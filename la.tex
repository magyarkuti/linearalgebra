\documentclass[9pt, a4paper, showtrims]{memoir}
\newcommand{\alert}{\relax}
\let\Aref\relax
\usepackage[x11names]{xcolor}
%%%%%%% pdflatex %%%%%%%%%%
%\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
%\usepackage[hungarian]{babel}[2015/11/24]
%%%%%%% pdflatex %%%%%%%%%%

%%%%%% lualatex %%%%%%%%%%%
%\usepackage{polyglossia}\setdefaultlanguage{magyar}
\usepackage[hungarian]{babel}[2015/11/24]
\usepackage{fontspec,microtype,amssymb}
\usepackage{unicode-math} %o math fontokhoz.Oszeakad az amssymb csomaggal, ha nem az amssymb van elobb.
\defaultfontfeatures{Ligatures=TeX}
\setmainfont{TeX Gyre Pagella}
%\setmainfont[BoldFont={TT Nooks-Regular},ItalicFont={Calluna-It}]{Calluna-Regular}
\setmathfont{TeX Gyre Pagella Math}%unicode-math ?
\newfontfamily\DejaSans{Dejavu Sans}
%\setsansfont{Kurier}[Scale=MatchLowercase]
\setsansfont{TeX Gyre Adventor}[Scale=MatchLowercase]
%\setsansfont{Dejavu Sans}[Scale=MatchLowercase] 
%\setsansfont[BoldFont={TT Nooks-Regular},ItalicFont={Calluna-It}]{Calluna-Regular}%
%\setmonofont{inconsolata}[Scale=MatchLowercase]
\setmonofont{TeX Gyre Cursor}
%%%%%% lualatex %%%%%%%%%%%

\frenchspacing
\usepackage{csquotes,amsmath,amsthm,systeme,fixme}
\usepackage[backend=biber,
            bibencoding=utf8,
            style=authoryear,
            autocite=inline,
            backref=true]{biblatex}
\addbibresource{\jobname.bib}
\BiblatexHungarianWarningOff
\fxsetup{status=draft, theme=color, layout={inline}}
\renewcommand{\fixmelogo}{\textcolor{black}{\colorbox{Firebrick1}{\textsf{\textbf{FIX}}}}}

\usepackage[unicode]{hyperref}
\hypersetup{final=true,
            pdftitle={Lineáris algebra},
            pdfauthor={Magyarkuti, Gyula},
            pdfsubject={linear algebra},
            pdfcreator={LuaLaTeX},
            pdfkeywords={algebra}
           }\usepackage{memhfixc}
\usepackage[inline]{enumitem}
\usepackage[a4paper]{geometry}
\usepackage[missing={GitHub: \today},dirty={Continuous integration},mark]{gitinfo2}
%\usepackage[mark,dirty={(Dirty)}]{gitinfo2}
%\edef\gitBranch{\gitBranch}
\renewcommand{\gitMarkFormat}{\normalfont\color{gray}\footnotesize\ttfamily}
\renewcommand{\gitMark}{Branch: \gitBranch\,@\,\gitFirstTagDescribe{}
    \textbullet{}
    Date: \gitAuthorIsoDate
}

% Ez akkor kell, ha San seriff fontok a part, chapter, section stb helyeken. 
\renewcommand{\booktitlefont}{\normalfont\Huge\scshape}
\renewcommand{\partnamefont}{\normalfont\huge\sffamily\raggedleft}
\renewcommand{\partnumfont}{\normalfont\huge\sffamily}
\renewcommand{\parttitlefont}{\normalfont\huge\sffamily\raggedleft}
\renewcommand{\chapnamefont}{\huge\sffamily\raggedleft}
\renewcommand{\chapnumfont}{\huge\sffamily}
\renewcommand{\chaptitlefont}{\Huge\sffamily\bfseries}
\setsecheadstyle{\Large\sffamily\bfseries\raggedright}
\setsubsecheadstyle{\large\sffamily\bfseries\raggedright}
\setsubsubsecheadstyle{\normalsize\sffamily\bfseries\raggedright}
\setparaheadstyle{\normalsize\sffamily\bfseries\raggedright}
\setsubparaheadstyle{\normalsize\sffamily\raggedright}
% Tartalomjegyzék is a San seriff
\renewcommand{\cftpartfont}{\sffamily\bfseries}
\renewcommand{\cftchapterfont}{\sffamily\bfseries\scshape}
\renewcommand{\cftsectionfont}{\sffamily}
\renewcommand{\cftsubsectionfont}{\sffamily}
\renewcommand{\cftpartpagefont}{\sffamily\bfseries}
\renewcommand{\cftchapterpagefont}{\sffamily\bfseries}
\renewcommand{\cftsectionpagefont}{\sffamily}
\renewcommand{\cftsubsectionpagefont}{\sffamily}

%\maxtocdepth{subsection}

\nouppercaseheads
\makeoddhead{myheadings}{\sffamily\footnotesize\leftmark}{}{\sffamily\footnotesize\thepage}
\makeevenhead{myheadings}{\sffamily\footnotesize\thepage}{\sffamily\footnotesize\myBotmark}{\sffamily\footnotesize\rightmark}
\makepsmarks{myheadings}{%
    \renewcommand\chaptermark[1]{%
    \markboth{%
%      \ifnum \value{secnumdepth} > 1
%      \if@mainmatter %
            \thechapter.~\chaptername:~%
%      \fi
%      \fi
        ##1}{\thepart.~\partname}}%
    }
\makeatletter
\let\ps@plain\ps@empty
\newcommand\arraybslash{\let\\\@arraycr}
\patchcmd{\@makechapterhead}
    {\printchaptername \chapternamenum \printchapternum}
    {\printchapternum.\@\chapternamenum \printchaptername}
    {}{}
\renewenvironment{proof}[1][\proofname]
    {\par\pushQED{\qed}%
    \normalfont \topsep6\p@\@plus6\p@\relax
    \trivlist
    \item[\hskip\labelsep
        \itshape
    #1\@addpunct{:}]\ignorespaces}
    {\popQED\endtrivlist\@endpefalse}
\makeatother

\newcommand{\addQEDstyle}[2]{\AtBeginEnvironment{#1}{\pushQED{\qed}\renewcommand{\qedsymbol}{#2}}\AtEndEnvironment{#1}{\popQED}}
%% qed trükkök:
%% https://tex.stackexchange.com/questions/16453/denoting-the-end-of-example-remark
%\swapnumbers %% a magyar.ldf megfordítja. A polyglossia nem. De a magyar ldf pontot is tesz a számcimke után

\renewcommand{\qedsymbol}{□}
\newcommand{\myqedsymbol}{$\lrcorner$}
\theoremstyle{plain}

\newtheorem{proposition}{állítás}[chapter]
\newtheorem{lemma}[proposition]{lemma}
\newtheorem*{SL}{Steinitz-lemma}
\newtheorem*{FA}{Az algebra alaptétele}
%
\theoremstyle{remark}
\newtheorem{note}[proposition]{megjegyzés}

\theoremstyle{definition}
\newtheorem{definition}[proposition]{definíció}
\newtheorem{corollary}[proposition]{következmény}
\newtheorem{defprop}[proposition]{definíció-állítás}
\addQEDstyle{definition}{\myqedsymbol}\addQEDstyle{proposition}{\myqedsymbol}\addQEDstyle{lemma}{\myqedsymbol}\addQEDstyle{note}{\myqedsymbol}\addQEDstyle{corollary}{\myqedsymbol}\addQEDstyle{SL}{\myqedsymbol}\addQEDstyle{defprop}{\myqedsymbol}


%%% https://tex.stackexchange.com/questions/319474/put-current-theorem-like-items-name-number-in-header
% \myBotmark feltöltése a lapon lévő utolsó tétel környezettel
\makeatletter
    \@ifdefinable\@my@claim@mark{\newmarks\@my@claim@mark}
    \newcommand*\myMark[1]{\marks\@my@claim@mark{#1}}
    \newcommand*\myBotmark{\botmarks\@my@claim@mark}
    \patchcmd{\@begintheorem}{% search for:
        \thm@swap\swappedhead\thmhead % more specific than before
    }{% replace with:
        \myMark{#2.\@ifnotempty{#1}{\ #1}\@ifnotempty{#3}{\ (#3)}}%
        \thm@swap\swappedhead\thmhead
    }{
        \typeout{>>> Made patch specific for amsthm.}
    }{
        \typeout{>>> Patch specific for amsthm FAILED!}
    }

%part
\long\def\@part[#1]#2{%
  \M@gettitle{#1}%
  \def\f@rtoc{#1}%
  \@nameuse{part@f@rtoc@before@write@hook}%
  \phantomsection
  \mempreaddparttotochook
  \ifnum \c@secnumdepth >-2\relax
    \refstepcounter{part}%
    \addcontentsline{toc}{part}%
      {\protect\partnumberline{\thepart}\f@rtoc}%
    \mempartinfo{\thepart}{\f@rtoc}{#2}%
  \else
    \addcontentsline{toc}{part}{\f@rtoc}%
    \mempartinfo{}{\f@rtoc}{#2}%
  \fi
  \mempostaddparttotochook
  \partmark{#1}%
  {\centering
   \interlinepenalty \@M
   \parskip\z@
   \ifnum \c@secnumdepth >-2\relax
     \printpartnum.\ \printpartname \partnamenum%%MGy
     \midpartskip
   \fi
   \printparttitle{#2}\par}%
  \@endpart}
\makeatother

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator{\gen}{gen}
\DeclareMathOperator{\crank}{crank}
\DeclareMathOperator{\rrank}{rrank}
\DeclareMathOperator{\srank}{srank}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\im}{Im}
%\DeclareMathOperator{\arg}{arg}

\def\scwords #1 #2 #3 {\textsc{#1} \textsc{#2} \textsc{#3} }
\newcommand{\uj}{\text{új}}
\newcommand{\rgi}{\text{régi}}
\newcommand{\ip}[2]{\langle#1,#2\rangle}
\newcommand{\Star}[1]{#1\ensuremath{^*}\kern-\scriptspace}
\newcommand{\CStar}{\Star{\ensuremath{\mathrm{C}}}}
%\citeindextrue
\makeindex
\synctex=1
\usepackage{graphicx, datatool}
\DTLloadrawdb[keys={Number,Quote,Author}]{quotes}{quotes.txt}% Load quotes
     \edef\RandomQuote{\number\numexpr1+\uniformdeviate\DTLrowcount{quotes}}% Identify random quote
     \dtlgetrow{quotes}{\RandomQuote}% Retrieve random quote
%     \dtlgetrow{quotes}{112}% Retrieve the row 113
     \dtlgetentryfromcurrentrow{\Number}{1}\dtlgetentryfromcurrentrow{\Quote}{2}\dtlgetentryfromcurrentrow{\Author}{3}%
     \typeout{A bölcsesség:}
     \typeout{\Number:\Quote -- \Author}
     \typeout{ }

%\usepackage{comment}
\begin{document}
\frontmatter*
{\centering
\thispagestyle{empty}
\noindent
\includegraphics[width=\linewidth]{Hortus_Deliciarum,_Die_Philosophie_mit_den_sieben_freien_Künsten.JPG}

\vspace{\fill}
{\printbooktitle{Lineáris Algebra}}\par
}
\newpage
\thispagestyle{empty}
\phantom{Ez egy szándékosan üres lap}
\newpage
\thispagestyle{empty}
\section*{\texttt{Verzió információk}}
%}
{\ttfamily
	\begin{center}
		\begin{tabular}{l|r}
			\hline
			References         & \gitReferences       \\
			Branch             & \gitBranch           \\
			Dirty              & \gitDirty            \\
			Hash               & \gitHash             \\
			Author Iso Date    & \gitAuthorIsoDate    \\
			\hline
			First Tag Describe & \gitFirstTagDescribe \\
			Reln               & \gitReln             \\
			Roff               & \gitRoff             \\
			Tags               & \gitTags             \\
			Describe           & \gitDescribe         \\
			\hline
		\end{tabular}
	\end{center}
}
\chapter*{Előszó}
\epigraphhead[70]{\epigraph{\Quote}{\textit{\Author}}}
\scwords%
A legfontosabb forrás \parencite{DancsPuskas2001}.

\ldots

Igyekszem strukturáltan írni.
Kicsi, atomszerű építőkövek egymás utáni megértése visz az anyagban előre,
ezek az egymástól feltűnő módon szeparált ,,állítások'' és azok érvekkel való alátámasztása,
amit ,,bizonyításnak'' is szokás mondani.
Az írásmód oka,
hogy evvel is hangsúlyozzam, hogy az olvasónak igyekeznie kell strukturáltan gondolkodni.
A hátulütője,
hogy hibásan azt a helytelen képzetet keltheti,
mintha az egyes állítások mintegy puzzle-ként állnának össze.
Nem, nem erről van szó.
A puzzle-ban minden elem egyenrangú,
az egyik elem hiánya éppen annyira fájdalmas mint a másiké.
Ez egyetlen matematikai diszciplína esetében sem igaz!
Az olvasónak igyekeznie kell,
hogy meglássa mi a legfontosabb gondolat a sok-sok állításnak,
mint építménynek egy-egy ,,nyilvánvaló következményében''.

Hogy e kis lépések egymástól még határozottabban váljanak el azt az írás
tipográfiája is erősíti azzal,
az állítás-szerű környezeteket a \,\myqedsymbol~,
és a bizonyítás környezetet a \,\qedsymbol~ karakterekkel zárom le.

Stb.

\bigskip\noindent
Magyarkuti Gyula
\hfill{Budapest, \ontoday}
\thispagestyle{empty}


\clearpage
\pagestyle{myheadings}
\tableofcontents*
\mainmatter*
%\part{F-dúr hegedűverseny No. 3, Op. 8, RV 293, ,,L'autunno''}
\part{Ősz}
\chapter{Előzmények}
\scwords A lineáris algebra tárgyalásához elengedhetetlenül szükséges általános algebrai ismereteket foglaljuk össze.
\section{Algebrai struktúrák}
\begin{definition}[$n$-változós művelet]\index{művelet}\index{algebrai struktúra}
	Legyen $H$ egy halmaz. Egy
	\[
		\varphi\colon H^n\to H
	\]
	függvényt $n$-változós \emph{műveletnek} nevezünk.
	Egy halmazt és rajta véges sok műveletet együtt \emph{algebrai struktúrának} mondunk.
	Jelölés:
	$$\left(H,\varphi_1,\ldots,\varphi_n  \right),$$
	ahol $H$ a halmaz és
	$\varphi_1,\ldots,\varphi_n$ a $H$ halmazon értelmezett műveletek.
\end{definition}
\begin{definition}[félcsoport]\index{félcsoport}
	Egy $\left( S,\ast \right)$ algebrai struktúrát \emph{félcsoportnak} mondjuk,
	ha $\ast$ egy kétváltozós \emph{asszociatív}\index{asszociatív}
	művelete az $S$ halmaznak,
	azaz minden $a,b,c\in S$ mellett
	\[
		a\ast\left( b\ast c \right)=\left( a\ast b\right)\ast c.\qedhere
	\]
\end{definition}
Lefordítva ez azt jelenti, hogy
\begin{enumerate}
	\item minden $a,b\in S$ mellett $a\ast b\in S$, és
	\item minden $a,b,c\in S$ esetén $a\ast\left( b\ast c \right)=a\ast\left( b\ast c \right)$
\end{enumerate}
\begin{definition}[neutrális elem]\index{neutrális elem}\index{neutrális elemes félcsoport}
	Az $\left( S,\ast \right)$ félcsoportban az $s\in S$ elem \emph{balról (jobbról) neutrális},
	ha $s\ast t=t$ ($t\ast s=t$) minden $t\in S$ mellett.
	Ha $s\in S$ balról is és jobbról is neutrális, akkor $s$-et egy \emph{neutrális elemnek}
	mondjuk.
	A félcsoportot \emph{neutrális elemes félcsoportnak} nevezzük, ha van benne neutrális elem.
\end{definition}
\begin{proposition}
	Ha egy félcsoportban, van egy balról neutrális elem és egy jobbról neutrális elem,
	akkor ezek megegyeznek.
	Emiatt egy neutrális elemes félcsoportban neutrális elem csak egy van.
\end{proposition}
\begin{proof}
	Legyen $s_1$ balról-- és $s_2$ jobbról neutrális elem.
	Ekkor
	\(
	s_2=s_1\ast s_2=s_1.
	\)
\end{proof}
A félcsoport additív írásmódja esetén természetes a neutrális elemet \emph{zérusnak},
míg multiplikatív írásmód esetén \emph{egységnek} nevezni.
\begin{definition}[csoport]\index{csoport}
	Egy $\left( G,\ast \right)$ algebrai struktúrát \emph{csoportnak} nevezünk,
	ha neutrális elemes félcsoport, amelyben minden $g\in G$-hez létezik $g'\in G$, hogy
	\[
		g\ast g'=e=g'\ast g.\tag{\dag}
	\]
	Itt $e\in G$ jelöli a $G$ csoport neutrális elemét.
\end{definition}
\begin{defprop}[inverz elem]\index{inverz}
	Legyen $\left( G,\ast \right)$ egy csoport.
	Ekkor minden $g\in G$-hez, csak egyetlen $g'\in G$ létezik,
	amelyre a fenti ($\dag$) azonosság fennáll.
	Adott $g$-hez ezt ez egyetlen $g'\in G$ elemet,
	amelyre ($\dag$) teljesül a $g$ elem \emph{inverzének} mondjuk.
\end{defprop}
\begin{proof}
	Jelölje $e$ a csoport neutrális elemét,
	és tegyük fel, hogy $g',g''$ inverz elemei $g$-nek.
	Azt mutatjuk meg, hogy $g'\ast g=e$ és $g\ast g''=e$ esetén a két inverz megegyezik.
	\[
		g'=g'\ast e=
		g'\ast\left( g\ast g'' \right)=
		\left(g'\ast g\right)\ast g'' =
		e\ast g''=g''.\qedhere
	\]
\end{proof}
Példaként gondoljuk meg, hogy az összes $H\to H$ függvények halmaza a kompozíció művelettel
neutrális elemes félcsoportot,
és az összes $H\to H$ kölcsönösen egyértelmű függvények halmaza a kompozíció művelettel csoportot alkotnak.
Ez utóbbi csoportot mondjuk \emph{permutáció csoportnak}\index{permutációk}.
\begin{proposition}[egyszerűsítési szabály]\index{egyszerűsítési szabály}
	Csoportban igaz az egyszerűsítési szabály, azaz
	\[
		a\ast c=b\ast c\implies a=b.\qedhere
	\]
\end{proposition}
\begin{proof}
	\begin{math}
		a=a\ast e
		=
		a\ast \left( c\ast c'\right)=
		\left( a\ast c \right)\ast c'=
		\left( b\ast c \right)\ast c'=
		b\ast\left( c\ast c' \right)=
		b\ast e=
		b.
	\end{math}
\end{proof}
\begin{definition}[Abel-csoport]\index{Abel-csoport}
	Egy $\left( G,\ast \right)$ csoportot \emph{Abel-csoportnak} nevezünk,
	ha a művelete \emph{kommutatív}\index{kommutatív} is,
	azaz minden $s,t\in G$ mellett $s\ast t=t\ast s$.
\end{definition}
\begin{definition}[gyűrű]\index{gyűrű}
	A kétműveletes $\left( R,+,\cdot \right)$ algebrai struktúrát \emph{gyűrűnek} nevezzük,
	ha
	\begin{enumerate}
		\item $\left( R,+ \right)$ Abel-csoport;
		\item $\left( R,\cdot \right)$ félcsoport;
		\item és a két műveletet összeköti a következő két disztributivitás:\index{disztributív}
		      \[
			      a\cdot\left( b+c \right)=a\cdot b + a\cdot c\qquad
			      \left( a + b \right)\cdot c=a\cdot c+b\cdot c.
		      \]
	\end{enumerate}
	Ha $\left( R,\cdot \right)$ neutrális elemes félcsoport, akkor azt mondjuk, hogy $R$ egy
	\emph{egységelemes gyűrű}, és ha $\left( R,\cdot \right)$ kommutatív félcsoport, akkor
	azt mondjuk, hogy $R$ egy \emph{kommutatív gyűrű}.
\end{definition}
\begin{definition}[test]
	Egy $\left( \mathbb{F},+,\cdot \right)$ kétműveletes algebrai struktúrát \emph{testnek}
	nevezünk,
	ha olyan kommutatív egységelemes gyűrű,
	amelyben minden nemzérus\footnote{Értsd: minden elemnek, amely a $+$ műveletre nézve neutrális elemtől különbözik.}
	elemnek van inverze\footnote{Értsd: a $\cdot$ szorzás neutrális elemére mint egységelemre nézve.},
	és $0\neq 1$\footnote{Értsd: az összeadásra nézve és a szorzásra nézve képzett neutrális elemek nem azonosak.}.
\end{definition}
A test az algebrai struktúra, ahol az összeadás és szorzás műveletekkel úgy számolhatunk, mint amit a valós számok során megszoktuk.
Példaként néhány tulajdonság.
\begin{proposition}
	Az $\left( R,+,\cdot \right)$ egységelemes gyűrűben minden $a\in R$ mellett
	\begin{equation*}
		a\cdot 0=0\text{ és }
		\left( -1 \right)\cdot a=-a.\qedhere
	\end{equation*}
\end{proposition}
\begin{proof}
	\begin{math}
		0+a\cdot 0=
		a\cdot 0=
		a\left( 0+0 \right)=
		a\cdot 0+a\cdot 0.
	\end{math}
	A jobboldali $a\cdot 0$-val való egyszerűsítés után kapjuk,
	hogy $0=a\cdot 0$.
	A második azonosságot az első felhasználásával kapjuk:
	\begin{math}
		0
		=
		0a
		=
		\left( 1+\left( -1 \right) \right)\cdot a
		=
		1\cdot a + \left( -1 \right)\cdot a
		=
		a +\left( -1 \right)\cdot a.
	\end{math}
	Az additív inverz definíciója és egyértelműsége szerint ez éppen azt jelenti, hogy $-a=\left( -1 \right)\cdot a$.
\end{proof}
Ami nagyon fontos, hogy egy gyűrűben nem feltétlen teljesül,
hogy elemek szorzata csak úgy lehet zérus, ha legalább az egyik elem zérus.
Számunkra a legfontosabb példa  a mátrixok gyűrűje\footnote{Lásd kicsit később.},
ahol pont ennek a hiánya jelenti nehézséget.

Egy testben ilyen nem fordulhat elő.
\begin{definition}[nullosztómentes gyűrű]
	Egy gyűrűt \emph{nullosztómentesnek} nevezzük,
	ha két elem szorzata csak úgy lehet nulla,
	ha legalább az egyik elem nulla.
\end{definition}
\begin{proposition}
	Egy test egyben nullosztómentes gyűrű, azaz
	ha $\mathbb{F}$ egy test, és $a,b\in\mathbb{F}$.
	Akkor
	\[
		ab=0\implies a=0\text{ vagy }b=0.\qedhere
	\]
\end{proposition}
\begin{proof}
	Tegyük fel, hogy $ab=0$.
	Ha $b\neq 0$, akkor létezik $b'\in\mathbb{F}$, hogy $bb'=1$.
	Így
	\[
		0= 0b'=\left( ab \right)b'=a\left( bb' \right)=a1=a.\qedhere
	\]
\end{proof}
\begin{proposition}
	Nullosztómentes gyűrűben nem zérus elemmel való szorzatot egyszerűsíteni lehet
	azaz, ha $a,b,c\in R,b\neq 0$ esetén
	\[
		ab=cb\implies a=c.\qedhere
	\]
\end{proposition}
\begin{proof}
	\begin{math}
		\left( a-c \right)b=ab-cb=0\implies a-c=0.
	\end{math}
\end{proof}
\begin{definition}[ideál]\index{ideál}\index{főideál}\index{főideál-gyűrű}
	Egy $\left( R,+,\cdot \right)$ kommutatív gyűrű egy $J\subseteq R$ nem üres részhalmazát
	\emph{ideálnak} nevezzük,
	ha
	\begin{enumerate}
		\item
		      minden $a,b\in J$ mellett $a+b\in J$;
		\item
		      minden $c\in R$ és minden $a\in J$ mellett $ca\in J$.
	\end{enumerate}
	Ha egy $d\in R$ adott, akkor a
	\[
		\left\{ da:a\in R \right\}
	\]
	halmaz egy ideálja $R$-nek.
	Ez a $d$ elem többszöröseiből álló ideál, amelyet \emph{főideálnak}\index{főideál} is nevezünk.
	Ha egy gyűrűben minden ideál egy főideál, akkor a gyűrűt \emph{főideál-gyűrűnek}\index{főideál-gyűrű} mondjuk.
\end{definition}
A generált ideál fogalma nagyon fontos.
\begin{defprop}[generált ideál]\index{generált ideál}
	Legyen adott a  kommutatív, egységelemes $\left( R,+,\cdot \right)$ gyűrűben véges sok $a_1,\ldots,a_r$ elem.
	Az e véges sok elemet tartalmazó ideálok közös része maga is ideál,
	és e metszet az eredeti véges halmazt
	tartalmazó \emph{legszűkebb ideál}.
	Jelöljük ezt $J\left( a_1,a_2,\ldots,a_r \right)$ módon.

	Tekintsük a
	$
		\left\{ \sum_{j=1}^ra_jb_j:b_1,\ldots,b_r\in R \right\}
	$
	halmazt.
	Világos,
	hogy ez egy ideál az $R$ gyűrűben.
	A gyűrű egységelemes,
	ezért ennek $J\left( a_1,\ldots,a_r \right)$ egy részhalmaza.
	Másrészt minden az $\left\{ a_1,\ldots,a_r \right\}$ elemeket tartalmazó ideál,
	egyben tartalmazza a
	$
		\left\{ \sum_{j=1}^ra_jb_j:b_1,\ldots,b_r\in R \right\}
	$
	halmazt is,
	ami azt jelenti, hogy
	\[
		J\left( a_1,\ldots,a_r \right)=
		\left\{ \sum_{j=1}^ra_jb_j:b_1,\ldots,b_r\in R \right\}
	\]
	az $a_1,\ldots,a_r$ elemeket tartalmazó legszűkebb ideál.
	Nevezzük ezt az ideált az $a_1,\ldots,a_r$ elemek \emph{generálta ideálnak} is.
\end{defprop}



Világos, hogy $\left\{ 0 \right\}$ és maga az egész $R$ ideálok.

A legfontosabb struktúrák számunkra a következők:
\begin{itemize}
	\item
	      Egységelemes gyűrű, amelyben a nullosztómentesség nem teljesül: mátrixok.
	\item
	      Kommutatív egységelemes gyűrű, amely nullosztómentes de mégsem test: polinomok.
	\item
	      Test:
	      a valós vagy a komplex számok.
\end{itemize}
\section{Polinomgyűrűk}
\begin{definition}[polinom]\index{polinom}
	Legyen $\mathbb{F}$ egy test.
	E test feletti polinomokon az összes
	\[
		p\left( t \right)=
		\alpha_0+\alpha_1t+\alpha_2t^2+\ldots+\alpha_nt^n
	\]
	alakú formális algebrai kifejezést értjük.
	Itt $n$ tetszőleges nem negatív egész
	és $\alpha_0,\ldots,\alpha_n$ tetszőleges, az $\mathbb{F}$ testbeli elemek.
	Az $\mathbb{F}$ test feletti összes polinomok halmazát $\mathbb{F}\left[ t \right]$ módon jelöljük.
\end{definition}
A fenti definícióban az \emph{algebrai kifejezés} szó arra utal,  hogy az
\begin{math}
	\alpha_0+\alpha_1t+\alpha_2t^2+\ldots+\alpha_nt^n
\end{math}
műveletek minden $t\in\mathbb{F}$ mellett értelmesek,
és eredményük egy újabb $\mathbb{F}$ testbeli elem.
Ha $t\in\mathbb{F}$ konkrétan meg van adva,
akkor a behelyettesítés után kapott elemet mondjuk a $p$ polinom helyettesítési értékének.

A \emph{formális algebrai kifejezés}\index{formális algebrai kifejezés} arra utal,
hogy egy polinomot az együtthatói határozzák meg,
azaz két polinom akkor és csak akkor azonos,
ha az megfelelő együtthatói azonosak.
Ez szemben áll avval, hogy ha a polinomokra mint függvényekre tekintenénk,
akkor a helyettesítési értékek egyenlősége jelentené a két polinom azonos voltát.
A formális szó tehát azt jelenti, hogy nem mint függvényre gondolunk,
hanem egyszerűen az adott $\alpha_0,\ldots,\alpha_n$ rögzített elemek -- ezeket mondjuk együtthatóknak --,
által előírt műveletekre.
Az az előírás ugyanis, hogy tetszőleges $t\in\mathbb{F}$ mellett hajtsuk végre az
\[
	\alpha_0+\alpha_1t+\alpha_2t^2+\ldots+\alpha_nt^n
\]
műveletsort.
A műveletsorról és nem annak eredményéről van szó.


Két műveletsor akkor azonos, ha ugyanazok a műveletsort meghatározó
$\left( \alpha_0,\alpha_1,\ldots,\alpha_n \right)$%
\footnote{Az előbbi zárójellel azt hangsúlyozzuk, hogy az együtthatók sorrendje is számít.}
együtthatók.%
\footnote{Persze felmerül a kérdés,
	hogy ha két polinom minden helyettesítési értéke azonos,
	akkor igaz-e,
	hogy mint formális polinomok is azonosak,
	tehát a két polinom együtthatói is rendre azonosak-e?
	A pozitív választ később látjuk nem véges számtest, például a valós vagy a komplex test, feletti polinomok esetén.
	Lásd \aref{pr:polinomfv}. állítás utáni megjegyzést \apageref{pr:polinomfv}. oldalon.%
}
A jelölések megértése is fontos.
$p\left( t \right)\in\mathbb{F}\left[ t \right]$ semmi mást nem jelent,
minthogy $p\left( t \right)$ egy polinom.
Persze a polinom nem keverendő össze a helyettesítési értékével,
hiszen az egyik egy algebrai kifejezés-együttes, a másik egy az adott testbeli elem.
Szokásos viszont, hogy ha nincs konkrét $t$ a szövegkörnyezetben, akkor is $p\left( t \right)$ jelöli a polinomot.
Néha egyszerűbben csak $p$-vel jelöljük, főleg akkor ha nincs szó behelyettesítésről,
emiatt érdektelen a változó jele.
Ritkábban, de előfordul, hogy egy konkrét értékre, mondjuk $s\in\mathbb{F}$-re kell kiértékelnünk a polinomot ilyenkor $p\left( s \right)$ jelöli azt a testbeli elemet,
amelyet $t$ helyett $s$-et téve az előírt műveletek kiértékelése után kapunk.
A szövegkörnyezetben mindig világosnak kell lennie, hogy $p\left( t \right)$ a polinomot jelenti,
vagy egy konkrét $t$-re kiértékelt testbeli elemet.

\begin{definition}[polinom foka]\index{polinom foka}
	Legyen $p\left( t \right)\in\mathbb{F}\left[ t \right]$ egy polinom.
	Azt mondjuk, hogy az $n$ nem negatív egész szám e \emph{polinom fokszáma},
	ha $n$ a legnagyobb indexű nem zérus együttható.
	A legnagyobb indexű nem zérus együtthatót \emph{főegyütthatónak}\index{főegyüttható} nevezzük.
	Azt mondjuk, hogy egy nemzérus polinom \emph{normált}\index{normált polinom}, ha $1$ a főegyütthatója.

	A $p\left( t \right)=0$ konstans zérus polinom foka megállapodás szerint legyen $-\infty$.
	A $p$ polinom fokszámát $\deg p$ módon jelöljük.
\end{definition}
Látni fogjuk, hogy a konstans zérus polinomra $\deg p=-\infty$ csak egy kényelmes jelölés.
Időnként a polinom fokszámával műveleteket is végzünk.
Megegyezés szerint ilyenkor $-\infty+a=-\infty$ minden $a$ nem negatív egész számra,
és $\left( -\infty \right)+\left( -\infty \right)=-\infty.$
A $-\infty$ szimbólumot minden egész számnál határozottan kisebbnek gondoljuk.

Két polinom összegét és szorzatát a szokásos módon definiáljuk:
\begin{definition}\label{def:polmuveletek}
	Legyen $p,q\in\mathbb{F}[t]$, két polinom.
	\[
		p\left( t \right)
		=
		\sum_{j=0}^n\alpha_jt^j
		\text{ és }
		q\left( t \right)
		=
		\sum_{j=0}^m\beta_jt^j,
		\qquad
		\alpha_j,\beta_j\in\mathbb{F},
		0\leq n,m\in\mathbb{Z}.
	\]
	Ekkor a $p$ és $q$ összegének definíciója:
	\[
		\left( p+q \right)\left( t \right)
		=
		\sum_{j=0}^{\max{\left\{ m,n \right\}}}\left( \alpha_j+\beta_j \right)t^j;
	\]
	míg a két polinom szorzatának definíciója:
	\[
		\left( pq \right)\left( t \right)
		=
		\sum_{j=0}^{n+m}c_jt^j
		\text{ ahol }
		c_j
		=
		\sum_{k=0}^j\alpha_k\beta_{j-k}.
		\qedhere
	\]
\end{definition}
Mind az összeadás, mind a szorzás definíciójában úgy kell a formulát érteni,
hogy amikor a hivatkozott együtthatók nem léteznek,
akkor értékük legyen zérus.
Például a szorzat legmagasabb indexű együtthatójára
$c_{n+m}=\alpha_n\beta_m$, hiszen az $\alpha_k,\beta_{n+m-k}$ számok
csak egyetlen $k$ mellett értelmezettek közösen, mikor $k=n$.
\footnote
{
	Ha már követjük a konvenciót,
	amely szerint a nem definiált együtthatókat zérusnak tekintjük,
	akkor persze
	\(
	\left( p+q \right)\left( t \right)
	=
	\sum_{j=0}^\infty\left( \alpha_j+\beta_j \right)t^j;
	\)
	és
	\(
	\left( pq \right)\left( t \right)
	=
	\sum_{j=0}^\infty\left( \sum_{k=0}^\infty\alpha_k\beta_{j-k} \right)t^j
	\)
	is írható.
	Itt a fenti összegek nem végtelen összegek, hiszen a két polinom együtthatói
	csak véges sok esetben különböznek zérustól.
	Így ebben az esetben a formálisan végtelen összeg definíciója
	egyszerűen a véges sok nem zérus elem összege.
}
\begin{proposition}
	Legyenek $p,q\in\mathbb{F}[t]$ polinomok az $\mathbb{F}$ test felett.
	Ekkor
	\begin{enumerate}
		\item $\deg \left( pq \right)=\deg p+\deg q$;
		\item $\deg \left( p+q \right)\leq\max\left\{ \deg p,\deg q \right\}$.\qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Figyeljünk arra, hogy a konstans zérus polinom esetében is működik a tétel,
	és vegyük észre, hogy az szorzat polinomra vonatkozó állítás azért igaz,
	mert a test nullosztómentes.
\end{proof}
A következő állítás igazolását az olvasóra bízom.
Aprólékosan igazoljuk a test axiómák gyakorlásaként.
Vigyázat: a szorzás asszociativitása nem is olyan egyszerű.
\begin{proposition}
	Egy $\mathbb{F}$ test feletti $\mathbb{F}\left[ t \right]$ formális polinomok
	a fent bevezetett összeadás és szorzás műveletekkel,
	nullosztómentes,
	kommutatív, egységelemes gyűrűt alkotnak.
\end{proposition}

\section{Polinomok oszthatósága és a maradékos osztás}
\begin{definition}[oszthatóság]\index{polinom osztója}
	Azt mondjuk, hogy a $p\in\mathbb{F}\left[ t \right]$ \emph{osztója} az $f\in\mathbb{F}\left[ t \right]$ nem zérus polinomnak,
	ha létezik $h\in\mathbb{F}\left[ t \right]$, hogy $f\left( t \right)=p\left( t \right)h\left( t \right)$.
	Ilyenkor $f$-et a $p$ egy \emph{többszörösének}\index{polinom többszöröse} is mondjuk.
	Jelölés: $p|f$.
\end{definition}
Világos, hogy egy $p$ polinom összes többszörösei -- tehát azok, amelyeknek $p$ osztója --
ideált alkotnak.
Ez a $p$ generálta legszűkebb ideál, azaz  a $J(p)=\left\{ fp:f\in\mathbb{F}\left[ t \right] \right\}$ főideál.
Ha $q\in J\left( p \right)$, akkor $J\left( q \right)\subseteq J\left( p \right)$, azaz ha $q$ egy többszöröse $p$-nek,
akkor $q$ minden többszöröse $p$-nek is többszöröse.
Ha $p,q$ polinomok,
amelyekre $p|q$ és $q|p$ akkor a két polinom csak konstans szorzóban különbözik egymástól.
Ha például a két polinom még normált is, akkor $p|q$ és $q|p$ csak $p=q$ esetben lehetséges.
A polinomgyűrű ideáljaira fokuszálva, azt gondoltuk éppen meg,
hogy \emph{$J\left( p \right)=J\left( q \right)$ normált $p,q$ polinomokra csak úgy teljesülhet, ha $p=q$},
azaz a polinomok gyűrűjében minden főideálnak csak egy generáló eleme van a normált polinomok körében.

A következő állítás szerint a polinomok közt is működik a maradékos osztás,
ahogyan azt az egész számok közt megszoktuk.
\begin{proposition}[maradékos osztás]
	Legyenek $p,q\in\mathbb{F}\left[ t \right]$ polinomok, $q\neq 0$.
	Ekkor létezik egyetlen $h,r\in\mathbb{F}\left[ t \right]$ polinom, amelyre
	\[
		p
		=
		hq+r;
		\quad
		\deg r < \deg q.\qedhere
	\]
\end{proposition}
\begin{proof}
	Először is azt vegyük észre, hogy $\deg p<\deg q$ esetben $r=p$,
	$h=0$ szereposztással készen is vagyunk.

	Tegyük fel tehát, hogy $n=\deg p\geq \deg q=m$, és lássuk be az állítást $n$ szerinti indukcióval.
	Ha $n=0$, akkor $p\left( t \right)=\alpha_0$ és $q\left( t \right)=\beta_0\neq 0$.
	Ekkor persze
	\[
		\alpha_0=\frac{\alpha_0}{\beta_0}\beta_0+0,
	\]
	ami azt jelenti, hogy $h\left( t \right)=\frac{\alpha_0}{\beta_0}$ és $r\left( t \right)=0$ szereposztás
	megfelelő.

	Most tegyük fel,
	hogy igaz az állítás $n+1$-nél kisebb fokú $p$ polinomokra ($n\geq 0$),
	és lássuk be egy pontosan $n+1$-ed fokú polinomra.
	Legyen tehát
	\[
		p\left( t \right)=\alpha_{n+1}t^{n+1}+\ldots+\alpha_0
		\quad\text{ és }\quad
		q\left( t \right)=\beta_{m}t^m+\ldots+\beta_0,
	\]
	ahol $m\leq n+1$.
	Tekintsük a következő polinomot:
	\[
		\frac{\alpha_{n+1}}{\beta_m}t^{n+1-m}q\left( t \right).
	\]
	Világos, hogy ennek főegyütthatója éppen $\alpha_{n+1}$ és foka éppen $n+1=\deg p$.
	Így a
	\[
		p_1\left( t \right)
		=
		p\left( t \right)-
		\frac{\alpha_{n+1}}{\beta_m}t^{n+1-m}q\left( t \right).
	\]
	polinomra $\deg p_1<\deg p$.
	Na most, ha $\deg p_1<\deg q$, akkor a bizonyítás első mondatában említett helyzetben vagyunk,
	tehát nyilvánvaló szereposztással az állítás igaz $p_1$-re és $q$-ra.
	Ha viszont $\deg p_1\geq \deg q$ még mindig igaz, akkor az indukciós feltétel szerint található
	$h,r\in\mathbb{F}\left[ t \right]$ polinom, amelyre igaz az állítás.
	Mindkét esetben találtunk tehát $h,r$ polinomokat, amelyre
	\[
		p\left( t \right)-
		\frac{\alpha_{n+1}}{\beta_m}t^{n+1-m}q\left( t \right)
		=
		p_1\left( t \right)
		=
		h\left( t \right)q\left( t \right)+r\left( t \right);
		\quad
		\deg r < \deg q
	\]
	teljesül.
	Ekkor persze
	\[
		p\left( t \right)
		=
		\left( h\left( t \right)
		+
		\frac{\alpha_{n+1}}{\beta_m}t^{n+1-m}
		\right)
		q\left( t \right)
		+
		r\left( t \right);
		\quad
		\deg r < \deg q
	\]
	is fennáll. Ezt kellett belátni az állítás egzisztencia részéhez.

	Az unicitás részhez tegyük fel, hogy valamely $h,h_1,r,r_1$ polinomokra
	\[
		h\left( t \right)q\left( t \right)+r\left( t \right)
		=
		p\left( t \right)
		=
		h_1\left( t \right)q\left( t \right)+r_1\left( t \right)
	\]
	teljesül, ahol $\deg r<\deg q$ és $\deg r_1<\deg q$.
	Persze átrendezve ekkor
	\[
		\left( h\left( t \right)-h_1\left( t \right) \right)q\left( t \right)
		=
		r_1\left( t \right)-r\left( t \right)
	\]
	is fennáll.
	Ekkor a fokszámokra figyelve
	\[
		\deg\left( h-h_1 \right)+\deg q
		=
		\deg\left( r_1-r \right)
		\leq
		\max\left\{ \deg r_1,\deg (-r) \right\}
		<
		\deg q.
	\]
	Ez csak akkor lehetséges, ha $\deg\left( h-h_1 \right)=-\infty$,
	ami azt jelenti, hogy $h=h_1$,
	amiből persze $r_1=r$ már látszik is.
\end{proof}
\begin{proposition}[a polinomgyűrű egy főideál-gyűrű]\label{pr:pgyurufoidealgyuru}
	A polinomok $\mathbb{F}\left[ t \right]$ kommutatív, egységelemes, nullosztómentes gyűrűjében
	minden a $\left\{ 0 \right\}$-tól különböző ideált generál az ideálban lévő egyetlen normált minimális fokszámú polinom.
	Így $\mathbb{F}\left[ t \right]$ egy főideál-gyűrű.
\end{proposition}
\begin{proof}
	Legyen a $J$ egy ideálja $\mathbb{F}\left[ t \right]$-nek,
	amely nem csak a zérus elemből áll.
	Vegyünk egy minimális fokszámú de nem zérus polinomot $J$-ben,
	tehát olyat,
	amely maga sem zérus és nála kisebb fokszámú polinom már nincs $J$-ben a $0$ elemen kívül.
	Legyen ez $d$.
	Most megmutatjuk, hogy minden $p\in J$-re $d|p$.
	A maradékos osztás szerint
	valamely $h,r$ polinomokra
	\[
		p\left( t \right)=
		h\left( t \right)d\left( t \right)+r\left( t \right);
		\text{ ahol }
		\deg r<\deg d.
	\]
	Mivel $p,d\in J$, és $J$ egy ideál, ezért $r\in J$.
	No de, $d$ konstrukciója szerint ilyen csak a zérus polinom van,
	ezért valóban $d|p$.
	Ez éppen azt jelenti, hogy
	\(
	J=\left\{ dh:h\in\mathbb{F}\left[ t \right] \right\}
	\)
	azaz $d$ generálja a $J$ ideált.
	Azt viszont már korábban is meggondoltuk,
	hogy egy főideált csak egyetlen normált polinom generál.

	Megmutattuk tehát,
	hogy egyetlen normált, minimális fokszámú polinom van $J$-ben,
	és minden $J$-beli polinom ennek többszöröse.
\end{proof}
Érdemes eltenni magunknak, hogy az ideál generáló eleme, tehát az ideálbeli elemek közös osztója
éppen az ideál minimális fokszámú nem zérus polinomja.
Ilyenből a normált polinomok közül csak egy van.

\begin{definition}
	Legyenek most $p_1,\ldots,p_k$ polinomok.
	\begin{enumerate}
		\item A $d$ polinom a \emph{legnagyobb közös osztója}\index{legnagyobb közös osztó} az adott polinomoknak,
		      ha
		      \begin{enumerate}
			      \item $d|p_j$ minden $j=1,\ldots,k$-ra,
			      \item ha $d_1|p_j$ minden $j=1,\ldots,k$ mellett akkor $d_1|d$ is fennáll,
			      \item $d$ normált.
		      \end{enumerate}
		      A $p_1,\ldots,p_k$ polinomokat \emph{relatív prímeknek}\index{relatív prím polinomok} nevezzük,
		      ha közös osztójuk csak a konstans polinomok,
		      azaz a $d\left( t \right)=1$ a legnagyobb közös osztó.
		\item A $d$ polinom a \emph{legkisebb közös többszöröse}\index{legkisebb közös többszörös} az adott polinomoknak,
		      ha
		      \begin{enumerate}
			      \item $p_j|d$ minden $j=1,\ldots,k$-ra,
			      \item ha $p_j|d_1$ minden $j=1,\ldots,k$ mellett akkor $d|d_1$ is fennáll.
			      \item $d$ normált.\qedhere
		      \end{enumerate}
	\end{enumerate}
\end{definition}
Persze az első kérdés, hogy van-e a polinomoknak legnagyobb közös osztója vagy legkisebb közös többszöröse, és hány ilyen van?
\begin{proposition}\label{pr:lkkt}
	Bármely $p_1,\ldots,p_r\in\mathbb{F}\left[ t \right]$ nem zérus polinomoknak
	létezik egyetlen legkisebb közös többszöröse.
	Jelesül, a legkisebb közös többszörös, a
	\(
	\cap_{j=1}^rJ\left( p_j \right)
	\) ideálnak, mint főideálnak az egyetlen normált generátora.
\end{proposition}
\begin{proof}
	Világos, hogy ideálok metszete is ideál, emiatt
	\(
	\cap_{j=1}^rJ\left( p_j \right)
	\)
	is ideál $\mathbb{F}\left[ t \right]$ gyűrűben.
	De itt minden ideál főideál, létezik tehát $d\in\mathbb{F}\left[ t \right]$ normált polinom, amelyre
	\[
		J\left( d \right)
		=
		\cap_{j=1}^rJ\left( p_j \right)
	\]
	Világos, hogy $d\in J\left( p_j \right)$ minden $j$-re,
	ergo $d$ többszöröse minden $p_j$-nek.
	Ha $p_j|d_1$ fennáll, minden $j$-re
	az azt jelenti, hogy $d_1\in J\left( p_j \right)$, minden $j$-re, azaz
	\(
	d_1
	\in
	\cap_{j=1}^rJ\left( p_j \right)
	=
	J\left( d \right),
	\)
	tehát $d|d_1$ valóban fennáll.

	Ha $d$ mellett $g$ is legkisebb közös többszörös, akkor $d|g$ és $g|d$ szerint $g$ és $d$ foka azonos,
	így csak egymás konstans szorosai lehetnek, de mivel mindketten normáltak, ezért e konstans csak 1 lehet.
\end{proof}
\begin{proposition}
	Bármely $p_1,\ldots,p_r\in\mathbb{F}\left[ t \right]$ nem zérus polinomoknak
	létezik egyetlen legnagyobb közös osztója.
	Jelesül,
	a legnagyobb közös osztó a $J\left( p_1,\ldots,p_r \right)$ ideálnak mint főideálnak az egyetlen generáló eleme.
	Ezért a $d$ legnagyobb közös osztó kifejezhető
	\[
		d\left( t \right)=f_1\left( t \right)p_1\left( t \right)+
		\ldots+
		f_r\left( t \right)p_r\left( t \right)
	\]
	alakban,
	valamely $f_1,\ldots,f_r\in\mathbb{F}\left[ t \right]$ polinomok segítségével.
\end{proposition}
\begin{proof}
	Láttuk, hogy létezik egyetlen normált $d$ polinom, amelyre $J\left( d \right)=J\left( p_1,\ldots,p_r \right)$.
	Világos, hogy $d|p_j$ minden $j=1,\ldots,r$ és $d\in J\left( p_1,\ldots,p_r \right)$,
	azaz
	\[
		d=f_1p_1+\ldots+f_rp_r
	\]
	valamely $f_1,\ldots,f_r$ polinomokra.
	Ha valamely $d_1$ polinomra $d_1|p_j$ minden $j=1,\ldots,r$ mellett,
	akkor a fenti azonosság szerint $d_1|d$ is fennáll.

	Az egyértelműség mint a legkisebb közös többszörösnél.
\end{proof}
Az előző állítás kiemelt azonosságát \emph{Bezout-azonosságnak}\index{Bezout-azonosság}
mondjuk.
\begin{proposition}
	Legyenek a $p_1,\ldots,p_r\in\mathbb{F}\left[ t \right]$ tetszőleges $\mathbb{F}$ test feletti polinomok.
	Ezek pontosan akkor relatív prímek,
	ha léteznek $f_1,\ldots,f_r\in\mathbb{F}\left[ t \right]$ polinomok, hogy
	\[
		f_1\left( t \right)p_1\left( t \right)+f_2\left( t \right)p_2\left( t \right)+\ldots+f_r\left( t \right)p_r\left( t \right)=1\qedhere
	\]
\end{proposition}

A szakaszt a maradékos osztás módszerének másik fontos következményeivel zárjuk.
Azt gondoljuk meg,
hogy a gyöktényező a polinomból mindig kiemelhető,
emiatt egy akármilyen test feletti $n$-ed fokú polinom gyökeinek száma $n$-nél nagyobb nem lehet.
\begin{proposition}
	Legyen $p\in\mathbb{F}\left[ t \right]$ egy nem zérus polinom,
	és $t_0$ egy gyöke, azaz $p\left( t_0 \right)=0$.
	Ekkor létezik $h\in\mathbb{F}\left[ t \right]$ nem zérus polinom,
	amelyre
	\[
		p\left( t \right)=\left( t-t_0 \right)h\left( t \right).\qedhere
	\]
\end{proposition}
\begin{proof}
	Maradékos osztással $p$-re és az elsőfokú $t-t_0$ polinomra
	\[
		p\left( t \right)=h\left( t \right)\left( t-t_0 \right)+r\left( t \right),
		\text{ ahol }
		\deg r<1.
	\]
	No de, $t_0$ egy gyök, tehát $0=p\left( t_0 \right)=r\left( t_0 \right)$.
	Ez azt jelenti, hogy $\deg r=-\infty$, ami éppen az állítás.
\end{proof}
\begin{definition}[gyök multiplicitása]\index{gyökök multiplicitása}
	Legyen $t_0$ gyöke a $p\left( t \right)$ polinomnak.
	Azt mondjuk, hogy a $k$ pozitív egész e $t_0$ gyök \emph{multiplicitása},
	ha van olyan $h\left( t \right)$ polinom, hogy
	\begin{math}
		p\left( t \right)=\left( t-t_0 \right)^kh\left( t \right),
	\end{math}
	de $h\left( t_0 \right)\neq 0$.
	Néha azt is mondjuk, hogy $t_0$ egy $k$-szoros gyöke $p$-nek.
\end{definition}
Teljesen világos, hogy a gyöktényező kiemelhetősége miatt minden gyök legalább egyszeres multiplicitású.
A következő állítás szerint
a gyökök száma még a multiplicitásukkal együtt számolva sem lehet több mint a polinom foka.
\begin{proposition}
	Legyen(ek) a $p\in\mathbb{F}\left[ t \right]$ nem zérus polinom különböző gyökei $t_1,\ldots,t_k$,
	és ezen gyökök multiplicitásai rendre $m_1,\ldots,m_k$.
	Ekkor $m_1+\ldots+m_k\leq\deg p$.
\end{proposition}
\begin{proof}
	A test nullosztó mentessége és a gyöktényező kiemelhetősége miatt
	\[
		p\left( t \right)=
		\left( t-t_1 \right)^{m_1}\cdot\left( t-t_2 \right)^{m_2}\ldots\left( t-t_k \right)^{m_k}\cdot
		h\left( t \right),
	\]
	ahol $h$ olyan nem zérus polinom, amelynek már nincsen gyöke.
	A fokszámok összehasonlításából kapjuk, hogy
	$m_1+\ldots+m_k\leq m_1+\ldots+m_k+\deg h=\deg p$.
\end{proof}
A fenti gondolat szerint, ha egy legfeljebb $n$-ed fokú polinomnak $n+1$ különböző gyöke van,
akkor csak úgy lehetséges, ha a polinom minden együtthatója nulla.
Ezt úgy is szoktuk fogalmazni,
hogy egy legfeljebb $n$-ed fokú polinomot $n+1$ helyettesítési értéke már egyértelműen meghatározza:
\begin{proposition}\label{pr:polinomfv}
	Tegyük fel, hogy a $p,q\in\mathbb{F}\left[ t \right]$ polinomok legfeljebb $n$-ed fokúak, ahol $n$ egy nemnegatív egész,
	és tegyük fel,
	hogy létezik $n+1$ különböző $t_0,t_1,\ldots,t_n$ pont a testben,
	amelyekre $p\left( t_j \right)=q\left( t_j \right)$ minden $j=0,\ldots,n$.
	Ekkor $p\left( t \right)=q\left( t \right)$,
	azaz $p$ és $q$ együtthatói azonosak.
\end{proposition}
\begin{proof}
	Legyen $h=p-q$.
	Világos, hogy $\deg h\leq n$ és $h$-nak van $n+1$ különböző gyöke.
	Az előző állítás szerint ez csak a $h=0$ polinomra igaz, ami azt jelenti,
	hogy $p$ és $q$ együtthatói azonosak.
\end{proof}
A maradékos osztás tételének szép következménye tehát,
hogy ha $\mathbb{F}$ egy nem véges test, és a $p,q\in\mathbb{F}\left[ t \right]$ polinomok,
akkor $p$-nek és $q$-nak pontosan akkor azonosak az együtthatói, ha
\(
p\left( t \right)=q\left( t \right)
\)
fennáll minden $t\in\mathbb{F}$ mellett.

Itt fontos, hogy $\mathbb{F}$ nem véges test,
hiszen például ha $\mathbb{F}=\left\{ 0,1 \right\}$ a két elemű test,
akkor a $p\left( t \right)=t^2+t$ polinomra minden $t\in\left\{ 0,1 \right\}$ mellett $p\left( t \right)=0$,
de a polinom együtthatói rendre a $\left\{ 0,1,1 \right\}$ számok a testből,
tehát ez nem a összeadásra nézve neutrális eleme az $\mathbb{F}\left[ t \right]$ polinomgyűrűnek.

Konklúzióképpen: megnyugodhatunk, hogy az iménti szörnyűség nem véges testek esetén nem fordulhat elő,
tehát mondjuk a valós vagy a komplex számtest felett mindegy,
hogy a polinomokat függvényeknek, vagy formális algebrai kifejezéseknek gondoljuk.
A lényeg hogy egy $n$-ed fokú polinomot az $n+1$ együtthatója, definíció szerint,
de az $n+1$ különböző helyen felvett helyettesítési értéke is egyértelműen meghatározza.

\section{Az Euklideszi-algoritmus}
Algoritmust keresünk polinomok legnagyobb közös osztójának és legkisebb közös többszörösének meghatározására.
Ha egy pillanatra $\left( p_1,\ldots,p_n \right)$ jelöli az adott $p_1,\ldots,p_n$ polinomok legnagyobb közös osztóját,
akkor nem nehéz meggondolni,
hogy
\[
	\left( \left( p_1,\ldots,p_{n-1} \right),p_n \right)=\left( p_1,\ldots,p_{n}\right).
\]
Ezt $n=3,4,\ldots$ számokra alkalmazva azt kapjuk,
hogy ha módszerünk van két polinom legnagyobb közös osztójának meghatározására,
akkor evvel már akárhány -- persze véges sok -- polinom legnagyobb közös osztója is meghatározható.
Analóg módon ugyanez igaz a legkisebb közös többszörösre is.
Azt gondoltuk meg tehát, hogy ha meg tudnánk határozni két polinom legnagyobb közös többszörösét és legkisebb közös osztóját,
akkor ugyanezt már meg tudnánk tenni több polinom esetén is.

Az Euklideszi-algoritmus két polinom legnagyobb közös osztójának meghatározására szolgál.
Az eddigi ismereteink szerint a $p,q$ legnagyobb közös osztója a $J\left( p,q \right)$ ideál legalacsonyabb
fokú, normált tagja.
Az Euklideszi-algoritmus egy véges sok lépésben végrehajtható algoritmus,
egy a definíciónál sokkal hatékonyabb eljárás, két polinom generálta főideál generáló elemének,
azaz a legnagyobb közös osztónak a meghatározására.

Az algoritmus megértése előtt emlékeznünk kell arra,
hogy ha $p,q\in\mathbb{F}\left[ t \right]$ valamely polinomok, akkor
\(
J\left( p,q \right)=\left\{ fp+gq:f,g\in\mathbb{F}\left[ t \right] \right\}
\)
jelöli a $p$ és a $q$ polinomokat tartalmazó legszűkebb ideált.
Világos, hogy ha $r_1,r_2\in J\left( p,q \right)$, akkor
$J\left( r_1,r_2 \right)\subseteq J\left( p,q \right)$.
\begin{proposition}[Euklidesz]\index{Euklideszi-algoritmus}
	Legyen $p,q\in\mathbb{F}\left[ t \right]$ nem zérus polinomok.
	Definiálja $p_{-1}=p,p_{0}=q$.
	Folytatva, ha valamely $i\geq 0$ számra $p_{i-1}$ és $p_i$ már definiált és $p_i\neq 0$,
	akkor a maradékos osztás szerint létezik egyetlen $h_{i+1},p_{i+1}\in\mathbb{F}\left[ t \right]$ polinom,
	amelyre
	\[
		p_{i-1}=h_{i+1}p_i+p_{i+1};
		\text{ ahol }
		\deg p_{i+1}<\deg p_i.\tag{\dag}
	\]
	Mivel minden egyes lépésben csökken a fokszám,
	ezért van olyan $s\geq 0$,
	hogy $p_s\neq 0$, de $p_{s+1}=0$.
	\\
	Erre a $p_s$ polinomra $J\left( p_s \right)=J\left( p,q \right)$, ezért
	$p_s$ normáltja a $p\text{ és a }q$ polinomok legnagyobb közös osztója.
\end{proposition}
\begin{proof}
	A fenti algoritmussal olyan
	$p_{-1}, p_0,p_1,\ldots,p_s,p_{s+1}$ polinomokat kapunk,
	amelyekre minden $i=0,\ldots,s$ mellett a (\dag) azonosság fennáll,
	és $\deg p_{s+1}=-\infty$,
	azaz $p_{s+1}=0$.

	A (\dag) azonosság szerint,
	minden $i=0,1,\ldots,s$ mellett
	\begin{math}
		p_{i-1}\in J\left( p_i,p_{i+1} \right)
	\end{math},
	amiből a
	\begin{math}
		J\left( p_{i-1},p_i \right)\subseteq J\left( p_i,p_{i+1} \right)
	\end{math}
	tartalmazás adódik.
	Másrészt a (\dag) azonosságot értelmezhetjük úgy is, hogy
	\(
	p_{i+1}\in J\left( p_{i-1},p_i \right)
	\),
	amiből persze
	\begin{math}
		J\left( p_i,p_{i+1} \right)\subseteq J\left( p_{i-1},p_i \right)
	\end{math}
	következik.
	Így minden $i=0,\ldots,s$-re végül is
	\[
		J\left( p_{i-1},p_i \right)
		=
		J\left( p_i,p_{i+1} \right).
	\]
	Alkalmazva ezt minden $i=0,\ldots,s$ mellett
	\[
		J\left( p,q \right)
		=
		J\left( p_{-1},p_0 \right)
		=
		J\left( p_0,p_1 \right)
		=
		J\left( p_1,p_2 \right)
		=\ldots=
		J\left( p_s,p_{s+1} \right)
		=J\left( p_s \right).\qedhere
	\]
\end{proof}
Most a legkisebb közös többszörös algoritmikus meghatározására törekszünk.
\begin{proposition}\label{pr:rprim}
	Legyenek a $p,q\in\mathbb{F}\left[ t \right]$ polinomok relatív prímek,
	és tegyük fel, hogy
	\begin{math}
		p|qr.
	\end{math}
	Ekkor $p|r$.
\end{proposition}
\begin{proof}
	Mivel a $p$ és a $q$ polinomok relatív prímek,
	ezért a Bezout-azonosság\index{Bezout-azonosság} szerint van $f$ és $g$ polinom, amelyekre
	\(
	fpr+gqr=r.
	\)
	A feltétel szerint $qr$ a $p$ többszöröse,
	így az iménti azonosság baloldala a $p$ többszöröse,
	ami éppen azt jelenti, hogy $p|r$.
\end{proof}
\begin{proposition}
	Tekintsük a $p,q\in\mathbb{F}\left[ t \right]$ normált polinomokat.
	Jelölje $d$ a legnagyobb közös osztót,
	és
	$m$ a legkisebb közös többszöröst.
	Ekkor
	\begin{displaymath}
		d\left( t \right)m\left( t \right)=p\left( t \right)q\left( t \right).\qedhere
	\end{displaymath}
\end{proposition}
\begin{proof}
	Legyen $p=dr_1$ és $q=dr_2$.
	Először megmutatjuk, hogy $r_1$ és $r_2$ relatív prímek.
	A Bezout-azonosság szerint valamely $f,g$ polinomra
	\(
	d=fdr_1+gdr_2.
	\)
	Kihasználva, hogy nullosztómentes gyűrűben nem zérus elemmel egyszerűsíteni lehet,
	kapjuk az
	\(
	1=fr_1+gr_2
	\)
	azonosságot.
	Ez azt jelenti, hogy $r_1$ és $r_2$ relatív prímek.
	%    Ha $s$ polinom közös osztójuk, akkor $ds$ is közös osztója $p$-nek és $q$-nak,
	%    amiből $ds|s$ következik.
	%    A fokszámokat összehasonlítva ez csak akkor lehetséges, ha $s$ konstans polinom, 
	%    azaz $s|1$ valóban fennáll.

	Most megmutatjuk, hogy
	\[
		dr_1r_2\tag{\dag}
	\]
	a legkisebb közös többszörös.
	Világos, hogy ez egy közös többszöröse a $p,q$ polinomoknak.
	Tegyük fel, hogy $s$ egy másik közös többszörös, azaz
	$s=ps_1$ és $s=qs_2$.
	Ekkor
	\begin{math}
		dr_1s_1=ps_1=s=qs_2=dr_2s_2,
	\end{math}
	amiből újra a nullosztómentesség szerint
	\[
		r_1s_1=r_2s_2.
	\]
	Na most,
	a fent kiemelt azonosság szerint szerint $r_1|r_2s_2$, ahol $r_1$ és $r_2$ relatív prímek.
	Ebből azonnal kapjuk, hogy $r_1|s_2$.
	No de, $s=qs_2=dr_2s_2$, amiből már látszik,
	hogy $s$ egy többszöröse a $dr_1r_2$ polinomnak.
	Az is világos, hogy (\dag) normált polinom, ez az $m$ legkisebb közös többszörös.
	Innen
	$d\cdot m=d(dr_1r_2)=(dr_1)(dr_2)=p\cdot q$ már nyilvánvaló.
\end{proof}
A fenti állítás csak két polinomra igaz, többre nem,
de nekünk csak két polinomra kell.
Úgy interpretáljuk,
hogy ha a szorzatot osztom maradékosan a legnagyobb közös osztóval, akkor a maradék mindig
zérus,
és a hányados éppen a legkisebb közös többszörös.
Azt gondoltuk meg tehát,
hogy az Euklideszi-algoritmus módszert ad két polinom legkisebb közös többszörösének algoritmikus meghatározására is.

Visszatérve a szakasz elején felvetett gondolatra,
ilyen módon véges sok lépésben végrehajtható algoritmust kapunk véges sok polinom legkisebb közös többszörösének meghatározására.
Például öt polinom legkisebb közös többszöröséhez,
egy Euklideszi-algoritmussal meghatározzuk az első kettő polinom legnagyobb közös osztóját,
majd egy újabb maradékos osztással az első kettő legkisebb közös többszörösét.
Ugyanezt teszem az így kapott és a harmadik polinommal,
az eredmény az első három polinom legkisebb közös többszöröse.
Az így kapott polinommal és a negyedik polinommal egy újabb Euklideszi-algoritmus és egy újabb maradékos osztás után kapjuk az első négy polinom legkisebb közös többszörösét,
majd ennek eredményével és az ötödik polinommal mint két polinomnak a legkisebb közös többszörösével kapjuk az eredeti öt polinom legkisebb közös többszörösét.
\section{Polinom faktorizáció}
Kicsi korunk óta sulykolják belénk, hogy minden egész szám előáll, méghozzá lényegében csak egyféleképpen
prímek szorzataként.
Ha ismerjük két szám prímtényezős előállítását, akkor nagyon könnyű megmondani a két szám legkisebb közös többszörösét,
vagy a legnagyobb közös osztóját.
Evvel a probléma csak annyi,
hogy nagyon nehéz megmondani két, esetleg jó nagy, szám prímtényezős előállítását,
így még az egész számok gyűrűjében is az Euklideszi-algoritmus a megfelelő, véges sok lépésben,
végrehajtható módszer a legnagyobb közös osztó és a legkisebb közös többszörös konkrét felírására.

Ebben a szakaszban azt mutatjuk meg, hogy prímtényezős előállításról szóló tétel a polinomok gyűrűjében is igaz marad.
Természetesen konkrét algoritmust nem adunk, hiszen ilyen még számokra sem igen van.%
\footnote{Természetesen számokra működik az az algoritmus, amit általános iskolában tanultunk,
	csak annyira lassú, hogy egy nagy szám primtényezős előállítását esetleg többszáz évig számolná a földkerekség valamennyi számítógépe.
	Ez egy szép példája annak,
	mennyire hasznos lehet az a tudás,
	ami valaminek a nem tudásáról szól,
	hiszen
	emiatt tudunk biztonságos módon két számla közt pénzt mozgatni.
	Lásd például: \url{https://en.wikipedia.org/wiki/RSA_(cryptosystem)}}
\begin{definition}[reducibilis polinom]\index{reducibilis polinom}\index{irreducibilis polinom}
	Egy polinomot \emph{reducibilisnek} mondunk,
	ha előáll mint két legalább elsőfokú polinom szorzata.
	Egy nem reducibilis polinomot \emph{irreducibilisnek} nevezünk.
\end{definition}
Világos, hogy minden legfeljebb elsőfokú polinom tetszőleges test felett irreducibilis.
A magasabb fokú polinomok esetében a probléma nagyban függ a testtől is,
ahonnan a polinom együtthatói származnak.
A következő állítás viszont minden test mellett igaz.
\begin{proposition}[polinom faktorizáció]\label{pr:polfact}
	Tetszőleges test feletti polinomgyűrűben,
	minden (normált) polinom előáll mint (normált) irreducibilis polinomok szorzata.
\end{proposition}
\begin{proof}
	Az előállítandó polinom foka szerinti teljes indukció.
	Elsőfokú polinom maga irreducibilis.

	Most tegyük fel, hogy az állítás igaz $n$-nél alacsonyabb fokú polinomokra
	és lássuk be $\deg p=n$ mellett, ahol $n>1$.
	Ha $p$ irreducibilis, akkor megint készen vagyunk.
	Ha $p=f g$ valamely $\deg f\geq 1$ és $\deg g\geq 1$ mellett,
	akkor $\deg f<n$ és $\deg g<n$.
	Az indukciós feltevés szerint $f$ és $g$, emiatt $p=fg$ is előáll irreducibilis polinomok szorzataként.
\end{proof}

Érdemes látni,
hogy ha a $d$ polinom egy irreducibilis $p$ polinom osztója,
akkor csak
$\deg d=0$, vagy $\deg d=\deg p$ lehetséges.
Ezért egy tetszőleges $f$ polinomra igaz, hogy
vagy $p|f$ vagy $p$ és $f$ relatív prímek.
Ugyanis,
ha az $f$ és $p$ polinomok $d$ legnagyobb közös osztója nem $1$,
akkor csak $d=p$ lehetséges, ergo $p|f$.
\begin{proposition}[prím tulajdonság]\index{prím polinom}
	Legyen $p\in\mathbb{F}\left[ t \right]$ irreducibilis polinom, amelyre
	$p|(f_1\cdots f_n)$, valamely $f_j\in\mathbb{F}\left[ t \right]$ polinomokra, ahol $j=1\dots,n\geq 1$.
	Ekkor létezik $1\leq j\leq n$, amelyre $p|f_j$.\qedhere
\end{proposition}
\begin{proof}
	A polinomok $n$ száma szerinti indukció.
	Az $n=1$ eset semmitmondó módon teljesül.
	Tegyük fel, hogy igaz az állítás $n$-nél kevesebb polinomra,
	és lássuk be $n$-re. Itt $n\geq 2$.
	Induljunk ki tehát abból, hogy
	\[
		p|\left( f_1\cdots f_{n-1} \right)\cdot f_n
	\]
	Ha $p$ osztója lenne az $f_1\cdots f_{n-1}$ szorzatnak,
	akkor az indukciós feltevés szerint készen is lennénk.
	Ha $p$ nem osztója a szorzatnak,
	akkor $p$ irreducibilis volta miatt relatív prímek.
	Ekkor \aref{pr:rprim}. állítás szerint $p|f_n$.
\end{proof}
Az is igaz, hogy ha egy polinom reducibilis, akkor az nem prím, de ezt a gyakorlatokra hagyjuk.
Összeségében látjuk tehát,
hogy a polinomok irreducibilitása és prím tulajdonsága azonos fogalmak.

Az irreducibilis polinomok prím tulajdonsága segítségével a polinomok faktorizáció egyértelműségét is igazolhatjuk.
\begin{proposition}
	Legyen $p \in\mathbb{F}\left[ t \right]$ egy legalább elsőfokú normált polinom.
	Ekkor ezek sorrendjétől eltekintve egyértelműen léteznek legalább elsőfokú, normált, irreduciblis $q_1,\ldots,q_s\in\mathbb{F}\left[ t \right]$ polinomok,
	hogy $p=q_1\cdots q_s$.
\end{proposition}
A ,,sorrendtől eltekintés'' alatt azt értjük, hogy ha $p$ előáll
\[
	p_1\cdots p_s=p=q_1\cdots q_r
\]
legalább elsőfokú, normált, irreducibilis polinomok szorzataként,
akkor $s=r$ és a $p_1,\ldots,p_s$ polinomok alkalmas átindexelése után $p_j=q_j$,
minden $j=1,\ldots,s$ mellett.
\begin{proof}
	\Aref{pr:polfact}. állítás
	szerint $p$ előáll, mint normált, irreducibilis polinomok szorzata.
	Itt legalább egy polinom legalább elsőfokú, hiszen $p$ legalább elsőfokú.
	Ha van a szorzat tagjai közt nulladfokú, akkor az csak 1 lehet a normáltság szerint,
	ezért egyszerűen elhagyható.
	Meggondoltuk tehát, hogy $p$ legalább elsőfokú, normált, irreducibilis polinomok szorzata.

	Az unicitást,
	a felbontandó polinom fokszáma szerinti indukcióval igazoljuk:
	Ha a polinom elsőfokú, akkor a felbontás egyértelműsége is nyilvánvaló.
	Tegyük fel, hogy igaz az egyértelműség $n$-nél alacsonyabb fokszámú polinomokra,
	és tegyük fel, hogy $\deg p=n>1$.
	Nézzünk két lehetséges előállítást
	\[
		p_1\cdots p_s=p=q_1\cdots q_r,
	\]
	ahol $p_1,\ldots,p_s,q_1,\ldots,q_r$ legalább elsőfokú, normált, irreducibilis polinomok.
	A $q_1$ polinom osztója a jobboldalnak, ezért a baloldalnak is.
	A prím tulajdonság miatt, \ref{pr:rprim}. állítás, $q_1$ osztója az egyik baloldali polinomnak.
	Alkalmas átindexelés után feltehető, hogy $q_1|p_1$.
	No de, $p_1$ is irreducibilis, és $\deg q_1\geq 1$ miatt csak $\deg q_1=\deg p_1$ lehetséges,
	tehát a normáltság szerint $q_1=p_1$.
	A polinomgyűrű nullosztó mentessége szerint az első polinomokkal egyszerűsíthetünk, ergo
	\[
		p_2\cdots p_s=q_2\cdots q_r,
	\]
	is fennáll.
	A fenti polinom már $n$-nél alacsonyabb fokú,
	így az indukciós feltevés szerint $s-1=r-1$,
	és alkalmas átindexelés után minden $j=2,\ldots,s$ esetén is teljesül a $p_j=q_j$ egyenlőség.
\end{proof}
Az egész szakaszt összefoglalhatjuk így is:
\begin{proposition}
	Minden legalább elsőfokú normált polinomhoz léteznek
	--
	méghozzá sorrendjüktől eltekintve egyértelműen léteznek
	--
	olyan $q_1,\ldots,q_s$ egymástól különböző, normált, irreducibilis polinomok;
	és olyan $n_1,\ldots,n_s$ pozitív egészek;
	amelyekre
	\[
		p\left( t \right)
		=
		q_1^{n_1}\left( t \right)\cdots q_s^{n_s}\left( t \right).\qedhere
	\]
\end{proposition}

\section{Mátrixok}
\begin{definition}[mátrix]
	Egy tetszőleges test feletti mátrixnak nevezzünk,
	a test elemeiből képzett táblázatot.
	Ha $m,n\in\mathbb{N}$ előre rögzített pozitív egészek és
	az $A$ táblázatnak $m$ sora és $n$ oszlopa van,
	akkor azt mondjuk, hogy $A$ egy $m\times n$ méretű mátrix.
	Az $\mathbb{F}$ test feletti $m\times n$-es mátrixok halmazát $\mathbb{F}^{m\times n}$
	módon jelöljük.

	Ha $A\in\mathbb{F}^{m\times n}$ egy mátrix,
	akkor $A_i$ jelöli az $i$-edik sort, ami persze egy $1\times n$-es mátrix;
	$A^j$ jelöli a $j$-edik oszlopot, ami persze egy $m\times 1$-s mátrix;
	$A_i^j$ jelöli az $i$-edik sor $j$-edik elemét.
	Sokszor használjuk az $A_{i,j}=A_i^j$ jelölést is.

	Időnként, azt hangsúlyozandó hogy mátrixokról van szó a mátrixot jelölő betűt kapcsos zárójelbe teszem.
	Pl. $\left[ A \right]\in\mathbb{F}^{m\times n}$.

	A mátrixot a mérete és az elemei határozzák meg.
	Emiatt két mátrix akkor azonos, ha azonos méretűek, és a megfelelő elemeik is azonosak.
\end{definition}
Az azonos típusú mátrixok közt műveleteket definiálunk:
\begin{definition}[mátrixok összege]\index{matrix@mátrixok összege}
	Rögzített $m,n\in\mathbb{N}$ mellett, ha $A,B\in\mathbb{F}^{m\times n}$,
	akkor ezek összege az a $C\in\mathbb{F}^{m\times n}$ mátrix, amelyre
	\[
		C_{i,j}=A_{i,j}+B_{i,j}
	\]
	minden $i=1,\ldots,m$ és $j=1,\ldots, n$.
	Jelölés: $C=A+B$.
\end{definition}
\begin{proposition}\label{pr:matrixokVS1}
	Az $m\times n$ méretű mátrixok az fent definiált összeadás művelettel
	Abel-csoportot alkotnak.
	A $[0]$-val jelölt neutrális elem az az $m\times n$-s mátrix,
	amelynek minden eleme a test zérus eleme:
	\[
		[0]_{i,j}=0;
	\]
	az $[A]$ mátrix additív inverze az az $[-A]$-val jelölt $m\times n$ méretű mátrix,
	amelyre
	\[
		[-A]_{i,j}=-([A]_{i,j}).\qedhere
	\]
\end{proposition}

Most definiáljuk egy számnak és egy mátrixnak a szorzatát.
\begin{definition}[szám és mátrix szorzata]\index{szám és mátrix szorzata}
	Ha $\alpha\in\mathbb{F}$ egy szám és $A\in\mathbb{F}^{m\times n}$ egy mátrix, akkor ezek szorzata
	az $\alpha A\in\mathbb{F}^{m\times n}$ módon jelölt mátrix, melynek elemeire
	\[
		[\alpha A]_{i,j}=\alpha[A]_{i,j}.\qedhere
	\]
\end{definition}
Könnyen ellenőrizhetőek a következő számolási szabályok:
\begin{proposition}\label{pr:matrixokVS2}
	Legyenek $A,B\in\mathbb{F}^{m\times n}$ mátrixok, és $\alpha,\beta\in\mathbb{F}$ tetszőleges számok.
	Ekkor
	\begin{enumerate}
		\item $\alpha\left( A+B \right)=\alpha A+\alpha B$;
		\item $\left( \alpha+\beta \right)A=\alpha A+\beta A$;
		\item $\left( \alpha\beta \right)A=\alpha\left( \beta A \right)$;
		\item $1 A=A$.\qedhere
	\end{enumerate}
\end{proposition}
Az utolsó két állítást, \ref{pr:matrixokVS1}. és \ref{pr:matrixokVS2}.,
együtt később úgy fogjuk fogalmazni,
hogy adott test feletti tetszőleges méretű mátrixok \emph{vektorteret}\index{vektortér} alkotnak.
Ha az $A_1,\ldots,A_k$ azonos méretű mátrixok adottak,
akkor tetszőleges $\alpha_1,\ldots,\alpha_k\in\mathbb{F}$
számok mellett értelmes az $A_1,\ldots,A_k$ mátrixoknak az $\alpha_1,\ldots,\alpha_k$ számokkal
mint együtthatókkal képzett \emph{lineáris kombinációja}\index{lineáris kombináció}:
\[
	\alpha_1A_1+\alpha_2A_2+\dots+\alpha_kA_k.
\]
Minden eddig megértett dolog persze az $m=1$ speciális esetben,
és az $n=1$ speciális esetben is igaz.
Ha $m=1$, akkor a mátrixot \emph{sorvektornak}\index{sorvektor};
ha pedig $n=1$, akkor a mátrixot \emph{oszlopvektornak}\index{oszlopvektor} mondjuk.

Most a mátrixok között a szorzás műveletet definiáljuk.
\begin{definition}[mátrixok szorzata]\index{matrix@mátrixok szorzata}
	Legyen $A\in\mathbb{F}^{m\times k}$ és $B\in\mathbb{F}^{k\times n}$ mátrix.
	Fontos, hogy $A$ oszlopainak száma azonos $B$ sorainak számával.
	Ezek $C=AB$ szorzata egy $C\in\mathbb{F}^{m\times n}$ mátrix,
	melynek elemeit az alábbi egyenlőség definiálja
	\[
		[C]_{i,j}=\sum_{s=1}^k[A]_{i,s}[B]_{s,j}.
	\]
	Itt persze $i=1,\ldots,m$ és $j=1,\ldots,n.$
\end{definition}
Figyeljünk arra, hogy a fenti definícióban az $A$ és $B$ mátrixok sorrendje is fontos.
Például, ha $n\neq m$, akkor $BA$ nem is értelmes.
Még $m=n$ esetén is $BA$ egy $k\times k$ méretű mátrix, míg $AB$ egy $n\times n$ méretű mátrix,
azaz az $AB$ szorzatnak általában véve semmi köze a $BA$ szorzathoz.

A bevezetett összeadás és szorzás műveletekre teljesül a disztributivitás is:
\begin{proposition}
	Legyen $A\in\mathbb{F}^{m\times k}$, valamint a $B,C\in\mathbb{F}^{k\times n}$ mátrixok.
	Ekkor
	\begin{displaymath}
		A\left( B+C \right)=AB+AC.
	\end{displaymath}
	Hasonlóan,
	ha $A,B\in\mathbb{F}^{m\times k}$, valamint a $C\in\mathbb{F}^{k\times n}$ mátrixok,
	akkor
	\begin{displaymath}
		\left( A+B \right)C=AC+BC.\qedhere
	\end{displaymath}
\end{proposition}
\begin{proof}
	Az első disztributivitást mutatjuk meg, a második evvel analóg.
	Világos, hogy mindkét oldalon azonos méretű mátrixok vannak.
	Minden szóba jövő $i,j$ index mellett
	\begin{multline*}
		\left[ A\left( B+C \right) \right]_{i,j}
		=
		\sum_{s=1}^k[A]_{i,s}\left[ B+C \right]_{s,j}
		=
		\sum_{s=1}^k[A]_{i,s}\left([B]_{s,j}+[C]_{s,j} \right)
		=
		\sum_{s=1}^k\left( [A]_{i,s}[B]_{s,j}\right)+\left([A]_{i,s}[C]_{s,j} \right)
		=
		\\
		\sum_{s=1}^k[A]_{i,s}[B]_{s,j}
		+
		\sum_{s=1}^k[A]_{i,s}[C]_{s,j}
		=
		\left[ AB \right]_{i,j}+\left[ AC \right]_{i,j}
		=
		[AB+AC]_{i,j}.
	\end{multline*}
	Ez éppen  az $A\left( B+C \right)=AB+AC$ mátrixazonosságot jelenti.
\end{proof}
Szinte banalitás,
de egy mátrix az elemei összesége, de a sorai összesége, sőt az oszlopai összesége is.
A következő ultrafontos állítás arról szól, hogy a szorzás operációt hogyan kell látnunk attól függően,
hogy a szorzat mátrixra,
mint elemei összeségére,
mint oszlopai összeségére,
vagy mint sorainak összeségére gondolunk.
\begin{proposition}
	Legyen $A\in\mathbb{F}^{m\times k}$ és $B\in\mathbb{F}^{k\times n}$ mátrix.
	Jelölje $C=AB$ ezek szorzatát ebben a sorrendben.
	Ekkor
	\begin{enumerate}
		\item A szorzat mátrix $i$-edik sorának $j$-edik eleme,
		      az $A$ mátrix $i$-edik sorának és a $B$ mátrix $j$-edik
		      oszlopának, mint speciális mátrixoknak a szorzata.
		      Magyarul: minden $1\leq i\leq m$ és $1\leq j \leq n$ mellett
		      \[
			      [C]_{i,j}=[A]_i\cdot [B]^j.
		      \]
		\item
		      A szorzat mátrix minden oszlopa az $A$ mátrix oszlopainak a $B$ mátrix megfelelő oszlopából vett elemekkel képzett
		      lineáris kombinációja.
		      Magyarul: minden $1\leq j\leq n$ mellett
		      \[
			      [C]^j=\sum_{s=1}^k[B]_s^j[A]^s.
		      \]
		\item
		      A szorzat mátrix minden sora a $B$ mátrix sorainak az $A$ mátrix megfelelő sorából vett elemekkel képzett
		      lineáris kombinációja.
		      Magyarul: minden $1\leq i\leq m$ mellett
		      \[
			      [C]_i=\sum_{s=1}^k[A]_i^s[B]_s.
		      \]
		\item
		      A szorzat mátrix az $A$ oszlopaiból, és a $B$ soraiból alkotott diádok összege.\index{diad@diád}
		      Magyarul:
		      \[
			      [C]=\sum_{s=1}^k[A]^s[B]_s.
		      \]
		      \emph{Diádnak}\index{diad@diád} nevezzük egy oszlop- és egy sorvektor szorzatát.
		      Ha az oszlopnak és a sornak rendre azonosak az elemei, akkor
		      \emph{szimmetrikus diádról} beszélünk.\index{szimmetrikus diád}
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}[1. bizonyítása]
	$[A]_i\cdot [B]^j=\sum_{s=1}^k[A]_{i,s}[B]_{s,j}=[C]_{i,j}.$
\end{proof}
\begin{proof}[2. bizonyítása]
	\(
	\left[ \sum_{s=1}^k[B]_s^j[A]^s \right]_i=
	\sum_{s=1}^k[B]_s^j[A]_i^s =
	\sum_{s=1}^k[A]_i^s[B]_s^j =
	[C]_{i,j}=
		[C]_i^j
	\)
	minden $i$-re.
\end{proof}
\begin{proof}[3. bizonyítása]
	\(
	\left[ \sum_{s=1}^k[A]_i^s[B]_s \right]^j=
	\sum_{s=1}^k[A]_i^s[B]_s^j=
	[C]_{i,j}=
		[C]_i^j
	\)
	minden $j$-re.
\end{proof}
\begin{proof}[4. bizonyítása]
	\(
	\left[ \sum_{s=1}^k[A]^s[B]_s \right]_{i,j}=
	\sum_{s=1}^k\left[ [A]^s[B]_s \right]_{i,j}=
	\sum_{s=1}^k [A]_i^s[B]_s^j=
	C_{i,j}
	\)
	minden $i$-re $j$-re.
\end{proof}
\begin{definition}[Kronecker-delta, identitás mátrix]\index{Kronceker-delta}\index{identitás mátrix}
	\emph{Kroencker-deltának} nevezzük az alábbi egyszerű szimbólumot:
	\[
		\delta_{i,j}=
		\begin{cases}
			1,\text{ ha }i=j; \\
			0,\text{ egyébként.}
		\end{cases}
	\]
	Adott $n\geq 1$ természetes számra az $n\times n$ méretű identitás mátrix azaz $I\in\mathbb{F}^{n\times n}$ mátrix,
	amelyre
	\[
		[I]_{i,j}=\delta_{i,j}.\qedhere
	\]
\end{definition}
Nyilvánvaló, hogy ha $A\in\mathbb{F}^{m\times n}$ mátrix és $I\in\mathbb{F}^{n\times n}$ méretű identitás mátrix,
akkor $A\cdot I=A$. Hasonlóan, ha most $I$ az $m\times m$ identitás mátrixot jelöli, akkor pedig $I\cdot A=A$ azonosság teljesül.
Persze, ha $A\in\mathbb{F}^{n\times n}$ négyzetes mátrix és $I\in\mathbb{F}^{n\times n}$ az ugyanilyen méretű identitás mátrix,
akkor
\[
	IA=AI=A
\]
is fennáll.

A mátrixok szorzásának legérdekesebb tulajdonsága a szorzás asszociativitása.
\begin{proposition}
	Legyen az $A,B,C$ mátrixok úgy megadva, hogy $AB$ is értelmes és $BC$ is értelmes legyen,
	azaz $A\in\mathbb{F}^{m\times k},B\in\mathbb{F}^{k\times l}, C\in\mathbb{F}^{l\times n}$.
	Ekkor
	\[
		A\left( BC \right)=\left( AB \right)C.\qedhere
	\]
\end{proposition}
\begin{proof}
	Először is azt vegyük észre,
	hogy ha az $A$ és a $C$ mátrixok egyike egy szám,
	és a másik két mátrix összeszorozható,
	akkor a mátrix szorzás definíciója szerint az állítás nyilvánvaló.

	Másodszor azt vegyük észre, hogy mindkét oldalon azonos méretű
	konkrétan $m\times n$ méretű mátrixok szerepelnek.

	Azt kell tehát még meggondolnunk,
	hogy az $i$-edik sor $j$-edik eleme mindkét oldalon ugyanaz.
	A jobboldalon ez
	\begin{multline*}
		[AB]_i\cdot [C]^j
		=
		\left( \sum_{s=1}^k[A]_i^s[B]_s \right)[C]^j
		=
		\sum_{s=1}^k\left([A]_i^s[B]_s\right)[C]^j
		=
		\sum_{s=1}^k[A]_i^s\left([B]_s[C]^j\right)
		\\
		=
		\sum_{s=1}^k[A]_i^s\left(\sum_{r=1}^l[B]_s^r[C]_r^j\right)
		=
		\sum_{s=1}^k\sum_{r=1}^l[A]_i^s([B]_s^r[C]_r^j).
	\end{multline*}
	A baloldalon
	az $i$-edik sor $j$-edik eleme hasonló számolgatással:
	\begin{multline*}
		[A]_i[BC]^j=
		[A]_i\left( \sum_{r=1}^l[B]^r[C]_r^j \right)
		=
		\sum_{r=1}^l[A]_i\left([B]^r[C]_r^j\right)
		=
		\sum_{r=1}^l\left([A]_i[B]^r\right)[C]_r^j
		\\
		\sum_{r=1}^l\left(\sum_{s=1}^k[A]_i^s[B]_s^r\right)[C]_r^j
		=
		\sum_{r=1}^l\sum_{s=1}^k\left([A]_i^s[B]_s^r\right)[C]_r^j.
	\end{multline*}
	A testben fennálló asszociativitás és kommutativitás miatt a bal- és a jobboldali kifejezés azonos.
\end{proof}
Ha tehát adottak az $A_1,A_2,\ldots,A_p$ mátrixok, úgy hogy a sorban egymás után következőknek értelmes a szorzata,
akkor ezek bármilyen zárójelezésével képzett szorzata is értelmes,
és ugyanazt a mátrixot eredményezi.
Mint azt a számok esetén megszoktuk,
használhatjuk az $A_1A_2\dots A_p$ jelölést ezen mátrixok bármelyik zárójelezésével képzett szorzatára.
Érdemes a fenti bizonyításból megjegyezni a formulát,
amelyet három mátrix szorzatára kapunk:
\[
	\left[ ABC \right]_{i,j}
	=
	\sum_{s=1}^k\sum_{r=1}^l\left[ A \right]_{i,s}\left[ B \right]_{s,r}\left[ C \right]_{r,j}.
\]
Foglaljuk össze az eddigieket négyzetes mátrixok mellett:
\begin{proposition}
	Legyen $n\in\mathbb{N}$ természetes szám, és tekintsük az $n\times n$ méretű mátrixok
	halmazát, ellátva ezt a halmazt a mátrix összeadással és a mátrixszorzással.
	Az $\left( \mathbb{F}^{n\times n},+,\cdot \right)$ algebrai struktúra egy egységelemes gyűrű.
\end{proposition}
Ez a gyűrű, az $n>1$ esetben biztosan nem kommutatív.
Például $n=2$ mellett
\[
	\begin{pmatrix}
		0 & 1 \\
		0 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		0 & 0 \\
		1 & 0
	\end{pmatrix}
	=
	\begin{pmatrix}
		1 & 0 \\
		0 & 0
	\end{pmatrix},
	\text{ amíg }
	\begin{pmatrix}
		0 & 0 \\
		1 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		0 & 1 \\
		0 & 0
	\end{pmatrix}
	=
	\begin{pmatrix}
		0 & 0 \\
		0 & 1
	\end{pmatrix}.
\]
Az sem igaz, hogy ez a gyűrű nullosztómentes lenne, hiszen például $n=2$ mellett az
\[
	A
	=
	\begin{pmatrix}
		0 & 1 \\
		0 & 0
	\end{pmatrix}
\]
mátrix nyilván nem a zérus mátrix (az összeadásra nézve neutrális elem),
de $A\cdot A=0$.
Ebből persze már az is következik, hogy a fenti $A$ mátrixnak nincs a szorzásra nézve inverze,
de ez e nélkül is nagyon egyszerűen látszik.%
\footnote{
	Ha
	\(
	\begin{pmatrix}
		a & b \\
		c & d
	\end{pmatrix}
	\)
	jobboldali inverze lenne akkor a jobb alsó sarokra figyelve $0\cdot b +0\cdot d=1$ lenne, de egy testben $1\neq 0$.
}
Gyakorlatként próbáljunk magasabb $n$ számok mellett is a kommutativitás és a nullosztómentesség hiányára
példát találni.

Nagyon fontos látni, hogy a kommutativitás hiánya, az eddigiektől eltérő számolási gyakorlatot eredményez.
A számolás közben a mátrixok sorrendjén nem változtathatunk.
Persze előfordul, hogy két mátrix szorzata nem függ a sorrendtől.
Ilyenkor a két mátrixot egymással \emph{felcserélhetőnek}, vagy \emph{kommutálónak} mondjuk.\index{kommutáló mátrixok}
Például, az identitás mátrixszal minden más mátrix kommutál.
A mátrix azon elemeit, ahol a sor- és az oszlopindexek azonosak
\emph{fődiagonálisbeli}\index{fődiagonális}
elemeknek mondjuk.
Egy mátrixot
\emph{diagonális alakúnak}\index{diagonális mátrix}
mondunk,
ha minden nem zérus eleme a fődiagonálisában van.
Az is világos, hogy a diagonális mátrixok egymással kommutálnak.
Fontos része az első féléves anyagnak, hogy ha két négyzetes mátrix szorzata az identitás mátrix,
akkor e két mátrix egymással kommutál.
Ez az eredmény távolról sem nyilvánvaló, és most nem is tudjuk belátni, mert ehhez már szükség van a Gauss\,--\,Jordan\index{Gauss\,--\,Jordan-elimináció} eliminációs algoritmusra
\footnote{Ha viszont tudjuk mi az a Gauss\,--\,Jordan-elimináció, akkor máris megérthetjük \aref{pr:ketoldal}. állítást. Ehhez ugorjunk \apageref{pr:ketoldal}. oldalra}, vagy a lineáris függetlenség fogalmára, 
amiket majd később vezetünk be.\label{pg:kommutal}\index{kommutáló mátrixok}

Nagyon is triviális mégis érdemes észrevenni, hogy a fentiek $n=1$ esetben nem jelentenek problémát.
Ilyenkor az $1\times 1$-es mátrixok tere voltaképpen azonos az $\mathbb{F}$-testtel, hiszen csak az a különbség,
hogy egy testbeli $a$ elemet $[a]$ módon írjuk. A szorzás és az összeadás definíciója ugyanazt adja,
ha mint a testbeli elemre, vagy az ebből képzett $1\times 1$-es mátrixra gondolunk.

Az $n\times n$-es négyzetes mátrixok másik érdemleges részstruktúrája az
\[
	\mathcal{F}=
	\left\{ c\cdot I:c\in\mathbb{F} \right\}.
\]
Itt $I$ az $n\times n$ méretű identitás mátrix, tehát $\mathcal{F}$ elemei azon diagonális alakú mátrixok,
ahol minden elem a diagonálisban azonos.
Világos, hogy két ilyen mátrix összege és szorzata is ilyen marad:
\[
	aI+bI=\left( a+b \right)I\text{ és } aI\cdot bI=\left( ab \right)I.
\]
Ez azt jelenti, hogy az $\left( \mathcal{F},+,\cdot \right)$ struktúra egy egységelemes gyűrű.
Világos, hogy itt bármely két elem kommutál, ergo egy kommutatív egységelemes gyűrűvel állunk szemben és
az is teljesen nyilván való, hogy minden nem zérus elemnek van a szorzásra nézve inverze.
Azt kaptuk tehát, hogy a fenti $\mathcal{F}$ minden $n$ mellett egy test.

\section{A komplex számok mint mátrixok}
\begin{definition}[Izomorf testek]\index{izomorfizmus}
	Legyenek $\mathbb{F}$ és $\mathbb{G}$ testek.
	Azt mondjuk, hogy a két test \emph{izomorf} egymással,
	ha létezik köztük \emph{művelettartó bijekció}, azaz létezik
	\[
		\varphi:\mathbb{F}\to\mathbb{G}
	\]
	bijekció, amely tartja a műveleteket is, azaz
	\[
		\varphi\left( a+b \right)=\varphi\left( a \right)+\varphi\left( b \right)
		\text{ és }
		\varphi\left( a\cdot b \right)=\varphi\left( a \right)\cdot \varphi\left( b \right).
	\]
	A művelettartó bijekciót \emph{izomorfizmusnak} nevezzük.
\end{definition}
Az izomorf testek közt nem teszünk különbséget. Úgy tekintjük őket, hogy csak jelölésükben különböznek.
Például, ha az $\mathbb{R}$ valós számokra gondolunk,
akkor a $2\times 2$-es diagonális alakú valós mátrixok közül azok, ahol a diagonális mindkét eleme azonos,
a valós testtel izomorf testet alkot.
\[
	\mathcal{R}=\left\{
	\begin{pmatrix}
		a & 0 \\
		0 & a
	\end{pmatrix}
	:a\in\mathbb{R}
	\right\}
	\text{ és }
	\varphi:\mathbb{R}\to\mathcal{R},
	\text{ ahol }
	\varphi\left( a \right)
	=
	\begin{pmatrix}
		a & 0 \\
		0 & a
	\end{pmatrix}.
\]

A $2\times 2$-es valós mátrixok egységelemes gyűrűjében tehát $\mathcal{R}$ egy olyan részgyűrű, ami még test is,
és izomorf az $\mathbb{R}$ valós számtesttel.
Voltaképpen azt csináltuk, hogy a valós számtestet beágyaztuk a $2\times 2$-es mátrixok közé,
azaz egy $a$ valós számot az
\begin{math}
	\begin{pmatrix}
		a & 0 \\
		0 & a
	\end{pmatrix}
\end{math}
mátrixszal reprezentálunk (írunk le).

A $2\times 2$ méretű valós mátrixok még sok-sok más testet is tartalmaznak.%
\footnote{Karakterizációjukat lásd: \parencite{MR1415833}-ben.}
Ezek közül számunkra a legfontosabb a következő részhalmaz.
\begin{defprop}[komplex számtest]
	Jelölje két tetszőleges $a,b\in\mathbb{R}$ valós szám mellett $M_{a,b}$ az
	$\left( a,b \right)$ valós számpárhoz tartozó
	\(
	M_{a,b}
	=
	\begin{pmatrix}
		a & -b \\
		b & a
	\end{pmatrix}
	\)
	mátrixot.
	Tekintsük az ilyen típusú mátrixok $\mathcal{C}$-vel jelölt halmazát:
	\[
		\mathcal{C}
		=
		\left\{
		M_{a,b}
		:a,b\in\mathbb{R}
		\right\}
	\]
	E részhalmaz
	\begin{enumerate}
		\item
		      zárt a mátrix összeadásra és a mátrix szorzásra,
		      így a $2\times 2$-es valós mátrixok egy speciális egységelemes részgyűrűje.
		\item
		      E részgyűrűben a mátrix szorzás kommutatív művelet,
		      és
		\item
		      és e részgyűrűben minden nem zérus mátrixnak van inverze is a mátrixszorzás műveletre nézve.
	\end{enumerate}
	Eszerint a $\left( \mathcal{C},+,\cdot \right)$ algebrai struktúra egy test.
	Ezt a testet nevezzük a \emph{komplex számtestnek}\index{komplex számok},
	vagy a \emph{komplex számtest mátrix reprezentációjának}.\index{komplex számok mátrix reprezentációja}
\end{defprop}
\begin{proof}
	Az $a,b,c,d\in\mathbb{R}$ valós számok mellett
	\[
		M_{a,b}+M_{c,d}=
		\begin{pmatrix}
			a & -b \\
			b & a
		\end{pmatrix}
		+
		\begin{pmatrix}
			c & -d \\
			d & c
		\end{pmatrix}
		=
		\begin{pmatrix}
			a+c & -b-d \\
			b+d & a+c
		\end{pmatrix}
		=
		M_{a+c,b+d}
	\]
	és hasonlóan a mátrix szorzás definíciója szerint
	\[
		M_{a,b}\cdot M_{c,d}=
		\begin{pmatrix}
			a & -b \\
			b & a
		\end{pmatrix}
		\cdot
		\begin{pmatrix}
			c & -d \\
			d & c
		\end{pmatrix}
		=
		\begin{pmatrix}
			ac-bd & -ad-bc \\
			bc+ad & -bd+ac
		\end{pmatrix}
		=
		M_{ac-bd,ad+bc}.
	\]
	Mivel $M_{1,0}$ a $2\times 2$-es identitás mátrix, ezért $\mathcal{C}$ a mátrix összeadásra és a mátrixszorzásra nézve
	egységelemes gyűrűt alkot.

	A szorzás kommutativitása is látszik a fenti számolásból, hiszen
	\[
		M_{c,d}\cdot M_{a,b}=M_{ca-db,cb+da}=M_{ac-bd,ad+bc}=M_{a,b}\cdot M_{c,d}
	\]
	a valós számok összeadásának és szorzásának
	kommutativitása miatt.

	Legyen most $M_{a,b}$ egy nem zérus mátrix, így $a^2+b^2\neq 0$.
	Világos, hogy
	\[
		M_{a,b}\cdot M_{a,-b}=M_{a^2+b^2,0}=\left( a^2+b^2 \right)M_{1,0}=\left( a^2+b^2 \right)I.
	\]
	Ebből már látszik is, hogy $M_{a,b}\cdot M_{\frac{a}{a^2+b^2},\frac{-b}{a^2+b^2}}=I$.
	Ez a már igazolt kommutativitással éppen azt jelenti, hogy minden nem zérus elemnek van multiplikatív inverze,
	ergo $\mathcal{C}$ valóban test.
\end{proof}
Ebben a $\mathcal{C}$ testben az
\(
M_{0,1}=
\begin{pmatrix}
	0 & -1 \\
	1 & 0
\end{pmatrix}
\)
elem olyan, hogy a négyzete a szorzásra nézve reprodukáló elemnek az összeadásra nézve képzett inverze:
\[
	M_{0,1}\cdot M_{0,1}
	=
	\begin{pmatrix}
		0 & -1 \\
		1 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		0 & -1 \\
		1 & 0
	\end{pmatrix}
	=
	\begin{pmatrix}
		-1 & 0  \\
		0  & -1
	\end{pmatrix}
	=
	M_{-1,0}
	=
	-I.
\]
Ez a tulajdonság azért figyelemre méltó,
mert ha a szokásoknak megfelelően a test multiplikatív neutrális elemét az $1$ szimbólummal jelöljük,
akkor olyan elemet találtunk a komplex számtestben,
amelynek négyzete éppen $-1$.
Tudjuk, hogy a valós számtest esetében ez nem lenne lehetséges.

Tekintsük most valamely $a,b\in\mathbb{R}$ mellett az
\[
	M_{a,b}=
	\begin{pmatrix}
		a & -b \\
		b & a
	\end{pmatrix}
	=
	\begin{pmatrix}
		a & 0 \\
		0 & a
	\end{pmatrix}
	+
	\begin{pmatrix}
		0 & -1 \\
		1 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		b & 0 \\
		0 & b
	\end{pmatrix}
\]
felbontást.
Ha bevezetjük az
\(
i=M_{0,1}=
\begin{pmatrix}
	0 & -1 \\
	1 & 0
\end{pmatrix}
\)
jelölést\index{i komplex szám@$i$ komplex szám}, akkor minden komplex szám
\[
	M_{a,b}=
	\begin{pmatrix}
		a & 0 \\
		0 & a
	\end{pmatrix}
	+
	i
	\cdot
	\begin{pmatrix}
		b & 0 \\
		0 & b
	\end{pmatrix}
\]
alakban írható.
Emlékezzünk arra,
hogy a valós számtest is részhalmaza a komplex számtestnek abban az értelemben,
ha minden $a$ valós számot az
\begin{math}
	\begin{pmatrix}
		a & 0 \\
		0 & a
	\end{pmatrix}
\end{math}
mátrixszal reprezentálunk. ($\mathcal{R}\subseteq\mathcal{C}$).
Ha tehát megegyezünk abban, hogy az $a$ valós számra nézve mi az
\(
\begin{pmatrix}
	a & 0 \\
	0 & a
\end{pmatrix}
\)
mátrixra gondolunk,%
\footnote{Kicsit pontosabban: a valós számok $2\times 2$-es mátrix reprezentációját használjuk.}
akkor azt kapjuk, hogy minden komplex szám
\[
	M_{a,b}=a+ib
\]
alakú, ahol $i$ egy olyan komplex szám, amelyre $i^2=-1$, $a,b\in\mathbb{R}$.
Ezt nevezzük a komplex szám \emph{normálalakjának}.\index{komplex szám normálalakja}

Ne felejtsük a műveleteket:
Láttuk, hogy $M_{a,b}+M_{ c,d}=M_{a+c,b+d}$.
Ez a normálalak reprezentáció mellett azt jelenti, hogy az összeadás definíciója csak
\[
	(a+ib)+(c+id)=(a+c)+i(b+d)\tag{\dag}
\]
lehet.
Hasonlóan emlékszünk, hogy
\begin{math}
	M_{a,b}\cdot M_{c,d}=M_{ac-bd,ad+bc}
\end{math},
ami normálalak reprezentáció mellett az
\[
	(a+ib)\cdot (c+id)=(ac-bd)+i(ad+bc)\tag{\ddag}
\]
definíciót eredményezi.

A következőket gondoltuk meg:
\begin{defprop}[komplex számtest a normálakkal]
	Definiálja
	\[
		\mathbb{C}
		=
		\left\{
		a+ib:a,b\in\mathbb{R}
		\right\}
	\]
	a \emph{komplex számok normálalakját}.
	Az összeadás műveletet definiálja (\dag), és a szorzás műveletet definiálja (\ddag).
	Az így kapott algebrai struktúra test, amely izomorf a komplex számtest mátrix reprezentációjával.
	Az izomorfizmust a
	\[
		\varphi:\mathcal{C}\to\mathbb{C},\quad
		\varphi
		\left(
		\begin{matrix}
				a & -b \\
				b & a
			\end{matrix}
		\right)
		=
		a+ib
	\]
	művelettartó bijekció hozza létre.
\end{defprop}

Ha már megértettük, hogy a komplex számok normálalak reprezentációja testet alkot,
akkor a (\dag) és (\ddag) definíciók megjegyzése nagyon könnyű.
Más nem is lehet:
Az összeadáshoz (\dag) csak el kell végezni a műveletet majd kiemelni $i$-t,
a szorzás definíciójához (\ddag)
az $i$ kiemelése után jutunk.

\section{A komplex számok abszolútértéke}
A valós számokra jól ismert abszolútérték függvényt terjesztjük ki komplex számtest elemeire.
\begin{definition}[valós rész, képzetes rész]\index{komplex szám valós része}\index{komplex szám képzetes része}
	Legyen $z\in\mathbb{C}$, $z=a+ib$.
	Ekkor $a\in\mathbb{R}$ a $z$ komplex szám \emph{valós része},
	és $b\in\mathbb{R}$ a $z$ komplex szám \emph{képzetes része}.
	$\Re z$ jelöli a valós részt, és $\Im z$ a képzetes részt.
\end{definition}
\begin{definition}[konjugált]\index{komplex szám konjugáltja}
	Legyen $z\in\mathbb{C}$, $z=a+ib$.
	Ekkor $z$ \emph{konjugáltja} $\bar{z}=a-ib$.
\end{definition}
A konjugált definíciója alapján, egyszerű számolgatással kapjuk az alábbi azonosságokat.
\begin{proposition}
	Minden $z\in\mathbb{C}$ komplex szám mellett fennállnak a konjugálás következő szabályai:
	\[
		\overline{\left( \bar{z} \right)}=z,\quad
		z+\bar{z}=2\Re z,\quad
		z-\bar{z}=2i\Im z,\quad
		z\in\mathbb{R}\text{ akkor és csak akkor, ha } z=\bar{z},\quad
		z\bar{z}=(\Re z)^2+(\Im z)^2\geq 0.
	\]
	Az összeadás és szorzás művelet is felcserélhető a konjugálással,
	azaz bármely két $z,w\in\mathbb{C}$ komplex szám esetén igaz, hogy
	\[
		\overline{z+w}=\bar{z}+\bar{w},\quad
		\overline{z\cdot w}=\bar{z}\cdot\bar{w},\quad
		\overline{z-w}=\bar{z}-\bar{w},\quad
		\overline{\left( \frac{z}{w} \right)}=\frac{\bar{z}}{\bar{w}}\quad\text{ feltéve, hogy }w\neq 0.\qedhere
	\]
\end{proposition}
Most használjuk először a valós számok rendezését.
Az $\mathbb{R}$ testen a $\geq $ reflexív, antiszimmetrikus, tranzitív relációt az algebrai műveletekkel a következő két axióma kapcsolja össze:
Minden $a,b,c\in\mathbb{R}$ mellett
\[
	a\geq b\text{ esetén }a+c\geq b+c,\qquad a,b\geq 0\text{ esetén }ab\geq 0.
\]
Az $a^2-b^2=\left( a+b \right)\left( a-b \right)$ azonosság szerint,
az $a,b$ nem negatív valós számok mellett $a\geq b$ akkor és csak akkor, ha $a^2\geq b^2$.
Speciálisan $a^2=b^2$ akkor és csak akkor, ha $a=b$.

\begin{definition}[komplex szám abszolútértéke]\index{abszolútérték}
	Legyen $z\in\mathbb{C}$ egy komplex szám.
	Láttuk, hogy
	$z\bar{z}\in\mathbb{R}$ és
	$z\bar{z}\geq 0$.
	E szám négyzetgyökét nevezzük a $z$ komplex szám \emph{abszolútértékének}.
	Jelölés: $|z|=\sqrt{z\bar{z}}$.
\end{definition}
Ha speciálisan $\Im z=0$,
tehát ha $z$ egy valós szám,
akkor e definíció szerint
\[
	|z|=
	\sqrt{z^2}=
	\begin{cases}
		z  & \text{, ha }z\geq 0     \\
		-z & \text{, ha }z<0\text{,}
	\end{cases}
\]
ami egybeesik a valós számok abszolútértékének definíciójával.
A fentiből azonnal látszik, hogy minden $a\in\mathbb{R}$ valós szám mellett $a\leq|a|$.

Mivel $z\bar{z}=\bar{z}z$, ezért $|z|=|\bar{z}|$, azaz komplex számnak és konjugáltjának azonos az abszolútértéke.
Szokásos technika, hogy a komplex szám abszolútértékére vonatkozó állításokat, az abszolútértékek négyzetére fogalmazzuk át.
Ez megtehető, hiszen nem negatív valós számok egyenlősége és azok négyzetének egyenlősége azonos fogalmak.
Például $|z|=0$, akkor és csak akkor, ha $z\bar{z}=0$, ami akkor és csak akkor teljesül ha $z=0$.
Hasonlóan, ha $z,w$ komplex számok, akkor $|zw|^2=zw(\overline{zw})=zw\bar{z}\bar{w}=z\bar{z}w\bar{w}=|z|^2|w|^2=\left( |z||w| \right)^2$,
ergo $|zw|=|z||w|$.

Világos, hogy $|\Re z|^2\leq(\Re z)^2+(\Im z)^2=z\bar{z}=|z|^2$, emiatt
\(
|\Re z|\leq |z|.
\)
Ezt alkalmazva tetszőleges $z,w$ komplex számok mellett
\[
	|
	\Re (z\bar{w})
	|
	\leq
	|z\bar{w}|
	=
	|z||w|,
\]
amit \emph{Cauchy\,--\,Schwartz-egyenlőtlenségnek}\index{Schwartz-egyenlőtlenség} mondunk.
Az abszolútérték legfontosabb tulajdonságai:
\begin{proposition}
	Legyen $z,w\in\mathbb{C}$ komplex szám.
	Ekkor
	\begin{enumerate}
		\item $|z|=0$ akkor és csak akkor, ha $z=0$,
		\item $|zw|=|z||w|$,
		\item $|z+w|\leq |z|+|w|$.\qedhere
	\end{enumerate}
\end{proposition}
Az utolsó egyenlőtlenséget \emph{háromszög-egyenlőtlenségnek}\index{háromszög-egyenlőtlenség} nevezik.
Egy kevésbé népszerű, de ekvivalens alakja
\[
	\left|\left( |z|-|w| \right)\right|\leq|z-w|.
\]
\begin{proof}
	Csak a háromszög-egyenlőtlenséget nem gondoltuk meg eddig:
	\begin{multline*}
		|z+w|^2=
		\left( z+w \right)\left( \overline{z+w} \right)
		=
		\left( z+w \right)\left( \bar{z}+\bar{w} \right)
		=
		z\bar{z}+w\bar{w}+z\bar{w}+\overline{z\bar{w}}
		=
		\\
		|z|^2+|w|^2+2\Re (z\bar{w})
		\leq
		|z|^2+|w|^2+2|\Re (z\bar{w})|
		\leq
		|z|^2+|w|^2+2|z||w|
		=
		\left( |z|+|w| \right)^2.
	\end{multline*}
	Az első egyenlőtlenség, hogy egy valós szám nem nagyobb az abszolútértékénél,
	a második egyenlőtlenség pedig a Cauchy\,--\,Schwartz-egyenlőtlenség.

	A kevésbé populáris alakhoz használjuk a már igazolt egyenlőtlenséget:
	\begin{math}
		|z|
		=
		|(z-w)+w|
		\leq
		|z-w|+|w|,
	\end{math}
	majd ugyanezt a $z$ és a $w$ számok felcserélése után mégegyszer felírva
	\[
		|z|-|w|\leq |z-w|
		\quad\text{ és }\quad
		|w|-|z|\leq |w-z|.
	\]
	Na most, azt ugyan nem tudjuk, hogy a $||z|-|w||$ szám a baloldali számok közül melyikkel azonos,
	de az egyikkel biztosan egybeesik.
	No de, mindegy is melyikkel, hiszen a jobboldal mindkét esetben a kívánt $|z-w|$.
\end{proof}
A definícó szerint $|z|^2=\bar{z}z$.
Véve mindkét oldal abszolútértékét,
azt kapjuk, hogy%
\footnote{Ezt az azonosságot a komplex számok
	\emph{\CStar-azonosságának}\index{Ccsillag@\CStar-azonosság}
	mondjuk},
\[
	|\bar{z}z|=|z|^2.
\]%

Persze valós számok esetén az iménti állítás 1., 2., 3. pontját korábban is sokszor használtuk már.
Vegyük észre, hogy fent a valós esetet is újra igazoltuk.

\section{A komplex számok trigonometrikus alakja}
Láttuk, hogy $z,w\in\mathbb{C}$ komplex szám mellett
\[
	\Re\left( z+w \right)=\Re z+\Re w
	\qquad
	\text{ és }
	\qquad
	\Im\left( z+w \right)=\Im z+\Im w.
\]
Ez azt jelenti, hogy ha az $a+ib$ komplex számot azonosítjuk az $\mathbb{R}^2$ sík
$\left( a,b \right)$ pontjával,
akkor egyszerűen koordinátánként kell összeadni a komplex számokat, mintha a $z$ komplex szám
az origóból az $\left( a,b \right)$ pontra mutató vektor lenne.

A kérdés, hogy ha így képzeljük a komplex számokat,
akkor a komplex számok szorzása mit jelent a komplex számoknak megfeleltetett vektorok körében?

\begin{defprop}[komplex szám trigonometrikus alakja]\index{komplex szám trigonometrikus alakja}
	Legyen $z\in \mathbb{C}$ egy nem zérus komplex szám.
	Ekkor létezik $\varphi\in\mathbb{R}$ valós szám, hogy
	\[
		z=
		|z|\left( \cos\varphi+i\sin\varphi \right)\tag{\dag}
	\]
	Egy ilyen $\varphi$ számot nevezzük a $z$ komplex szám egy \emph{argumentumának}, és néha $\arg z$ módon jelöljük.
	\index{komplex szám argumentuma}
	A (\dag) alak a komplex szám \emph{trigonometrikus alakja}.

	Két nem zérus komplex szám egyenlősége azt jelenti, hogy az abszolútértékük azonos,
	és az argumentumaik különbsége $2\pi$ többszöröse.
\end{defprop}
\begin{proof}
	Világos, hogy ha $z=a+ib\neq 0$, akkor $a^2+b^2>0$, így
	\[
		z=a+ib
		=
		\sqrt{a^2+b^2}\left( \frac{a}{\sqrt{a^2+b^2}}+i\frac{b}{\sqrt{a^2+b^2}} \right).
	\]
	Ha $x$ jelöli a fenti zárójelben a valós részt és $y$ a képzetes részt,
	akkor $x^2+y^2=1$.
	Így az $\left( x,y \right)$ pár a sík egységkörének egy pontja.
	A trigonometrikus függvények középiskolai definíciója szerint,
	ha $\varphi$ jelöli az $\left( 1,0 \right)$ pontot az $\left( x,y \right)$
	ponttal összekötő ív hosszát, -- az óramutató járásával ellentétes irányban mérve a körcikk peremén --
	akkor $x=\cos\varphi$ és $y=\sin\varphi$.
\end{proof}
\begin{proposition}
	Legyenek a $z,w\in \mathbb{C}$ nem nulla komplex számok a trigonometrikus alakjukban felírva,
	azaz
	\[
		z=|z|\left( \cos\varphi+i\sin\varphi \right),\qquad
		w=|w|\left( \cos\psi+i\sin\psi \right).
	\]
	Ekkor a két szám szorzatának abszolútértéke az abszolútértékek szorzata
	és a szorzat egy argumentumát is megkapjuk mint az argumentumok összege.
	Magyarul:
	\[
		zw=
		|z||w|\left( \cos\left( \varphi+\psi \right)+i\sin\left( \varphi+\psi \right) \right).
	\]
	Speciálisan minden $n\in\mathbb{Z}$ egész számra\index{Moivre-formula}
	\[
		z^n=|z|^n\left( \cos n\varphi+i\sin n\varphi \right).\qedhere
	\]
\end{proposition}
A szakasz bevetőjében feltett kérdésre tehát a válasz,
hogy a $z$ komplex számmal való szorzást a sík olyan geometriai transzformációjának képzelhetjük,
amely egy $\arg z$ szöggel való forgatásból és egy $|z|$-szeres origó középpontú nyújtásból áll.
\begin{definition}[egységgyök]\index{komplex $n$-edik egységgyökök}
	Legyen az $n\in\mathbb{N}$ természetes szám rögzítve.
	Tetszőleges $k=0,1,\ldots,n-1$ mellett jelölje
	\[
		\epsilon_k^{(n)}=\cos k\frac{2\pi}{n}+i\sin k\frac{2\pi}{n}.
	\]
	az úgynevezett \emph{$n$-edik komplex egységgyököket}.
\end{definition}
Az $n$-edik komplex egységgyökök a komplex számsík pontosan $n$ különböző pontját alkotják,
és az $n$-edik hatványuk $1$, azaz
$(\epsilon_k^{(n)})^n=1$.
Emiatt minden $w\neq 0$ komplex szám mellett pontosan $n$ komplex gyöke van a $t^n-w$ polinomnak.
Ha ugyanis $w$ trigonometrikus alakja $w=|w|\left( \cos\psi+i\sin\psi \right)$,
akkor legyen például $z_0=\sqrt[n]{|w|}\left( \cos(\psi/n)+i\sin(\psi/n) \right)$,
és így $(z_0\epsilon_k^{(n)})^n=w$ minden $k=0,\ldots,n-1$ szám mellett.
\section{Polinom faktorizáció a komplex- és a valós számtest felett}
A komplex számtest felett minden nem konstans polinomnak van gyöke.
Formálisabban:
\begin{FA}
	Minden legalább elsőfokú komplex együtthatós polinomnak van komplex gyöke.
	Azaz
	ha $p\left( t \right)\in\mathbb{C}[t]$ egy nem konstans polinom,
	akkor létezik $z\in\mathbb{C}$ komplex szám,
	amelyre $p\left( z \right)=0$.
\end{FA}
Az állítást itt nem tudjuk igazolni és egyelőre érdemes bizonyítás nélkül elfogadni.
Illusztrációként megértettük, hogy miért igaz $p\left( t \right)=\alpha_0+t^n$ alakú polinomra.
A felépítés jelen pontján tekintsük a komplex számtest egy még nem igazolt tulajdonságának.

Most összefoglaljuk, hogy az algebra alaptétele mit jelent a komplex test feletti, majd a valós test feletti polinomok
faktorizációjára nézve.
Láttuk, hogy minden normált polinom előáll mint normált irreducibilis polinomok szorzata,
emiatt azt kell meggondolnunk, hogy mik az irreducibilis polinomok.
\begin{proposition}
	A $\mathbb{C}\left[ t \right]$ polinomgyűrűben minden legalább másodfokú polinom reducibilis.
\end{proposition}
\begin{proof}
	Legyen $p$ egy legalább másodfokú komplex polinom.
	Az algebra alaptétele miatt van gyöke, és
	tudjuk hogy a gyöktényező mindig kiemelhető.
	Így azt kapjuk, hogy $p(t)=\left( t-z \right)h\left( t \right)$ alakú,
	emiatt $2\leq\deg p=1+\deg h$, ergo
	$p$ valóban előállt mint két legalább elsőfokú polinom szorzata.
\end{proof}
A polinomok szorzattá bontásáról szóló tételnek a komplex számtest feletti speciális esete tehát:
\begin{proposition}\label{pr:PolFact}
	A $\mathbb{C}[t]$ polinomgyűrű minden legalább elsőfokú normált polinomjához léteznek
	a sorrendjüktől eltekintve egyetlen $z_1,\ldots,z_s\in\mathbb{C}$ egymástól különböző komplex számok,
	és léteznek $n_1,\ldots,n_s$ pozitív egészek,
	amelyekre
	\[
		p\left( t \right)=
		\left( t-z_1 \right)^{n_1}
		\cdots
		\left( t-z_s\right)^{n_s}.\qedhere
	\]
\end{proposition}
Persze ez azt jelenti, hogy egy $n$-ed fokú komplex polinomnak mindig pontosan $n$ komplex gyöke van,
ha a gyökök számát multiplicitással számoljuk.
Az állítást egy kicsit egyszerűbben úgy is fogalmazhatjuk,
hogy
\emph{
	minden nem konstans komplex polinom előáll mint első fokú komplex polinomok szorzata.
}
Az iménti mondat nyilván nem igaz a ,,komplex'' szót a ,,valós'' szóra cserélve,
ugyanis minden negatív diszkriminánsú másodfokú valós polinom jó is ellenpéldának.
Ez a jelenség az oka annak,
hogy a komplex számtest felett sokkal kényelmesebb dolgoznunk mint a valós számok felett.

Most nézzük, hogy mit jelent az algebra alaptétele a valós test feletti polinomgyűrűre nézve.
\begin{proposition}
	Legyen $p\left( t \right)\in\mathbb{C}[t]$ a komplex polinomgyűrű egy olyan eleme,
	amelynek minden együtthatója valós.
	Ekkor a $z\in \mathbb{C}$ komplex szám pontosan akkor gyöke $p$-nek,
	ha $\bar{z}$ is gyöke $p$-nek.
	Sőt, ha $z$ egy $k$-szoros multiplicitású gyök,
	akkor $\bar{z}$ is pontosan $k$-szoros multiplicitású gyök.
\end{proposition}
\begin{proof}
	A $p$ polinomra tehát
	\begin{math}
		p\left( t \right)
		=
		\sum_{j=0}^n\alpha_jt^j,
	\end{math}
	ahol $\alpha_0,\alpha_1,\ldots,\alpha_n\in\mathbb{R}$.
	A konjugálás tulajdonságai szerint egy $z$ komplex számra
	\[
		p\left( z \right)=
		\overline{
			\sum_{j=0}^n\alpha_jz^j
		}
		=
		\sum_{j=0}^n\overline{\alpha_jz^j}
		=
		\sum_{j=0}^n\bar{\alpha_j}\bar{z}^j
		=
		\sum_{j=0}^n\alpha_j\bar{z}^j
		=
		p\left(\bar{z} \right).
	\]
	Emiatt valós együtthatós $p$ polinomra és $z$ komplex számra
	$p\left( z \right)=0$ akkor és csak akkor, ha $p\left( \bar{z} \right)=0.$

	Ha a gyök speciálisan valós szám, akkor a multiplicitásra vonatkozó állítás semmit mondó.
	Ha most $z$ egy nem valós komplex gyök,
	akkor $p(t)=\left( t-z \right)\left( t-\bar{z} \right)h\left( t \right)$, ahol
	$h\left( z \right)=0$ pontosan akkor, ha $h\left( \bar{z} \right)=0$.
	Így, ha $z$ egy $k$ szoros gyök,
	akkor
	\[
		p\left( t \right)=\left( t-z \right)^{k}\left( t-\bar{z} \right)^{k}\cdot h\left( t \right)
	\]
	alakú, ahol $h$-nak már sem $z$ sem $\bar{z}$ nem gyöke.
\end{proof}
Minden valós együtthatós polinom előáll mint első vagy másodfokú polinomok szorzata:
\begin{proposition}\label{pr:RealPolFact}
	Az $\mathbb{R}[t]$ polinomgyűrű minden legalább elsőfokú normált polinomjához
	léteznek $x_1,\ldots,x_r\in\mathbb{R}$ valós számok,
	léteznek $\alpha_1,\beta_1,\ldots,\alpha_s,\beta_s$ valós együttható párok,
	és léteznek
	$n_1,\ldots,n_r, m_1,\ldots,m_s$ pozitív egészek úgy, hogy
	\[
		p\left( t \right)
		=
		\left( t-x_1 \right)^{n_1}
		\cdots
		\left( t-x_r \right)^{n_r}
		\cdot
		\left(\alpha_1 + \beta_1t +t^2\right)^{m_1}
		\cdots
		\left(\alpha_s + \beta_st +t^2\right)^{m_s}.
	\]
	Itt $r,s\geq 0$, de $r+s>0$, és $n_1+\dots+n_r+2\left( m_1+\dots+m_s \right)=\deg p$,
	továbbá a jobboldalon szereplő másodfokú polinomok irreducibilisek.
\end{proposition}
\begin{proof}
	Tekintsük a $p$ polinomot mint a komplex számtest feletti polinomgyűrű egy elemét,
	és alkalmazzuk a komplex polinom faktorizációról szóló tételt.
	A $p$ így előáll mint első fokú, esetleg komplex polinomok szorzata.
	A gyököket osszuk két része.
	Legyenek $x_1,\ldots,x_r$ a különböző valós gyökök, amelyek multiplicitása rendre
	$n_1,\ldots,n_r$.
	Legyenek $z_1,\overline{z_1},\ldots,z_s,\overline{z_s}$ a különböző nem valós de komplex gyökök,
	ahol $m_1,\ldots,m_s$ rendre a konjugált gyökpárok multiplicitása.
	Így
	\[
		p\left( t \right)=
		\left( t-x_1 \right)^{n_1}
		\cdot\ldots\cdot
		\left( t-x_r \right)^{n_r}
		\cdot
		\left( \left( t-z_1 \right)\cdot\left( t-\bar{z_1} \right) \right)^{m_1}
		\cdot\ldots\cdot
		\left( \left( t-z_s \right)\cdot\left( t-\bar{z_s} \right) \right)^{m_s}.
	\]
	Világos, hogy $r,s\geq 0$, de $r+s>0$, és $n_1+\dots+n_r+2\left( m_1+\dots+m_s \right)=\deg p$.
	A komplex faktorokra végezzük el a szorzást,
	így
	$
		\left( t-z_k \right)\left( t-\bar{z_k} \right)
		=
		t^2-2\Re z_kt+|z_k|^2
	$.
	Így $\alpha_k=|z_k|^2$ és $\beta_k=-2\Re z_k$, választással készen is vagyunk.
\end{proof}
A fenti állításból két dolog azonnal látszik.
Az első,
hogy \emph{valós számtest felett minden legalább harmadfokú polinom reducibilis},
a második pedig,
hogy ha egy valós együtthatós polinomnak nincs valós gyöke,
akkor csak páros fokú lehet, vagy ami ugyanaz:
\emph{minden páratlan fokú valós együtthatós polinomnak van valós gyöke.}

%\chapter{Gauss\,--\,Jordan-elimináció}
%\scwords Lineáris egyenletrendszerek megoldását automatizáljuk.\index{lineáris egyenletrendszer}


\chapter{A vektortér fogalma}
\scwords A lineáris algebra kezdő fejezetéhez érkeztünk,
miután áttekintettük azokat az általános algebrai ismereteket,
amelyek nélkül nem tárgyalhatók a lineáris algebrához szükséges gondolatok.
\begin{definition}[Vektortér]\index{vektortér}
	Legyen adva egy $\mathbb{F}$ test, és egy $V$ halmaz.
	Tegyük fel, hogy adott egy $+:V\times V\to V$ két változós művelet (ezt összeadásnak nevezzük)
	és adott egy $\cdot:\mathbb{F}\times V\to V$ szintén kétváltozós művelet (ezt skalárral való szorzásnak, vagy számmal való szorzásnak mondjuk).
	A $V$-t az $\mathbb{F}$ test feletti \emph{vektortérnek} nevezzük,
	ha $\left( V,+ \right)$ egy Abel-csoport,
	és a számmal valós szorzás műveletre teljesülnek az alábbi axiómák:
	\begin{enumerate}
		\item Minden $\alpha\in\mathbb{F}$ és minden $u,v\in V$ mellett
		      \begin{math}
			      \alpha\cdot\left( u+v \right)=\alpha\cdot u+\alpha\cdot v,
		      \end{math}
		\item Minden $\alpha,\beta\in\mathbb{F}$ és minden $u\in V$ mellett
		      \begin{math}
			      \left( \alpha+\beta \right)\cdot u=\alpha\cdot u+\beta\cdot u,
		      \end{math}
		\item Minden $\alpha,\beta\in\mathbb{F}$ és minden $u\in V$ mellett
		      \begin{math}
			      \alpha\cdot\left( \beta\cdot u \right)=\left( \alpha\beta \right)\cdot u,
		      \end{math}
		\item Minden $u\in V$ esetén $1\cdot u=u$.\qedhere
	\end{enumerate}
\end{definition}
Nagyon hasonlóan ahhoz, ahogyan testben is meggondoltuk igazak a következő számolási szabályok.
Minden $\alpha\in\mathbb{F}$ mellett
\begin{enumerate}[label=\roman*.)]
	\item $\alpha\cdot 0=0$,
	\item $0\cdot v=0$,
	\item $\left( -1 \right)\cdot v=-v$,
	\item $\alpha\cdot v=0$ esetén $\alpha=0$ vagy $v=0$.
\end{enumerate}
A számmal való szorzás $\cdot:\mathbb{F}\times V\to V$ művelet eredményének szokásos rövidítése,
hogy a kissé körülményes $\alpha\cdot v$ helyett csak $\alpha v$-t írunk.
Az is előfordul,
különösen amikor egy konkrét vektortér konkrét műveletéről van szó,
hogy az $\alpha v$ és a $v\alpha$ jelölést is
ugyanarra az $\alpha\cdot v\in \mathbb{F}$ elemre használjuk.

Alapvető példa vektortérre a mátrixok tere.
Az $m\times n$ méretű mátrixok $\mathbb{F}^{m\times n}$ halmaza a mátrix összeadással és a számmal való szorzással
vektorteret alkot az $\mathbb{F}$ test felett.
Ha $n=1$, akkor kapjuk az oszlopvektorok $\mathbb{F}^m$ terét, ami így szintén egy $\mathbb{F}$ feletti vektortér.
Speciális esetként $\mathbb{R}^m$ egy $\mathbb{R}$ feletti vektortér,
$\mathbb{C}^m$ egy $\mathbb{C}$ feletti vektortér.

Fontos példa még, egy adott $X$ halmazból az $\mathbb{F}$ testbe képező összes függvények halmaza
a függvények közt szokásos összeadás művelettel, és számmal való szorzással.
Ezt a teret $\mathbb{F}^X$ módon szokás jelölni, és minden nehézség nélkül ellenőrizhető,
hogy $\mathbb{F}^X$ egy $\mathbb{F}$ feletti vektortér.
Hasonlóan látszik, hogy például az összes valós--valós folytonos függvények is egy $\mathbb{R}$
feletti vektorteret alkotnak, vagy ha ezek közül csak a differenciálható függvényekre szorítkozunk,
akkor ezen függvénytér is vektortér az $\mathbb{R}$ test felett.

Rögzítenünk kell magunkban, hogy a vektortér definíciójában a test is fontos szerepet játszik.
Más test felett ugyanaz az Abel-csoport már egy másik vektortérhez tartozik.
Világos például, hogy $\mathbb{R}$ az $\mathbb{R}$ test felett vektortér a szokásos műveletekkel,
de látjuk majd, hogy egészen más tulajdonságai vannak annak a vektortérnek,
amit akkor kapunk,
ha az $\mathbb{R}$ valós számok Abel-csoportját,
mint a $\mathbb{Q}$ racionális test feletti vektortérnek tekintjük.

Játékos példaként gondoljuk meg,
hogy a pozitív valós számok egy az $\mathbb{R}$ feletti vektorteret alkot a következő fura műveletekkel:
Tetszőleges $a,b$ pozitív valós szám mellett
\(
a\#b=a\cdot b,
\)
majd tetszőleges $\alpha$ valós szám és $a$ pozitív valós számra legyen
\(
\alpha\ast a=a^\alpha.
\)
\footnote{Itt $a\ast\alpha$ mit jelent?}

A vektorteret sokszor csak az additív művelet alaphalmazával jelöljük.
Kicsit pontosabb ha az alaphalmaz mellett a testet is konkrétan specifikáljuk,
de sokszor feltesszük, hogy a szövegkörnyezetből nyilvánvaló, hogy mely testre gondolunk.
Hasonlóan pontosabb lenne a két műveletet is mindig kijelölni, mikor egy vektortérre hivatkozunk,
de ha világos, hogy mi a vektortérbeli elemek közt az additív művelet, és hogy mit jelent egy test elemeivel szorozni,
akkor elhagyjuk a műveletek kijelölését.
A legpontosabb, -- de persze a legkörülményesebb -- jelölés az lenne, hogy pl.
,,tekintsünk egy $\left( V,+,\cdot \right)$ vektorteret az $\mathbb{F}$ test fölött.''
Ehelyett sokszor csak azt mondjuk, hogy
,,legyen $V$ egy vektortér''.
Ilyenkor a szövegkörnyezetből világosnak kell lennie, hogy mi a test, mit jelent a számmal való szorzás, és mi az összeadás a $V$ halmazon.

\section{Vektortér alterei}
\begin{definition}[altér]\index{vektortér altere}\index{altér}
	Legyen $\left( V,+,\cdot \right)$ egy vektortér az $\mathbb{F}$ test felett.
	Egy $M\subseteq V$ részhalmaz a $V$ vektortér \emph{altere},
	ha $M$ maga is vektorteret alkot a $V$-ben definiált additív művelettel,
	és a $V$-ben definiált számmal való szorzással.
\end{definition}
\begin{proposition}
	Legyen $V$ egy vektortér, és $M\subseteq V$ egy részhalmaza.
	Az $M$ pontosan akkor altér,
	ha
	\begin{enumerate}
		\item $0\in M$,
		\item $u,v\in M$ esetén $u+v\in M$,
		\item $u\in M$ és $\alpha\in\mathbb{F}$ esetén $\alpha u\in M$.\qedhere
	\end{enumerate}
\end{proposition}
Tetszőleges $V$ vektortérre a $\left\{ 0 \right\}$ és maga $V$ mindig alterek,
ezeket \emph{triviális altereknek} is szokás mondani.\index{triviális altér}

Most két fontos fogalmat vezetünk be.
Egy halmazt tartalmazó legszűkebb altér fogalmát,
és a halmaz lineáris burkának fogalmát.
Ki fog derülni, hogy a lineáris burok mindig egybeesik a legszűkebb altérrel.
\begin{defprop}[generált altér]\index{generált altér}
	Egy vektortérben, akárhány altér közös része altér.
	Emiatt értelmes a következő definíció.
	Ha $H\subseteq V$ egy részhalmaza a $V$ vektortérnek,
	akkor jelölje
	$\gen H$ a $H$ halmazt tartalmazó összes alterek metszetét.
	Ezt az alteret nevezzük a $H$ halmaz által \emph{generált altérnek.}
\end{defprop}
\begin{proposition}
	Legyen $V$ egy vektortér.
	Ekkor
	\begin{enumerate}
		\item Minden $H\subseteq V$ halmazra $H\subseteq \gen H$,
		\item Ha $H\subseteq K\subseteq V$, akkor $\gen H\subseteq \gen K$,
		\item Minden $H\subseteq V$ mellett $\gen\left( \gen H \right)=\gen H$.
	\end{enumerate}
	A $\gen H$ a $H$ halmazt tartalmazó alterek közt a legszűkebb.
	Így $H\subseteq V$ pontosan akkor altér, ha $\gen H=H$.
\end{proposition}
\begin{definition}[lineáris kombináció, lineáris burok]\index{lineáris kombináció}\index{lineáris burok}
	Ha adott a $V$ vektortérben véges sok $v_1,\ldots,v_r$ vektor,
	akkor a vektortér minden
	\[
		\alpha_1v_1+\dots+\alpha_r v_r
	\]
	alakú vektorát a $v_1,\ldots,v_r$ vektorok egy \emph{lineáris kombinációjának} mondjuk.

	Legyen $H\subseteq V$ egy tetszőleges halmaz.
	Ekkor $\lin H$ jelöli $H$ összes véges részhalmazának összes lineáris kombinációinak halmazát,
	azaz
	\[
		\lin H=
		\left\{ \sum_{j=1}^n\alpha_jv_j:n\in\mathbb{N},v_1,\ldots,v_n\in H,\alpha_1,\ldots,\alpha_n\in\mathbb{F} \right\}
	\]
	Definíció szerint nulla darab vektor lineáris kombinációja a vektortér zérus eleme,
	tehát $\lin \emptyset=\left\{ 0 \right\}.$
	A $\lin H$ halmazt nevezzük a $H$ halmaz \emph{lineáris burkának}.
\end{definition}
\begin{proposition}
	Egy $V$ vektortér minden $H\subseteq V$ részhalmazának lineáris burka,
	a $H$ halmazt tartalmazó legszűkebb altér, azaz
	\[
		\lin H=\gen H.\qedhere
	\]
\end{proposition}
\begin{proof}
	Világos, hogy $H\subseteq \lin H$, világos hogy $\lin H$ egy altér,
	és az is nyilvánvaló, hogy ha $H\subseteq M$ egy tetszőleges altér,
	akkor $\lin H\subseteq M$.

	A $\lin H$ egy a $H$-t tartalmazó altér,
	és $\gen H$ az összes ilyen alterek metszete, ezért
	$\gen H\subseteq\lin H$.
	Másoldalról, a $\lin H$ altér részhalmaza minden $H$-t tartalmazó altérnek,
	így azok közös részének is,
	ergo
	$\lin H\subseteq \gen H$.
\end{proof}
Ha $H$ egy véges halmaz, akkor $\lin H$ kicsit egyszerűbben írható.
Mint arról már a definícióban is szó volt $\lin \emptyset=\left\{ 0 \right\}$.
Ha $H$ egy elemű, akkor $\lin (\left\{ v_1 \right\})=\left\{ \alpha v_1:\alpha\in\mathbb{F} \right\}$.
Ha $H=\left\{ v_1,v_2 \right\}$ két elemű, akkor
$\lin({v_1,v_2})=\left\{ \alpha_1v_1+\alpha_2v_2:\alpha_1,\alpha_2\in\mathbb{F} \right\}$.
Hasonlóan, ha $H=\left\{ v_1,\ldots,v_r \right\}$ halmaz $r$ elemből áll akkor elegendő csak az $r$ elemből álló lineáris kombinációkat képezni, azaz
\[
	\lin \left( \left\{ v_1,\ldots,v_r \right\} \right)
	=
	\left\{ \sum_{j=1}^r\alpha_jv_j:\alpha_1,\ldots,\alpha_r\in\mathbb{F} \right\}.
\]
\begin{definition}[generátorrendszer, végesen generált vektortér]\index{generátorrendszer}\index{végesen generált vektortér}
	Egy vektortér egy $H$ részhalmazáról azt mondjuk, hogy \emph{generálja a vektorteret}
	vagy, hogy $H$ egy \emph{generátorrendszere} $V$-nek, ha
	\[
		\lin H=V.
	\]
	A $V$ vektorteret \emph{végesen generáltnak} mondunk, ha létezik véges generátorrendszere.
\end{definition}

A generátorrendszer cseréről szóló \ref{le:gencsere}.~lemmának kiemelten fontos szerepe van felépítésünkben.
Egyrészt használjuk majd a Steinitz-lemma\index{Steinitz-lemma} (\ref{le:Steinitz}) igazolásában,
másrészt ennek segítségével tisztázzuk majd azt a kérdést,
hogy hogyan alakulnak egy vektor ,,koordinátái'', ha a vonatkoztatási rendszert változtatjuk.

\begin{lemma}[generátorrendszer csere]\label{le:gencsere}
	Legyen $\left\{ x_1,\ldots,x_m \right\}$ egy generátorrendszere valamely vektortérnek,
	és tegyük fel, hogy valamely $y$ vektorra
	\[
		y=\sum_{j=1}^m\eta_jx_j,
	\]
	ahol $\eta_k\neq 0$ valamely $1\leq k\leq m$ mellett.
	Ekkor $y$ becserélhető a $k$-adik helyen a generátorrendszerbe,
	úgy hogy az generátorrendszer maradjon, azaz a
	\[
		\left\{ x_1,\ldots,x_{k-1},y,x_{k+1},\ldots,x_m \right\}
	\]
	vektorrendszer is generátorrendszer.
\end{lemma}
\begin{proof}
	Fejezzük ki $x_k$-t az $y$-ra felírt formulából:
	\(\displaystyle
	x_k=\frac{1}{\eta_k}y+\sum_{\substack{j=1\\j\neq k}}^m\frac{-1}{\eta_k}\eta_jx_j.
	\)
	Ha $a$ eredetileg
	\(\displaystyle
	a=\sum_{j=1}^m\alpha_jx_j
	\)
	alakú, akkor $x_k$ helyére betéve, a fent kifejezett formulát és bevezetve a
	$\delta=\frac{\alpha_k}{\eta_k}$ jelölést, azt kapjuk hogy:
	\begin{multline*}
		a=\alpha_kx_k+\sum_{\substack{j=1\\j\neq k}}^m\alpha_jx_j=
		\\
		=
		\alpha_k
		\left(
		\frac{1}{\eta_k}y+\sum_{\substack{j=1\\j\neq k}}^m\frac{-1}{\eta_k}\eta_jx_j
		\right)
		+\sum_{\substack{j=1\\j\neq k}}^m\alpha_jx_j
		=
		\frac{\alpha_k}{\eta_k}y+
		\sum_{\substack{j=1\\j\neq k}}^m\left( \alpha_j-\frac{\alpha_k}{\eta_k}\eta_j \right)x_j=
		\\
		=\delta y+
		\sum_{\substack{j=1\\j\neq k}}^m\left( \alpha_j-\delta\eta_j \right)x_j.
	\end{multline*}
	Azt kaptuk tehát, hogy ha egy vektor kifejezhető az eredeti vektorrendszerből az
	\[
		\left( \alpha_1,\ldots,\alpha_m \right)
	\]
	együtthatókkal, akkor ugyanez a vektor a módosított vektorrendszerből is kifejezhető,
	méghozzá az
	\[
		\left(
		\underbrace{\alpha_1-\delta\eta_1}_{1.},
		\underbrace{\alpha_2-\delta\eta_2}_{2.},
		\ldots,
		\underbrace{\alpha_{k-1}-\delta\eta_{k-1}}_{k-1.},
		\underbrace{\quad\delta\quad}_{k.},
		\underbrace{\alpha_{k+1}-\delta\eta_{k+1}}_{k+1.},
        \ldots,
		\underbrace{\alpha_m-\delta\eta_m}_{m.}
		\right)
	\]
	együtthatókkal.
\end{proof}
\section{Elimináció}
Szokásos jelölés és a kívánatos szemlélet kialakításában is fontos szerepet játszik a következő táblázat.
Ha $\left\{ x_1,\ldots,x_m \right\}$ egy generátorrendszer az azt jelenti, hogy minden $a\in V$ vektor
előáll mint az $x_1,\ldots,x_m$ vektorok valamilyen együtthatókkal vett lineáris kombinációja.
Ha tehát $a=\alpha_1x_1+\dots+\alpha_mx_m$, akkor azt a következőképpen fejezzük ki.
\[
	\begin{array}{c|c}
		       & a        \\
		\hline
		x_1    & \alpha_1 \\
		\vdots & \vdots   \\
		x_k    & \alpha_k \\
		\vdots & \vdots   \\
		x_m    & \alpha_m \\
		\hline
	\end{array}
\]
Tekinthető ez egy $m\times 1$ típusú bekeretezett mátrixnak, a hol a sorok címkéi a generátorrendszer elemei,
az egyetlen oszlop címkéje pedig az a vektor, amelynek az előállításáról van szó.

A generátorrendszer csere lemma arról is szólt, hogy ha $y$-t a $k$-adik helyen cseréljük a generátorrendszerbe,
akkor a fenti táblázat hogyan változik.
Azt kaptuk, hogy a
\begin{eqnarray}\label{alg:G-J}
	\begin{array}{c|cc}
		       & y              & a                       \\
		\hline
		x_1    & \eta_1         & \alpha_1                \\
		\vdots & \vdots         & \vdots                  \\
		x_k    & \boxed{\eta_k} & \alpha_k                \\
		\vdots & \vdots         & \vdots                  \\
		x_m    & \eta_m         & \alpha_m                \\
		\hline
		       & \delta         & \frac{\alpha_k}{\eta_k}
	\end{array}
	&\implies&
	\begin{array}{c|cc}
		       & y      & a                     \\
		\hline
		x_1    & 0      & \alpha_1-\eta_1\delta \\
		\vdots & \vdots & \vdots                \\
		y      & 1      & \delta                \\
		\vdots & \vdots & \vdots                \\
		x_m    & 0      & \alpha_m-\eta_m\delta \\
		\hline
		       &        &
	\end{array}
\end{eqnarray}
transzformációt kell végrehajtani.
Persze ugyanezt azt egyetlen $a$ vektor helyett kiszámolhatjuk például az $a,b,c$ vektorokra is.
Továbbra is az $y$-t cseréljük a generátorrendszerbe, így a
\begin{eqnarray*}
	\begin{array}{c|cccc}
		       & y              & a                       & b                      & c                       \\
		\hline
		x_1    & \eta_1         & \alpha_1                & \beta_1                & \gamma_1                \\
		\vdots & \vdots         & \vdots                  & \vdots                 & \vdots                  \\
		x_k    & \boxed{\eta_k} & \alpha_k                & \beta_k                & \gamma_k                \\
		\vdots & \vdots         & \vdots                  & \vdots                 & \vdots                  \\
		x_m    & \eta_m         & \alpha_m                & \beta_m                & \gamma_m                \\
		\hline
		       & \delta         & \frac{\alpha_k}{\eta_k} & \frac{\beta_k}{\eta_k} & \frac{\gamma_k}{\eta_k} \\
	\end{array}
	&\implies&
	\begin{array}{c|cccc}
		       & y      & a                       & b                      & c                       \\
		\hline
		x_1    & 0      & \alpha_1-\eta_1\delta_a & \beta_1-\eta_1\delta_b & \gamma_1-\eta_1\delta_c \\
		\vdots & \vdots & \vdots                  & \vdots                 & \vdots                  \\
		y      & 1      & \delta_a                & \delta_b               & \delta_c                \\
		\vdots & \vdots & \vdots                  & \vdots                 & \vdots                  \\
		x_m    & 0      & \alpha_m-\eta_m\delta_a & \beta_m-\eta_m\delta_b & \gamma_m-\eta_m\delta_c \\
		\hline
		       &        &
	\end{array}
\end{eqnarray*}
a transzformációt hajtjuk végre.
Most arra figyeljünk,
hogy végül is a keretben lévő $m\times 4$-es mátrixszal sorműveleteket hajtottunk végre:
\begin{itemize}
	\item
	      Eldöntöttük, hogy az $y,a,b,c$ vektorok közül az $y$-t cseréljük be a generátorrendszerbe,
	      mégpedig a $k$-adik helyen.
	      Ezt jeleztük az $y$ oszlopa $k$-adik helyen lévő elemének bekeretezésével.
	      A keretben csak nem zéró szám lehet.
	\item
	      Első lépésként a keretezett számmal osztottuk a $k$-adik sort. Ez az új táblázat $k$-adik sora.
	      Pusztán segítségképpen ugyanezt a sort mint egy számolási segédsort az első táblázat alá másoltuk.
	\item
	      Az új táblázat első sora az eredeti első sornak és a segédsor $\eta_1$-szeresének különbsége.
	      A második sor az eredeti második sornak és a segédsor $\eta_2$-szeresének különbsége.
	      Hasonlóan, $m\neq k$-ra az $m$-edik sor az eredeti $m$-edik sornak és a segédsor $\eta_m$-szeresének különbsége.
\end{itemize}

Látjuk tehát, hogy összesen két fajta sorművelettel alakítottuk a kiindulási mátrixot:
\begin{enumerate}
	\item Egy sort szoroztunk egy nem zérus számmal,
	\item Egy sorhoz hozzáadtuk egy másik sor számszorosát.
\end{enumerate}
Világos, hogy ez ugyanaz, mintha a feladat az lett volna,
hogy a fenti két sorművelet használva az $y$ oszlopában minden számot el kell tüntetni,
a $k$-adikat pedig $1$-re kell beállítani.
Ez a \emph{Gauss\,--\,Jordan-elimináció}.\index{Gauss\,--\,Jordan-elimináció}

\subsection{Homogén lineáris egyenletrendszer}
Az alábbi feladatot lineáris egyenletrendszernek nevezzük.
\[
	\begin{array}{rrlcl}
		a_{1,1}x_1+ & \cdots & +a_{1,n}x_n & =      & b_1 \\
		a_{2,1}x_1+ & \cdots & +a_{2,n}x_n & =      & b_2 \\
		            &        &             & \vdots &     \\
		a_{m,1}x_1+ & \cdots & +a_{m,n}x_n & =      & b_m
	\end{array}
\]
Itt az $n,m\in\mathbb{N}$, az $a_{i,j}\in\mathbb{F}$ testbeli számok előre adottak
minden $i=1,\ldots,m$ és minden $j=1,\ldots,n$ mellett.
Adottak még az egyenletek jobboldalát képező $b_i\in\mathbb{F}$ számok.
A feladat megoldása annyit tesz, hogy keressük az $x_1,\ldots,x_n$ ismeretlenek
összes olyan értékét az $\mathbb{F}$ testből,
amelyre fenti egyenletek mind teljesülnek.
Ha a jobboldali számokra $b_i=0$ minden $i=1,\ldots,n$ mellett,
akkor
\emph{homogén lineáris egyenletrendszerről}%
\index{homogén lineáris egyenletrendszer}
beszélünk,
egyébként a rendszert
\emph{inhomogénnek}%
\index{inhomogén lineáris egyenletrendszer}
mondjuk.
A fenti rendszer együtthatóiból álló
\[
	A=
	\begin{pmatrix}
		a_{1,1} & \cdots & a_{1,n} \\
		a_{2,1} & \cdots & a_{2,n} \\
		\vdots  & \ddots & \vdots  \\
		a_{m,1} & \cdots & a_{m,n}
	\end{pmatrix}
\]
mátrixot
\emph{együttható-mátrixnak}%
\index{együttható-mátrix}
mondjuk.

Ha az egyenletek egyikét egy nemzérus számmal szorozzuk,
és az egyenletek egyikéhez hozzáadjuk egy másik egyenlet számszorosát,
akkor a megoldások nem változnak,
azaz ekvivalens átalakítást hajtunk végre.

Egy Gauss\,--\,Jordan-eliminációs\index{Gauss\,--\,Jordan-elimináció}  lépésre tekinthetünk úgy is,
mint rögzített $i,j$ mellett a $j$-edik változó kiküszöbölésére
-- azaz eliminációjára --
valamennyi nem az $i$-edik sorból,
és az $i$-edik sorban e $j$-edik változó együtthatójának $1$-re állítására.
Valóban,
ha az $i$-edik sort osztjuk az $a_{i,j}\neq 0$ számmal, akkor e sor $j$-edik
eleme $1$-re változik.
Ha a $k$-adik ($k\neq i$) sorból kivonjuk az imént normált sor
$a_{k,j}$-szeresét, akkor az eredmény sor $j$-edik helyén zérust kapunk.
Ha ezt minden $k=1,\ldots,m$ mellet kiszámoljuk, akkor éppen egy Gauss\,--\,Jordan-%
eliminációt hajtunk végre az $a_{i,j}$ pivot elem\footnote{Értsd: sarok elem.}\index{pivot elem} választásával.
Az eliminációs lépés hatására az $j$-edik változó az $i$-edik kivételével
minden más sorból eltűnt, és az $i$-edik sorban pontosan $1$ együtthatóval szerepel.

Ugyan ez egy táblázatban megfogalmazva.
Itt a baloldali mátrix az együttható mátrix, amelynek
a $j$-edik oszlopa van külön kiemelve.
$L_1,\ldots,L_m$ jelöli az együttható-mátrix sorait
A jobboldali mátrix az elimináció eredménye, a megfelelő sorműveletekkel az egyes sorok
címkéiben:
\[
	\begin{array}{ccc|c}
		\cdots & a_{1,j}         & \cdots & L_1    \\
		       & \vdots          &        & \vdots \\
		\cdots & \boxed{a_{i,j}} & \cdots & L_i    \\
		       & \vdots          &        & \vdots \\
		\cdots & a_{m,j}         & \cdots & L_m
	\end{array}
	\implies
	\begin{array}{ccc|c}
		\cdots & 0      & \cdots & L_1-a_{1,j}\frac{1}{a_{i,j}}L_i \\
		       & \vdots &        & \vdots                          \\
		\cdots & 1      & \cdots & \frac{1}{a_{i,j}}L_i            \\
		       & \vdots &        & \vdots                          \\
		\cdots & 0      & \cdots & L_m-a_{m,j}\frac{1}{a_{i,j}}L_i
	\end{array}
\]
Azt kell látnunk, hogy egy eliminációs lépés az $a_{i,j}$ pivot elem választásával pontosan ugyanazt eredményezi,
mint az együttható-mátrix $j$-edik $\left[ A \right]^j$ oszlopának becserélése a generátorrendszer $i$-edik helyére.

Az algoritmus célja, hogy
a generátorrendszerbe annyi oszlopot cseréljünk be amennyit csak tudunk,
de persze egy már korábban becserélt vektort nem cserélünk ki egy újabbra.
Az algoritmus akkor áll meg, ha nem tudunk új oszlopot becserélni.
Ez pontosan akkor fordul elő,
ha a táblázat \emph{minden nem zérus sora egy már becserélt vektorhoz tartozik.}

%Ennek két oka lehet.
%Vagy minden oszlopot becseréltünk, vagy
%valamennyi eddig be nem cserélt oszlop minden tagja zérus az eredeti generátorrendszer nem cserélt helyein.
%Ez utóbbi mondatot egyszerűbben úgy fogalmazhatjuk, 
%hogy ha a táblázat minden nem zérus sora egy becserélt vektorhoz tartozik,
%akkor véget ér az algoritmus.

A lineáris egyenletrendszerek nyelvére átültetve ez azt jelenti,
hogy az algoritmus célja, a változók eliminálása, lehetőség szerint minnél többet.
Egy sorban (és egy oszlopban\footnote{Ez magától teljesül, erre nem kell figyelnünk.}) csak egy eliminált változó szerepelhet.
Az algoritmus akkor áll meg,
ha a táblázatnak minden nem zérus sorában már van eliminált változó.

Az algoritmus utolsó táblájából a homogén lineáris egyenletrendszer általános megoldása könnyen leolvasható:
Dobjuk el a zérus sorokat, és foglalkozzunk a maradék táblázattal.
Nevezzük az eliminált változókat \emph{kötöttnek}
a többi változót \emph{szabadnak}.
Minden sor egy és csak egy kötött, azaz eliminált, változót tartalmaz.
A szabad változók, az az a többiek már ha vannak ilyenek egyáltalán, egy nem eliminált változóhoz tartoznak.
Adjunk a szabad változóknak tetszőleges értéket,
majd minden sorban ebből már egyértelműen kifejezhető a kötött változók értéke.

\paragraph{Összefoglalva}
Az eliminált változókat
\emph{kötött változónak}\index{kotott@kötött változó}
nevezzük,
és a többi változót
\emph{szabad változónak}\index{szabad változó}
mondjuk. 
Persze az algoritmusban a kötött változók száma és a szabad valtozók száma mindig az együttható mátrix oszlopainak számával azonos.

A következő tétel rendkívül fontos, még akkor is ha teljesen nyilvánvaló a most tárgyalt algoritmus alapján.
\begin{proposition}
	Ha egy homogén lineáris egyenletrendszerben több ismeretlen van mint egyenlet,
	akkor a rendszernek van nem zéró megoldása.
\end{proposition}
\begin{proof}
	Gauss\,--\,Jordan-algoritmus\index{Gauss\,--\,Jordan-elimináció}  legutolsó táblázatában
	a kötött változó száma legfeljebb a sorok száma az pedig szigorúan kisebb mint az összes ismeretlenek száma.
	Van tehát szabad változó, amelynek értéke tetszőleges lehet.
\end{proof}
Mivel a változók vagy kötöttek vagy szabadok,
ezért a kötött változók számának és a szabad változók számának az összege a változók száma.
Ebben a pillanatban még nem látszik,
de később ki fog derülni,
hogy a kötött így a szabad változók száma is független az algoritmus során választott pivot elemektől.\index{pivot elem}
Érdemes a rendszer egy megoldásra úgy gondolni, mint egy $x\in\mathbb{F}^n$ oszlopvektorra,
amelynek $i$-edik koordinátája az $x_i$ változó aktuális értéke.
Ilyen módon az $x_1,\ldots,x_n$ pontosan akkor elégíti ki a lineáris egyenletrendszert,
ha
\[
	Ax=b
\]
mátrixegyenlet teljesül, ahol a $b\in\mathbb{F}^n$ az a vektor melynek $i$-edik koordinátája $b_i$.

Érdemes itt konkrét példákkal szemléltetni a homogén lineáris egyenletrendszer általános megoldásának felírását:
\begin{enumerate}
	\item
	      Oldjuk meg a
	      \[
		      \systeme{ x+4y+7z=0,
			      2x+5y+8z=0,
			      3x+6y+8z=0
		      }
	      \]
	      homogén  lineáris egyenletrendszert.
	      A Gauss\,--\,Jordan-algoritmus\index{Gauss\,--\,Jordan-elimináció}  lehet például a következő:
	      \[
		      \begin{array}{|rrr}
			      a         & b & c \\
			      \hline
			      \boxed{1} & 4 & 7 \\
			      2         & 5 & 8 \\
			      3         & 6 & 8 \\
			      \hline
			      \delta    & 4 & 7
		      \end{array}
		      \implies
		      \begin{array}{r|rrr}
			        & a & b          & c   \\
			      \hline
			      a & 1 & 4          & 7   \\
			        & 0 & \boxed{-3} & -6  \\
			        & 0 & -6         & -13 \\
			      \hline
			        & 0 & \delta     & 2
		      \end{array}
		      \implies
		      \begin{array}{r|rrr}
			        & a & b & c          \\
			      \hline
			      a & 1 & 0 & -1         \\
			      b & 0 & 1 & 2          \\
			        & 0 & 0 & \boxed{-1} \\
			      \hline
			        & 0 & 0 & \delta
		      \end{array}
		      \implies
		      \begin{array}{r|rrr}
			        & a & b & c \\
			      \hline
			      a & 1 & 0 & 0 \\
			      b & 0 & 1 & 0 \\
			      c & 0 & 0 & 1
		      \end{array}
	      \]
	      Ez azt jelenti, hogy az eredeti feladat ekvivalens az
	      \[\systeme{x=0,y=0,z=0}\]
	      feladattal, amelynek nyilvánvalóan csak a zéró vektor a megoldása.
	\item
	      Tekintsük a következő feladatot:
	      \[
		      \begin{array}{rl}
			      x_1+3x_2+4x_3+5x_4-x_5=  & 0 \\
			      -2x_1+x_2-x_3+4x_4-5x_5= & 0 \\
			      2x_1+x_2+3x_3+3x_5=      & 0 \\
			      3x_1+x_2+4x_3-x_4+5x_5=  & 0
		      \end{array}
	      \]
	      Az elimináció lehet a következő:
	      \begin{multline*}
		      \begin{array}{r|rrrrr}
			       & a_1       & a_2 & a_3 & a_4 & a_5 \\
			      \hline
			       & \boxed{1} & 3   & 4   & 5   & -1  \\
			       & -2        & 1   & -1  & 4   & -5  \\
			       & 2         & 1   & 3   & 0   & 3   \\
			       & 3         & 1   & 4   & -1  & 5   \\
			      \hline
			       & \delta    & 3   & 4   & 5   & -1
		      \end{array}
		      \implies
		      \begin{array}{r|rrrrr}
			          & a_1 & a_2       & a_3 & a_4 & a_5 \\
			      \hline
			      a_1 & 1   & 3         & 4   & 5   & -1  \\
			          & 0   & \boxed{7} & 7   & 14  & -7  \\
			          & 0   & -5        & -5  & -10 & 5   \\
			          & 0   & -8        & -8  & -16 & 8   \\
			      \hline
			          & 0   & \delta    & 1   & 2   & -1
		      \end{array}
		      \implies
		      \\
		      \begin{array}{r|rrrrr}
			          & a_1 & a_2 & a_3 & a_4 & a_5 \\
			      \hline
			      a_1 & 1   & 0   & 1   & -1  & 2   \\
			      a_2 & 0   & 1   & 1   & 2   & -1  \\
			          & 0   & 0   & 0   & 0   & 0   \\
			          & 0   & 0   & 0   & 0   & 0
		      \end{array}
	      \end{multline*}
	      Az eredeti egyenletrendszer tehát ekvivalens a következő egyenletrendszerrel:
	      \[
		      \begin{array}{rl}
			      x_1+x_3-x_4+2x_5= & 0 \\
			      x_2+x_3+2x_4-x_5= & 0
		      \end{array}
	      \]
	      Itt $x_1,x_2$ a kötött változók és $x_3,x_4,x_5$ a szabad változók.
	      Ezek értéke tetszőleges lehet, mondjuk $x_3=s$, $x_4=r$, $x_5=t$ és ekkor
	      $x_1=-s+r-2t$ valamint $x_2=-s-2r+t$.
	      Az általános megoldás ezért
	      \[
		      \left\{
		      \begin{pmatrix}
			      -s+r-2t \\
			      -s-2r+t \\
			      s       \\
			      r       \\
			      t
		      \end{pmatrix}
		      :s,r,t\in\mathbb{R}
		      \right\}
		      =
		      \left\{ s
		      \begin{pmatrix}
			      -1 \\-1\\1\\0\\0
		      \end{pmatrix}
		      +
		      r
		      \begin{pmatrix}
			      1 \\-2\\0\\1\\0
		      \end{pmatrix}
		      +
		      t
		      \begin{pmatrix}
			      -2 \\1\\0\\0\\1
		      \end{pmatrix}
		      :s,r,t\in\mathbb{R}
		      \right\}.
	      \]
	      A szabad változók száma tehát $3$,
	      és a rendszer megoldáshalmaza a
	      \[
		      \begin{pmatrix}
			      -1 & 1  & -2 \\
			      -1 & -2 & 1  \\
			      1  & 0  & 0  \\
			      0  & 1  & 0  \\
			      0  & 0  & 1
		      \end{pmatrix}
	      \]
	      mátrix oszlopai generált altér.
\end{enumerate}

Egy homogén lineáris egyenletrendszernek a zérus vektor mindig megoldása,
ezt nevezzük \emph{triviális megoldásnak}.\index{triviális megoldás}
A Gauss\,--\,Jordan-eliminációs\index{Gauss\,--\,Jordan-elimináció}  algoritmusból világos a következő gondolat.
A rendszernek pontosan akkor nincs nem triviális megoldása,
ha nincs szabad változó, azaz minden változó kötött:
\begin{proposition}
	Egy $Ax=0$ homogén lineáris egyenletrendszernek pontosan akkor a triviális megoldás az egyetlen megoldása,
	ha az eliminációs algoritmusban minden oszlop a generátorrendszerbe cserélhető.
\end{proposition}
\subsection{Mátrixok bázisfaktorizációja 1.}
Érdemes a Gauss\,--\,Jordan-eliminációt\index{Gauss\,--\,Jordan-elimináció}  az egyenletrendszerek megoldásától függetlenül is szemlélni.
Ide másolom az előző feladat megoldásának táblázatát:
\[
	\begin{array}{r|rrrrr}
		    & a_1 & a_2 & a_3 & a_4 & a_5 \\
		\hline
		a_1 & 1   & 0   & 1   & -1  & 2   \\
		a_2 & 0   & 1   & 1   & 2   & -1  \\
		    & 0   & 0   & 0   & 0   & 0   \\
		    & 0   & 0   & 0   & 0   & 0
	\end{array}
\]
Ez utolsó táblázat felfedi az eredeti $a_1,a_2,a_3,a_4,a_5$ vektorok közti kapcsolatot.
A táblázat értelmezése szerint
\begin{eqnarray*}
	a_3&=& a_1+a_2\\
	a_4&=& -a_1+2a_2\\
	a_5&=& 2a_1-a_2
\end{eqnarray*}
Ezt mátrixszorzásként interpretálva azt kapjuk, hogy
\[
	\begin{pmatrix}
		1  & 3 & 4  & 5  & -1 \\
		-2 & 1 & -1 & 3  & -5 \\
		2  & 1 & 3  & 0  & 3  \\
		3  & 1 & 4  & -1 & 5
	\end{pmatrix}
	=
	\begin{pmatrix}
		1  & 3 \\
		-2 & 1 \\
		2  & 1 \\
		3  & 1
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		1 & 0 & 1 & -1 & 2  \\
		0 & 1 & 1 & 2  & -1
	\end{pmatrix}
    \tag{\dag}\label{bazisfakt}
\]
hiszen a baloldali mátrix minden oszlopa az $a_1,a_2$ oszlopok lineáris kombinációja.

\begin{definition}
	Legyen $A\in\mathbb{F}^{m\times n}$ mátrix.
	Az $A$ mátrix oszlop- (sor-) vektorterének nevezzük az $A$ oszlopai (sorai) generálta alterét az $\mathbb{F}^m$ ($\mathbb{F}^n$)
	vektortérnek.
    \index{oszlopvektortér}\index{sorvektortér}
\end{definition}
Az előző példa szerint az ottani $4\times 5$ méretű mátrix oszlopvektorterének az $\left\{ a_1,a_2 \right\}$ két elemű rendszer
egy generátorrendszere, hiszen ha $W$ jelöli az oszlopok generálta alteret, azaz
\(W=\lin\left\{ a_1,a_2,a_3,a_4,a_5 \right\}\), akkor 
\[
    \left\{ a_1,a_2,a_3,a_4,a_5 \right\}\subseteq
    \lin{\left\{ a_1,a_2\right\}},
\]
amiből a lineáris burkot képezve
\[
    W
    \subseteq
    \lin{\left\{ a_1,a_2\right\}}
    \subseteq
    W
\]
következik, ergo $W=\lin\left\{ a_1,a_2 \right\}$ valóban fennáll.

Nézzük újra \apageref{bazisfakt}. oldalon a (\dag) matrix faktorizációt.
Látható, hogy nem csak az oszlop vektortérnek, de a sorvektortérnek is találtunk egy két elemű generátorrendszerét:
Jelölje most $a_1,a_2,a_3,a_4$ a (\dag) baloldalán lévő mátrix sorait.
E mátrix legyen $A$.
A $B$ mátrix legyen a (\dag) jobboldalán az első mátrix és $C$ a második.
Persze $A=B\cdot C$, ami szerint $A$ minden sora $C$ sorainak lineáris kombinációja.
Itt konkrétan
\begin{eqnarray*}
    a_1&=& 1c_1+3c_2\\
    a_2&=& (-2)c_1+1c_2\\
    a_3&=& 2c_1+1c_2\\
    a_4&=& 3c_1+(-1)c_2,
\end{eqnarray*}
ahol $c_1,c_2$ jelöli a $C$ mátrix sorait.
Tehát, ha most $V$ jelöli az $A$ mátrix sorvektortetét, azaz
\(V=\lin\left\{ a_1,a_2,a_3,a_4 \right\}\),
akkor 
\[
    \left\{ a_1,a_2,a_3,a_4 \right\}
    \subseteq
    \lin\left\{ c_1,c_2 \right\},
\]
amiből a lineáris burkot képezve
\[
    V\subseteq\lin\left\{ c_1,c_2 \right\}\subseteq V
\]
fennáll. 
Az utolsó tartalmazás most azért igaz, 
mert $C$ mátrix az $A$ utolsó eliminációs táblázatának nem zérus soraiból áll, 
de az elimináció minden egyes lépésében a sorok egy lináris kombinációját képezzük, 
ergo a táblázat minden egyes sora az eredeti sorok alkotta vektortérben -- ami itt $V$ -- marad.

Vegyük észre, hogy a fenti eljárással azt gondoltuk meg, 
hogy tetszőleges $m\times n$ méretű mátrix
sorvektorterének és oszlopvektorterének van azonos $r$ elemszámú generátorrendszere,
ahol $1\leq r\leq\min\left\{ m,n \right\}$.

Az alábbi állítás a mátrix szorzás definíciójának következménye.
\begin{proposition}
	Legyen $A\in\mathbb{F}^{m\times n}$ egy nem zérus mátrix.
	\begin{enumerate}
		\item
		      Tegyük fel, hogy $A$ oszlopvektorterének adott egy $r$-elemű generátorrendszere.
		      Ekkor a generátorrendszer $r$ db.~vektorát a $B$ mátrix oszlopaiba rendezve egy $m\times r$ mátrixot kapunk.
		      Ehhez a $B$ mátrixhoz létezik $C$ mátrix, amely $r\times n$ méretű és
		      \(
		      A=B\cdot C.
		      \)
		\item
		      Most azt tegyük fel, hogy $A$ sorvektorterében adott egy $r$-elemű generátorrendszer.
		      Ekkor a generátorrendszer $r$ db.~sorát a $C$ mátrix soraiba rendezve egy $r\times n$ mátrixot kapunk.
		      Ehhez a $C$ mátrixhoz létezik $B$ mátrix, amely $m\times r$ méretű és
		      \(
		      A=B\cdot C.
		      \)
              \qedhere
	\end{enumerate}
\end{proposition}

\subsection{Inhomogén lineáris egyenletrendszer}
Láttuk, hogy egy $m$ egyenletet és $n$ ismeretlent tartalmazó lineáris egyenletrendszer ekvivalens az
\[
	Ax=b
\]
feladattal, ahol $A\in\mathbb{F}^{m\times n}$ az együttható-mátrix és $b\in\mathbb{F}^m$
a jobboldalakból alkotott vektor.
A következő egyszerű észrevétel szerint ha az inhomogén rendszernek találunk valahogyan egyetlen megoldását és ismerjük a homogén rendszer általános megoldását,
akkor már az inhomogén rendszer általános megoldása is egyszerűen felírható.
\begin{proposition}
	A fenti jelölések megtartása mellett
	legyen $x_0\in\mathbb{F}^m$ egy tetszőlegesen rögzített megoldása az $Ax=b$ inhomogén lineáris egyenletrendszernek.
	Ekkor
	\[
		\left\{ x:Ax=b \right\}
		=
		\left\{ x_0+z:Az=0 \right\},
	\]
	azaz az inhomogén rendszer megoldáshalmaza,
	azonos a homogén rendszer megoldás halmazának egy partikuláris megoldással való
	eltoltjával.
\end{proposition}
\begin{proof}
	Legyen először $x$ egy megoldás, azaz $Ax=b$.
	Mivel $x_0$ is egy megoldás, ezért $Ax_{0}=b$ is teljesül.
	Persze $x=x_0+\left( x-x_0 \right)=x_0+z$, ahol $z=x-x_0$ egy olyan vektor,
	amelyre $Az=Ax-Ax_0=b-b=0$.

	Másodszor tekintsünk egy $x=x_0+z$ alakú vektort, ahol $Az=0$.
	Ekkor persze $Ax=Ax_0+Az=b+0=b$.
\end{proof}
Egy nem nyilvánvaló következmény,
hogy akármelyik partikuláris megoldással toljuk is el az
inhomogén rendszer megoldását mindig ugyanazt a halmazt kapjuk,
emiatt mindegy melyik partikuláris megoldást rögzítettük.

Persze a kérdés, hogy hogyan találunk egyáltalán partikuláris megoldást.
Máshogyan fogalmazva: mikor van egyáltalán megoldása egy inhomogén rendszernek?
\begin{proposition}
	Az $Ax=b$ inhomogén lineáris egyenletrendszernek pontosan akkor van megoldása,
	ha $b$ előáll mint $A$ oszlopainak valamilyen lineáris kombinációja,
	azaz a $b$ vektor az $A$ mátrix oszlopvektorteréhez tartozik.
\end{proposition}
Konkrétan adott $A$ és $b$ mellett Gauss\,--\,Jordan-algoritmussal\index{Gauss\,--\,Jordan-elimináció}  el tudjuk dönteni, hogy
$b$ vektor eleme-e az $A$ mátrix oszlopvektorterének.
Egészítsük ki az $A$ mátrixot az utolsó oszlopában a $b$ vektorral.
Most hajtsuk végre a Gauss\,--\,Jordan-eliminációt, de úgy hogy pivot-elemet,\index{pivot elem}
csak az első n oszlopból válasszunk, ugyanúgy mintha csak a homogén rendszert oldanánk meg.
Persze minden egyes eliminációs lépésben számoljuk ki az utolsó oszlopvektor elemeit is.
Az algoritmus úgy ér véget,
hogy az $A$ mátrixnak megfelelő részben minden nem zérus sor tartalmaz eliminált változót.

Ha van olyan sor ahol az első $n$ elem zérus, de az utolsó elem nem zérus,
akkor nyilván nincs megoldás.
Minden egyéb esetben van megoldás,
hiszen a csupa zéró sorokhoz tartozó egyenletek az ismeretlenek minden értéke mellett fennállnak,
de maradék a nem zérus sorok esetén a kötött változót
kifejezhetjük a szabad változók tetszőleges -- mondjuk zérus -- értéke mellett.

Az inhomogén rendszernek tehát pontosan akkor van megoldása,
ha az utolsó oszlop minden nem zérus eleme olyan egyenlethez tartozik,
ahol szerepel eliminált változó.

Ha a fenti eliminációra, mint a generátorrendszer csere lemma alkalmazására gondolunk,
akkor azt kapjuk, hogy a megoldhatóság szükséges és elegendő feltétele,
hogy a végső táblában az utolsó, a $b$-hez tartozó oszlopnak csak olyan helyeken lehetnek nem zérus tagjai,
amely helyek korábban már a generátorrendszerbe becserélt oszlopvektorokhoz tartoznak,
ami persze nem jelent többet, mint hogy $b$ eleme az $A$ oszlopvektorterének.
Az algoritmus annyiban több, mint a korábban megértett feltétel,
hogy a megoldható esetben rögtön kapunk egy partikuláris megoldást is.

Nézzünk egy példát.
\[
	\begin{array}{rl}
		x_1+3x_2+4x_3+5x_4-x_5=  & 6  \\
		-2x_1+x_2-x_3+4x_4-5x_5= & -5 \\
		2x_1+x_2+3x_3+3x_5=      & 7  \\
		3x_1+x_2+4x_3-x_4+5x_5=  & 10
	\end{array}
\]
A Gauss\,--\,Jordan-algoritmus:\index{Gauss\,--\,Jordan-elimináció}
\begin{multline*}
	\begin{array}{c|rrrrr|r}
		 & a_1       & a_2 & a_3 & a_4 & a_5 & b  \\
		\hline
		 & \boxed{1} & 3   & 4   & 5   & -1  & 6  \\
		 & -2        & 1   & -1  & 4   & -5  & -5 \\
		 & 2         & 1   & 3   & 0   & 3   & 7  \\
		 & 3         & 1   & 4   & -1  & 5   & 10 \\
		\hline
		 & \delta    & 3   & 4   & 5   & -1  & 6
	\end{array}
	\implies
	\begin{array}{c|rrrrr|r}
		    & a_1 & a_2 & a_3 & a_4 & a_5       & b  \\
		\hline
		a_1 & 1   & 3   & 4   & 5   & -1        & 6  \\
		    & 0   & 7   & 7   & 14  & -7        & 7  \\
		    & 0   & -5  & -5  & -10 & \boxed{5} & -5 \\
		    & 0   & -8  & -8  & -16 & 8         & -8 \\
		\hline
		    & 0   & -1  & -1  & -2  & \delta    & -1
	\end{array}
	\implies
	\\
	\begin{array}{c|rrrrr|r}
		    & a_1 & a_2 & a_3 & a_4 & a_5 & b  \\
		\hline
		a_1 & 1   & 2   & 3   & 3   & 0   & 5  \\
		    & 0   & 0   & 0   & 0   & 0   & 0  \\
		a_5 & 0   & -1  & -1  & -2  & 1   & -1 \\
		    & 0   & 0   & 0   & 0   & 0   & 0  \\
	\end{array}
\end{multline*}
A rendszernek tehát van megoldása.
A legegyszerűbb ha a szabad változókat zérusra állítjuk,
ergo $x_1=5$, $x_5=-1$, $x_2=0$, $x_3=0$, $x_4=0$ egy partikuláris megoldás.
Az általános megoldás tehát
\[
	\left\{
	\begin{pmatrix}
		5 \\0\\0\\0\\-1
	\end{pmatrix}
	+s
	\begin{pmatrix}
		-2 \\1\\0\\0\\1
	\end{pmatrix}
	+t
	\begin{pmatrix}
		-3 \\0\\1\\0\\1
	\end{pmatrix}
	+r
	\begin{pmatrix}
		-3 \\0\\0\\1\\2
	\end{pmatrix}
	:s,t,r\in\mathbb{R}
	\right\}.
\]
\begin{proposition}
    Legyen $A\in\mathbb{F}^{m\times n}$ mátrix és $b\in\mathbb{F}^n$ egy előre megadott vektor.
	Az $Ax=b$ inhomogén lineáris egyenletrendszernek pontosan akkor van egy és csak egy
	megoldása, ha $b$ az $A$ képteréhez tartozik 
    és $A$ minden oszlopa a Gauss\,--\,Jordan-eliminációs algoritmussal
    \index{Gauss\,--\,Jordan-elimináció} 
    a generátorrendszerbe cserélhető.
\end{proposition}
Ebben az esetben persze az algoritmusból adódó partikuláris megoldás az egyetlen megoldás.

Ha például a fenti harmadik eliminációs táblázatban a $b$ oszlopban a 2. elem nem zérus szám lenne,
akkor a $0,1,0,0$ számokkal mint oszlopvektorral jelölt $b$ jobboldal mellett az $Ax=b$ rendszernek nem lenne megoldása. 
Ez azért van, mert az utolsó táblázathoz tartozó homogén lineáris egyenletrendszer ekvivalens az első táblázat rendszerével, és a 
\(
\begin{pmatrix}0\\1\\0\\0\end{pmatrix}
\)
oszlopot helyettesítve az első táblázat $b$ oszlopába, az elimináció során a $b$ oszlop nem változik, hiszen a 2. sorban nem választottunk pivot elemet, ergo a segédsorban $b$ alatt minden lépésben zérus állna.

Ezt végig gondolhatjuk tetszőleges méretű mátrixra is, így kapjuk a következő állítást.
\begin{proposition}
    Legyen $A\in\mathbb{F}^{m\times n}$ mátrix.
    Hajtsunk végre Gauss\,--\,Jordan-eliminációt a mátrixon.
    Az utolsó táblázatnak pontosan akkor nincs csupa zérus elemből álló sora,
    ha minden $b\in\mathbb{F}^m$ vektorra az 
    $Ax=b$ inhomogén lineáris egyenletrendszernek van megoldása.
    \label{pr:nincsmo}
\end{proposition}
\begin{proof}
    Ha az elimináció utolsó táblázatában nincs zérus sor, akkor minden sorban van egyetlen eliminált változó. 
    Akármi is tehát a jobboldal, ha a szabad változókat zérusra állítjuk akkor kapunk egy megoldást.
    Megfordítva, ha mondjuk a $k$-adik sor a csupa zérus elemből álló sor az utolsó táblázatban, 
    akkor az a jobboldal, ahol egyedül a $k$-adik koordináta nem zérus,
    egy nem megoldható $Ax=b$ rendszert definiál.
\end{proof}
	Tudjuk, hogy az $A$ mátrixra alkalmazott Gauss\,--\,Jordan-algoritmus\index{Gauss\,--\,Jordan-elimináció}  akkor ér véget, amikor
	minden sorra teljesül, hogy az vagy a csupa zérus sor, vagy az pontosan egy kötött ismeretlent tartalmaz.
	Eszerint akkor és csak akkor nincs zérus sor, ha a kötött ismeretlenek száma azonos a sorok számával.

Ebből már adódik, hogy ha kevesebb ismeretlene mint sora van egy rendszernek, 
akkor mindig megválasztható olyan jobboldali vektor, 
amellyel az inhomogén rendszer nem megoldható.
\begin{proposition}
	Legyen $A\in\mathbb{F}^{m\times r}$ olyan mátrix, ahol $r<m$.
	Ekkor létezik $b\in\mathbb{F}^m$ vektor,
	amellyel felírt $Ax=b$ inhomogén lineáris egyenletrendszernek már nincs megoldása.
\end{proposition}
\begin{proof}
	Tegyük fel tehát, hogy kevesebb ismeretlen van mint sor.
    Ekkor persze a kötött változók száma is kisebb mint a sorok száma,
    ergo van zérus sor az utolsó eliminációs táblázatban.
    Ekkor \aref{pr:nincsmo} állítás szerint, létezik olyan jobboldal, amivel képzett inhomogén rendszer már nem megoldható.
\end{proof}

A Gauss\,--\,Jordan-elimináció utolsó táblázatára vonatkozó \aref{pr:nincsmo}. állítás, gyönyörű
következménye a négyzetes mátrixok kétoldalisági tulajdonsága.
\begin{proposition}
    Tegyük fel, hogy $A,B\in\mathbb{F}^{m\times m}$ négyzetes mátrixok szorzata az identitás mátrix,
    azaz $AB=I$.
    Ekkor $BA=I$ is fennáll.
    \label{pr:ketoldal}
\end{proposition}
\begin{proof}
    Ha $AB=I$, akkor nyilvánvalóan minden $b\in\mathbb{F}^m$ jobboldal mellett,
    az $Ax=b$ inhomogén lineáris egyenletrendszerek van megoldása. 
    Például $x=Bb$ egy megoldás.
    \Aref{pr:nincsmo}. állítás szerint ez csak úgy lehet, ha az utolsó eliminációs táblázatban nincs csupa zérus sor.
    Ezek szerint az utolsó táblázatban a sorok száma megegyezik az eliminált változók számával,
    de négyzetes mátrixról beszélünk, tehát a oszlopok száma, azaz az $Ax=0$ homogén rendszerben
    az összes változók száma is azonos a kötött változók számával.
    Ekkor persze nincs szabad változó, tehát az $Ax=0$ homogén rendszernek csak a triviális megoldás a megoldása.
    A mátrix szorzás definíciója szerint ez úgy is fogalmazható, hogy
    tetszőleges $C\in\mathbb{F}^{m\times n}$ mátrixra, az $AC=0$ feltételből $C=0$ következik.
    Na most
    \[
        A\left(BA-I  \right)=A\left( BA \right)-AI=\left( AB \right)A-AI=IA-AI=A-A=0.
    \]
    Meggondoltuk tehát, hogy $BA-I=0$, ergo $BA=I$ is fennáll.
\end{proof}
\subsection{Inverz mátrix}
Gondoljunk arra, hogy milyen módszerrel automatizálhatnánk a Gauss\,--\,Jordan-algoritmust.\index{Gauss\,--\,Jordan-elimináció}
Mivel sorműveletekről van szó, természetes gondolat, hogy az $A\in\mathbb{F}^{m\times n}$
mátrix transzformálásához $m\times m$ méretű mátrixokat használjunk,
amelyekkel balról szorozzuk $A$-t.
Valóban, ha $m\times m$ méretű identitás mátrix
\begin{enumerate}
	\item
	      $i$-edik sorát szorozzuk egy $\delta$ számmal,
	      akkor egy olyan mátrixot kapunk,
	      amellyel való balszorzása $A$-nak éppen az $A$ mátrix
	      $i$-edik sorát szorozza $\delta$-val.
	\item
	      $k$-adik sorából kivonjuk az $i$-edik sor $\delta$-szorosát,
	      akkor egy olyan mátrixot kapunk,
	      amellyel való balszorzása $A$-nak éppen az $A$ mátrix
	      $k$-adik sorából vonja ki az $i$-edik sor $\delta$-szorosát.
	\item
	      $k$-adik és $j$-edik sorát felcseréljük,
	      akkor egy olyan mátrixot kapunk,
	      amellyel való balszorzása $A$-nak éppen az $A$ mátrix
	      $k$-adik és $j$-edik sorát cseréli fel.
\end{enumerate}

Tekintsünk most, egy $m\times m$ méretű négyzetes mátrixot.
Tegyük fel, hogy a mátrix minden oszlopát a generátorrendszerbe cserélhetjük
a szokásos Gauss\,--\,Jordan-eliminációs algoritmus\index{Gauss\,--\,Jordan-elimináció}  során.
Láttuk korábban, hogy ez azt jelenti, hogy az $Ax=b$ inhomogén lineáris egyenletrendszer
minden $b$ vektor mellett egyértelműen megoldható.

Legyen $\left\{ e_1,\ldots,e_m \right\}$ az $\mathbb{F}^m$ szokásos generátorrendszere,
azaz $e_k$-nak éppen a $k$-adik koordinátája 1, a többi zérus.
Ha $B\in\mathbb{F}^m$ az a mátrix,
amelynek $k$-edik oszlopa az $Ax=e_k$ inhomogén lineáris egyenletrendszer egyetlen $x_k\in\mathbb{F}^m$ megoldása,
akkor a mátrix szorzás definíciója szerint
\[
	AB=I
\]
teljesül.
A $B$ mátrixot tehát $m$ darab $m$ lépéses Gauss\,--\,Jordan-eliminációval\index{Gauss\,--\,Jordan-elimináció}  meg tudnánk határozni.

Egyszerűbb,
ha egyetlen eliminációs algoritmus használunk, de arra az $m\times 2m$ méretű mátrixra,
amelyet úgy kapunk, hogy az a $A$ mátrix jobboldalához illesztjük az $I\in\mathbb{F}^{m\times m}$ identitás mátrixot.
Az elimináció során,
$A$ minden sorát a generátorrendszerbe cseréljük, de persze a jobbra illesztett identitás
mátrix sorait is transzformáljuk.
Feltevésünk szerint $m$ lépés után áll le az algoritmus.
Ekkor egy  olyan táblázat van előttünk, amelynek első $m$ oszlopa egy olyan négyzetes
mátrixot alkot, amelynek minden oszlopa és minden sora egyetlen 1-est tartalmaz a többi elem zérus.
Vegyük észre,
hogy egy ilyen mátrix a sorok felcserélésével az identitás mátrixszá transzformálható.

Az $Ax=e_k$ egyenlet megoldása a jobboldali $m\times m$ mátrix $k$-adik oszlopából olvasható le.
Ha például az első sor $j$-edik elemén van 1-es,
akkor az azt jelenti, hogy az első sorba elimináltuk az $j$-edik ismeretlent,
tehát a megoldás $j$-edik koordinátáját tartalmazza az $m+k$-adik oszlop első eleme.

Ha tehát úgy cseréljük fel a táblázat sorait,
hogy a baloldali $m\times m$ méretű mátrix váljon az identitás mátrixszá,
akkor az $Ax=e_k$ egyenlet egyetlen partikuláris megoldása jelentkezik az egész táblázat
$m+k$ adik oszlopában.
Mivel a keresett $B$ mátrixnak éppen ez a $k$-adik oszlopa,
ezért maga a $B$ mátrix áll a táblázat jobboldali $m\times m$ méretű részén.

Most meggondoljuk, hogy $BA=I$ is fennáll.
Az iménti algoritmussal az $[A|I]$ mátrixot transzformáltuk a $[I|B]$ mátrixszá.
A transzformáció során a Gauss\,--\,Jordan-eliminációt használtuk,\index{Gauss\,--\,Jordan-elimináció}  amely a szakasz elején említett 1) és 2) típusú mátrixokkal mint balszorzásokkal hajtható végre.
Az algoritmus végén még sorokat is felcseréltünk, ami egy 3) típusú balszorzást jelent.
Ha az 1) vagy 2) vagy 3) típusú mátrixot \emph{elemi mátrixnak}\index{elemi mátrix}
nevezünk,
akkor úgy fogalmazhatunk,
hogy van véges sok $P_1,\ldots,P_r\in\mathbb{F}^{m\times m}$ elemi mátrix, amelyre
\(
(P_r\cdots P_1)\left[ A|I \right]=\left[ I|B \right].
\)
Ekkor persze $(P_r\cdots P_1)A=I$ és $P_r\cdots P_1=B$,
amiből már következik, hogy
\[
	BA=I.
\]

\begin{definition}[nem szinguláris mátrix]
	Egy $A\in\mathbb{F}^{m\times m}$ négyzetes mátrixot
	\emph{invertálhatónak}\index{invertálható mátrix} vagy
	\emph{regulárisnak} vagy \emph{nem szingulárisnak} nevezünk,
	ha létezik $B\in\mathbb{F}^{m\times m}$ négyzetes mátrix,
	amelyre
	\[
		AB=BA=I
	\]
	Mivel egy egységelemes gyűrűben ha van inverz, akkor csak egyetlen egy van,
	ezért adott $A$ invertálható mátrixhoz csak egy $B$ mátrix van, amely kielégíti a fenti definíciót.
	Ezt a $B$ mátrixot nevezzük az $A$ inverzének és $A^{-1}=B$ módon jelöljük.
\end{definition}

\begin{proposition}
	Legyen $A\in\mathbb{F}^{m\times m}$ mátrix. Az alábbi feltevések ekvivalensek:
	\begin{enumerate}
		\item
		      $A$ invertálható.
		\item
		      Létezik $B\in\mathbb{F}^{m\times m}$ mátrix, amelyre
		      \begin{math}
			      AB=I.
		      \end{math}
		\item
		      Gauss\,--\,Jordan-eliminációval\index{Gauss\,--\,Jordan-elimináció}  az $A$ minden oszlopa a generátorrendszerbe cserélhető.
	\end{enumerate}
	Ha a fenti feltevések egyike (ergo mindegyike) fennáll,
	akkor az 2)-ben szereplő $B$ mátrixból csak egy van,
	mégpedig az $A^{-1}$ inverz mátrix.
\end{proposition}
\begin{proof}
	Körben igazolunk:
	\begin{description}
		\item[$1.\Rightarrow 2.$] Nyilvánvaló.
		\item[$2.\Rightarrow 3.$]
		      Tegyük fel -- indirekt --, hogy már $r$ transzformáció után a Gauss\,--\,Jordan-algoritmus megáll, ahol $r<m$.
		      Ez azt jelenti, hogy az $A$ oszlop-vektorterének van $r$ elemű generátorrendszere.
		      E generátorrendszer vektorait mint oszlopokat egy $A_1$ mátrixba téve egy $m\times r$ mátrixot kapunk.
		      Mivel az oszlopok lineáris burkában az $A$ valamennyi oszlopa szerepel,
		      van olyan $r\times m$ méretű $A_2$ mátrix, amelyre
		      \(
		      A_1A_2=A.
		      \)
		      Azt kaptuk tehát, hogy
		      \[
			      A_1\left( A_2B \right)=\left( A_1A_2 \right)B=AB=I.
		      \]
		      Ebből az következik,
		      hogy tetszőleges $b\in\mathbb{F}^{m}$ vektor mellett az
		      \begin{math}
			      x=A_2Bb\in\mathbb{F}^{r}
		      \end{math}
		      vektor megoldása az $A_1x=b$ inhomogén lineáris egyenletrendszernek,
		      ami ellentmondás, hiszen $A_1$ mátrixnak kevesebb oszlopa van mint sora.
		\item[$3.\Rightarrow 1.$] Éppen ezt igazoltuk a szakasz elején.
	\end{description}
	Ha a feltevések fennállnak, akkor
	\[
		A^{-1}=A^{-1}I=A^{-1}\left( AB \right)=\left( A^{-1}A \right)B=IB=B.\qedhere
	\]
\end{proof}
Ha tehát $A$ és $B$ négyzetes mátrixok, amelyekre $AB=I$,
akkor 2) szerint $A$ invertálható, és $A^{-1}=B$, ezért
$BA=A^{-1}A=I$ is fennáll.
Ahogyan azt a mátrixok bevezetésekor \apageref{pg:kommutal}. oldalon megígértük,
most megmutattuk azt, hogy a négyzetes mátrixok gyűrűjében ha két mátrix szorzata a gyűrű
egységeleme, akkor ezek a mátrixok kommutálnak.\index{kommutáló mátrixok}\index{kommutál}

Egy konkrét példa megoldásával ismételjük át a szakasz elején megértett algoritmust.
A feladat, hogy keressük meg az
\(
A=
\begin{pmatrix}
	1 & 1 & 1 & 0 \\
	0 & 3 & 1 & 2 \\
	2 & 3 & 1 & 0 \\
	1 & 0 & 2 & 1
\end{pmatrix}
\)
mátrix inverzét!
Az inverz létezésének szükséges és elegendő feltétele, hogy 4 lépést tudjunk végrehajtani az eliminációs algoritmusban.
Egy lehetséges megoldás a következő:
\begin{multline*}
	\begin{array}{cccc|cccc}
		\boxed{1} & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
		0         & 3 & 1 & 2 & 0 & 1 & 0 & 0 \\
		2         & 3 & 1 & 0 & 0 & 0 & 1 & 0 \\
		1         & 0 & 2 & 1 & 0 & 0 & 0 & 1 \\
		\hline
		\delta    & 1 & 1 & 0 & 1 & 0 & 0 & 0
	\end{array}\Rightarrow
	\begin{array}{cccc|cccc}
		1 & 1  & 1  & 0         & 1  & 0 & 0 & 0 \\
		0 & 3  & 1  & 2         & 0  & 1 & 0 & 0 \\
		0 & 1  & -1 & 0         & -2 & 0 & 1 & 0 \\
		0 & -1 & 1  & \boxed{1} & -1 & 0 & 0 & 1 \\
		\hline
		0 & -1 & 1  & \delta    & -1 & 0 & 0 & 1
	\end{array}\Rightarrow
	\\
	\begin{array}{cccc|cccc}
		1 & 1  & 1          & 0 & 1  & 0  & 0 & 0  \\
		0 & 5  & \boxed{-1} & 0 & 2  & 1  & 0 & -2 \\
		0 & 1  & -1         & 0 & -2 & 0  & 1 & 0  \\
		0 & -1 & 1          & 1 & -1 & 0  & 0 & 1  \\
		\hline
		0 & -5 & \delta     & 0 & -2 & -1 & 0 & 2
	\end{array}\Rightarrow
	\begin{array}{cccc|cccc}
		1 & 6          & 0 & 0 & 3  & 1           & 0            & -2           \\
		0 & -5         & 1 & 0 & -2 & -1          & 0            & 2            \\
		0 & \boxed{-4} & 0 & 0 & -4 & -1          & 1            & 2            \\
		0 & 4          & 0 & 1 & 1  & 1           & 0            & -1           \\
		\hline
		0 & \delta     & 0 & 0 & 1  & \frac{1}{4} & -\frac{1}{4} & -\frac{1}{2}
	\end{array}\Rightarrow
	\\
	\begin{array}{cccc|cccc}
		1 & 0 & 0 & 0 & -3 & -\frac{1}{2} & \frac{3}{2}  & 1            \\
		0 & 0 & 1 & 0 & 3  & \frac{1}{4}  & -\frac{5}{4} & -\frac{1}{2} \\
		0 & 1 & 0 & 0 & 1  & \frac{1}{4}  & -\frac{1}{4} & -\frac{1}{2} \\
		0 & 0 & 0 & 1 & -3 & 0            & 1            & 1
	\end{array}\Rightarrow
	\begin{array}{cccc|cccc}
		1 & 0 & 0 & 0 & -3 & -\frac{1}{2} & \frac{3}{2}  & 1            \\
		0 & 1 & 0 & 0 & 1  & \frac{1}{4}  & -\frac{1}{4} & -\frac{1}{2} \\
		0 & 0 & 1 & 0 & 3  & \frac{1}{4}  & -\frac{5}{4} & -\frac{1}{2} \\
		0 & 0 & 0 & 1 & -3 & 0            & 1            & 1
	\end{array}
\end{multline*}
Kaptuk hát, hogy az $A$ mátrix inverze az
\begin{math}
	A^{-1}=\frac{1}{4}
	\begin{pmatrix}
		-12 & -2 & 6  & 4  \\
		4   & 1  & -1 & -2 \\
		12  & 1  & -5 & -2 \\
		-12 & 0  & 4  & 4
	\end{pmatrix}
\end{math}
mátrix.

Picit kevesebb helyet foglal az algoritmus, ha lusták vagyunk a generátorrendszerbe már becserélt
oszlopok kiírására.
Ekkor érdemes kiírni a sorok és oszlopok címkéjét, nehogy eltévedjünk.
Az előző feladat így alakul:
\begin{multline*}
	\begin{array}{r|cccc|cccc}
		    & a_1       & a_2 & a_3 & a_4 & e_1 & e_2 & e_3 & e_4 \\
		\hline
		e_1 & \boxed{1} & 1   & 1   & 0   & 1   & 0   & 0   & 0   \\
		e_2 & 0         & 3   & 1   & 2   & 0   & 1   & 0   & 0   \\
		e_3 & 2         & 3   & 1   & 0   & 0   & 0   & 1   & 0   \\
		e_4 & 1         & 0   & 2   & 1   & 0   & 0   & 0   & 1   \\
		\hline
		    & \delta    & 1   & 1   & 0   & 1   & 0   & 0   & 0
	\end{array}\Rightarrow
	\begin{array}{r|ccc|cccc}
		    & a_2 & a_3 & a_4       & e_1 & e_2 & e_3 & e_4 \\
		\hline
		a_1 & 1   & 1   & 0         & 1   & 0   & 0   & 0   \\
		e_2 & 3   & 1   & 2         & 0   & 1   & 0   & 0   \\
		e_3 & 1   & -1  & 0         & -2  & 0   & 1   & 0   \\
		e_4 & -1  & 1   & \boxed{1} & -1  & 0   & 0   & 1   \\
		\hline
		    & -1  & 1   & \delta    & -1  & 0   & 0   & 1
	\end{array}\Rightarrow
	\\
	\begin{array}{r|cc|cccc}
		    & a_2 & a_3        & e_1 & e_2 & e_3 & e_4 \\
		\hline
		a_1 & 1   & 1          & 1   & 0   & 0   & 0   \\
		e_2 & 5   & \boxed{-1} & 2   & 1   & 0   & -2  \\
		e_3 & 1   & -1         & -2  & 0   & 1   & 0   \\
		a_4 & -1  & 1          & -1  & 0   & 0   & 1   \\
		\hline
		    & -5  & \delta     & -2  & -1  & 0   & 2
	\end{array}\Rightarrow
	\begin{array}{r|c|cccc}
		    & a_2        & e_1 & e_2         & e_3          & e_4          \\
		\hline
		a_1 & 6          & 3   & 1           & 0            & -2           \\
		a_3 & -5         & -2  & -1          & 0            & 2            \\
		e_3 & \boxed{-4} & -4  & -1          & 1            & 2            \\
		a_4 & 4          & 1   & 1           & 0            & -1           \\
		\hline
		    & \delta     & 1   & \frac{1}{4} & -\frac{1}{4} & -\frac{1}{2}
	\end{array}\Rightarrow
	\\
	\begin{array}{r|cccc}
		    & e_1 & e_2          & e_3          & e_4          \\
		\hline
		a_1 & -3  & -\frac{1}{2} & \frac{3}{2}  & 1            \\
		a_3 & 3   & \frac{1}{4}  & -\frac{5}{4} & -\frac{1}{2} \\
		a_2 & 1   & \frac{1}{4}  & -\frac{1}{4} & -\frac{1}{2} \\
		a_4 & -3  & 0            & 1            & 1
	\end{array}\Rightarrow
	\begin{array}{r|cccc}
		    & e_1 & e_2          & e_3          & e_4          \\
		\hline
		a_1 & -3  & -\frac{1}{2} & \frac{3}{2}  & 1            \\
		a_2 & 1   & \frac{1}{4}  & -\frac{1}{4} & -\frac{1}{2} \\
		a_3 & 3   & \frac{1}{4}  & -\frac{5}{4} & -\frac{1}{2} \\
		a_4 & -3  & 0            & 1            & 1
	\end{array}
\end{multline*}

\section{Lineárisan független rendszerek}
\begin{definition}[lineárisan összefüggő vektorrendszer]\index{lineárisan összefüggő rendszer}
	Egy véges $\left\{ y_1,\ldots,y_n \right\}$ vektorrendszert \emph{lineárisan összefüggőnek}
	mondunk, ha van olyan vektora, amely kifejezhető a többi vektor lineáris kombinációjaként.
\end{definition}
Úgy is fogalmazhatnánk, hogy az $\left\{ y_1,\ldots,y_n \right\}$ rendszer pontosan akkor
lineárisan összefüggő, ha létezik $1\leq k\leq n$ index, amelyre
\[
	y_k\in\lin\left\{ y_1,\ldots,y_{k-1},y_{k+1},\ldots,y_n \right\}.
\]

Nyilvánvaló példa, hogy ha a vektorrendszer a $0$ vektort tartalmazza,
akkor lineárisan összefüggő.

Hasonlóan, ha ugyanaz a vektor többször is szerepel a vektorrendszerben, 
akkor a rendszer szintén lineárisan összefüggő.

Fontos emlékeznünk, hogy 0 db.~vektor lineáris kombinációja megegyezés szerint a vektortér zérus eleme.
Így ha a vektorrendszer egyetlen elemű, például $\left\{ v \right\}$, akkor a $v$-n kívüli vektorok rendszere az üres rendszer, így a $v$-n kívüli elemek lineáris kombinációja egyedül a zérus vektor.
Ezek szerint $\left\{ v \right\}$ pontosan akkor lineárisan összefüggő,
ha $v=0$.
Olyan vektorrendszerre tehát, 
amelyre nem igaz a lineárisan összefüggőség definíciója példa egy egyedüli nem zérus vektor,
de akár az $\left\{  \right\}$ üres rendszer is.

\begin{proposition}
	Legyen $\left\{ y_1,\ldots,y_n \right\}$ vektorrendszer rögzítve a $V$ vektortérben.
	A vektorrendszerre tett alábbi feltevések egymással ekvivalensek.
	\begin{enumerate}
		\item Lineárisan összefüggő;
		\item Van olyan elem a vektortérben, amely nem csak egyféleképpen áll elő mint az $y_1,\ldots,y_n$
		      vektorok lineáris kombinációja,\\
		      azaz:
		      létezik z\in V, amelyre $z=\sum_{j=1}^n\xi_jy_j$ és $z=\sum_{j=1}^n\eta_jy_j$
		      és létezik $1\leq k\leq n$, amelyre $\xi_k\neq\eta_k$.
		\item Vannak olyan nem mind zérus $\alpha_1,\ldots,\alpha_n$ skalárok, amelyekkel
		      \[
			      \sum_{j=1}^n\alpha_jy_j=0,
		      \]
              azaz a vektorrendszernek van nem triviális lineáris kombinációja, amely a zérus vektort eredményezi.
              \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Körben bizonyítunk.
	\begin{description}
		\item[$1.\Rightarrow 2.$]
		      Tegyük fel, hogy $y_k=\sum_{j=1}^{k-1}\eta_jy_j+\sum_{j=k+1}^n\eta_jy_j$.
		      Ekkor az alábbi együttható rendszerek
		      \[
			      \left( \eta_1,\ldots,\eta_{k-1},0,\eta_{k+1},\ldots,\eta_n \right)
			      \qquad
			      \left( 0,\ldots,0,1,0,\ldots,0 \right)
		      \]
		      a $k$-adik helyen biztosan különböznek,
		      hiszen $0\neq 1$,
		      és mind a két együttható rendszerrel képzett lineáris kombináció ugyanazt az $y_k$ vektort eredményezi.
		\item[$2.\Rightarrow 3.$]
		      Világos, hogy
		      \[
			      0=z-z=
			      \sum_{j=1}^n\left( \xi_j-\eta_j \right)y_j
		      \]
		      és a $k$-adik skalár nem zérus.
		\item[$3.\Rightarrow 1.$]
		      Tegyük fel most, hogy
		      \(
		      \sum_{j=1}^n\alpha_jy_j=0,
		      \)
		      és, hogy $\alpha_k\neq 0.$
		      Ekkor
		      \[
			      y_k=\sum_{\substack{=1\\j\neq k}}^n-\frac{1}{\alpha_k}\alpha_jy_j
		      \]
		      azaz a $k$-adik vektor tekinthető mint a többi vektor valamely lineáris kombinációja.
	\end{description}
	Ezt kellett belátni.
\end{proof}
Fontos észrevétel a következő.
\begin{proposition}
	Minden lineárisan összefüggő vektorrendszert tartalmazó vektorrendszer maga is lineárisan összefüggő.
\end{proposition}
\begin{definition}
    Egy nem véges vektorrendszert \emph{lineárisan összefüggőnek} nevezünk,
	ha van véges részrendszere, amely lineárisan összefüggő.
\end{definition}
Persze továbbra is igaz, hogy egy összefüggő rendszert tartalmazó rendszer is összefüggő marad.

A lineárisan összefüggőség a lineáris burok operációval is megfogható:
\begin{proposition}
    Egy vektortér $H$ vektorrendszere pontosan akkor lineárisan összefüggő, ha létezik $y\in H$, amelyre $y\in\lin(H\smallsetminus\left\{ y \right\})$.
\end{proposition}
\begin{proof}
    Ha $H$ lineárisan összefüggő, akkor van $\left\{ x_1,\ldots,x_k \right\}$ véges összefüggő vektorrendszere.
    Itt az egyik vektor előáll mint a többi lineáris kombinációja. 
    Ha ezt a vektort $y$ jelöli, akkor $y\in\lin\left( H\smallsetminus\left\{ y \right\} \right)$.
    Megfordítva, ha valamely $y\in H$ mellett van $\left\{ x_1,\ldots,x_k \right\}\subseteq H\smallsetminus\left\{ y \right\}$ vektorrendszer, amelynek egy lineáris kombinációja éppen $y$, 
    akkor az $\left\{ x_1,\ldots,x_k,y \right\}$ véges rendszer egy lineárisan összefüggő rendszere $H$-nak, ergo $H$ valóban lineárisan összefüggő.
\end{proof}
\begin{definition}[lineárisan független vektorrendszer]\index{lineárisan független rendszer}
	Egy vektorrendszer \emph{lineárisan független}, ha nem lineárisan összefüggő.
\end{definition}
Az előző állítás tagadásával azonnal meg is kapjuk a lineárisan független rendszerek jellemzését
a lineáris burok operáció segítségével.
\begin{proposition}
    Egy vektortér $H$ vektorrendszere pontosan akkor lineárisan független, ha minden $y\in H$ mellet $y\notin\lin(H\smallsetminus\left\{ y \right\})$ is teljesül.
\end{proposition}
Fontos észrevétel a következő:
\begin{proposition}
    Egy lineárisan független rendszer minden részrendszere is lineáris független marad.
\end{proposition}
A definíció szerint egy nem véges vektorrendszer akkor lineárisan független, ha minden véges részrendszere is az.
Egy véges vektorrendszer lineárisan függetlenségét, pedig a következő egymással ekvivalens állítások karakterizálják.
\begin{proposition}
	Legyen $\left\{ y_1,\ldots,y_n \right\}$ vektorrendszer rögzítve a $V$ vektortérben.
	A vektorrendszerre tett alábbi feltevések egymással ekvivalensek.
	\begin{enumerate}
		\item Lineárisan független;
		\item A vektorrendszer lineáris burkában minden elem egyetlen egyféleképpen áll elő,
		      mint az $y_1,\ldots,y_n$ vektorok lineáris kombinációja.
		\item Az $y_1,\ldots,y_n$ vektoroknak csak a \emph{triviális lineáris kombinációja}\index{triviális lineáris kombináció} zérus,
		      azaz
		      \[
			      \sum_{j=1}^n\alpha_jy_j=0\text{ esetén }\alpha_1=\alpha_2=\dots=\alpha_n=0.\qedhere
		      \]
	\end{enumerate}
\end{proposition}
\begin{proof}
	Nyilvánvaló a lineáris összefüggés karakterizációjából.
\end{proof}
Egy a zéró vektort tartalmazó vektorrendszer persze lineárisan összefüggő,
és egy nem zérus vektorból álló egyelemű vektorrendszer lineárisan független.
A következő állítás sokszor teszi kényelmessé a gondolatmenetünket.
\begin{proposition}
	Legyen $\left\{ y_1,\ldots,y_n \right\}$ egy olyan legalább két elemű vektorrendszer,
	amelynek első eleme nem a zérus vektor, tehát $y_1\neq 0$.
	A vektorrendszer pontosan akkor lineárisan összefüggő,
	ha létezik olyan eleme,
	amely pusztán az előző elemek lineárisan kombinációja.

	Formálisabban: akkor és csak akkor,
	ha
	$\exists k\quad 2\leq k\leq n : y_k\in\lin\left\{ y_1,\ldots,y_{k-1} \right\}$
\end{proposition}
\begin{proof}
	Tegyük fel, hogy a vektorrendszer lineárisan összefüggő.
	Ekkor van olyan a zérus vektort eredményező lineáris kombinációja
	\(
	\alpha_1y_1+\dots+\alpha_ny_n=0,
	\)
	ahol nem az összes együttható nulla.
	Legyen $k$ a lineáris kombinációban a legnagyobb nem nulla együttható indexe.
	Világos, hogy $k\neq 1$,
	hiszen $y_1\neq 0$.
	Persze a $k$ feletti együtthatók mind nullák,
	emiatt
	\[
		\alpha_1y_1+\dots+\alpha_ky_k=0.
	\]
	Itt már $\alpha_k\neq 0$, tehát $y_k$ kifejezhető az előző vektorok segítségével:
	\[
		y_k=
		\frac{-1}{\alpha_k}\alpha_1y_1+\dots+\frac{-1}{\alpha_{k-1}}\alpha_{k-1}y_{k-1}.\qedhere
	\]
\end{proof}

A lehető legszűkebb lineárisan független rendszer az $\left\{  \right\}$ üres vektorrendszer,
amelynek egyetlen eleme sincs.
A lehető legbővebb generátorrendszer az egész vektortér.
Ennél sokkal érdekesebb és fontosabb a lineárisan független rendszerek közül a lehető legbővebbet keresni,
és generátorrendszerek közül a lehető legszűkebbet keresni.
\begin{definition}[maximális lineárisan független-- és minimális generátorrendszer]\index{maximális lineárisan független rendszer}\index{minimális generátorrendszer}
	Egy lineárisan független rendszert \emph{maximális lineárisan független rendszernek} nevezünk,
	ha nem lehet bővíteni úgy, hogy lineárisan független maradjon.

	Egy generátorrendszert \emph{minimális generátorrendszernek} mondunk, ha nem lehet szűkíteni úgy,
	hogy generátorrendszer maradjon.
\end{definition}
\begin{proposition}
    Legyen $H$ egy (nem feltétlen véges) vektorrendszer a V vektortérben.
    \begin{enumerate}
        \item Tegyük fel, hogy $y\notin H$ és $H$ lineárisan független.
            \\
            A $H\cup\left\{ y \right\}$ pontosan akkor lineárisan összefüggő, ha $y\in\lin H$.
        \item Tegyük fel, hogy $y\in H$ és $H$ generátorrendszer.
            \\
            A $H\smallsetminus\left\{ y \right\}$ pontosan akkor generátorrendszer, ha $y\in\lin\left( H\smallsetminus \left\{ y \right\} \right)$.
            \qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Voltaképpen négy különböző állítást kell igazolnunk:
    \begin{description}
        \item[$1.\Leftarrow$]
            Ha $y\in\lin H$, akkor $H$-nak van $\left\{ x_1,\ldots,x_k \right\}$ véges részrendszere,
            amelyre az $\left\{ x_1,\ldots,x_k,y \right\}$ vektorrendszer $y$ elemére igaz, hogy a többi elem lineáris kombinációja,
            ergo $H\cup\left\{ y \right\}$ lineárisan összefüggő.
        \item[$1.\Rightarrow$]
            Ha $H\cup\left\{ y \right\}$ lineárisan összefüggő, akkor van $H\cup\left\{ y \right\}$-nak véges részrendszere, amely lineárisan összefüggő.
            Ebben a rendszerben $y$-nak szerepelnie kell hiszen $H$ lineárisan független.
            Azt kapjuk tehát, hogy bizonyos $x_1,\ldots,x_k\in H$ vektorok mellett az $\left\{ x_1,\ldots,x_k,y \right\}$ vektorrendszer lineáris összefüggő.
            No de, $H$ lineárisan független, ezért csak az utolsó vektor, az $y$ vektor lehet az előző vektorok lineáris burkában.
        \item[$2.\Rightarrow$]
            Ha $H\smallsetminus\left\{ y \right\}$ generátorrendszer, akkor minden vektor így az $y$ is eleme a $\lin\left( H\smallsetminus\left\{ y \right\} \right)$ altérnek.
        \item[$2.\Leftarrow$]
            Ha $y\in\lin\left( H\smallsetminus\left\{ y \right\} \right)$, akkor $H\subseteq\lin\left( H\smallsetminus\left\{ y \right\} \right)$.
            Emiatt $\lin H\subseteq\lin\left( \lin\left( H\smallsetminus\left\{ y \right\} \right) \right)\subseteq\lin\left( H\smallsetminus\left\{ y \right\} \right)$.
            Ebből persze $\lin\left( H\smallsetminus\left\{ y \right\} \right)=\lin H$ következik.
            No de, $H$ generátorrendszer, ergo $H\smallsetminus\left\{ y \right\}$ is az.
            \qedhere
    \end{description}
\end{proof}
A előző állítás 1. pontját úgy érdemes értelmeznünk, hogy a $V$ vektortér egy $H\subseteq V$ lineáris független rendszere akkor és csak akkor nem bővíthető a lineárisan függetlenség megtartásával, ha $\lin H=V$, azaz $H$ egy generátorrendszer.
\\
A 2. pont is hasonló: A $H$ generátorrendszer egyetlen eleme sem elhagyható a generátorrendszer tulajdonság megtartásával akkor és csak akkor, ha minden $y\in H$ mellett $y\notin\lin\left( H\smallsetminus\left\{y \right\} \right)$, azaz $H$ egy lineárisan független rendszer.

Bebizonyítottuk tehát az alábbi fontos állítást.
\begin{proposition}
	Egy vektortér egy vektorrendszerére az alábbi feltevések ekvivalensek.
	\begin{enumerate}
		\item A vektorrendszer maximális lineárisan független rendszer.
		\item A vektorrendszer egyszerre lineárisan független és generátorrendszer.
		\item A vektorrendszer minimális generátorrendszer.\qedhere
	\end{enumerate}
    \label{pr:maxmin}
\end{proposition}
\begin{lemma}[független rendszer csere]\label{le:fgtlncsere}
	Legyen $\left\{ y_1,\ldots,y_n \right\}$ egy lineárisan független rendszere valamely vektortérnek,
	és tegyük fel, hogy a tér valamely $x$ vektorára
	\[
		x=\sum_{j=1}^n\xi_jy_j,
	\]
	ahol $\xi_k\neq 0$ az egyik $1\leq k\leq n$ mellett.
	Ekkor $y$ becserélhető a $k$-adik helyen a független rendszerbe
	úgy, hogy az független maradjon, azaz az
	\[
		\left\{ y_1,\ldots,y_{k-1},x,y_{k+1},\ldots,y_n \right\}
	\]
	vektorrendszer is lineárisan független.
\end{lemma}
\begin{proof}
	Megmutatjuk, hogy a cserélt rendszernek egyedül a triviális lineáris kombinációja zérus.
	Legyen tehát
	\[
		0
		=\sum_{\substack{j=1\\j\neq k}}^n\eta_jy_j+\eta_kx
		=\sum_{\substack{j=1\\j\neq k}}^n\eta_jy_j+\sum_{j=1}^n\eta_k\xi_jy_j
		=\sum_{\substack{j=1\\j\neq k}}^n\left( \eta_j+\eta_k\xi_j \right)y_j+\eta_k\xi_ky_k.
	\]
	Mivel az eredeti rendszer lineárisan független,
	ezért az utóbbi lineáris kombináció minden együtthatója a test null eleme.
	A $k$-adikkal kezdve $\eta_k\xi_k=0$, $\xi_k\neq 0$, így $\eta_k=0$.
	Ezt a nem $k$-adik együtthatókba visszahelyettesítve
	már azt kapjuk, hogy $\eta_j=0$ minden $j\neq k$ mellett is.
\end{proof}
\subsection{Mátrixok bázisfaktorizációja 2.}
A szakaszban bevezetett fogalmak illusztrációjaként érdemes visszatérni \apageref{bazisfakt}.~oldalra,
ahol láttuk hogyan találunk generátorrendszert egy mátrix oszlopvektorterében és sorvektorterében.
A kényelmes olvasás okán, ide másolom az eliminációt, annyi változtatással, hogy 
most kiírjuk az eredeti generátorrendszer $e_1,e_2,e_3,e_4$ elemeit is.
Persze itt $e_j\in\mathbb{R}^4$, melynek egyedül a $j$-edik koordinátája 1, a többi 0.
\begin{multline*}
    \begin{array}{r|rrrrr}
         & a_1       & a_2 & a_3 & a_4 & a_5 \\
        \hline
      e_1   & \boxed{1} & 3   & 4   & 5   & -1  \\
      e_2   & -2        & 1   & -1  & 4   & -5  \\
      e_3   & 2         & 1   & 3   & 0   & 3   \\
      e_4   & 3         & 1   & 4   & -1  & 5   \\
        \hline
         & \delta    & 3   & 4   & 5   & -1
    \end{array}
    \implies
    \begin{array}{r|rrrrr}
            & a_1 & a_2       & a_3 & a_4 & a_5 \\
        \hline
        a_1 & 1   & 3         & 4   & 5   & -1  \\
        e_2 & 0   & \boxed{7} & 7   & 14  & -7  \\
        e_3 & 0   & -5        & -5  & -10 & 5   \\
        e_4 & 0   & -8        & -8  & -16 & 8   \\
        \hline
            & 0   & \delta    & 1   & 2   & -1
    \end{array}
    \implies
    \\
    \begin{array}{r|rrrrr}
            & a_1 & a_2 & a_3 & a_4 & a_5 \\
        \hline
        a_1 & 1   & 0   & 1   & -1  & 2   \\
        a_2 & 0   & 1   & 1   & 2   & -1  \\
        e_3 & 0   & 0   & 0   & 0   & 0   \\
        e_4 & 0   & 0   & 0   & 0   & 0
    \end{array}
\end{multline*}
Világos, hogy a kezdő $\left\{ e_1,e_2,e_3,e_4 \right\}$
generátorrendszer egyben lineárisan független rendszer is.
Az első lépésben $a_1$ állt be $e_1$ helyére, 
majd a második lépésben az $a_2$ vektort cseréltük be az $e_2$ helyére.
A független rendszer cseréről szóló \ref{le:fgtlncsere}.~lemmát alkalmazva azt kapjuk, hogy az utolsó táblázat $\left\{ a_1,a_2,e_3,e_4 \right\}$ rendszere is lineárisan független.
No de, lineárisan független rendszer részrendszere is az, 
így az $\left\{ a_1,a_2 \right\}$ oszlopok az oszlopvektortér egy független rendszerét adják.

Ahogy azt korábban is láttuk -- lásd \apageref{bazisfakt}.~oldalt -- $\left\{ a_1,a_2 \right\}$ egy generátorrendszere is az oszlopvektortérnek, így már látjuk is, hogy $\left\{ a_1,a_2 \right\}$
az oszlopvektortér egy két elemű lineárisan független generátorrendszerét alkotja.

Emlékezzünk, hogy az utolsó táblázat nem zérus sorai a sorvektortér egy generátorrendszerét szolgáltatja. 
Ha az $a_j$ oszlopot az $i$-edik sorban cseréljük be, akkor a végső táblázat $j$-edik oszlopának $i$-edik sorában van 1-es egyébként a $j$-edik oszlop többi eleme zérus.
Ebből látszik, hogy a végső táblázat nem zérus sorai a sorvektortér egy lineárisan független generátorrendszerét alkotják.

Összefoglalásként az eliminációból leolvasható az
\[
	\begin{pmatrix}
		1  & 3 & 4  & 5  & -1 \\
		-2 & 1 & -1 & 3  & -5 \\
		2  & 1 & 3  & 0  & 3  \\
		3  & 1 & 4  & -1 & 5
	\end{pmatrix}
	=
	\begin{pmatrix}
		1  & 3 \\
		-2 & 1 \\
		2  & 1 \\
		3  & 1
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		1 & 0 & 1 & -1 & 2  \\
		0 & 1 & 1 & 2  & -1
	\end{pmatrix}
\]
bázisfaktorizáció, ahol a jobboldali első mátrix oszlopai a baloldali $A$ mátrix oszlopvektorterének lineárisan független generátorrendszerét alkotja, 
és a jobboldali második mátrix sorai az $A$ mátrix sorvektorterének lineárisan független generátorrendszerét alkotja.

A fenti bázisfaktorizáció tetszőleges mátrix mellett is végrehajtható, 
így a Gauss\,--\,Jordan-eliminációnak és a két cserélési lemmának -- \ref{le:fgtlncsere} és \ref{le:gencsere} -- az alábbi szép következményét kapjuk:
\begin{proposition}
    Tetszőleges $m\times n$ méretű nem zérus mátrix sorvektorterének és oszlopvektorterének is van azonos elemszámú lineárisan független, generátorrendszere. Ha $r$ e két vektorrendszer közös elemszáma, akkor $1\leq r\leq\min\left\{ m,n \right\}$.
\end{proposition}
Ha az itt elemzett mátrixra gondolunk, akkor felvethető, hogy mi történik, ha az eliminációs algoritmus során a fentitől eltérő pivot elemeket választunk?
Világos, hogy a pivot elemek más-más választása az eredeti mátrix más-más bázisfaktorizációjához vezet. 
A bázisfaktorizáció tehát messze nem egyértelmű.
A probléma, hogy ha adott néhány vektor, akkor mi a kapcsolat a belőlük kiválasztható maximális linárisan független rendszerek közt. 
Igaz-e, hogy a vektorokból kiválasztható maximális lineárisan független rendszerek -- tehát a vektorok lineáris burkában mint altérben maximális lineárisan független rendszereknek -- az elemszáma mindig azonos? Vagy előfordulhat például, hogy egy vektortérnek van egy két elemű és egy három elemű maximális lineárisan független rendszere?
Ami evvel azonos: előfordulhat-e, hogy egy vektortérben van egy 2 elemű és egy 3 elemű lineárisan független, generátorrendszer? 
\chapter{A Steinitz-lemma}\index{Steinitz-lemma}
\scwords Független- és generátorrendszerek elemszáma közti kapcsolat vezet a bázis és a dimenzió fogalmához.
Az derül ki, hogy a lineárisan független, generátorrendszerek elemszáma egy az adott vektortérre jellemző szám,
azaz bármely lineárisan független, generátorrendszernek ugyanannyi eleme van.

A félév legfajsúlyosabb állítása következik, az egyszerű bizonyítása ellenére.
A pontos megfogalmazás előtt emlékezzünk a generátorrendszer cseréjére vonatkozó \aref{le:gencsere}. lemmára.
\begin{SL}\index{Steinitz-lemma}
	Legyenek $n$ és $m$ nem negatív egészek.
	Tegyük fel, hogy az $\left\{ y_1,\ldots,y_n \right\}$ egy lineárisan független rendszer,
	és az
	$\left\{ x_1,\ldots,x_m \right\}$ egy generátorrendszer.
	Ekkor
	\begin{enumerate}
		\item $n\leq m$ és;
		\item az $x_1,\ldots,x_m$ vektorok alkalmas átindexelésével kapott
		      \(
		      \left\{ y_1,\ldots,y_n,x_{n+1},\ldots,x_m \right\}
		      \)
		      vektorrendszer is generátorrendszer.%
		      \footnote{Úgy kell a jelöléseket érteni, hogy az $n=0$, de az $n=m$ eset is lehetséges.
		      Az $n=0$ esetben az $y$-okkal jelölt vektorok egyike sem,
		      míg az $n=m$ esetben az $x$-el jelölt vektorok egyike sem szerepel az
		      \(
		      \left\{ y_1,\ldots,y_n,x_{n+1},\ldots,x_m \right\}
		      \)
		      vektorrendszer elemei közt.}%
		      \qedhere
	\end{enumerate}
	\label{le:Steinitz}
\end{SL}
\begin{proof}
	Legyen $k$ a legnagyobb a $\left\{ 0,\ldots,n \right\}$ egészek közül, amelyre
	\begin{enumerate}
		\item $k\leq m$, és
		\item az $x_1,\ldots,x_m$ vektorok alkalmas átindexelésével az
		      \[
			      \left\{ y_1,\ldots,y_k,x_{k+1},\ldots,x_m \right\}\tag{\dag}
		      \]
		      vektorrendszer is generátorrendszer.
	\end{enumerate}
	Ilyen $k$ biztosan van, hiszen $k=0$ triviálisan jó.
	Összesen azt kell meggondolnunk, hogy $k=n$.
	Ha $k<n$ lenne,
	\begin{itemize}
		\item
		      akkor létezne $y_{k+1}$ vektor.
		      No de, ez az $y_{k+1}$ nem szerepel az $\left\{ y_1,\ldots,y_k \right\}$ lineáris burkában,
		      ami (\dag) generátorrendszer volta miatt csak úgy lehetséges,
		      hogy $k\neq m$, azaz $k<m$, ergo $k+1\leq m$.
		\item
		      \Aref{le:gencsere}. lemma szerint a (\dag) vektorrendszerben az $y_{k+1}$ vektor
		      avval az $x$-el
		      --- a generátorrendszer tulajdonság megtartásával is ---
		      kicserélhető,
		      amely $x$ szerepel az $y_{k+1}$ vektornak a (\dag)-beli
		      vektorokkal képzett lineáris kombinációjában.
	\end{itemize}
	Ez ellentmondás, hiszen $k$ a legnagyobb olyan szám,
	amelyre a bizonyítás elején szereplő 1. és 2. feltételek egyszerre állnak fenn.
\end{proof}
\section{Rang-tétel}
\begin{definition}[mátrix feszítőrangja]\index{feszítőrang}
	Legyen $A\in\mathbb{F}^{n\times m}$ egy tetszőleges nem zérus mátrix.
    Azt mondjuk, hogy \emph{feszítőrangja} $r$, ha $r$ a legkisebb olyan pozitív egész, amelyre $A$ előáll
	\[
		A=BC
	\]
	alakban, ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$.
\end{definition}
Világos, hogy tetszőleges nemzérus négyzetes mátrixra ez jól definiált és $1\leq r \leq \min\{n,m\}$.
A Rang-tételnek a szokásosnál egy kicsit erősebb formája következik.

\begin{proposition}[Rang-tétel]
	Minden nemzérus mátrixban
	a maximális lineárisan független oszloprendszerek
	és a max\-i\-má\-lis lineárisan független sorrendszerek azonos elemszámúak,
	és ez a szám azonos a mátrix feszítőrangjával.
    \label{pr:rang}
\end{proposition}
\begin{proof}
	Jelölje $r$ az $A\in\mathbb{F}^{n\times m}$ mátrix feszítőrangját.
	Legyen $r_c$ a mátrix egyik rögzített maximális lineárisan független oszloprendszerének elemszáma.
	\begin{itemize}
		\item
		      Ezen oszlopokat egy $B\in\mathbb{F}^{n\times r_c}$ mátrixba téve
		      -- a maximalitás miatt -- létezik olyan $C\in\mathbb{F}^{r_c\times m}$ mátrix,
		      amelyre $A=BC$, azaz $r\leq r_c$.
		\item
		      Most tekintsünk egy tetszőleges olyan
		      \(
		      A=BC
		      \)
		      felbontást,
		      ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$.
		      Jelölje $W$ a $B$ mátrix oszlopai lineáris burkát.
		      Az $A$ mátrix fent rögzített maximális lineárisan független oszloprendszere egy lineárisan független rendszer a
		      $W$ vektortérben,
		      és $B$ oszlopai pedig egy generátorrendszer ugyanebben a vektortérben,
		      így a Steinitz-lemma\index{Steinitz-lemma}  szerint $r_c\leq r$.
	\end{itemize}
	Evvel megmutattuk, hogy bármely két maximális lineárisan független oszloprendszer azonos elemszámú, és számuk megegyezik a mátrix feszítőrangjával.


	Legyen $r_w$ az $A$ mátrix egyik rögzített maximális lineárisan független sorrendszerének elemszáma.
	\begin{itemize}
		\item
		      Ezen sorokat egy $C\in\mathbb{F}^{r_w\times m}$ mátrixba téve
		      -- a maximalitás miatt -- létezik olyan $B\in\mathbb{F}^{n\times r_w}$ mátrix,
		      amelyre $A=BC$, azaz $r\leq r_w$.
		\item
		      Most tekintsünk egy tetszőleges olyan
		      \(
		      A=BC
		      \)
		      felbontást,
		      ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$.
		      Jelölje most $V$ a $C$ mátrix sorai lineáris burkát.
		      Az $A$ mátrix fent rögzített maximális lineárisan független sorrendszere egy lineárisan független rendszer e
		      $V$ vektortérben,
		      és $C$ sorai pedig egy generátorrendszert alkotnak ugyanebben a $V$ vektortérben,
		      így a Steinitz-lemma szerint $r_w\leq r$.
	\end{itemize}
	Evvel azt is megmutattuk,
	hogy bármely két maximális lineárisan független sorrendszer azonos elemszámú,
	és számuk megegyezik a mátrix feszítőrangjával.
\end{proof}
Ha $A\in\mathbb{F}^{n\times m}$ mátrix, amelynek feszítőrangja $r$,
akkor $A$ bármelyik maximális lineárisan független oszloprendszerének $r$ az elemszáma.
Ezeket az oszlopokat egy $B\in\mathbb{F}^{n\times r}$ mátrixba rendezve az $A$ oszlopvektortének
egy generátorrendszerét kapjuk,
így létezik $C\in\mathbb{F}^{r\times m}$ mátrix, amelyre $A=BC$ teljesül.
Mivel két mátrix szorzata diádok összegeként is felírható, \index{diad@diád}
úgy azt kaptuk, hogy egy $r$ feszítőrangú mátrix felírható mint $r$ darab diád,
tehát 1 rangú mátrix, összegeként.
Itt a diádokat alkotó oszlop rendszer és sorrendszer is lineárisan független.
A következő gondolat szerint több is igaz.
\begin{proposition}
	Tegyük fel, hogy az $A\in\mathbb{F}^{n\times m}$ nem zérus mátrix, 
    amelynek feszítőrangja $r$ előáll
	\[
		A=\sum_{j=1}^rb_j\cdot c_j
	\]
	$r$ darab diád összegének alakjában,
    ahol $b_j\in\mathbb{F}^{n\times 1}$ oszlopvektorok és $c_j\in\mathbb{F}^{1\times m}$ sorvektorok.
	Ekkor a diádok oszlopaiból illetve soraiból alkotott
	\[
		\left\{ b_j:j=1,\ldots,r \right\} \text{ és }
		\left\{ c_j:j=1,\ldots,r \right\}
	\]
	rendszerek lineárisan független rendszereket alkotnak.
\end{proposition}
\begin{proof}
	Legyen a $B$ mátrix $j$-edik oszlopa $b_j$ és a $C$ mátrix $j$-edik sora $c_j$.
	Világos, hogy $B\in\mathbb{F}^{n\times r}, C\in\mathbb{F}^{r\times m}$ és
	$A=BC$.
	Ha $B$ oszlopai nem alkotnának lineárisan független rendszert, akkor
	lenne $B=B_1B_2$ előállítás,
	ahol $B_1\in\mathbb{F}^{n\times s}, B_2\in\mathbb{F}^{s\times r}$,
	ahol $s<r$.
	Ekkor persze
	\[
		A=BC=\left( B_1B_2 \right)C=B_1\left( B_2C \right)
	\]
	Itt $B_1$ mátrixnak $s$ oszlopa van, a
	$B_2C$ mátrixnak $s$ sora van,
	ami ellentomond annak, hogy $A$ feszítőrangja $r$.
	A sorrendszer lineárisan függetlensége is hasonlóan adódik.
\end{proof}

\section{Dimenzió}
A Steinitz-lemma következményeképpen:\index{Steinitz-lemma}
\begin{corollary}
	Egy vektortérben bármely két véges, lineárisan független, generátorrendszer elemszáma azonos.
	Konkrétabban, ha
	\[
		\left\{ x_1,\ldots,x_m \right\} \text{ és } \left\{ y_1,\ldots,y_n \right\}
	\]
	lineárisan független generátorrendszerek, akkor $n=m$.
	\label{co:baziselemszam}
\end{corollary}
\begin{definition}[végesen generált vektortér]\index{végesen generált vektortér}
	Egy vektorteret \emph{végesen generáltnak} nevezünk,
	ha létezik véges elemszámú generátorrendszere.
\end{definition}
Teljesen világos, hogy ha van egy vektortérben véges generátorrendszer,
akkor van minimális generátorrendszer is, azaz van a térben lineárisan független generátorrendszer.
Ezt rögzítjük a következőekben.
\begin{proposition}
	Minden végesen generált vektortérnek van olyan vektorrendszere,
	amely egyszerre lineárisan független és generátorrendszer.
	\label{pr:bazisletezik}
\end{proposition}
\begin{proof}
	Tekintsünk egy véges generátorrendszert.
    Világos, hogy ennek van minimális generátorrendszere,
    hiszen ha egyik eleme sem hagyható el, úgy hogy generátorrendszer maradjon, akkor minimális
    generátorrendszerrel állunk szemben. 
    Véges sok elemből indulunk ki, 
    így véges sok elem esetleges eldobása után egy minimális generátorrendszert kapunk.

    Láttuk korábban (\ref{pr:maxmin}. állítás), hogy egy generátorrendszer minimalitása éppen a rendszer lineárisan függetlenségét jelenti.
\end{proof}
A lineárisan független generátorrendszerek olyan sűrűn fordulnak elő a tárgyalásban,
hogy rövidebb külön nevet adni nekik.
\begin{definition}[bázis]\index{baa@bázis}
	Egy vektorrendszert \emph{bázisnak} nevezünk, ha ez egyszerre lineárisan független és generátorrendszer.
\end{definition}
\Aref{pr:bazisletezik}. állítást tehát úgy fogalmazhatjuk, hogy végesen generált vektortérnek van bázisa,
és hasonlóan \aref{co:baziselemszam}. következmény pedig azt jelenti,
hogy egy vektortérben bármely két bázis azonos elemszámú.
Ez utóbbi tény ad értelmet a következő definíciónak:
\begin{proposition}
	Egy végesen generált vektortérről azt mondjuk, hogy $n$ dimenziós, vagy $n$ a dimenzió száma,
	ha a vektortérben van $n$ elemű bázis.
\end{proposition}
A tényt, hogy a $V$ vektortér $n$ dimenziós $\dim\left( V \right)=n$ módon jelöljük.

Fontos látni, hogy éppen azt gondoltuk meg, hogy \emph{minden végesen generált vektortérben van bázis},
\footnote{Ez nem végesen generált vektorterekre is igaz, de itt nem igazoljuk,
	viszont később sem használjuk.}
és \emph{bármely két bázis pontosan annyi vektorból áll mint a tér dimenziója.}
A végesen generált vektortereket sokszor szinonimaként \emph{véges dimenziósnak} is mondjuk.\index{véges dimenziós}

A következő állítás az eddigiek összefoglalása:
\begin{proposition}
	Tekintsünk egy $m$-dimenziós vektorteret, és abban egy $m$-elemű
	$\left\{ x_1,\ldots,x_m \right\}$
	vektorrendszert.
	E vektorrendszerre tett alábbi feltevések ekvivalensek.
	\begin{enumerate}
		\item Lineárisan független;
		\item Maximális lineárisan független rendszer;
		\item Generátorrendszer;
		\item Minimális generátorrendszer;
		\item Bázis.\qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Az első négy feltétel ekvivalenciájával kezdünk.
	\begin{itemize}
		\item[1.\Rightarrow 2.]
		      Mivel a tér $m$-dimenziós, ezért van $m$-elemű generátorrendszere,
		      így a Steinitz-lemma szerint nincs $m$-nél több elemet tartalmazó lineárisan független
		      rendszere, ergo bármely $m$ elemet tartalmazó lineárisan független rendszer maximális is.
		\item[2.\Rightarrow 3.]
		      Láttuk korábban.
		\item[3.\Rightarrow 4.]
		      Mivel a tér $m$-dimenziós, ezért van $m$-elemű lineárisan független rendszere,
		      így a Steinitz-lemma szerint nincs $m$-nél kevesebb elemet tartalmazó generátorrendszere,
		      ergo bármely $m$ elemet tartalmazó generátorrendszer rendszer minimális is.
		\item[4.\Rightarrow 1.]
		      Láttuk korábban.
	\end{itemize}

	Az első négy feltétel tehát ugyanazt jelenti.
	Így ha 1.-et feltesszük, akkor 3. is fennáll, ami azt jelenti, hogy 1. feltétel és 5. feltétel is ekvivalensek.
\end{proof}\index{Steinitz-lemma}
A Steinitz-lemma kulcs szerepet játszott dimenzió fogalmának megértésében,
hiszen a bázis elemszáma nem lehetne a tér dimenziója, anélkül hogy tudnánk a tényt:
bármely két bázis azonos elemszámú!
Márpedig egy vektortérben nagyon sok bázis van.
A Steinitz-lemma 2. pontja segít ennek megértéséhez.

\begin{proposition}
	Egy végesen generált vektortér bármely lineárisan független rendszere kiegészíthető bázissá.
	\label{pr:lfgtenbazissa}
\end{proposition}
\begin{proof}
	Tegyük fel, hogy a tér $m$ dimenziós, ami azt jelenti, hogy van
	\[
		\left\{ x_1,\ldots,x_m \right\}
	\]
	$m$ elemű lineárisan független generátorrendszere.
	Legyen $\left\{ y_1,\ldots,y_n \right\}$ egy lineárisan független.
	A Steinitz-lemma szerint ez a rendszer kiterjeszthető egy
	\[
		\left\{ y_1,\ldots,y_n,x_{n+1},\ldots,x_m \right\}
	\]
	generátorrendszerré, ami persze bázis is.
	Ezt kellett belátni.
\end{proof}

Meggondoltuk tehát, hogy bármely véges generátorrendszerből elhagyható néhány elem úgy,
hogy a rendszer lineárisan független generátorrendszerré váljon,
és hasonlóan bármely lineárisan független rendszerhez, hozzátehető néhány elem úgy, hogy a
rendszer lineárisan független generátorrendszerré váljon.

A független rendszerek cseréjéről (\ref{le:fgtlncsere}.) és a generátor rendszerek cseréjéről (\ref{le:gencsere}.) szóló lemmák
együttes alkalmazásaként kapjuk a következő lemmát.

\begin{lemma}[bázis csere]\label{le:baziscsere}
	Legyen $\left\{ x_1,\ldots,x_m \right\}$ egy bázisa valamely vektortérnek,
	és tegyük fel,
	hogy egy $y$ vektorra
	\[
		y=\sum_{j=1}^m\eta_jx_j,
	\]
	ahol $\eta_k\neq 0$ valamely $1\leq k\leq m$ mellett.
	Ekkor $y$ becserélhető a $k$-adik helyen a bázisba,
	úgy hogy az is bázis maradjon, azaz az
	\[
		\left\{ x_1,\ldots,x_{k-1},y,x_{k+1},\ldots,x_m \right\}
	\]
	vektorrendszer is egy bázis.
\end{lemma}

\begin{proposition}
	Egy végesen generált $V$ vektortér minden $M\subseteq V$ altere is végesen generált.
    Ekkor $\dim(M)\leq \dim(V)$ és egyenlőség csak $V=M$ esetben lehetséges.
\end{proposition}
\begin{proof}
	Mivel $V$-ben nincs tetszőleges nagy lineárisan független rendszer,
	ezért $M$-ben van véges elemszámú maximális lineárisan független rendszer.
	Erről tudjuk, hogy egyben generátorrendszer is.

	Van tehát $V$-ben is $M$-ben is bázis.
	Mivel az $M$-beli bázis egyben lineárisan független rendszer,
	ezért a Steinitz-lemma\index{Steinitz-lemma} szerint 
    elemszáma nem nagyobb mint a rögzített bázis mint generátorrendszer elemszáma,
    azaz $\dim(M)\leq \dim(V).$

    Ha $\dim(M)=\dim(V)=n$, akkor $M$ az altér egy $\left\{ x_1,\ldots,x_n \right\}$ bázisa olyan lineárisan független rendszer $V$-ben, 
    amelynek annyi eleme van, mint a $V$ dimenziója.
    Így ez egy $V$-beli generátorrendszer is, ezért 
    $M=\lin\left( \left\{ x_1,\ldots,x_n \right\}\right)=V. $
\end{proof}
\subsection{Rang és feszítőrang}
\Aref{pr:rang}.~állítást, azaz a Rang-tételt a dimenzió fogalmának ismeretében úgy is fogalmazhatjuk, hogy \emph{tetszőleges nem zérus mátrixra az oszlopvektortér dimenziója egybeesik a feszítőranggal, és a sorvektortér dimenziója is azonos a feszítőranggal.}
\begin{definition}[mátrix rangja]\index{rang}
    Egy mátrix \emph{rangja} az oszlopai által generált vektortér dimenziója.
\end{definition}
A Rang-tétel tehát azt állítja, hogy nem zérus mátrixra a feszítőrang, a rang, azaz az oszlopvektortér dimenziószáma, és a sorvektortér dimenziószáma azonosak.

A zérus mátrixra a feszítőrang fogalmát nem definiáltuk, de az nyilvánvaló, hogy ebben az esetben az oszlopvektortér és a sorvektortér is 0 dimenziós.

Most felejtsük el egy pillanatra, hogy igazoltuk már a Rang-tételt.
A feszítőrang fogalma nélkül, de a dimenzió fogalma ismeretében gondoljuk át a Rang-tétel klasszikus alakját. 
\begin{proposition}
    Tetszőleges mátrixra az oszlopvektortér és a sorvektortér azonos dimenziós alterek.
\end{proposition}
\begin{proof}
    A Gauss\,--\,Jordan-elimináció algoritmusának segítségével $A=B\cdot C$
    a mátrix bázisfaktorizációja, ahol $A$ oszlopainak rendszere a mátrix oszlopvektorterének egy bázisa,
    és $C$ sorainak rendszere a sorvektortér egy bázisa.
    Mivel itt $B$ oszlopainak száma azonos $C$ sorainak számával, ezért az oszlopvektortér és a sorvektortér azonos dimenziós vektorterek.
\footnote{%
Érdemes felfigyelni rá, hogy milyen döbbenetesen szimpla gondolat, de persze a mélyén a dimenzió fogalma, azaz a Steinitz-lemma rejlik.}
\end{proof}% 
Persze ebből az alakból is látszik, hogy az oszlopok közül bárhogyan is választok ki egy maximális lineárisan független rendszert e vektorrendszerek elemszáma azonos lesz.
Más szavakkal: A Gauss\,--\,Jordan-elimináció során, akárhogyan is választok pivot elemeket,
az utolsó táblázatban a nem csupa zérus sorok száma mindig azonos, mégpedig egyenlő a sorvektortér dimenziójának számával, azaz a mátrix rangjával.
\chapter{Koordinátázás}
\scwords A koordináta-tér fogalmát vezetjük be.
Látni fogjuk, hogy véges dimenziós vektortérre ,,lényegében'' az egyetlen példa,
az $\mathbb{F}^n$ tér.
\section{Lineáris operátor fogalma}
\begin{definition}[lineáris operátor]
	Legyenek $V,W$ ugyanazon $\mathbb{F}$ test feletti vektorterek.
	Az $A:V\to W$ függvényt \emph{lineáris operációnak}\index{lineáris operáció} mondjuk,
	ha az
	\begin{displaymath}
		A\left( \alpha x+\beta y \right)=
		\alpha A\left( x \right)+\beta A\left( y \right)
	\end{displaymath}
	azonosság teljesül minden $x,y\in V$ és minden $\alpha,\beta\in\mathbb{F}$ mellett.

	Ha $V=W$, akkor a \emph{lineáris transzformáció}\index{lineáris transzformáció} kifejezést is használjuk,
	ha pedig $W=\mathbb{F}$,
	akkor szokásos még a \emph{lineáris funkcionál}\index{lineáris funkcionál} szó összetétel is.

	A $V\to W$ összes lineáris operációk halmazát $L\left( V,W \right)$ módon jelöljük.
	A $V=W$ esetben a rövidség kedvéért csak $L\left( V \right)$-t írunk.
\end{definition}
Láttuk korábban, hogy ha $A$ egy $m\times n$ méretű mátrix,
akkor $x\in\mathbb{F}^n$ oszlopvektor mellett az
\[
	x\mapsto A\cdot x,\
\]
mátrix szorzás egy $\mathbb{F}^n\to\mathbb{F}^m$ lineáris operációt definiál.
Hasonlóan,
ha most $x\in\mathbb{F}^m$ sorvektort jelöl akkor az
\[
	x\mapsto x\cdot A
\]
szorzás egy $\mathbb{F}^m\to\mathbb{F}^n$ lineáris operációt jelent.

\begin{defprop}
	Legyen $A\in L\left( V,W \right)$ egy lineáris operáció.
	Ennek értékkészlete egy altér,
	amelyet $\im A$ módon jelölünk.
	Azon pontok halmaza,
	amelyeket $A$ operátor a $W$ tér zérus elemébe képez egy alteret alkotnak,
	amelyet $\ker A$ módon jelölünk.

	Az $L\left( V,W \right)$ lineáris operációk halmaza a szokásos függvény műveletekkel vektorteret alkot.
\end{defprop}
A jelölések tehát:
\[
	\im A=\left\{ y\in W:\text{létezik } x\in V, A(x)=y \right\},
	\qquad
	\ker A=
	\left\{ x\in V:A\left( x \right)=0 \right\}.
\]
\begin{proposition}
	Egy lineáris leképezés pontosan akkor injektív, ha $\ker A=\left\{ 0 \right\}$.
	Egy injektív lineáris leképezés inverz függvénye egy $\im A\to V$ lineáris operátor.
	Az $A\in L\left( V,W \right)$ injektív lineáris leképezés inverzét $A^{-1}$ módon jelöljük.

	Lineáris leképezések kompozíciója is lineáris.
\end{proposition}
Tekinthetünk az $A$ mátrixra mint az $x\mapsto A\cdot x$ lineáris operátorra.
Ennek inverze az $x\mapsto A^{-1}\cdot x$ operátor,
ahol $A^{-1}$ az inverz mátrixot jelöli, hiszen
$Ax=y$ pontosan akkor teljesül, ha $x=A^{-1}y$.

Itt jegyezem meg, hogy a tradíciókat követve egy $A$ lineáris operáció esetén az $x$ vektor
$A\left( x \right)$ képét a zárójeleket elhagyva $Ax$ módon írjuk.
Hasonlóan az $A\circ B$ kompozíciót is $AB$ módon jelöljük.

\begin{proposition}
	Legyen $A\in L\left( V,W \right)$ egy lineáris operáció.
	Ekkor
	\begin{enumerate}
		\item ha $A$ injektív és $\left\{ y_1,\ldots,y_m \right\}\subseteq V$
		      lineárisan független,
		      akkor
		      $\left\{ Ay_1,\ldots,Ay_m \right\}$ is lineárisan független.
		\item ha $A$ szürjektív és $\left\{ x_1,\ldots,x_n \right\}\subseteq V$
		      generátorrendszer,
		      akkor
		      $\left\{ Ax_1,\ldots,Ax_n \right\}$ is generátorrendszere $W$-nek.
		\item ha $A$ bijekció és $\left\{ e_1,\ldots,e_n \right\}\subseteq V$
		      bázis,
		      akkor
		      $\left\{ Ae_1,\ldots,Ae_n \right\}$ is bázis $W$-ben.
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{definition}[izomorf vektorterek]\index{izomorf vektorterek}\index{izomorfizmus}
	Egy $A:V\to W$ függvényt \emph{izomorfizmusnak}\index{izomorfizmus} mondunk,
	ha $A$ lineáris bijekció.
	Az ugyanazon test feletti $V,W$ vektortereket egymással \emph{izomorf vektortérnek} mondjuk,
	ha létezik $A:V\to W$ izomorfizmus.
\end{definition}
Gondoljuk meg, hogy az azonos test feletti vektorterek izomorfizmusa egy ekvivalencia reláció,
azaz (1) minden vektortér izomorf saját magával; (2) ha $V$ izomorf $W$-vel,
akkor $W$ is $V$-vel; (3) ha $V_1$ és $V_2$ izomorfak, továbbá ha $V_2$ és $V_3$ izomorfak, akkor
$V_1$ és $V_3$ is izomorfak.
\begin{proposition}
	Minden az $\mathbb{F}$ test feletti véges dimenziós $V$ vektortér izomorf az $\mathbb{F}^{\dim(V)}$
	koordináta-térrel.
\end{proposition}
\begin{proof}
	Rögzítsünk egy $\left\{ e_1,\ldots,e_n \right\}$ bázist $V$-ben.
	Definiálja $\Psi:\mathbb{F}^n\to V$ a következő függvényt:
	\[
		\begin{pmatrix}
			\alpha_1 \\ \vdots\\ \alpha_n
		\end{pmatrix}
		\mapsto\sum_{j=1}^n\alpha_je_j
	\]
	Könnyű számolással ellenőrizhető,
	hogy $\Psi$ egy lineáris operáció,
	a bázis lineárisan függetlenségét használva adódik $A$ injektivitása,
	a szürjektivitás pedig a bázis generátorrendszer tulajdonságát használva igazolható.
\end{proof}
\begin{definition}[koordináta]\index{koordináta}
	Legyen $\left\{ e_1,\ldots,e_n \right\}$ egy bázisa a $V$ vektortérnek.
	Tudjuk, hogy minden $v\in V$ vektor egyetlen egyféleképpen, de előáll mint a bázisvektorok egy lineáris kombinációja.
	Ha ez $v=\sum_{j=1}^n\alpha_je_j$, akkor az
	\begin{math}
		\begin{pmatrix}
			\alpha_1 \\ \vdots\\ \alpha_n
		\end{pmatrix}
		\in
		\mathbb{F}^n
	\end{math}
	vektort az $x$ vektor $\left\{ e_1,\ldots,e_n \right\}$ bázisban felírt koordináta-vektorának nevezzük.
\end{definition}
Pont azt mutattuk meg, hogy egy rögzített bázis mellett az a leképezés,
amely egy vektorhoz hozzárendeli a bázisban felírt koordinátáit, egy izomorfizmus
a vektortér és a koordináta-tere közt.
Az eddigiek összefoglalásaként kapjuk az alábbi állítást.
\begin{proposition}
	Legyenek $V$ és $W$ ugyanazon test feletti végesen generált vektorterek.
	E két vektortér pontosan akkor izomorf, ha azonos dimenziósak.
\end{proposition}
\begin{proof}
	Ha $V$ izomorf $W$-vel, akkor létezik köztük izomorfizmus.
	No de, izomorfizmus bázist bázisra visz, ami azt jelenti, hogy azonos elemszámú
	bázisa van mindkét térnek.

	Ha $\dim(V)=\dim(W)=n$, akkor mindkét vektortér izomorf $\mathbb{F}^n$ vektortérrel,
	ergo egymással is izomorfak.
\end{proof}
Fontos látni, hogy a vektor koordináta-vektora függ a bázis megválasztásától.
Akár ha csak a bázisban az elemek sorrendjét megváltoztatjuk,
már akkor is változik a vektort koordináta-vektora.
\Aref{le:gencsere}., \aref{le:fgtlncsere}.  és \aref{le:baziscsere}. lemmák összefoglalásaként kapjuk,
hogy milyen módon változnak egy rögzített vektor koordinátái,
ha a bázisban egy elemet megváltoztatunk.
\begin{lemma}[bázistranszformáció]\label{le:bazistrafo}
	Legyen $\left\{ x_1,\ldots,x_m \right\}$ egy bázisa valamely vektortérnek,
	és tegyük fel,
	hogy egy $y$ olyan vektor
	\(
	y=\sum_{j=1}^m\eta_jx_j,
	\)
	ahol $\eta_k\neq 0$ valamely $1\leq k\leq m$ mellett.
	Láttuk $y$ becserélhető a $k$-adik helyen a bázisba,
	tehát az
	\(
	\left\{ x_1,\ldots,x_{k-1},y,x_{k+1},\ldots,x_m \right\}
	\)
	vektorrendszer is bázis marad.
	Tegyük fel továbbá, hogy $a\in V$ vektor koordinátái ismertek az eredeti bázisban,
	\(
	a=
	\sum_{j=1}^m\alpha_j x_j.
	\)
	Ekkor ugyanez az $a$ vektor az új bázisban kifejezve
	\(
	a=
	\sum_{\substack{j=1,\\j\neq k}}^n\left( \alpha_j-\eta_j\delta \right)x_j
	+\delta y_k,
	\)
	ahol $\delta=\frac{\alpha_k}{\eta_k}$.

	Egy táblázatban összefoglalva:
	\begin{eqnarray}\label{alg:bazistrafo}
		\begin{array}{c|cc}
			       & y              & a                       \\
			\hline
			x_1    & \eta_1         & \alpha_1                \\
			\vdots & \vdots         & \vdots                  \\
			x_k    & \boxed{\eta_k} & \alpha_k                \\
			\vdots & \vdots         & \vdots                  \\
			x_m    & \eta_m         & \alpha_m                \\
			\hline
			       & \delta         & \frac{\alpha_k}{\eta_k}
		\end{array}
		&\implies&
		\begin{array}{c|c}
			                 & a                     \\
			\hline
			x_1              & \alpha_1-\eta_1\delta \\
			\vdots           & \vdots                \\
			y                & \delta                \\
			\vdots           & \vdots                \\
			x_m              & \alpha_m-\eta_m\delta \\
			\hline
			\phantom{\delta} &
		\end{array}
	\end{eqnarray}
\end{lemma}


%%%\part{E-dúr hegedűverseny No. 1, Op. 8, RV 269, ,,La primavera''}
\chapter{Alterek Minkowski-összege és direkt összege}\index{Minkowski-összeg}
\scwords Az alterek struktúráját vizsgáljuk a fejezetben.
\section{Minkowski-összeg}
\begin{definition}\index{Minkowski-összeg}
	Legyen $V$ egy $\mathbb{F}$ test feletti vektortér és $H_1, H_2\subseteq V$ részhalmazok.
	E két halmaz \emph{összegén} vagy \emph{Minkowski-összegén}
	a
	\[
		H_1+H_2=\left\{ a+b:a\in H_1,b\in H_2 \right\}
	\]
	halmazt értjük.
	Hasonlóan, $\alpha\in\mathbb{F}$ szám mellett jelölje
	\[
		\alpha H_1=\left\{ \alpha a:a\in H_1 \right\}
	\]
	az $\alpha$ szám és a $H_1$ halmaz szorzatát.
\end{definition}
Világos, hogy ha például $H_1$ üres, akkor $H_1+H_2$ is, és az $\alpha H_1$ halmazok is üresek.

Az alábbi tulajdonságok, a vektortér axiómák közvetlen következményei.
Azokat a tulajdonságokat foglaljuk össze, amelyeket a vektortér axiómákból meg tudunk menteni
a Minkowski-összegre és a számmal való szorzásra.
\begin{proposition}\label{pr:Minkowski}
	Legyenek $A,B\subseteq V$ a $V$ vektortér részhalmazai,
	továbbá $\alpha,\beta\in\mathbb{F}$ számok.
	Ekkor
	\begin{enumerate}
		\item $A+B=B+A$;
		\item $\left( A+B \right)+C=A+\left( B+C \right)$;
		\item $A+\left\{ 0 \right\}=A$;
		\item $\alpha\left( A+B \right)=\alpha A+\alpha B$;
		\item $\left( \alpha+\beta \right) A\subseteq \alpha A+\beta A$%
		      \footnote{
			      Egy $A\subseteq V$ halmazt \emph{affin halmaznak}\index{affin halmaz}
			      szokás mondani,
			      ha zárt az $1$ összegű lineáris kombinációra
			      (\emph{affin kombinációra}\index{affin kombináció})
			      nézve,
			      tehát ha a $\lambda A+\left( 1-\lambda \right)A\subseteq A$
			      tartalmazás teljesül minden $\lambda\in\mathbb{F}$
			      mellett.
			      Ha $A$ affin halmaz,
			      akkor a fordított egyenlőség is igaz feltéve, hogy $\alpha+\beta\neq 0$.
			      Ugyanis
			      $\alpha A+\beta A
				      =
				      \left( \alpha+\beta \right)\left( \frac{1}{\alpha+\beta}\left( \alpha A+\beta A \right) \right)
				      =
				      \left( \alpha+\beta \right)\left( \frac{\alpha}{\alpha+\beta}A+\frac{\beta}{\alpha+\beta}A \right)
				      \subseteq
				      \left( \alpha+\beta \right)A$.
		      };
		\item $\alpha\left( \beta A \right)=\left( \alpha\beta \right)A$;
		\item $1\cdot A=A$;
	\end{enumerate}
	Igaz továbbá, hogy ha $A\neq \emptyset$, akkor $A+V=V$ és $0\cdot A=\left\{ 0 \right\}$.
\end{proposition}
Ha $A\subseteq V$ legalább két elemű,
akkor $A+\left( -A \right)\neq\left\{ 0 \right\}$, ami azt mutatja hogy a fenti 5. tartalmazás
általában nem teljesülhet egyenlőségre.

Az alábbi állítás csak átfogalmazása az altér definíciójának.
\begin{proposition}
	A $V$ vektortér $N\subseteq V$ részhalmaza pontosan akkor altér $V$ a vektortérben,
	ha
	(a) $N\neq \emptyset$ és az
	(b) $\alpha N+\beta N\subseteq N$ tartalmazás minden $\alpha,\beta\in\mathbb{F}$
	mellett fennáll.
\end{proposition}

Látható, hogy egy affin halmaz eltoltja affin marad,
hiszen ha
$N$ egy affin halmaz és $x\in V$ egy vektor,
akkor
$
	\lambda\left( x+N \right)+\left( 1-\lambda \right)\left( x+N \right)
	=
	x+\lambda N+\left( 1-\lambda \right)N
	\subseteq
	x+N
$.
Mivel egy altér nyilvánvalóan affin halmaz is,
ezért egy altér eltoltja is affin halmaz.
Még speciálisabban egy lineáris egyenletrendszer megoldáshalmaza is fontos példa affin halmazra.

Láttuk korábban, hogy alterek közös része is altér.
Ezt úgy interpretálhatjuk, hogy a legbővebb olyan altér,
amelyet két előre megadott altér tartalmaz ezen alterek halmazelméleti közös része.
Felmerül, hogy mi a legszűkebb olyan altér, amely mindkét előre megadott alteret tartalmazza.
A válasz kézenfekvő, hiszen éppen ez a generált altér fogalma.
Ha $H_1,H_2\subseteq V$ tetszőleges halmazok, akkor
$\lin\left\{ H_1\cup H_2 \right\}$ a $H_1$ és $H_2$ halmazokat tartalmazó legszűkebb altér.
Amennyiben altereket tartalmazó legszűkebb alteret keresünk,
akkor többet is állíthatunk:
\begin{proposition}
	Legyenek $M,N\subseteq V$ alterek.
	Ekkor
    \[
	    \lin(M\cup N)=M+N,
    \]
	tehát alterek Minkowski-összege\index{Minkowski-összeg} is altér, 
    sőt ez az $M$ és az $N$ altereket tartalmazó legszűkebb altér.
\end{proposition}
\begin{proof}
    Először is gondoljuk meg, hogy $M+N$ egy altér.
    Világos, hogy $M\cup N\subseteq M+N$,
    ezért $\lin\left( M\cup N \right)\subseteq \lin\left( M+N \right)=M+N$.
    Másrészt a $\lin\left( M\cup N \right)$ halmazra, 
    mint az $M\cup N$ halmazból képzett összes lineáris kombinációk halmazára gondolva
    látszik, hogy $M+N\subseteq \lin\left( M\cup N \right)$.
\end{proof}

A következő állítás szerint,
ha előre adott két altér, akkor ezek összegének dimenziója mindig azonos
a két alteret tartalmazó legszűkebb altér dimenziójának,
és a két altér által tartalmazott legbővebb altér  dimenziójának összegével.
\begin{proposition}\label{pr:drosszeg2}
	Legyenek $M$ és $N$ a $V$ vektortér végesen generált alterei.
	Ekkor az $M+N$ altér is végesen generált,
	továbbá
	\[
		\dim\left( M+N \right)=\dim(M)+\dim(N)-\dim \left( M\cap N \right).\qedhere
	\]
\end{proposition}
\begin{proof}
	Világos, hogy $M$ egy generátorrendszerének és $N$ egy generátorrendszerének egyesítése
	generátorrendszere az $M+N$ altérnek is.

	Legyen most $\left\{ p_1,\ldots,p_r \right\}$ bázis $M\cap N$-ben.
	Mivel egy lineárisan független rendszer kiegészíthető egy bázissá,
	ezért legyen
	$\left\{ m_1,\ldots,m_s,p_1,\ldots,p_r \right\}$ bázis $M$-ben,
	és
	$\left\{ p_1,\ldots,p_r,n_1,\ldots,n_t \right\}$ bázis $N$-ben,
	ahol $r,s,t$ nem negatív egészek.%
	\footnote{
		Figyeljünk arra, hogy $r=0$ is lehet, ha $M$ és $N$ diszjunktak.
		Ekkor $\left\{  \right\}$ a bázis a $\left\{ 0 \right\}$ altérben,
		azaz nincs egyetlen $p$-sem.
		Az összes $r$ elemet tartalmazó szumma ilyenkor üres, tehát értéke zérus.
	}
	Nyilvánvaló, hogy az
	\[
		\left\{ m_1,\ldots,m_s,p_1,\ldots,p_r,n_1,\ldots,n_t \right\}\tag{\dag}
	\]
	rendszer egy véges generátorrendszere az $M+N$ altérnek.
	Most megmutatjuk,
	hogy a $(\dag)$ rendszer lineárisan független is.
	Legyen ehhez
	\[
		\sum_{j=1}^s\alpha_j m_j+\sum_{j=1}^r\gamma_j p_j+\sum_{j=1}^t\beta_jn_j=0.\tag{\ddag}
	\]
	Ekkor persze
	\(
	\sum_{j=1}^s\alpha_j m_j =
	-\sum_{j=1}^r\gamma_j p_j
	-\sum_{j=1}^t\beta_jn_j
	\in
	M\cap N.
	\)
	Léteznek tehát $\delta_1,\ldots,\delta_r$ számok,
	amelyekre
	\(
	\sum_{j=1}^s\alpha_j m_j=
	\sum_{j=1}^r\delta_j p_j,
	\)
	emiatt a $(\ddag)$ egyenlőség a következő módon alakul:
	\[
		\sum_{j=1}^r\left( \delta_j+\gamma_j \right)p_j+\sum_{j=1}^t\beta_jn_j=0.
	\]
	No de, $\left\{ p_1,\ldots,p_r,n_1,\ldots,n_r \right\}$ lineárisan független,
	ezért itt minden együttható zérus.
	Speciálisan $\beta_j=0$ minden $j=1,\ldots,t$ mellett.
	Ezt $(\ddag)$-be visszaírva kapjuk, hogy
	\[
		\sum_{j=1}^s\alpha_j m_j+\sum_{j=1}^r\gamma_j p_j=0,
	\]
	amiből az $\left\{ m_1,\ldots,m_s,p_1,\ldots,p_r \right\}$ rendszer lineárisan függetlenségét használva kapjuk,
	hogy $\alpha_1=\dots=\alpha_s=\gamma_1=\dots=\gamma_r=0$.


	Megmutattuk tehát, hogy $(\dag)$ vektorrendszer bázisa $M+N$-nek.
	Mivel ennek elemszáma
	\begin{math}
		s+r+t=\left( s+r \right)+\left( r+t \right)-r,
	\end{math}
	ezért készen is vagyunk.
\end{proof}
\section{Direkt összeg}
Szokásos szóhasználat alterek esetén, hogy két alteret \emph{diszjunktnak}\index{diszjunkt alterek}
mondunk,
ha metszetük csak a vektortér zérus elemét tartalmazza.
A következő gondolat nem csak kettő hanem véges sok altérről szól.
Gondoljuk meg, hogy akárhány de véges sok altér összege is altér.
\begin{proposition}
    \label{pr:direktosszegsorrend}
    Legyenek az $M_1,\ldots,M_s\subseteq V$ alterek a $V$ vektortér alterei ($s\geq 2$).
    Az alábbi feltételek ekvivalensek:
    \begin{enumerate}
		\item 
            $\left( \sum_{j=1}^{k-1}M_j \right)\cap M_k
			=
			\left\{ 0 \right\}
            $
		    minden $k=2,\ldots,s$ mellett.
        \item
            A nullvektor csak egyetlen egyféleképpen áll elő, mint $M_j$-beli vektorok összege,
            azaz $\sum_{j=1}^su_j=0, u_j\in M_j$ esetén minden $j=1,\ldots,s$ mellett $u_j=0$ is teljesül.
        \item
            $\left( \sum_{\substack{j=1\\j\neq k}}^sM_j \right)\cap M_k
			=
			\left\{ 0 \right\}
            $
		    minden $k=1,\ldots,s$ mellett.
    \end{enumerate}
    Szokásos szóhasználat, hogy azt mondjuk, hogy az $M_1,\ldots,M_s$ altereknek \emph{értelmezhető a direkt összege},   vagy a \emph{direkt összege értelmes}, ha a fenti feltételek egyike (ezért mindegyike) fennáll.
\end{proposition}
\begin{proof}[Az $1.\Rightarrow 2.$ igazolása]
    Ha $2.$ nem áll fenn, akkor valamely $u_j\in M_j$ vektorokra $\sum_{j=1}^su_j=0$ úgy áll fenn,
    hogy nem minden $u_j=0$.
    Legyen $k$ a legnagyobb olyan $j$ index, amihez nem zérus vektor tartozik.
    Világos, hogy van ilyen index; 
    világos, hogy $u_k\neq 0$, de
    \[
        u_1+\dots+u_k=0;
    \]
    így az is világos, hogy $2\leq k\leq s$.
    Ekkor persze $u_k$ egy nem zérus vektor az $\left( \sum_{j=1}^{k-1}M_j \right)\cap M_k$ altérben.
    Megmutattuk tehát, hogy ha $2.$ nem áll fenn, akkor valamely $k$ mellett $1.$ sem teljesül.
\end{proof}
\begin{proof}[A $2.\Rightarrow 3.$ igazolása]
        Legyen $v_k\in M_k$ olyan, hogy valamely $v_j\in M_j$ vektorokra 
        $v_k=\sum_{\substack{j=1\\j\neq k}}^sv_j$
        teljesül. 
        Ekkor persze 
        $v_k+\sum_{\substack{j=1\\j\neq k}}^s-v_j=0$. 
        No de, a $2.$ feltétel szerint ez csak úgy lehet, ha minden vektor zérus, speciálisan $v_k=0$.
        Az mutattuk meg tehát, hogy minden $k$ mellett a $k$-adik altér diszjunkt az összes többi összegétől.
\end{proof}
\begin{proof}[Végül a $3.\Rightarrow 1.$]
    Nyilvánvalóan következik a
    $\sum_{j=1}^{k-1}M_j
    \subseteq
    \sum_{\substack{j=1\\j\neq k}}^sM_j
    $
    tartalmazásból.
\end{proof}
Ezek szerint a direkt összeg akkor értelmes, ha az alterek mindegyike diszjunkt az összes többi altér direkt összegétől.
Azt gondoltuk meg éppen, hogy ezzel ekvivalens feltevés, 
hogy minden altér diszjunkt csak a sorban előtte álló alterek direkt összegétől.
E második tulajdonság úgy tűnhet mintha függne az alterek sorrendjétől, 
de mivel ekvivalens az első tulajdonsággal,
ezért persze nem függ az alterek sorrendjétől.
Mind a kettő ugyanazt fejezi ki: A zérus vektor csak egyféleképpen áll elő, mint az egyes alterekből vett vektorok összege.

A következő definíció szerint, amennyiben az alterek direkt összege értelmes, úgy a direkt összeg egyszerűen a szóban forgó alterek Minkowski-összege.
\begin{definition}[direkt összeg]
	Legyenek az $M_1,\ldots,M_s\subseteq V$ alterek a $V$ vektortér alterei ($s\geq 2$).
	Azt mondjuk, hogy az $N\subseteq V$ altér az $M_1,\ldots,M_s$ alterek \emph{direkt összege}\index{direkt összeg},
	ha
	\begin{enumerate}
		\item $\sum_{j=1}^sM_j=N$,
		\item $\left( \sum_{\substack{j=1\\j\neq k}}^sM_j \right)\cap M_k
			      =
			      \left\{ 0 \right\}$
		      minden $k=1,\ldots,s$ mellett.
	\end{enumerate}
	Ekkor a $N$-et $N=M_1\oplus\dots\oplus M_s$ módon jelöljük.
\end{definition}
Külön érdemes figyelni az $s=2$, tehát csak két altér direkt összegének esetére.
Ilyenkor a fenti definíció azt jelenti,
hogy az $M_1$ és $M_2$ diszjunkt alterekre $N=M_1+M_2$.

\Aref{pr:direktosszegsorrend}. állításban 3. feltétel előnye, hogy ennek alapján teljesen világos, hogy az
$M_1\oplus\dots\oplus M_s$ direkt összeg nem függ az alterek sorrendjétől.
Az evvel ekivalens 1. feltevés előnye pedig,
hogy ha képeznünk kell az $M_1,\ldots,M_s$ alterek direkt összegét,
akkor rekurzívan járhatunk el:
ha a korábbi lépésben ellenőriztük, hogy az $M_1\oplus\dots\oplus M_{n-1}$ értelmes, akkor
a következő lépésben csak azt kell ellenőriznünk,
hogy $M_n$ diszjunkt az első $n-1$ altér direkt összegétől.
\begin{proposition}
	Legyen az $M_1,\ldots,M_s,N\subseteq V$ a $V$ vektortér altere.
	Az $N=M_1\oplus\dots\oplus M_s$ pontosan akkor teljesül,
	ha
	\begin{enumerate}
		\item minden $k=1,\ldots,s$ mellett $M_k\subseteq N$, és
		\item
            minden $v\in N$ vektorhoz létezik egyetlen $\left\{ v_1,\ldots,v_s \right\}\subseteq V$ vektorrendszer,
            amelyre $v_j\in M_j, j=1,\ldots,s$ és
		      \[
			      v=\sum_{j=1}^sv_j.\qedhere
		      \]
	\end{enumerate}
\end{proposition}
\begin{proof}
	Tegyük fel, hogy $N$ direkt összege az $M_1,\ldots,M_s$ altereknek.
	Minden szóba jövő $k$-ra $M_k\subseteq\sum_{j=1}^sM_j=N.$
	Mivel $N=M_1+\dots+M_s$, ezért minden $v\in N$-hez létezik $v_j\in M_j$, hogy
	$v=v_1+\dots+v_s$.
	Most tegyük fel, hogy $v$ előáll $v=u_1+\dots+u_s$ alakban is,
	ahol $u_j\in M_j$ minden $j=1,\ldots,s$ mellett.
	Ekkor 
	\[
        0=\sum_{\substack{j=1}}^s\left( v_j-u_j \right), \text{ ahol } v_j-u_j\in M_j.
	\]
	No de, a direkt összeg értelmes, ezért ez csak úgy lehet, hogy $v_j=u_j$ áll fenn minden $j=1,\ldots,s$ mellett,
    azaz az előállítás valóban egyértelmű is.

	Megfordítva,
	most azt tegyük fel, hogy a két feltétel fennáll.
    Ekkor $1.$ szerint $\sum_{j=1}^sM_j\subseteq N+\dots+N=N$,
	és $2.$ szerint $N\subseteq M_1+\dots+M_s$,
	amiből már
	\begin{math}
		\sum_{j=1}^sM_j=N
	\end{math}
	következik is.
    \\
    Persze 2. speciálisan az $N$ altér nullvektorára is igaz, ami azt jelenti, hogy az alterek direkt összege értelmes.
\end{proof}
Érdemes észrevenni,
hogy a fenti állítás 2. feltétele nem más mint az alábbi két feltétel együttesének tömör megfogalmazása:
\emph{
	\begin{itemize}
		\item[2a.] $N\subseteq M_1+\dots+M_s$,
		\item[2b.] $\sum_{j=1}^sv_j=0$, $v_j\in M_j$ esetén minden $j=1,\ldots,s$ mellett $v_j=0$.
	\end{itemize}
}

Megint csak a két vektortér speciális esetére figyelve
azt igazoltuk, hogy $N$ pontosan akkor az $M_1$ és $M_2$ direkt összege,
ha $M_1$ és $M_2$ olyan alterei $N$-nek, hogy $N$ minden eleme előáll,
de csak egyféleképpen egy $M_1$ és egy $M_2$-beli vektor összegeként.
\begin{proposition}\label{pr:drosszeg}
	Legyenek az $M_1,\ldots,M_s\subseteq V, (s\geq 2)$ alterek a $V$ vektortér olyan végesen generált alterei,
	amelyekre
	\(
	N=M_1\oplus\dots\oplus M_s.
	\)
	Ekkor
	\(
	\dim(N)=
	\dim(M_1)+\dots+\dim(M_s).
	\)
\end{proposition}
\begin{proof}
	Az $s=2$ eset éppen \aref{pr:drosszeg2}. állítás,
	hiszen a $\left\{ 0 \right\}$ triviális altér nulla dimenziós.
	Most tegyük fel, hogy $s$-nél kisebb számokra igaz az állítás,
	és lássuk be $s$-re. $s>2.$
	Legyen $M=\sum_{j=1}^{s-1}M_j.$
	Világos, hogy $M=M_1\oplus\dots\oplus M_{s-1}$ és $M\oplus M_s=N$.
	A már igazolt $s=2$ eset és az indukciós feltevés szerint
	\[
		\dim(N)
		=
		\dim(M)+\dim(M_s)
		=
		\sum_{j=1}^{s-1}\dim(M_j)+\dim(M_s)
		=
		\sum_{j=1}^{s}\dim(M_j).\qedhere
	\]
\end{proof}
Az állításból azonnal adódik,
hogy ha az $M_1, M_2, \dots,M_s$ alterek bázisait, egy közös vektorrendszerbe tesszük,
akkor az így kapott vektorrendszer az $N$ tér egy bázisává válik.
Világos ugyanis, hogy az egyesített rendszer $N$-nek generátorrendszere,
és az elemeinek száma \aref{pr:drosszeg}.~állítás szerint éppen $\dim(N)$.
Azt kaptuk tehát, hogy ez egy minimális generátorrendszere $N$-nek,
ergo egy bázis.


\section{Direkt kiegészítő}
\begin{proposition}\index{direkt kiegészítő}
	Legyen $V$ egy véges dimenziós vektortér, és $M\subseteq V$ egy altér.
	Ekkor létezik $N\subseteq V$ altér,
	amelyre $M\oplus N=V$.
	Egy ilyen alteret az $M$ altér \emph{direkt kiegészítőjének} nevezünk.
	Az $N$ altér valamennyi direkt kiegészítője egymással izomorf.
\end{proposition}
\begin{proof}
    Vegyünk fel $M$-ben egy $\{m_1,\ldots,m_r\}$ bázist.
    Ez persze $V$-ben is lineárisan független, 
    ezért kiegészíthető a $V$ egy $\left\{ m_1,\ldots,m_r,n_1,\ldots,n_s \right\}$ bázisává.
    Definiálja $N=\lin\left\{ n_1,\ldots,n_s \right\}$.
    
    Ha $x\in N\cap M$, akkor $x$ egyrészt $x=\sum_{j=1}^r\alpha_jm_j$, 
    másrészt $x=\sum_{j=1}^s\beta_jn_j$ alakú.
    Ebből azt kapjuk, hogy
    \[
        \sum_{j=1}^r\alpha_jm_j+\sum_{j=1}^s-\beta_jn_j=0.
    \]
    No de, $V$ fenti bázisa lineárisan független is,
    ezért a kiemelt lineáris kombináció minden együtthatója zérus, ami azt jelenti, hogy $x=0$,
    azaz $M\cap N=\left\{ 0 \right\}$.
    Az $M$ és $N$ alterek direkt összege tehát értelmes.
    Az is világos, hogy ha $x\in V$ egy tetszőleges vektor, akkor valamely $\alpha_1,\ldots,\alpha_r$ és valamely $\beta_1,\ldots,\beta_s$ együtthatók mellett
    \[
        \sum_{j=1}^r\alpha_jm_j+\sum_{j=1}^s\beta_jn_j=x,
    \]
    hiszen a $V$ fenti bázisa egyben $V$ generátorrendszere is.
    Ha itt az első szumma értékét $u$-val jelöljük, a második szumma értékét pedig $v$-vel,
    akkor az $u\in M$ és a $v\in N$ vektorokra $u+v=x$,
    ergo $M+N=V$.
    Ez azt jelenti, hogy konstruáltunk $N\subseteq V$ alteret, amelyre $M\oplus N=V$ teljesül.

    Ha $L\subseteq V$ egy másik olyan altere $V$-nek, amire $M\oplus L=V$ fennáll,
    akkor felhasználva a dimenziók összeadásáról szóló állítást azt kapjuk, hogy
    \[
        \dim(L)=\dim(V)-\dim(M)=\dim(N).
    \]
    Emlékezzünk arra, hogy végesen generált vektorterek izomorf voltának szükséges és elegendő feltétele, hogy a két altér dimenzió száma  azonos legyen.
    Megmutattuk tehát, hogy az $M$ altér bármely direkt kiegészítője egymással izomorf.
\end{proof}
Egy altérnek sok-sok direkt kiegészítője lehet.
Például tekintsük a folytonos függvények $C\left[ 0,1 \right]$ vektorterét $\mathbb{R}$ felett,
és legyen $N$ az az altér, amely azon $f\in C\left[ 0,1 \right]$ függvényeket tartalmazza,
amelyekre $f\left( 0 \right)=0$.
Látható, hogy tetszőleges $g\in C\left[ 0,1 \right]$ függvényre, amelyre $g\left( 0 \right)\neq 0$,
a $g$ által generált egydimenziós altér direkt kiegészítője az $N$ altérnek.%
\footnote{
	Ha véges dimenziós példára vágyunk,
	helyettesítsük $C\left[ 0,1 \right]$-et a legfeljebb $n$-ed fokú
	$\mathbb{R}$ feletti polinomok $n+1$ dimenziós vektorterével.
}
\chapter{Vektortér faktortere}
\scwords A vektortér konstrukciók végéhez érkeztünk.
Láttuk, hogy alterek közösrésze, összege is vektorteret alkot.
A faktortér az utolsó vektortér konstrukciós eljárásunk.
\begin{definition}
	Legyen $V$ egy vektortér és $M\subseteq V$ egy adott altér.
	Definiáljuk a $\sim$ relációt a vektortér elemei felett:
	$x\sim y$ pontosan akkor, ha $x-y\in M$.
\end{definition}
Látható, hogy $\sim$ ekvivalencia reláció,
hiszen
\begin{enumerate}
	\item reflexív,
	      ugyanis $x-x=0\in M$ minden $x\in V$-re,
	\item szimmetrikus,
	      ugyanis $x-y\in M$ mellett $y-x=-\left( x-y \right)\in -M=M$,
	\item tranzitív,
	      ugyanis $x-y\in M$ és $y-z\in M$ esetén $x-z=\left( x-y \right)+\left( y-z \right)\in M+M=M$
\end{enumerate}
Legyen adott $x\in V$ mellett az $x$ elemet tartalmazó ekvivalencia osztály $M_x$,
azaz
\[
	M_x=
	\left\{ u\in V:u\sim x \right\}.
\]
Tudjuk, hogy az összes ekvivalencia osztályok $\left\{ M_x:x\in V \right\}$ halmazrendszere
a $V$ egy partícióját alkotja,
ami azt jelenti, hogy $V=\cup_{x\in V}M_x$;
ha valamely $x,y\in V$ mellett $M_x\cap M_y\neq\emptyset$,
akkor $M_x=M_y$;
és minden $x\in V$ mellett $M_x\neq\emptyset$.

Most azt gondoljuk meg, hogy minden egyes ekvivalencia osztály tekinthető úgy is mint,
bármelyik elemével való eltoltja az $M$ vektortérnek.
Speciálisan az is adódik,
hogy az ekvivalencia osztályok affin\index{affin halmaz} halmazok.
\begin{proposition}
	A fenti jelölések mellett $M_x=u+M$ minden $u\in M_x$ mellett.
	Speciálisan, minden $x\in V$ mellett $M_x=x+M$.
\end{proposition}
\begin{proof}
	Megmutatjuk, hogy $M_x\subseteq u+M$.
	Legyen $v\in M_x$ tetszőleges.
	Ekkor $v\sim x$, $u\sim x$, ezért $v\sim u$.
	Ez azt jelenti, hogy $v-u\in M$, amiből már adódik,
	hogy $v\in u+M$.

	Megfordítva, most igazoljuk, hogy $u+M\subseteq M_x$.
	Legyen tehát $v\in u+M$.
	Ekkor $v-u\in M$, ergo $v\sim u$.
	No de, $u\sim x$ is fel van téve,
	emiatt $v\sim x$, azaz $v\in M_x$.
\end{proof}
Most definiálni szeretnénk az ekvivalencia osztályok halmazán összeadás és egy testbeli elemmel való
szorzás műveletet.
Az összeadás művelet legyen a korábban definiált Minkowski--összeg.\index{Minkowski-összeg}
Ez valóban művelet az ekvivalencia osztályokra megszorítva,
hiszen ha $M_{x_1}$ és $M_{x_2}$ két ekvivalencia osztály,
akkor
\[
	M_{x_1}+M_{x_2}=\left( x_1+M \right)+\left( x_2+M \right)
	=
	x_1+x_2+M+M=
	\left( x_1+x_2 \right)+M
	=
	M_{x_1+x_2},\tag{\dag}
\]
azaz két ekvivalencia osztály Minkowski--összege is egy ekvivalencia osztály.
Sőt, az is adódik, hogy
\emph{
	egy-egy vektorhoz tartozó ekvivalencia osztályok Minkowski--összege
	azonos e két vektor összegéhez tartozó ekvivalencia osztállyal.}

A skalárral való szorzás már nem ilyen egyszerű,
hiszen $0\cdot M_x=\left\{ 0 \right\}$, de ez utóbbi halmaz általában nem egy ekvivalencia osztály,
ezért az ekvivalencia osztályok halmazán a skalárral való szorzás nem egy művelet.
\begin{definition}[ekvivalencia osztály számszorosa]
	Legyen $\alpha\in\mathbb{F}$ egy szám és $M_{x}$ egy ekvivalencia osztály.
	Definiálja
	\[
		\alpha\ast M_x=
		\begin{cases}
			\alpha M_x & \text{, ha }\alpha\neq 0 \\
			M          & \text{, ha }\alpha=0.
		\end{cases}\qedhere
	\]
\end{definition}
Egyrészt vegyük észre,
hogy $\alpha\neq 0$ mellett
$\alpha M_x=\alpha\left( x+M \right)=\alpha x+\alpha M=\alpha\cdot x+M=M_{\alpha\cdot x}$,
másrészt azt lássuk,
hogy az $\alpha=0$ esetben
$M=0+M=M_0=M_{0\cdot x}$, ami azt jelenti, hogy a fenti definícióra az
\[
	\alpha\ast M_x=M_{\alpha\cdot x}\tag{\ddag}
\]
azonosság is teljesül minden $x\in V$ vektorra és minden $\alpha\in\mathbb{F}$ számra.
Kiderült tehát, hogy a most bevezetett $\ast$ skalárral való szorzást alkalmazva
egy ekvivalencia osztálynak egy számmal való szorzata egy ekvivalencia osztály lesz,
sőt
\emph{egy vektorhoz tartozó ekvivalencia osztály $\alpha$ szorosa azonos e vektor $\alpha$-szorosához tartozó
	ekvivalencia osztállyal}.
\begin{defprop}[faktortér]
	Legyen $V$ egy az $\mathbb{F}$ test feletti vektortér,
	és $M\subseteq V$ egy rögzített altér.
	Jelölje $\sim$ azt $V$ vektortér elemein értelmezett relációt,
	amelyre $x\sim y$, akkor és csak akkor, ha $x-y\in M$.
	Láttuk, hogy ez ekvivalencia reláció.
	Jelölje
	\[
		V/M=\left\{ M_x:x\in V \right\}
	\]
	az ekvivalencia osztályok halmazát.
	Definiáljuk a $V/M$ halmazrendszer elemei mint halmazok közt az összeadás nevű műveletet mint a halmazok 
	Minkowski--összegét;
    és a skalárral való szorzást műveletet mint a $\ast$ fent bevezetett szorzást.
    (Láttuk, hogy az
	\[
		M_{x_1}+M_{x_2}=M_{x_1+x_2},
		\qquad
		\text{és}
		\qquad
		\alpha\ast M_x=M_{\alpha\cdot x}
	\]
    azonosságok minden $x_1,x_2,x\in V$ és miden $\alpha\in\mathbb{F}$ mellett fennállnak.)
	Ekkor az itt bevezetett $(V/M,+,\ast)$ struktúra az $\mathbb{F}$ test feletti vektorteret alkot.
	Ezt a vektorteret nevezzük a $V$ vektortér $M$ altere szerinti \emph{faktorterének}\index{faktortér}.
\end{defprop}
\begin{proof}
	Meggondoltuk már, hogy a Minkowski--összeg és a $\ast$ szorzás eredménye egy ekvivalencia osztály.
	Most vegyük sorra a vektortér axiómákat:
	Az
	\begin{enumerate}
		\item[1.] $M_{x_1}+M_{x_{2}}=M_{x_2}+M_{x_1}$;
		\item[2.] $\left( M_{x_1}+M_{x_2} \right)+M_{x_3}=
			      M_{x_1}+\left( M_{x_2}+M_{x_3} \right)$;
	\end{enumerate}
	axiómák tetszőleges halmazokra, nem csak ekvivalencia osztályokra is fennállnak.

	Az ekvivalencia osztályok közt $M_0=M$ neutrális elem,
	hiszen minden $x\in V$ mellett $M_x+M=\left( x+M \right)+M=x+\left( M+M \right)=x+M=M_x$.
	\begin{enumerate}
		\item[3.] Minden $M_x$ ekvivalencia osztályra $M_x+M=M_x$;
		\item[4.] Minden $M_x$ ekvivalencia osztályra $M_x+M_{-x}=M$,\\
		      hiszen
		      $M_x+M_{-x}=M_{x+\left( -x \right)}=M_0=M$,
		      felhasználva a már igazolt $(\dag)$ azonosságot.
	\end{enumerate}
	A $\ast$ szorzás és az összeadás kapcsolatát leíró aximák ellenőrzéséhez használjuk
	$(\dag)$ és $(\ddag)$ azonosságokat:
	\begin{enumerate}
		\item[5.] $\left( \alpha+\beta \right)\ast M_x=\alpha\ast M_x+\beta\ast M_x$,\\
		      hiszen
		      $\left( \alpha+\beta \right)\ast M_x
			      =
			      M_{\left( \alpha+\beta \right)x}
			      =
			      M_{\alpha x+\beta x}
			      =
			      M_{\alpha x}+M_{\beta x}
			      =
			      \alpha\ast M_x+\beta\ast M_x$;
		\item[6.] $\alpha\ast\left( M_{x_1}+M_{x_2} \right)
			      =
			      \alpha\ast M_{x_1}+\alpha\ast M_{x_2}$,\\
		      hiszen
		      $
			      \alpha\ast\left( M_{x_1}+M_{x_2} \right)
			      =
			      \alpha\ast\left( M_{x_1+x_2} \right)
			      =
			      M_{\alpha(x_1+x_2)}
			      =
			      M_{\alpha x_1+\alpha x_2}
			      =
			      M_{\alpha x_1}+M_{\alpha x_2}
			      =
			      \alpha\ast M_{x_1}+\alpha\ast M_{x_2}$;
		\item[7.] $
			      \left( \alpha\beta \right)\ast M_{x}=
			      \alpha\ast\left( \beta\ast M_{x} \right)$,\\
		      hiszen
		      $
			      \left( \alpha\beta \right)\ast M_{x}
			      =
			      M_{\left( \alpha\beta \right)x}
			      =
			      M_{\alpha\left( \beta x \right)}
			      =
			      \alpha\ast M_{\beta x}
			      =
			      \alpha\ast\left( \beta\ast M_x \right)
		      $;
		\item[8.]
		      $
			      1\ast M_{x}=M_{x},
		      $
		      \\hiszen
		      $
			      1\ast M_{x}=M_{1\cdot x}=M_x.
		      $
	\end{enumerate}
	Ezt kellett belátni.
\end{proof}
\section{Izomorfia tételek}
\begin{proposition}
	Legyen $M$ a $V$ vektortér egy altere.
	Definiálja $\varphi:V\to V/M$ az $x\mapsto M_x$ függvényt.
	\begin{enumerate}
		\item Ekkor $\varphi:V\to V/M$ egy szürjektív lineáris operáció.
		\item Legyen most $N$ az $M$ egy direkt kiegészítője,\index{direkt kiegészítő}
		      azaz $M\oplus N=V$.
		      Definiálja $\Phi$ a $\varphi$ megszorítását az $N$ direkt kiegészítőre.
		      Ekkor $\Phi:N\to V/M$ egy izomorfizmus.\qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	1. A ($\dag$) és ($\ddag$) szerint
	$
		\varphi\left( \alpha x_1+\beta x_2 \right)
		=
		M_{\alpha x_1+\beta x_2}
		=
		M_{\alpha x_1}+M_{\beta x_2}
		=
		\alpha\ast M_{x_1}+\beta\ast M_{x_2}
		=
		\alpha\ast\varphi\left( x_1 \right)+\beta\ast\varphi\left( x_2 \right)
	$,
	ami éppen azt jelenti, hogy $\varphi$ egy lineáris operáció.
	Ha $A\in V/M$ egy ekvivalencia osztály, és $x\in A$ tetszőleges eleme,
	akkor $\varphi\left( x \right)=M_x=A$, ergo $\varphi$ szürjekció.

	2. Világos, hogy $\Phi$ lineáris leképezés leszűkítéseként maga is lineáris.

	Most nézzük, hogy $\Phi$ is szürjekció marad.
	Ha $A\in V/M$ egy ekvivalencia osztály,
	akkor létezik $x\in V$, amelyre $\varphi\left( x \right)=A$.
	No de, $x=u+v$ alakú, ahol $u\in M$ és $v\in N$, és $M=M_0$ az $V/M$ vektortér neutrális eleme.
	Így
	$
		A=\varphi\left( x \right)=\varphi\left( u+v \right)=\varphi\left( u \right)+\varphi\left( v \right)
		=M+\varphi\left( v \right)=\varphi\left( v \right).
	$

	Az injektiv tulajdonsághoz legyen $v\in \ker\Phi$,
	azaz $v\in N$, amelyre $\varphi\left( v \right)=M$.
	Ez a $\varphi$ definíciója miatt azt jelenti, hogy $M_v=M$, azaz $v\in M$.
	Azt kaptuk tehát, hogy $v\in N\cap M=\left\{ 0 \right\}$,
	ami csak úgy lehetséges,
	hogy $v=0$.
	Megmutattuk tehát, hogy $\ker\Phi=\left\{ 0 \right\}$,
	ami $\Phi$ injektivitását jelenti.
\end{proof}
Az állítás legelső következménye,
hogy az $M$ altér bármely direkt kiegészítője\index{direkt kiegészítő} izomorf a $V/M$ faktortérrel,
emiatt bármely direkt kiegészítő izomorf bármely direkt kiegészítővel is.
Feltéve, hogy
létezik direkt kiegészítő, izomorfiától eltekintve csak egyetlen egy létezik.
De van-e, mindig direkt kiegészítő?
Véges dimenziós esetben már láttuk az igenlő választ.
Nem véges dimenziós vektorterekre is igaz az állítás, de itt nem igazoljuk.
\begin{definition}[co-dimenzió]\index{co-dimenzió}
	Azt mondjuk, hogy a $V$ vektortér $M$ altere \emph{$k$ co-dimenziós},
	ha létezik $k$-dimenziós direkt kiegészítője $M$-nek,
	azaz létezik olyan $N$ altere $V$-nek, amelyre $M\oplus N=V$
	és $\dim(N)=k$.
	A tényt,
	hogy $M$ altérnek létezik $k$-dimenziós direkt kiegészítője
	$\codim M=k$ módon jelöljük.
\end{definition}
\begin{proposition}
	Legyenek $V,W$ ugyanazon test feletti vektorterek,
	és $A\in L\left( V,W \right)$ egy lineáris operáció.
	Ekkor a
	\(
	V/\ker A
	\)
	faktortér izomorf az $\im A$ vektortérrel.
\end{proposition}
\begin{proof}
	Jelölje $M=\ker A$ a $V$ vektortér alterét.
	Definiálni szeretnénk egy $\Omega:\im A\to V/M$ izomorfizmust.
	Ha $Ax_1=y=Ax_2$, akkor $x_1-x_2\in M$, azaz $x_1\sim x_2$, ergo $M_{x_1}=M_{x_2}$.
	Emiatt $y\in\im A$ mellett
	\[
		y\mapsto M_x, \text{ ahol } Ax=y
	\]
	jól definiálja az $\Omega:\im A\to V/M$ függvényt.

	1. $\Omega$ lineáris operáció:
	Legyen $Ax_1=y_1$ és $Ax_2=y_2$.
	Ekkor
	$A\left( \alpha x_1+\beta x_2 \right)=\alpha y_1+\beta y_2$, így
	\[
		\Omega\left( \alpha y_1+\beta y_2 \right)
		=
		M_{\alpha x_1+\beta x_2}
		=
		\alpha\ast M_{x_1}+\beta\ast M_{x_2}
		=
		\alpha\ast \Omega\left( y_1 \right)+\beta\ast\Omega\left( y_2 \right).
	\]

	2. $\Omega$ szürjekció:
	Ha $H\in V/M$ egy ekvivalencia osztály,
	akkor tetszőleges $x\in H$ mellett legyen $y=Ax$.
	Ekkor
	\[
		\Omega\left( y \right)=M_x=H,
	\]
	amivel azt igazoltuk,
	hogy minden ekvivalencia osztály egy $\im A$-beli vektor $\Omega$ képe.


	3. $\Omega$ injekció:
	Legyen $y\in\ker\Omega$, azaz $y\in W$, amelyre $\Omega\left( y \right)$ a $V/M$ faktortér neutrális eleme,
	azaz
	$\Omega\left( y \right)=M$.
	Ez $\Omega$ definíciója szerint csak úgy lehet, ha $M=M_x$, ahol $Ax=y$.
	No de, $M=M_0$, így $y=A0=0$.
	Megmutattuk tehát, hogy $\ker \Omega=\left\{ 0 \right\}$,
	ami éppen $\Omega$ injektivitása.
\end{proof}
\begin{proposition}
	Tegyük fel, hogy $M$ a $V$ vektortér olyan altere,
	amelynek van véges dimenziós direkt kiegészítője.
	Ekkor a $V/M$ faktortér is végesen generált, és
	\[
		\codim M=\dim(V/M).\qedhere
	\]
\end{proposition}
\begin{proof}
	Válasszunk $N$ véges dimenziós alterét $V$-nek,
	amelyre $V=M\oplus N$.
	Láttuk, hogy $V/M$ és $N$ izomorf vektorterek,
	emiatt $V/M$ is végesen generált, és
	$\dim \left( V/M \right)=\dim(N)=\codim(M)$.
\end{proof}
\begin{proposition}
	Legyenek $V,W$ vektorterek,
	továbbá $A\in L\left( V,W \right)$ egy olyan lineáris operáció,
	amelyre az $\im A\subseteq W$ végesen generált.
	Ekkor a $\ker A$ altérnek van $\im A$-val izomorf direkt kiegészítője, emiatt
	\[
		\codim\left( \ker A \right)=\dim\left( \im A \right).\qedhere
	\]
\end{proposition}
\begin{proof}
	Legyen $\im A$ egy bázisa $\left\{ Ax_1,\ldots,Ax_r \right\}$.
    Ha például $x_i$ előállna, mint a többi $x_j$ vektor lineáris kombinációja,
    akkor az $A$ linearitása miatt $Ax_i$ is előállna ugyanazon együtthatókkal mint a többi $Ax_j$
    lineáris kombinációja.
    Ez azt jelenti, hogy $\left\{ x_1,\ldots,x_r \right\}$ is lineárisan független vektorrendszer.
	Definiálja $N=\lin\left\{ x_1,\ldots,x_r \right\}$.
    Persze $\left\{ x_1,\ldots,x_r \right\}$ egy bázisa $N$-nek,
    ergo $\dim(N)=r$.

	Ha $x\in\ker A\cap N$, akkor valamely $\alpha_j$ skalárokkal
	$x=\sum_{j=1}^r\alpha_jx_j$ és $Ax=0$.
	Így $0=\sum_{j=1}^r\alpha_jAx_j$, ami csak $\alpha_1=\dots=\alpha_r=0$
	esetben lehetséges az $\left\{ Ax_1,\ldots,Ax_r \right\}$ rendszer lineárisan függetlensége szerint.
    Megmutattuk tehát, hogy $\ker A\cap N=\left\{ 0 \right\}$, ergo $\ker A$-nak és $N$-nek értelmes a direkt összege.

	Ha $x\in V$ tetszőleges vektor, akkor $Ax\in\im A$, emiatt
	$Ax$ valamely skalárokkal $Ax=\sum_{j=1}^r\alpha_jAx_j$ alakú.
	Világos, hogy így $x-\sum_{j=1}^r\alpha_jx_j\in\ker A$,
	ami igazolja, hogy $V=\ker A+N$.

	Láttuk tehát, hogy $V=\ker A\oplus N$, azaz $\ker A$-nak valóban találtunk véges dimenziós direkt kiegészítőjét.
	Az is világos, hogy $\codim\left( \ker A \right)=\dim(N)=r=\dim\left( \im A \right)$.
\end{proof}
Magától értetődik a következő két észrevétel.
\begin{proposition}
	Ha $A\in L\left( V,\mathbb{F} \right)$ egy nem zérus lineáris funkcionál a $V$ vektortéren,
	akkor $\ker A$ egy 1 co-dimenziós altere $V$-nek.
\end{proposition}
\begin{proposition}[Rang--defektus-tétel]\index{Rang--defektus-tétel}
	Legyen $V$ egy véges dimenziós vektortér,
	és $W$ egy tetszőleges vektortér,
	továbbá $A\in L\left( V,W \right)$ egy lineáris operáció.
	Ekkor
	$\im A$ is véges dimenziós, és
	\[
		\dim\left( \ker A \right)+\dim\left( \im A \right)=\dim(V).\qedhere
	\]
\end{proposition}
%\chapter*{Rang-tétel mégegszer}
%
%\begin{definition}[rang, oszloprang, sorrang, feszítőrang]\index{vektorrendszer rangja}\index{oszloprang}\index{sorrang}\index{feszítőrang}
%    Egy véges vektorrendszer \emph{rangján} a vektorrendszer generálta altér dimenzióját értjük.
%    Egy mátrix \emph{oszloprangján} a mátrix oszlopai mint vektorrendszer rangját értjük.
%    Egy mátrix \emph{sorrangján} a mátrix sorai mint vektorrendszer rangját értjük.
%    Ha $A\in\mathbb{F}^{n\times m}$ nemzérus mátrix,
%    akkor legkisebb olyan $r$ számot, amelyre
%    létezik $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$ mátrix úgy, hogy 
%    $A=BC$ szorzatfelbontás teljesül,
%    az $A$ mátrix \emph{feszítőrangjának} nevezzük.
%
%    Jelölések: Ha $\left\{ x_1,\ldots,x_m \right\}$ a szóban forgó vektorrendszer, akkor
%    \[
%        \rank\left\{ x_1,\ldots,x_m \right\}=\dim\lin\left\{ x_1,\ldots,x_m \right\}
%    \]
%    Ha $A\in\mathbb{F}^{n\times m}$ egy mátrix,
%    akkor 
%    \[
%        \crank{A}=\rank\left\{ [A]^{j}:j=1,\ldots,m \right\},
%        \quad
%        \rrank{A}=\rank\left\{ [A]_k:k=1,\ldots,n\right\},
%    \]
%    továbbá $\srank{A}$ jelöli a feszítőrangját $A$-nak.
%\end{definition}
%\begin{proposition}(Rang-tétel)\label{pr:rang}
%    Tetszőleges test feletti tetszőleges mátrix mellett, a fent bevezetett három rang-koncepció azonos.
%
%    Formálisabban: Minden $A\in\mathbb{F}^{n\times m}$ mellett
%    \[
%        \crank{A}=\srank{A}=\rrank{A}\qedhere
%    \]
%\end{proposition}
%\begin{proof}
%    Induljunk ki a feszítőrang fogalmából.
%    Legyen $r=\srank{A}$, és 
%    \[
%        A=BC,\tag{\dag}
%    \]
%    ahol $B\in\mathbb{F}^{n\times r},C\in\mathbb{F}^{r\times m}$.
%    Azt mutatjuk meg, hogy ekkor $B$ oszloprendszere minimális generátorrendszere $A$ oszlop-vektorterének
%    és $C$ sorrendszere minimális generátorrendszere $A$ sor-vektorterének.
%
%    Vegyük észre, hogy $\srank{A}\leq \crank{A}$.
%    Ugyanis ha az $A$ mátrix oszlop-vektorterének veszünk egy tetszőleges választott
%    $b_1,\ldots,b_k$ generátorrendszerét, 
%    akkor van $B\in\mathbb{F}^{n\times k}$ és $C\in\mathbb{F}^{k\times m}$ mátrix, hogy $A=BC$.
%    Az $r=\srank{A}$ szám az ilyen $k$ számok legkisebbike, tehát valóban $r\leq\crank{A}$.
%    \\
%    Most tekintsük a (\dag)-ben rögzített szorzatot.
%    Jelölje $W$ a $B$ mátrix és $V$ az $A$ mátrix oszlopvektorterét.
%    Mivel $BC$ oszlopai $B$ oszlopainak lineáris kombinációja, 
%    ezért $A$ minden oszlopa beleesik $W$-be, 
%    így az $A$ oszlopainak lineáris burka is részhalmaza $W$-nek,
%    azaz 
%    \[
%        V\subseteq W.
%    \]
%    A $B$ mátrixnak $r$ darab oszlopa van, 
%    tehát $\dim W\leq r$.
%    Látjuk tehát, hogy 
%    \[\dim W\leq r\leq\crank{A}=\dim V,
%    \]
%    ami csak úgy lehetséges, 
%    hogy $V=W$.
%    A $B$ oszlopai tehát $V$-nek is generátorrendszerét alkotják,
%    és $r$ minimalitása szerint egy elem sem elhagyható a generátorrendszer tulajdonság
%    elvesztése nélkül.
%
%    A sorokra vonatkozó indoklás analóg.
%    Először is $\srank{A}\leq \rrank{A}$.
%    Ugyanis ha az $A$ mátrix sorvektorterének veszünk egy tetszőleges választott
%    $c_1,\ldots,c_k$ generátorrendszerét, 
%    akkor van $B\in\mathbb{F}^{n\times k}$ és $C\in\mathbb{F}^{k\times m}$ mátrix, hogy $A=BC$.
%    Az $r=\srank{A}$ szám az ilyen $k$ számok legkisebbike, tehát valóban $r\leq\rrank{A}$.
%    \\
%    Most tekintsük a (\dag)-ben rögzített szorzatot.
%    Jelölje $W$ a $C$ mátrix és $V$ az $A$ mátrix sorvektorterét.
%    Mivel $BC$ sorai $C$ sorainak lineáris kombinációja, 
%    ezért $A$ minden sora $W$-be esik,
%    így az $A$ sorainak lineáris burka is részhalmaza $W$-nek,
%    azaz 
%    \[
%        V\subseteq W.
%    \]
%    A $C$ mátrixnak $r$ sora van, 
%    tehát $\dim W\leq r$.
%    Látjuk tehát, hogy 
%    \[\dim W\leq r\leq\crank{A}=\dim V,
%    \]
%    ami csak úgy lehetséges, 
%    hogy $V=W$.
%    A $C$ oszlopai tehát $V$-nek is generátorrendszerét alkotják,
%    és $r$ minimalitása szerint egy elem sem elhagyható a generátorrendszer tulajdonság
%    elvesztése nélkül.
%
%    Ezt kellett belátni. 
%\end{proof}
%\begin{definition}[mátrix rangja]\index{matrix@mátrix rangja}
%    Mivel a sorrang, az oszloprang, a feszítőrang minden mátrix mellett azonos,
%    ezért a továbbiakban a közös értékre a \emph{rang} szót is használjuk.\footnote{Lásd: \parencite{Wardlaw2005}}
%\end{definition}
%\begin{note}
%    Érdemes a Rang-tétel következő összegzését megjegyezni.
%    Legyen $A\in\mathbb{F}^{n\times m}$ mátrix, amelynek $r$ a rangja.
%    Ekkor létezik $A=BC$ felbontása, ahol $B\in\mathbb{F}^{n\times r},C\in\mathbb{F}^{r\times m}$.
%    Ez a felbontás persze nem egyértelmű, hiszen $A$ oszlop-vektorterének nagyon sok bázisa van.
%    Viszont minden ilyen felbontásban $B$ oszloprendszere az $A$ oszlop-vektorterének, 
%    míg $C$ sorrendszere az $A$ sorvektorterének minimális generátorrendszerét, ergo bázisát alkotja.
%\end{note}
%Következményképpen érdemes meggondolni a mátrix és inverzének felcserélhetőségére vezető állítást.
%\begin{proposition}
%    Legyenek $A,B\in\mathbb{F}^{n\times n}$ négyzetes mátrixok, amelyekre AB=I.
%    Ekkor BA=I is teljesül.
%\end{proposition}
%\begin{proof}
%    Az identitás mátrix rangja nyilván $n$.
%    E mátrix feszítőrangjának definíciójára gondolva, 
%    az előző megjegyzés szerint $A$ oszlopai $\mathbb{F}^n$ lineárisan független rendszerét alkotják.
%    Vegyük észre, hogy a mátrix szorzás asszociativitását is kihasználva
%    \[
%        A\left( BA-I \right)=A\left( BA \right)-AI=\left( AB \right)A-AI=IA-AI=A-A=0.
%    \]
%    Na most, 
%    ha $BA\neq I$ lenne, 
%    akkor a $BA-I$ mátrixnak lenne egy olyan nem zérus oszlopa, 
%    melynek elemeivel mint együtthatókkal képzett lineáris kombinációja az $A$ oszlopainak 
%    a zéró vektort eredményezi.
%    Ez persze ellentmond az $A$ oszloprendszer lineáris függetlenségének,
%    tehát $BA=I$ valóban fennáll.%
%    \footnote{%
%        A feszítőrang fogalmának ismerete nélküli -- talán még elemibb -- bizonyítás: \parencite{doi:10.4169/college.math.j.48.5.366}%
%    }%
%\end{proof}
%\begin{defprop}[invertálható mátrix]
%    Legyen $A\in\mathbb{F}^{n\times n}$ egy négyzetes mátrix.
%    Az alábbi feltételek egymással ekvivalensek.
%    \begin{enumerate}
%        \item Van egyetlen olyan $B\in\mathbb{F}^{n\times n}$ mátrix,
%            amelyre $AB=I$,
%        \item Van olyan $B\in\mathbb{F}^{n\times n}$ mátrix,
%            amelyre $AB=I$,
%        \item Van egyetlen olyan $B\in\mathbb{F}^{n\times n}$ mátrix,
%            amelyre $BA=I$,
%        \item Van olyan $B\in\mathbb{F}^{n\times n}$ mátrix,
%            amelyre $BA=I$,
%        \item $\rank A=n$,
%        \item $A$ oszlopai lineárisan független rendszer alkotnak,
%        \item $A$ sorai lineárisan független rendszert alkotnak.
%    \end{enumerate}
%    Ha a fenti feltételek egyike (ergo mindegyike) fennáll, 
%    akkor azt mondjuk, hogy $A$ mátrix \emph{invertálható}\index{invertálható mátrix}.
%    Szinonimaként használjuk még a \emph{nemszinguláris}\index{nemszinguláris mátrix}, 
%    vagy az \emph{reguláris}\index{reguláris mátrix} szavakat.
%    Ha egy mátrix nem invertálható, akkor \emph{szingulárisnak}\index{szinguláris mátrix} nevezzük.
%
%    Egy invertálható négyzetes mátrix esetén azt az egyetlen $B$ mátrixot, amelyre
%    \(
%    AB=I
%    \)
%    fennáll az $A$ inverzének mondjuk, és $A^{-1}=B$-vel jelöljük.
%    Világos, hogy
%    \[
%        AA^{-1}=I=A^{-1}A,\quad (A^{-1})^{-1}=A.\qedhere
%    \]
%\end{defprop}
%\begin{proof}
%    A 2., 4., 5., 6., 7. állítások ekvivalenciája nyilvánvaló az előzőek szerint.
%    Ha $AB=I=AC$, akkor $A\left( B-C \right)=0$ így $A$ oszloprendszere lineáris függetlensége 
%    miatt $B=C$. 
%    Ezzel $2.\Rightarrow 1.$ implikációt is beláttuk.
%    Az 1. és 3. feltevések ekvivalenciája az előző állítás miatt teljesül.
%\end{proof}
%\chapter*{Elemi fogalmak mégegyszer}
%Tegyük fel, hogy, hogy ismerjük az alábbi fogalmakat:
%\begin{enumerate}
%    \item Vektorrendszer lineáris függetlensége;
%    \item Altér, lineáris burok, generátorrendszer;
%    \item Mátrix szorzás,
%    \item Gauss\,--\,Jordan-elimináció.\index{Gauss\,--\,Jordan-elimináció}  Pontosan azt tesszük fel, hogy ha $Q$ egy négyzetes mátrix, melynek oszlopai
%        lineárisan függetlenek, akkor az elemi sor műveletekkel az identikus mátrixszá transzformálható.
%        Mivel az elemi sorműveletek, bal-szorzások alkalmas mátrixszal, 
%        azt kapjuk, hogy létezik $P$ mátrix, amelyre $PQ=I$.
%\end{enumerate}
%Amit határozottan kerülünk, és azt tesszük fel, hogy nem ismerjük,
%\begin{enumerate}
%    \item Bázisok fogalma,
%    \item Különböző bázisok azonos számossága,
%    \item determináns.
%\end{enumerate}
%Külön probléma a mátrix rangjának definíciója.
%Nem definiálhatom, 
%mint az oszlop vagy sorvektortér dimenzióját, hiszen a felépítés jelen szintjén még nincs bázis.
%Természetesen azt sem tudjuk még, hogy a maximális lineárisan független sor- vagy oszloprendszer választástól függetlenül mindig azonos elemszámú.
%A mátrix feszítőrangja viszont definiálható.
%\begin{definition}[mátrix feszítőrangja]
%    Legyen $A\in\mathbb{F}^{n\times m}$ egy tetszőleges nem zérus mátrix.
%    Azt mondjuk, hogy feszítőrangja $r$, ha $r$ a legkisebb olyan pozitív egész, amelyre $A$ előáll
%    \[
%        A=BC
%    \]
%    alakban, ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times n}$.
%\end{definition}
%Világos, hogy tetszőleges nemzérus négyzetes mátrixra ez jól definiált és $1\leq r \leq \min\{n,m\}$.
%A Rang-tételnek a szokásosnál egy kicsit erősebb formáját lehet megfogalmazni a dimenzió fogalmának bevezetése nélkül,
%ami a lenti 3. állítás.
%
%\begin{proposition}
%    Az alábbi állítások egymás következményei:
%    \begin{enumerate}
%        \item Homogén lineáris egyenletrendszernek, amelynek több ismeretlene van, mint egyenlete
%            mindig létezik nem triviális megoldása.
%        \item Lineárisan független vektorrendszer elemszáma nem nagyobb mint egy generátorrendszer elemszáma.
%        \item Minden nemzérus mátrixban 
%            a maximális lineárisan független oszloprendszerek 
%            és maximális lineárisan független sorrendszerek azonos elemszámúak, 
%            és ez a szám egybeesik a mátrix feszítőrangjával.
%        \item
%            Legyen $A,B\in\mathbb{F}^{n\times n}$ négyzetes mátrixok.
%            Ekkor $AB=I$ esetén $BA=I$ is fennáll.\qedhere
%    \end{enumerate}
%\end{proposition}
%\begin{proof}[1. \Rightarrow 2.]
%    Legyen $\left\{ y_1,\ldots,y_n \right\}$ egy generátorrendszer,
%    és $\left\{ x_1,\ldots,x_m \right\}$ olyan vektorrendszer a vektortérben, ahol $m>n$.
%    Meg kell mutatnunk, hogy ez utóbbi egy lineárisan összefüggő.
%    Világos, hogy minden $1\leq k\leq m$ mellett
%    \[
%        x_k=\sum_{j=1}^n\alpha_{j,k}y_j.
%    \]
%    Egyelőre tetszőleges $\xi_1,\ldots,\xi_m$ együtthatók mellett
%    \begin{eqnarray}
%        \sum_{k=1}^m\xi_kx_k=
%        \sum_{k=1}^m\sum_{j=1}^n\xi_k\alpha_{j,k}y_j=
%        \sum_{j=1}^n\left( \sum_{k=1}^m\alpha_{j,k}\xi_k \right)y_j
%        \label{eq:sys}
%    \end{eqnarray}
%    Tekintsük az $\left( \alpha_{j,k} \right)$ együtthatók generálta
%    homogén lineáris egyenletrendszert. 
%    Itt $j=1,\ldots,n$ és $k=1,\ldots,m$.
%    Mivel $m>n$, ezért az ismeretlenek száma több mint az egyenletek száma.
%    Létezik tehát nem triviális megoldás\index{triviális megoldás}, azaz léteznek nem mind nulla
%    $\xi_1,\ldots,\xi_m$ számok, amelyekre minden $j=1,\ldots,n$ esetén
%    \[
%        \sum_{k=1}^m\alpha_{j,k}\xi_k=0.
%    \]
%    Találtunk tehát az $\left\{ x_1,\ldots,x_m \right\}$ vektorrendszernek egy
%    nem triviális, 
%    de a zéró vektort eredményező,
%    lineáris kombinációját (\ref{eq:sys}).
%\end{proof}
%\begin{proof}[2.\Rightarrow 3.]
%    Jelölje $r$ az $A\in\mathbb{F}^{n\times m}$ mátrix feszítőrangját.
%    Legyen $r_c$ a mátrix egyik rögzített maximális lineárisan független oszloprendszerének elemszáma.
%    \begin{itemize}
%        \item 
%            Ezen oszlopokat egy $B\in\mathbb{F}^{n\times r_c}$ mátrixba téve 
%            -- a maximalitás miatt -- létezik olyan $C\in\mathbb{F}^{r_c\times m}$ mátrix, 
%            amelyre $A=BC$, azaz $r\leq r_c$.
%        \item
%            Most tekintsünk egy tetszőleges olyan
%            \(
%            A=BC
%            \)
%            felbontást, 
%            ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$.
%            Jelölje $W$ a $B$ mátrix oszlopai lineáris burkát. 
%            Az $A$ mátrix fent rögzített maximális lineárisan független oszloprendszere egy lineárisan független rendszer a 
%            $W$ vektortérben,
%            és $B$ oszlopai pedig egy generátorrendszer ugyanebben a vektortérben.
%            A 2. állitás szerint $r_c\leq r$.
%    \end{itemize}
%    Evvel megmutattuk, hogy bármely két maximális lineárisan független oszloprendszer azonos elemszámú, és számuk megegyezik a mátrix feszítőrangjával.
%
%
%    Legyen $r_w$ az $A$ mátrix egyik rögzített maximális lineárisan független sorrendszerének elemszáma.
%    \begin{itemize}
%        \item 
%            Ezen sorokat egy $C\in\mathbb{F}^{r_w\times m}$ mátrixba téve 
%            -- a maximalitás miatt -- létezik olyan $B\in\mathbb{F}^{n\times r_w}$ mátrix, 
%            amelyre $A=BC$, azaz $r\leq r_w$.
%        \item
%            Most tekintsünk egy tetszőleges olyan
%            \(
%            A=BC
%            \)
%            felbontást, 
%            ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$.
%            Jelölje most $V$ a $C$ mátrix sorai lineáris burkát. 
%            Az $A$ mátrix fent rögzített maximális lineárisan független sorrendszere egy lineárisan független rendszer e
%            $V$ vektortérben,
%            és $C$ sorai pedig egy generátorrendszert alkotnak ugyanebben a $V$ vektortérben.
%            A 2. állitás szerint $r_w\leq r$.
%    \end{itemize}
%    Evvel azt is megmutattuk, 
%    hogy bármely két maximális lineárisan független sorrendszer azonos elemszámú, 
%    és számuk megegyezik a mátrix feszítőrangjával.
%\end{proof}
%\begin{proof}[3.\Rightarrow 4.]
%    Tegyük fel, hogy $AB=I$.
%    Az identitás mátrix rangja nyilván $n$.
%    A 3. állitás miatt a feszítőrang is $n$.
%    Ha $A$ oszlopai nem lennének lineárisan függetlenek,
%    akkor lenne $A=CD$ felbontás, ahol $C\in\mathbb{F}^{n \times r}$ és $D\in\mathbb{F}^{r\times n}$ valamely $r<n$ mellett.
%    Ekkor persze $I=AB=\left( CD \right)B=C\left( DB \right)$ is teljesülne, 
%    ahol $DB\in\mathbb{F}^{r\times n}$ ellentmondva az identitás mátrix feszítőrangja definíciójának.
%    Világos, hogy
%    \[
%        A\left( BA-I \right)=
%        A\left( BA \right)-AI=
%        \left( AB \right)A-AI=IA-AI=0.
%    \]
%    Figyelembe véve, hogy $A$ oszlopai lineáris függetlenek, ez csak úgy lehetséges, ha $BA-I$ a zéró mátrix, ergo $BA=I$.
%\end{proof}
%\begin{proof}[4.\Rightarrow 1.]
%    Legyen $A\in\mathbb{F}^{n\times m}$ a homogén lineáris egyenletrendszer együttható mátrixa,
%    ahol $n$ az egyenletek száma, $m$ az ismeretlenek száma.
%    Azt kell megmutatnunk, hogy az oszloprendszer lineárisan összefüggő.
%    Ha független lenne, akkor
%    egészítsük ki e mátrixot alulról $m-n$ darab csupa nullákat tartalmazó sorral.
%    Mivel $m>n$ ezért a kiegészített $Q\in\mathbb{F}^{m\times m}$ mátrix legalsó sora csak nullát tartalmaz.
%    Mivel $A$ oszlopai lineárisan függetlenek, ezért $Q$ oszlopai is azok.
%    Emiatt létezik $P\in\mathbb{F}^{m\times m}$ mátrix, amelyre $PQ=I$.
%    A 3. állitás szerint $I=QP$ is teljesül, 
%    ami azt jelenti, hogy $I$ utolsó sora a csupa nullákat tartalmaz, ami ellentmondás.
%    Beláttuk tehát, hogy $A$ oszlopai lineárisan összefüggőek, azaz az eredeti egyenletrendszernek van nem triviális megoldása.
%\end{proof}
%
\part{Tavasz}
\chapter{Mátrixok és lineáris operációk}
\scwords Mátrixok és lineáris operációk kapcsolatát vizsgáljuk.
Azt már a mátrixok bevezetésekor is láttuk, hogy egy $n\times m$ méretű mátrix egyben tekinthető valamely
$\mathbb{F}^m\to\mathbb{F}^n$ lineáris operációnak olyan módon,
hogy az egy $x\in\mathbb{F}^m$ oszlopvektorhoz a mátrix szorzás definíciójának megfelelően
az $A\cdot x\in\mathbb{F}^n$ oszlopvektort rendeli.
Ebben a fejezetben a mátrixok ezen interpretációját erősítjük tovább.

\section{Rang--defektus-tétel következménye}
Láttuk, hogy tetszőleges $A\in L\left( V,W \right)$ lineáris operáció mellett
\[
	\nu\left( A \right)+\rho\left( A \right)=\dim(V),
\]
ahol $\nu\left( A \right)=\dim\left( \ker A \right)$ az $A$ defektusa,
$\rho\left( A \right)=\dim\left(\im A \right)$ az $A$ rangja.
Ebből adódóan, ha $A\in L\left( V \right)$ egy lineáris transzformáció,
akkor $\nu\left( A \right)=0$ és $\rho\left( A \right)=\dim(V)$ egymással ekvivalens feltevések.

Itt az első feltétel pontosan $A$ injektivitását,
míg a második feltétel pontosan $A$ szürjektivitását jelenti.
Azt látjuk tehát,
hogy lineáris transzformációk esetén a transzformáció injektivitása és szürjektivitása egyszerre teljesül,
vagy egyszerre nem teljesül:
\begin{proposition}
	Legyen $V$ egy véges dimenziós vektortér, és $A\in L\left( V \right)$ egy lineáris transzformáció.
	\begin{enumerate}
		\item
		      Ekkor az alábbi feltételek egymással ekvivalensek.
		      \begin{enumerate}
			      \item $A$ injektív;
			      \item $A$ szürjektív;
			      \item $A$ vektortér-izomorfizmus.
			      \item Létezik $B\in L\left( V \right)$ lineáris transzformáció, amelyre $A\circ B=I$.
		      \end{enumerate}
		\item
		      Ha a fenti d) fennáll, akkor $B\circ A=I$ is teljesül.\qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Az első három pont ekvivalens voltát már meggondoltuk.

	Ha $A\circ B=I$, akkor $A$ szürjektív, hiszen az $y\in V$ vektor előáll,
	mint a $By$ vektor $A$ képe.

	Megfordítva, ha $A$ szürjektív, akkor a) szerint injektív is, van tehát $V\to V$ inverze.
	De lineáris függvény inverze is lineáris,
	így a $B=A^{-1}$ jelölést bevezetve találtunk $B\in L\left( V \right)$
	transzformációt, amelyre $A\circ B=I$.

	Most tegyük fel, hogy valamely $B\in L\left( V \right)$-re $A\circ B=I$.
	Ekkor a) szerint $A$ injektív is, ergo $\ker A=\left\{ 0 \right\}$.
	No de, minden $x\in V$ mellett
	\[
		A\left( (B\circ A)x-x \right)
		=
		A\left( B\left( Ax \right) \right)-Ax
		=
		\left( A\circ B \right)\left( Ax \right)-Ax
		=
		I\left( Ax \right)-Ax
		=
		Ax-Ax=0,
	\]
	ezért $(B\circ A)x-x\in\ker A=\left\{ 0 \right\}$.
	$(B\circ A)x=x$ minden $x\in V$ mellett, ami éppen azt jelenti, hogy $B\circ A=I$ is fennáll.
\end{proof}
Kiderült tehát, hogy ugyanúgy mint amikor mátrixok regularitásáról volt szó,
az $A\in L\left( V \right)$ lineáris transzformáció pontosan akkor injektív,
ha van olyan $B\in L\left( V \right)$ lineáris transzformáció,
amelyre $AB=I$ (vagy $BA=I$) teljesül.
Ekkor $A^{-1}=B$.

\section{Mátrixok tere mint koordináta-tér}
Világos, hogy adott $\mathbb{F}$ test feletti $n\times m$ méretű mátrixok az $\mathbb{F}$ feletti
vektorteret alkotnak.
Az is világos, hogy ha $A_{i,j}$ jelöli azt az $n\times m$ méretű mátrixot,
amelynek minden helyén $0$ van az $i$-edik sor $j$-edik helyének kivételével, ahol $1$ szerepel,
akkor az
\[
	\left\{ A_{i,j}:i=1,\ldots,n;j=1,\ldots,m\right\}
\]
mátrixok rendszere egy bázis az $\mathbb{F}^{n\times m}$ térben.
Így persze $\dim \mathbb{F}^{n\times m} = n\cdot m$.

Most áttérünk az $L\left( V,W \right)$ lineáris operációk vektorterének vizsgálatára.
Először azt gondoljuk meg, hogy lineáris transzformáció egy bázison tetszőlegesen és egyértelműen előírható.
\begin{proposition}\label{pr:bazisonegyertelmu}
	Legyenek $V$ és $W$ ugyanazon test feletti vektorterek.
	Legyen $V$-ben a $\left\{ v_1,\ldots,v_m \right\}$ egy bázis,
	és rögzítsünk $W$-ben egy tetszőleges $m$ elemű $\left\{ w_1,\ldots,w_m \right\}$ vektorrendszert.

	Ekkor létezik egyetlen egy $A\in L\left( V,W \right)$ lineáris operáció,
	amelyre $Av_j=w_j$ minden $j=1,\ldots,m$ esetén.
\end{proposition}
\begin{proof}
	Definiáljuk az $A:V\to W$ függvényt a következőképpen.
	Minden $v\in V$ egyértelműen áll elő mint $v=\sum_{j=1}^m\alpha_jv_j$.
	Egy ilyen $v$ mellett legyen
	\[
		A\left( v \right)=\sum_{j=1}^m\alpha_jw_j.
	\]
	Világos, hogy $A:V\to W$ függvény jól definiált, hiszen a rögzített bázisban minden elem előáll és egyetlen egyféleképpen
	áll elő mint a bázis elemek egy lineáris kombinációja.
	Megmutatjuk, hogy az így definiált $A$ függvény egy lineáris operáció.
	Legyen $x_1=\sum_{j=1}\alpha_jv_j$ és
	$x_2=\sum_{j=1}\beta_jv_j$.
	Tetszőleges $\alpha,\beta\in\mathbb{F}$ mellett
	\[
		\alpha x_1+\beta x_2=
		\sum_{j=1}^m\left( \alpha\alpha_j+\beta\beta_j \right)v_j,
	\]
	tehát $A$ definíciója szerint
	\[
		A\left( \alpha x_1+\beta x_2 \right)=
		\sum_{j=1}^m\left( \alpha\alpha_j+\beta\beta_j \right)w_j
		=
		\alpha\sum_{j=1}^m\alpha_jw_j+
		\beta\sum_{j=1}^m\beta_jw_j=
		\alpha A\left( x_1 \right)+\beta A\left( x_2 \right).
	\]
	Ez éppen $A$ függvény linearitását jelenti.
	Az is világos, hogy $v_j=0v_1+\ldots+1v_j+\ldots +0v_m$, tehát az $A$ függvény definíciója értelmében
	\[
		Av_j=w_j
	\] valóban fennáll minden $j=1,\ldots,m$ mellett.

	Most tegyük fel, hogy $B\in L\left( V,W \right)$ szintén teljesíti a $Bv_j=w_j$ feltételeket.
	Ekkor tetszőleges $v\in V$ mellett, ha $v=\sum_{j=1}^m\alpha_jv_j$ alakú,
	akkor az $A$ definíciója és $B$ linearitása miatt
	\[
		Av=\sum_{j=1}^m\alpha_jw_j=
		\sum_{j=1}^m\alpha_jBv_j=
		B\left( \sum_{j=1}^m\alpha_jv_j \right)=Bv,
	\]
	ami éppen azt jelenti, hogy $A=B$.
\end{proof}
\begin{proposition}
	Legyenek az $\left\{ e_1,\ldots,e_m \right\}\subseteq V$ és az
	$\left\{ f_1,\ldots,f_n \right\}\subseteq W$ bázisok rögzítve.
	Definiáljuk valamely rögzített $i\in\left\{ 1,\ldots,n \right\}$ és valamely rögzített
	$j\in \left\{ 1,\ldots,m \right\}$ mellett az $A_{i,j}\in L\left( V,W \right)$ lineáris operációt az
	\begin{equation}
		A_{i,j}\left( e_k \right)=\delta_{j,k}f_i
		\label{eq:aij}
	\end{equation}
	azonosságokkal, ahol $k=1,\ldots,m$.
	Ekkor az $\left\{ A_{i,j}: i=1,\ldots,n;j=1,\ldots,m \right\}$
	lineáris operációk rendszere egy bázisa az $L\left( V,W \right)$ vektortérnek,
	ezért
	\[
		\dim \left( L\left( V,W \right) \right)=n\cdot m.\qedhere
	\]
\end{proposition}
\begin{proof}
	Azt kell először is látni, hogy (\ref{eq:aij}.) azonosságok összesen az előző \ref{pr:bazisonegyertelmu}.~állítás
	alkalmazását írják elő rögzített $i,j$ mellett az
	\begin{math}
		\left\{ e_1,\ldots,e_j,\ldots,e_m \right\}
	\end{math}
	és az
	\begin{math}
		\left\{ 0,\ldots,0,f_i,0,\ldots,0 \right\}
	\end{math}
	két pontosan $m$ elemű vektorrendszerre.
	Ezt úgy is fogalmazhatjuk, hogy az $A_{i,j}$ az a lineáris operáció,
	amely a bázis minden nem $j$-edik elemét zérusra viszi, és a $j$-edik bázis elemet
	pedig $f_i$-re.
	\Aref{pr:bazisonegyertelmu}. állítás szerint vannak ilyen $A_{i,j}$ lineáris transzformációk,
	és minden $i,j$ pár mellett csak egyetlen egy van.

	Most megmutatjuk, hogy az $\left\{ A_{i,j}:i=1,\ldots,n;j=1,\dots m \right\}$ lineárisan független
	rendszer.
	Legyenek az $\alpha_{i,j}\in\mathbb{F}$ számok olyanok,
	amelyekre
	\begin{math}
		\sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}=0.
	\end{math}
	Ekkor tetszőleges $k\in\left\{ 1,\ldots,m \right\}$ esetén
	\[
		0=
		\left( \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j} \right)e_k
		=
		\sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}e_k
		=
		\sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}\delta_{j,k}f_i
		=
		\sum_{i=1}^n\alpha_{i,k}f_i.
	\]
	No de, az $\left\{f_1,\ldots,f_n  \right\}$ egy lineárisan független rendszer,
	ergo csak a  triviális lineáris kombinációja zérus,
	ergo $\alpha_{i,k}=0$ minden $i=1,\ldots,n$ mellett.
	Persze ez minden $k$ mellett megismételhető, tehát azt kaptuk, hogy valamennyi $\alpha_{i,j}$ együttható
	zérus.

	Most azt mutatjuk meg, hogy az $\left\{ A_{i,j}:i=1,\ldots,n;j=1,\dots m \right\}$ egy
	generátorrendszere az $L\left( V,W \right)$ vektortérnek.
	Legyen $A\in L\left( V,W \right)$ egy rögzített lineáris operáció.
	Definiáljuk az $\alpha_{i,j}$ számokat,
	mint az $Ae_{j}\in W$ vektor $\left\{ y_1,\ldots,y_n \right\}$ bázisban felírt koordináta-vektorának
	$i$-edik elemét.
	Magyarul
	\[
		Ae_j=
		\sum_{i=1}^n\alpha_{i,j}f_i.
	\]
	Ekkor minden $k\in\left\{ 1,\ldots,m \right\}$ mellett
	\[
		\left( \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j} \right)e_k
		=
		\sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}e_k
		=
		\sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}\delta_{j,k}f_i
		=
		\sum_{i=1}^n\alpha_{i,k}f_i
		=Ae_k.
	\]
	Ez azt jelenti, hogy az $A$ és az
	\begin{math}
		\sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}
	\end{math}
	lineáris operátorok az $\left\{ e_1,\ldots,e_m \right\}$ bázis minden elemén megegyeznek.
	Láttuk, hogy bázison felvett értékek már egyértelműen meghatározzák a lineáris operációt,
	ezért e két lineáris operáció is azonos.
	Találtunk tehát $\alpha_{i,j}$ számokat, amelyekre
	\begin{math}
		A=
		\sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j},
	\end{math}
	ami azt jelenti, hogy $A$ előáll mint az $A_{i,j}$ függvények valamely lineáris kombinációja.
\end{proof}
Érdemes későbbre is eltennünk magunkban,
hogy hogyan találtunk az adott $A$ operátorhoz azon $\alpha_{i,j}$ számokat,
amelyekre az
\[
	A=
	\sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}
	=
	\sum_{j=1}^m\sum_{i=1}^n\alpha_{i,j}A_{i,j}
\]
egyenlőség teljesül:
$\alpha_{i,j}$ a $j$-edik bázis elem $A$ képének $i$-edik koordinátája.
Ugyanez a koordináta-vektor fogalmával:
Az $Ae_j\in W$ vektornak az $\left\{ y_1,\ldots,y_n \right\}$ bázisban felírt koordináta-vektorának az elemei
adják az $\alpha_{1,j},\alpha_{2,j},\ldots,\alpha_{n,j}$ számokat, formálisabban:
\[
	\left[ Ae_j \right]_{\left\{ y_1,\ldots,y_n \right\}}
	=
	\begin{pmatrix}
		\alpha_{1,j} \\ \alpha_{2,j}\\ \vdots \\ \alpha_{n,j}
	\end{pmatrix}.
\]

A következő gondolat előtt arra kell emlékeznünk,
hogy minden véges dimenziós vektortér izomorf a koordináta-terével.
Láttuk, hogy $\dim \left( L\left( V,W \right) \right)=\dim(V)\cdot\dim(W)$.
De mi lesz $L\left( V,W \right)$ koordináta-tere?
A válaszhoz rögzítenünk kell a bázis elemek egy sorrendjét.
\begin{definition}[lineáris operátor mátrixa]
	(\Aref{eq:aij}.) azonosságokkal definiált $A_{i,j}$ lineáris operátorokat állítsuk a következő sorrendbe:
	\[
		\{
		\underbrace{A_{1,1},A_{2,1},\ldots,A_{n,1}}_{j=1},
		\underbrace{A_{1,2},A_{2,2},\ldots,A_{n,2}}_{j=2},
		\underbrace{A_{1,3},A_{2,3},\ldots,A_{n,3}}_{j=3},
		\underbrace{\dots,\ldots,\dots,\ldots,\dots}_{j=4,\ldots,m-1},
		\underbrace{A_{1,m},A_{2,3},\ldots,A_{n,m}}_{j=m}
		\}
	\]
	Láttuk, hogy a fent konstruált $\alpha_{i,j}$ számokkal
	\[
		A=
		\sum_{j=1}^m\sum_{i=1}^n\alpha_{i,j}A_{i,j}.
	\]
	Ez azt jelenti, hogy az $A\in L\left( V,W \right)$ függvénynek a fenti bázisban felírt koordináta-vektora
	az $\alpha_{i,j}$ elemekből a fenti sorrendben képzett oszlopvektor, azaz
	\[
		\left[ A \right]=
		\begin{pmatrix}
			\alpha_{1,1} \\ \alpha_{2,1}\\ \vdots \\ \alpha_{n,1}\\
			\alpha_{1,2} \\ \alpha_{2,2}\\ \vdots \\ \alpha_{n,2}\\
			\alpha_{1,3} \\ \alpha_{2,3}\\ \vdots \\ \alpha_{n,3}\\
			\vdots       \\ \vdots\\
			\alpha_{1,m} \\ \alpha_{2,m}\\ \vdots \\ \alpha_{n,m}
		\end{pmatrix}\tag{\dag}
	\]
	Mint minden vektortér így az $L\left( V,W \right)$ is izomorf a koordináta-terével, ergo $L\left( V,W \right)$ izomorf
	az $\mathbb{F}^{n\cdot m}$ vektortérrel.

	Érdemes a koordinátatér elemeit, azaz az $n\cdot m$ elemből álló oszlopvektorokat $n$ koordinátánként megtörni,
	és evvel egy $n\times m$-es mátrixba rendezni.
	Ezt a jelölést alkalmazva az $A$ lineáris operációnak a fenti bázisban felírt koordináta vektora az az
	$n\times m$-es mátrix,
	amelyre
	\[
		[A]=
		\begin{pmatrix}
			\alpha_{1,1} & \alpha_{1,2} & \alpha_{1,3} & \dots  & \alpha_{1,m} \\
			\alpha_{2,1} & \alpha_{2,2} & \alpha_{2,3} & \dots  & \alpha_{2,m} \\
			\alpha_{3,1} & \alpha_{3,2} & \alpha_{3,3} & \dots  & \alpha_{3,m} \\
			\vdots       & \vdots       & \vdots       & \ddots & \vdots       \\
			\alpha_{n,1} & \alpha_{n,2} & \alpha_{n,3} & \dots  & \alpha_{n,m}
		\end{pmatrix}.
	\]
	Ezt a mátrixot nevezzük az
	$A\in L\left( V,W \right)$ \emph{lineáris operáció mátrixának}\index{lineáris operáció mátrixa}
	az előre rögzített
	$\left\{e_1,\ldots,e_m  \right\}\subseteq V$ és $\left\{ f_1,\ldots,f_n \right\}$ bázisok mellett.
\end{definition}
Meggondoltuk tehát, hogy a kapcsolat lineáris operáció és mátrixa között azonos avval a kapcsolattal,
ami általában egy vektor és koordinátái közt van.

Nagyon fontos, hogy bármilyen lineáris operáció mátrixát fel tudjuk írni, 
emiatt foglaljuk össze az eddigieket.
\begin{proposition}
	Tegyük fel, hogy $A\in L\left( V,W \right)$ egy lineáris operáció.
	Rögzítsük a $V$ és a $W$ vektortér egy-egy bázisát.
	Legyen tehát $\{e_1,\ldots,e_m\}\subseteq V$ egy bázis és
	$\left\{ f_1,\ldots,f_n \right\}\subseteq W$ egy bázis.
	Az $A$ operációnak a fenti rögzített bázisokban felírt mátrixa az az
	$n\times m$ méretű
	$[A]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}$
	mátrix,
	\footnote{Ha világos, hogy mely bázisokat rögzítjük akkor a nehézkes
		$[A]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}$
		jelölés helyett csak $[A]$-t írunk.
		Persze mindig tudnunk kell, hogy az $A$ lineáris operátor mely bázisokban felírt mátrixáról van szó.
	}
	amelynek $j$-edik oszlopa az $Ae_j$ vektor $\left\{ y_1,\ldots,y_n \right\}$ bázisban felírt koordinátája.
	Formálisabban:
	\[
		[A]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}^j
		=
		\left[ Ae_j \right]_{\left\{ y_1,\ldots,y_n \right\}}.
		\qedhere
	\]
\end{proposition}
Speciálisan, ha $V=W$, akkor $A:V\to V$ lineáris transzformációról beszélünk.
Amikor egy lineáris transzformáció mátrixáról van szó,
akkor az mindig úgy értendő, hogy a $V$ vektortérnek mint az értelmezési tartománynak és a
$V$ vektortérnek mint értékkészletnek is ugyanazt a bázisát választjuk.
Ekkor tehát $A$ lineáris transzformáció
$[A]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ e_1,\ldots,e_m \right\}}}$
mátrixára:
\footnote{
	A nehézkes
	$[A]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ e_1,\ldots,e_m \right\}}}$
	helyett egyszerűbben
	$[A]_{\left\{ e_1,\ldots,e_m \right\}}$, vagy még inkább ha a szövegkörnyezetből nyilvánvaló, hogy mely bázisra gondolunk,
	akkor csak a $[A]$ jelölést használjuk.
}
\[
	[A]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ e_1,\ldots,e_m \right\}}}^j
	=
	\left[ Ae_j \right]_{\left\{ e_1,\ldots,e_m \right\}}.
\]

Meggondoltuk tehát, hogy az $L\left( V,W \right)$ lineáris operátorok vektortere izomorf az
$\mathbb{F}^{\dim(W)\times \dim(V)}$ mátrixok vektorterével.
Az izomorfizmus tehát az a leképezés,
amely egy lineáris transzformációhoz hozzárendeli annak
-- valamely előre megadott bázisokban felírt --
mátrixát.
Emiatt persze $A,B\in L\left( V,W \right)$ és $\alpha,\beta\in\mathbb{F}$ mellett
\begin{equation}\label{eq:vt}
	\left[ \alpha A+\beta B \right]=\alpha\left[ A \right]+\beta\left[ B \right].
\end{equation}
Izomorf struktúrák közt nem érdemes különbséget tenni, viszont vigyáznunk kell arra,
hogy olyan fogalmakat definiáljunk, amelyek invariánsak az izomorfiára.
Például lineáris operáció rangja definíció szerint a képtere dimenziója,
mátrix rangja definíció szerint a feszítőrang, azaz a legkisebb
$r$ szám amelyre a mátrix felírható $n\times r$ és egy $r\times m$ méretű mátrix szorzataként.
Látni fogjuk, hogy lineáris operátornak és mátrixának rangja azonos.
\begin{proposition}
	Legyen $A\in L\left( V,W \right)$ lineáris operátor és $x\in V$ egy vektor.
	Rögzítsük az $\left\{ e_1,\ldots,e_m \right\}\subseteq V$ és az $\left\{ f_1,\ldots,f_n \right\}\subseteq W$ bázisokat.
	Ekkor
	\[
		[Ax]_{\left\{ f_1,\ldots,f_n \right\}}
		=
		[A]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}
		\cdot
		[x]_{\left\{ e_1,\ldots,e_m \right\}}.
	\]
	Emiatt $\im A$ és $\im[A]$ egymással izomorf alterek, hasonlóan $\ker A$ és $\ker[A]$ egymással
	izomorf vektorterek, így dimenziójuk is azonos.
	Konkrétan $A$ lineáris operátornak és az $[A]$ mátrixának a rangja is defektusa is azonos.\index{rang}\index{defektus}
\end{proposition}
\begin{proof}
	Legyen
	\begin{math}
		[x]_{\left\{ e_1,\ldots,e_m \right\}}
		=
		\begin{pmatrix}
			\xi_1 \\ \xi_2 \\ \vdots \\ \xi_m
		\end{pmatrix}.
	\end{math}
	Ez azt jelenti,
	hogy $x=\sum_{j=1}^m\xi_je_j$, így
	$Ax=\sum_{j=1}^m\xi_jAe_j$.
	Véve az $Ax$ vektor $\left\{ f_1,\ldots,f_n \right\}$ bázisban felírt koordináta-vektorát
	\begin{multline*}
		[Ax]_{\left\{ f_1,\ldots,f_n \right\}}
		=
		\left[ \sum_{j=1}^m\xi_jAe_j \right]_{\left\{ f_1,\ldots,f_n \right\}}
		=
		\sum_{j=1}^m\xi_j[Ae_j]_{\left\{ f_1,\ldots,f_n \right\}}
		=
		\sum_{j=1}^m\xi_j[A]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}^j
		=\\
		[A]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}
		\cdot
		[x]_{\left\{ e_1,\ldots,e_m \right\}}.
	\end{multline*}

	Ebből persze már könnyen adódik, ahogy $Ax=0$ ekvivalens $[A][x]=0$ feltétellel, emiatt
	$\ker A$ és $\ker [A]$ izomorfak,
	így $\nu(A)=\nu([A])$.

	Hasonlóan $\im A$ és $\im [A]$ is izomorfak.
	A rangtétel szerint $[A]$ mátrix rangja megegyezik az oszlopvektorai generálta altérben lévő maximális lineárisan
	független rendszer elemszámával.
	Mivel egy vektor pontosan akkor van $[A]$ képterében, ha előáll mint az oszlopai lineáris kombinációja,
	ezért $[A]$ rangja azonos $[A]$ képterének dimenziójával,
	így $\rho(A)=\rho([A])$.
\end{proof}

\section{Lineáris operátorok szorzata}
Emlékezzünk arra, hogy a lineáris operáció mátrixát úgy kaptuk, hogy a koordinátáit $n$ elemenként
megtörve az oszlopvektort egy mátrixszá rendeztük át.
Ennek az átrendezésnek az igazi értelme, hogy ilyen módon a mátrix szorzás művelet a lineáris operátorok kompozíciójával
kapcsolódik össze.
\begin{proposition}
	Legyenek $V,Z,W$ ugyanazon test feletti véges dimenziós vektorterek,
	$B \in L\left( V,Z \right)$ és $A \in L\left( Z,W \right)$.
	Rögzítsük az
	\[
		\left\{ e_1,\ldots,e_m \right\}\subseteq V,\quad
		\left\{ z_1,\ldots,z_r \right\}\subseteq Z,\quad
		\left\{ f_1,\ldots,f_n \right\}\subseteq W
	\]
	bázisokat.
	Jelölje $C=A\circ B$ a kompozíció lineáris operátort.

	Ekkor $C$ mátrixa az $A$ és $B$ mátrixszának szorzata.
	Formálisabban:
	\[
		\left[ C \right]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}
		=
		\left[ A \right]_{\substack{\left\{ z_1,\ldots,z_r \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}
		\cdot
		\left[ B \right]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ z_1,\ldots,z_r \right\}}}.\qedhere
	\]
\end{proposition}
\begin{proof}
	Először is ellenőrizzük, hogy értelmes-e az állításban felírt formula.
	$\left[ A \right]$ mérete $n\times r$, $\left[ B \right]$ mérete $r\times m$.
	Így az $[A]\cdot[B]$ szorzat értelmes és a szorzás eredménye egy $n\times m$ mátrix.
	A $[C]$ szintén egy $n\times m$ méretű mátrix,
	így a bal és a jobboldal összehasonlítása is értelmes.

	Már csak azt kell meggondolni,
	hogy a baloldali mátrix minden eleme azonos a jobboldali szorzatmátrix megfelelő elemével.
	Az $\alpha,\beta,\gamma$ szimbólumokkal jelöljük az $[A],[B], [C]$ mátrixok megfelelő elemeit.
	Ez azt jelenti, hogy
	\[
		Be_j=\sum_{k=1}^r\beta_{k,j}z_k,\quad
		Az_k=\sum_{i=1}^n\alpha_{i,k}f_i,\quad
		Ce_j=\sum_{i=1}^n\gamma_{i,j}f_i.
	\]
	Így azt kapjuk, hogy minden $j=1,\ldots,m$ mellett
	\begin{multline*}
		\sum_{i=1}^n\gamma_{i,j}f_i
		=
		Ce_j
		=A\left( Bej \right)
		=
		\sum_{k=1}^r\beta_{k,j}Az_k
		=
		\sum_{k=1}^r\beta_{k,j}\left( \sum_{i=1}^n\alpha_{i,k}f_i \right)
		=\\
		\sum_{k=1}^r\sum_{i=1}^n\beta_{k,j}\alpha_{i,k}f_i
		=
		\sum_{i=1}^n\sum_{k=1}^r\alpha_{i,k}\beta_{k,j}f_i
		=
		\sum_{i=1}^n\left( \sum_{k=1}^r\alpha_{i,k}\beta_{k,j} \right)f_i.
	\end{multline*}
	No de, az $\{f_1,\ldots,f_n\}$ rendszer lineárisan független,
	tehát a lineáris burkában minden elem előállítása egyértelmű,
	ami éppen azt jelenti, hogy minden $i=1,\ldots,n$ és minden $j=1,\ldots,m$ mellett
	\[
		[C]_{i,j}=\gamma_{i,j}
		=\sum_{k=1}^r\alpha_{i,k}\beta_{k,j}
		=\left( \left[ A \right]\cdot\left[ B \right] \right)_{i,j}.\qedhere
	\]
\end{proof}
Azért, hogy teljes legyen az analógia a lineáris operátorok kompozíciója és a mátrixok szorzása műveletek közt,
a lineáris operátorok $A\circ B$ kompozícióját is $A\cdot B$ módon, vagy még egyszerűbben $AB$ módon jelöljük.
A jelölést a szóhasználat is követi:
\begin{definition}[lineáris transzformációk szorzata]
	A lineáris operátorok kompozíció műveletét \emph{szorzatnak}\index{lineáris transzformációk szorzata} is mondjuk.
\end{definition}
\noindent Ilyen módon ha $A$ és $B$ két olyan lineáris operátor, amelynek kompozíciója -- azaz szorzata -- értelmes,
akkor a mátrixaik szorzata is értelmes, továbbá
\begin{equation}\label{eq:gyuru}
	[AB]=[A][B].
\end{equation}
\begin{definition}[lineáris operátor hatványai]
	A lineáris transzformációk a bevezetett szorzás művelettel egységelemes gyűrűt alkotnak,
	ahol az egységelem az
	$I:V\to V$ az identitás operáció.

	Legyen $A\in L\left( V \right)$ egy lineáris transzformáció.
	Ennek $0$-dik hatványát definiálja $A^0=I$,
	Ha valamely $n$ nem negatív egész mellett $A^n$ már definiált,
	akkor legyen $A^{n+1}=AA^n$.
\end{definition}
Világos, hogy ha $n,m$ nem negatív egészek, akkor $A^{n+m}=A^n\cdot A^m$.
Az is nyilvánvaló, hogy az $I$ identitás operációnak akármelyik bázisban felírt mátrixa ugyanaz a mátrix,
mégpedig az identitás mátrix (ahol minden elem zérus, kivétel a diagonális elemek, amelyek értéke 1.)
\begin{proposition}
	Legyen $A\in L\left( V \right)$ egy lineáris transzformáció, és legyen rögzítve a tér egy bázisa,
	amelyben felírjuk $A$ transzformáció $[A]$ mátrixát.
	Ekkor
	\(
	\left[ A^n \right]=\left[ A \right]^n
	\),
	továbbá az $A$ transzformáció pontosan akkor izomorfizmus, 
    ha az $[A]$ mátrixa reguláris\index{reguláris mátrix} és
	\begin{math}
		\left[ A^{-1} \right]
		=
		\left[ A \right]^{-1}.
	\end{math}
\end{proposition}
\begin{proof}
	A (\ref{eq:gyuru}) azonosságot alkalmazva $B=A$ mellett nyilvánvaló indukcióval kapjuk az
	$\left[ A^n \right]=\left[ A \right]^n$
	azonosságot.

	Világos, hogy $A$ pontosan akkor izomorfizmus,
	ha létezik $B\in L\left( V \right)$, amelyre $AB=I$.
	No de, ez ekvivalens avval, hogy $[A][B]=[I]$, ami pedig a szükséges és elegendő feltétele
	$[A]$ mátrix invertálhatóságának.
	Ekkor $[A]^{-1}=[B]=\left[ A^{-1} \right]$.
\end{proof}
A (\ref{eq:vt}) és a (\ref{eq:gyuru}) azonosságok szerint 
egy lineáris operátorhoz hozzárendelni annak valamely bázisban
felírt mátrixát egy olyan bijekció, ami tartja a gyűrű műveleteket.
Az $L\left( V \right)$ és a $F^{\dim(V)\times \dim(V)}$ tehát olyan egységelemes (nem kommutatív) gyűrűk,
amelyek közt van a gyűrű műveleteket tartó bijekció (gyűrű-izomorfizmus)\index{gyűrű-izomorfizmus}.

\chapter{Általános bázistranszformáció}
\scwords Egy vektor koordinátái függnek a bázis megválasztásától.
Az elemi bázistranszformáció arra szolgál hogy felírjuk az új bázisban a vektor koordinátáit,
amikor az új bázis és a régi bázis csak egyetlen vektorban különbözik.
A fejezetben általánosabban oldjuk meg a problémát, mikor
az új bázis és a régi bázis elemei tetszőlegesen különbözhetnek.

Vegyük fel a $V$ vektortér egy-egy bázisát.
Nevezzük az $\left\{ e_1,\ldots,e_n \right\}$ bázist régi bázisnak,
és nevezzük az $\left\{ f_1,\ldots,f_n \right\}$ bázis elemeit új bázisnak.
A kérdések a következők:
\begin{enumerate}
	\item Ha ismerjük egy $x\in V$ vektor régi bázisra vonatkozó koordinátáit,
	      akkor hogyan számítható ugyanennek a vektornak az új bázisban felírt koordináta-vektora?
	\item Ha ismerjük egy $A\in L\left( V \right)$ lineáris transzformációnak a régi bázisban felírt mátrixát,
	      akkor hogyan számolható ki az $A$-nak valamely új bázisban felírt mátrixa?
	\item Ha ismerjük egy $A\in L\left( V, W \right)$ lineáris operációnak a régi bázis páron felírt mátrixát,
	      akkor hogyan számítható $A$-nak az valamely új bázis páron felírt mátrixa?
\end{enumerate}
\begin{definition}
	Legyenek az
	$\left\{ e_1,\ldots,e_n \right\}$  és
	$\left\{ f_1,\ldots,f_n \right\}$ bázisok rögzítve.
	Tekintsük azt a $B$ lineáris transzformációt,
	amelyre $Be_j=f_j$ teljesül minden $j=1,\ldots,n$ mellett.
	Ezt a lineáris transzformációt nevezzük az
	$\left\{ e_1,\ldots,e_n \right\}$ bázisról az
	$\left\{ f_1,\ldots,f_n \right\}$ bázisra való \emph{áttérés transzformációjának}\index{az@áttérés transzformáció}.
\end{definition}
Világos, hogy ha $B$ az áttérés transzformáció, akkor ennek
a régi bázisban felírt $[B]$ mátrixa egy olyan $n\times n$ méretű mátrix,
amelynek $j$-edik oszlopa a $Be_j=f_j$ vektornak a régi bázisban felírt koordinátája.
Mivel $B$ szürjektív, ergo injektiv is, tehát $B$ egy izomorfizmus,
ennek megfelelően a $\left[ B \right]$ mátrix reguláris mátrix, azaz létezik
$\left[ B \right]^{-1}$ inverze.
\footnote{
	Könnyen látható, hogy $B$-nek a régi és az új bázisban felírt mátrixa azonos, de ez később egyszerű következményként is adódik.}
\section{Vektor koordinátái az új bázisban}
\begin{proposition}
	Legyen az $\left\{ e_1,\ldots,e_n \right\}$ a régi bázis,
	és $\left\{ f_1,\ldots,f_n \right\}$ az új bázis.
	Jelölje $B$ a régi bázisról az új bázisra való áttérést
	Ekkor tetszőleges $x\in V$ vektor mellett
	\[
		\left[ x \right]_\uj
		=
		\left[ B \right]_\rgi^{-1}
		\cdot
		\left[ x \right]_\rgi.\qedhere
	\]
\end{proposition}
\begin{proof}
	Legyen
	$
		\left[ x \right]_\uj
		=
		\begin{pmatrix}
			\alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n
		\end{pmatrix}.
	$
	Ez azt jelenti, hogy $x=\sum_{j=1}^n\alpha_jf_j$.
	Így felírva az $x$ vektornak a régi bázisra vonatkozó koordináta-vektorát
	\[
		\left[ x \right]_\rgi
		=\left[ \sum_{j=1}^n\alpha_jf_j \right]_\rgi
		=\sum_{j=1}^n\alpha_j\left[ f_j \right]_\rgi
		=\sum_{j=1}^n\alpha_j\left[ B \right]_\rgi^j
		=\left[ B \right]_\rgi\cdot \left[ x \right]_\uj.
	\]
	A kívánt azonosságot kapjuk,
	ha balról szorozzuk mindkét oldalt $\left[ B \right]^{-1}_\rgi$ inverz mátrixszal.
\end{proof}

\section{Lineáris operátorok mátrixa új bázis párban}
\begin{proposition}
	Legyen $A\in L\left( V,W \right)$ lineáris operáció
	és tegyük fel, hogy ismerjük $A$ mátrixát az
	$\left\{ e_1,\ldots,e_m \right\}\subseteq V$ és az
	$\left\{ f_1,\ldots,f_n \right\}\subseteq W$ régi bázisokban.
	Legyenek az $\left\{ v_1,\ldots,v_m \right\}\subseteq V$
	és a $\left\{ w_1,\ldots,w_n \right\}\subseteq W$ az új bázisok.
	Definiálja $B\in L\left( V \right)$ a $V$ tér áttérés lineáris transzformációját
	és $D\in L\left( W \right)$ a $W$ tér áttérés transzformációját.
	Ekkor
	\[
		\left[ A \right]_{\uj}=
		\left[ D \right]_{\rgi}^{-1}\left[ A \right]_{\rgi}\left[ B \right]_{\rgi}.\qedhere
	\]
\end{proposition}
\begin{proof}
	Azt mutatjuk meg, hogy a baloldali mátrix és a jobboldali szorzat mátrix megfelelő oszlopai megegyeznek.
	A $j$-edik oszlopra:
	\begin{multline*}
		\left[ A \right]_{\uj}^j
		=
		\left[ A \right]_{\substack{\left\{ v_1,\ldots,v_m \right\}\\ \left\{ w_1,\ldots,w_n \right\}}}^j
		=
		\left[ Av_j \right]_{\left\{ w_1,\ldots,w_n \right\}}
		\\
		=
		\left[ D \right]^{-1}_{\left\{ f_1,\ldots,f_n \right\}}\cdot
		\left[ Av_j \right]_{\left\{ f_1,\ldots,f_n \right\}}
		=
		\left[ D \right]^{-1}_{\left\{ f_1,\ldots,f_n \right\}}\cdot
		\left[ A \right]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}
		\cdot
		\left[ v_j \right]_{\left\{ e_1,\ldots,e_m \right\}}
		\\
		=
		\left[ D \right]^{-1}_{\left\{ f_1,\ldots,f_n \right\}}\cdot
		\left[ A \right]_{\substack{\left\{ e_1,\ldots,e_m \right\}\\ \left\{ f_1,\ldots,f_n \right\}}}
		\cdot
		\left[ B \right]_{\left\{ e_1,\ldots,e_m \right\}}^j
		\\
		=
		\left[ D \right]_\rgi^{-1}
		\left[ A \right]_\rgi
		\left[ B \right]_\rgi^j
		=
        \left[ \left[ D \right]^{-1}_\rgi\left[ A \right]_\rgi\left[ B \right]_\rgi \right]^j.
	\end{multline*}
	Közben használtuk a mátrix szorzás művelet asszociativitását,
	és azt a tényt,
	hogy tetszőleges $\left[ E \right],\left[ F \right]$ mátrixok mellett
    $\left[ \left[ E \right]\left[ F \right] \right]^j=\left[ E \right]\left( \left[F  \right]^j \right)$, azaz
    az $\left[ \left[ E \right]\left[ F \right] \right]$ szorzat $j$-edik oszlopa azonos az $\left[ E \right]$ mátrixnak az $\left[ F \right]$ mátrix $j$-edik oszlopával való szorzatával.
\end{proof}
\section{Lineáris transzformáció mátrixa az új bázisban}
\begin{proposition}
	Legyen $A\in L\left( V \right)$ egy lineáris transzformáció.
	Ekkor
	\[
		\left[ A \right]_{\uj}
		=
		\left[ B \right]^{-1}_\rgi\
		\left[ A \right]_\rgi
		\left[ B \right]_\rgi.\qedhere
	\]
\end{proposition}
\begin{proof}
	A lineáris operátorokra vonatkozó állítás speciális esete,
	mikor $W=V$ és $D=B$.
\end{proof}
A fenti állítás minden $A\in L\left( V \right)$ lineáris transzformáció mellett igaz.
Speciálisan, ha alkalmazzuk az $A=B$ esetre, akkor látjuk,
hogy az áttérés mátrixa azonos, ha az új bázisban, vagy a régi bázisban írjuk fel.
\begin{proposition}
	Legyen $B$ az áttérés lineáris transzformáció.
	Ekkor
	\[
		\left[ B \right]_\rgi=\left[ B \right]_\uj
		\quad\text{és persze}\quad
		\left[ B^{-1} \right]_\rgi=\left[ B^{-1} \right]_\uj.\qedhere
	\]
\end{proposition}

\chapter{Invariáns alterek}
\begin{definition}
	Legyen $A\in L\left( V \right)$ egy lineáris transzformációja a $V$ vektortérnek,
	és $M\subseteq V$ a tér egy altere.
	Az mondjuk, hogy $M$ egy \emph{invariáns altere}\index{invariáns altér}
	$V$-nek, ha minden $v\in M$ mellett $Av\in M$ is teljesül.
\end{definition}
Ha nem világos, hogy mely lineáris transzformációról van szó,
akkor azt is mondjuk, hogy az $M$ altér $A$-invariáns, vagy azt,
hogy az $M$ altér invariáns az $A$ transzformációra nézve.

Úgy is fogalmazhatnánk, hogy az $M$ altér akkor invariáns altér,
ha az $A$ transzformációnak az $M$-re való $A|_M$ megszorítása az $M$
altér egy lineáris transzformációja, azaz
\[
	A|_M\in L\left( M \right).
\]

Nyilvánvaló példa invariáns alterekre a teljes $V$ vektortér és a $\{0\}$ altér.
Tetszőleges $A$ lineáris transzformáció mellett $\ker A$ és $\im A$ mindig invariáns alterek.
Ugyanis, ha $u\in\ker A$, akkor $A\left( Au \right)=A0=0$, tehát $Au\in\ker A$.
Az $\im A$ altér esete még egyszerűbb: A tér minden elemének képe $\im A$-ban van, speciálisan persze ez $\im A$ elemeire is igaz.

A célunk, hogy a teret előállítsuk lehetőleg minél alacsonyabb dimenziós terek direkt összegeként.

\begin{definition}[legszűkebb invariáns altér]
	Legyen $A\in L\left( V \right)$ egy lineáris transzformáció.
	Világos, hogy akárhány $A$-ra nézve invariáns altér metszete is $A$-invariáns altér,
	így egy $H\subseteq V$ halmazt tartalmazó
	\emph{legszűkebb $A$-invariáns altér}\index{legszűkebb invariáns altér}
	a $H$ halmazt tartalmazó $A$-invariáns alterek közös része.
	Formálisan
	\[
		\lin\left( H;A \right)
		=
		\bigcap_{
			\substack{
				H\subseteq N\\
				N\text{altér }\\
				N\text{invariáns}}
		}N.\qedhere
	\]
\end{definition}
A legérdekesebb eset, amikor $H$ egy elemű halmaz.
A $H=\left\{ v \right\}$ esetben a kissé nehézkes
$\lin\left( \left\{ v \right\};A \right)$ helyett egyszerűbben $\lin\left( v;A \right)$-t írunk.

\begin{proposition}
	Egy $A\in L\left( V \right)$ lineáris transzformációra és egy $v\in V$ vektorra
	\[
		\lin\left( v;A \right)
		=
		\lin\left\{ v,Av,A^2v,\dots \right\}.\qedhere
	\]
\end{proposition}
\begin{proof}
	Mivel $\lin\left( v;A \right)$ egy $v$-t tartalmazó $A$-invariáns altér,
	ezért $\left\{ v,Av,\ldots,A^kv,\dots \right\}\subseteq \lin\left( v;A \right)$,
	amiatt
	\[
		\lin\left\{ v,Av,A^2v,\dots \right\}
		\subseteq
		\lin\left( v;A \right).
	\]
	Most vegyük észre, hogy
	\begin{math}
		\lin\left\{ v,Av,A^2v,\dots \right\}
	\end{math}
	is egy $v$-t tartalmazó $A$-invariáns altér és $\lin\left( v;A \right)$ ilyenek közt a legszűkebb, ezért
	\[
		\lin\left( v;A \right)
		\subseteq
		\lin\left\{ v,Av,A^2v,\dots \right\}.\qedhere
	\]
\end{proof}
\begin{lemma}
	Tegyük fel, hogy egy $A\in L\left( V \right)$ lineáris transzformációra és egy
	$v\in V$ vektorra a
	\[
		\left\{ v,Av,\ldots,A^kv \right\}
	\]
	lineárisan összefüggő ($k\geq 1$).
	Ekkor minden $n\geq k$ mellett
	\[
		A^nv\in\lin\left\{ v,Av,\ldots,A^{k-1}v \right\}.\qedhere
	\]
	\label{le:of}
\end{lemma}
\begin{proof}
	Ha $v=0$ akkor az állítás nyilvánvaló.
	Ha $v\neq 0$, akkor a $\left\{ v \right\}$ rendszer lineárisan független.
	Legyen $t$ a legkisebb olyan szám, hogy a rendszerhez $A^tv$-t hozzávéve az lineárisan összefüggővé válik.
	Ilyen módon tehát
	\[
		\left\{ v,Av,\ldots,A^{t-1}v \right\} \text{ lineárisan független}\tag{\dag}
	\]
	de
	\[
		\left\{ v,Av,\ldots,A^{t-1}v, A^tv \right\} \text{ lineárisan összefüggő.}\tag{\ddag}
	\]
	Világos, hogy $1\leq t\leq k$.
	Jelölje $N=\lin\left\{ v, Av,\ldots,A^{t-1}v \right\}$.

	Most indukcióval megmutatjuk, hogy minden $m\geq 0$ számra
	\[
		A^{t+m}v\in N.
	\]

	Ha $m=0$, akkor $A^tv\in N$, hiszen a fenti (\ddag) lineárisan összefüggő rendszerre,
	az egyik elem előáll mint az előző elemek lineáris kombinációja.
	No de, ez elem csak az utolsó lehet,
	hiszen az utolsó elem nélküli (\dag) rendszer még lineárisan független.

	Most tegyük fel, hogy $A^{t+m}v\in N$ és lássuk be, hogy $A^{t+m+1}v\in N$ is teljesül.
	Ezek szerint
	\begin{math}
		A^{t+m}v=\sum_{j=0}^{t-1}\alpha_jA^jv
	\end{math}
	alakú. Erre $A$-t alkalmazva
	\begin{displaymath}
		A^{t+m+1}v
		=
		\sum_{j=0}^{t-1}\alpha_jA^{j+1}v
		=
		\sum_{j=1}^{t}\alpha_{j-1}A^{j}v
		=
		\left( \sum_{j=1}^{t-1}\alpha_{ j-1}A^{j}v \right)+\alpha_{t-1}A^tv\in N+N=N.\qedhere
	\end{displaymath}
\end{proof}
\begin{proposition}
	Legyen $V$ egy véges dimenziós vektortér.
	$A\in L\left( V \right)$ lineáris transzformáció,
	és $v\in V$ egy $v\neq 0$ vektor.
	Ekkor létezik egyetlen $1\leq k\leq \dim(V)$ szám,
	amelyre
	\[
		\left\{ v,Av,\ldots,A^{k-1}v \right\} \text{ lineárisan független}\tag{\dag}
	\]
	de
	\[
		\left\{ v,Av,\ldots,A^{k-1}v, A^kv \right\} \text{ lineárisan összefüggő.}\tag{\ddag}
	\]
	A fenti (\dag) rendszer bázisa $\lin\left( v;A \right)$-nak.
\end{proposition}
\begin{proof}
	Először a $k$ szám konstrukciója:
	Mivel $v\neq 0$, ezért $\left\{ v \right\}$ lineárisan független.
	Ha $\left\{ v,Av \right\}$ lineárisan összefüggő,
	akkor $k=1$ és készen vagyunk.
	Egyébként tekintsük a $\left\{ v,Av,A^2v \right\}$ rendszert.
	Ha ez lineárisan összefüggő, akkor $k=2$-vel készen vagyunk.
	Ha ez lineárisan független, akkor tekintsük a
	$\left\{ v,Av,A^2v,A^3v \right\}$ vektorrendszert.
	Ha lineárisan összefüggő, akkor $k=3$ és készen vagyunk,
	ha lineárisan független, akkor folytatjuk egy újabb elem hozzá vételével.
	Az eljárás előbb-utóbb megáll a vektorrendszer összefüggővé válásával,
	hiszen a Steinitz-lemma \index{Steinitz-lemma} szerint egy véges dimenziós vektortérben legfeljebb
	$\dim(V)$ elemszámú lineárisan független vektorrendszer van.

	Mivel $\lin\left( v;A \right)$ egy $v$-t tartalmazó $A$-invariáns altér,
	ezért
	\[
		\left\{ v,Av,\ldots,A^{k-1}v \right\}
		\subseteq
		\lin\left( v;A \right).
	\]
	Az előző lemma szerint
	\[
		\left\{ v,Av,A^2v,\dots \right\}
		\subseteq
		\lin\left\{ v,Av,\ldots,A^{k-1}v \right\}
	\]
	ezért
	\[
		\lin\left( v;A \right)
		=
		\lin\left\{ v,Av,A^2v,\dots \right\}
		\subseteq
		\lin\left\{ v,Av,\ldots,A^{k-1}v \right\}
		\subseteq
		\lin\left( v;A \right).
	\]
	Ez azt jelenti, hogy a (\dag) vektorrendszer egy lineárisan független generátorrendszere
	a $\lin\left( v;A \right)$ térnek, tehát valóban bázisa is.
\end{proof}
A $\lin\left( v;A \right)$ tehát a fenti $k$-ra egy $k$-dimenziós altér,
amelynek bázisa $\left\{A^{k-1}v,\ldots,Av,v \right\}$.
A játék kedvéért írjuk fel az $A|_{\lin\left( v;A \right)}$ transzformáció mátrixát
ebben a bázisban:\label{ar:elsomegjelenes}
\[
	\begin{array}{r|ccccc}
		         & A^kv         & A^{k-1}v & A^{k-2}v & \dots      & Av     \\
		\hline
		A^{k-1}v & \alpha_{k-1} & 1        & 0        & \dots\dots & 0      \\
		A^{k-2}v & \alpha_{k-2} & 0        & 1        & \dots      & \vdots \\
		\vdots   & \vdots       & \vdots   & \ddots   & \ddots     & 0      \\
		Av       & \alpha_1     & 0        & 0        & \dots      & 1      \\
		v        & \alpha_0     & 0        & 0        & \dots      & 0
	\end{array},
	\text{ ahol }A^kv=\sum_{j=0}^{k-1}\alpha_jA^jv.
\]
\section{Transzformációk sajátértéke}
A célunk, hogy a lehető legalacsonyabb dimenziós invariáns altereket találjunk.
Így persze az 1 dimenziós invariáns alterek a legérdekesebbek.
Ez vezet a sajátérték fogalmához.
\begin{definition}[sajátérték, sajátvektor, spektrum]
	Legyen $A\in L\left( V \right)$.
	Azt mondjuk, hogy a $\lambda\in\mathbb{F}$ szám az $A$ lineáris transzformáció
	\emph{sajátértéke}\index{sajátérték},
	ha létezik $v\in V$, $v\neq 0$ vektor, amelyre
	\[
		Av=\lambda v
	\]
	teljesül.

	Ha $\lambda$ egy sajátértéke $A$-nak,
	akkor az összes olyan nem zérus $v$ vektort,
	amelyre $Av=\lambda v$ fennáll az $A$ transzformáció $\lambda$ sajátértékéhez tartozó
	\emph{sajátvektorainak}\index{sajátvektor} nevezzük.

	Egy $A$ lineáris transzformáció összes sajátértékeinek halmazát az
	$A$ \emph{spektrumának}\index{spektrum} nevezzük, és
	$\sigma\left( A \right)$-val jelöljük.
\end{definition}
A spektrum tehát az $\mathbb{F}$ test azon részhalmaza, amelyre
\[
	\sigma\left( A \right)
	=
	\left\{
	\lambda\in\mathbb{F}:\exists v\in V, v\neq 0, Av=\lambda v
	\right\}
\]
teljesül.
Világos, hogy ha $\lambda$ egy sajátértéke $A$-nak, akkor a $\lambda$-hoz tartozó
sajátvektorok halmaza éppen a
\[
	\ker\left( A-\lambda I \right)
\]
altér nem zérus elemei.
Emiatt a fenti alteret a $\lambda$ sajátértékhez tartozó \emph{sajátaltérnek}\index{sajátaltér}
mondjuk.

Vegyük észre,
hogy $v\in V$ vektor pontosan akkor sajátvektora $A$-nak, ha $\dim\left( \lin\left( v;A \right) \right)=1$.
Hasonlóan az is könnyű, hogy $\lambda$ pontosan akkor sajátértéke $A$-nak,
ha az $A-\lambda I$ transzformáció szinguláris.

\chapter{Transzformációk polinomjai}

\begin{definition}[transzformáció polinomja]
	Legyen $V$ az $\mathbb{F}$ test feletti vektortér,
	$A\in L\left( V \right)$ egy lineáris transzformáció, és
	legyen $p\in\mathbb{F}\left[t \right]$ egy az $\mathbb{F}$ test feletti
	polinom,
	amely $p\left( t \right)=
		\sum_{j=0}^n\alpha_jt^j$
	alakú.
	Definiálja $p\left( A \right)\in L\left( V \right)$
	az $A$ transzformáció $p$ polinomját
	\[
		p\left( A \right)
		=
		\sum_{j=0}^n\alpha_jA^j.\qedhere
	\]
\end{definition}
\begin{proposition}[Számolási szabályok]
	Legyen $p,q\in\mathbb{F}\left[ t \right]$ polinomok,
	$A\in L\left( V \right)$ lineáris transzformáció.
	\begin{enumerate}
		\item Ha $r=p+q$, akkor $r\left( A \right)=p\left( A \right)+q\left( A \right)$.
		\item Ha $r=pq$, akkor $r\left( A \right)=p\left( A \right)q\left( A \right)$.\qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Az első állítás azért teljesül, mert
	\(
	\left( \alpha A^k+\beta A^k \right)
	=
	\left( \alpha+\beta \right)A^k.
	\)

	A második állításhoz azt vegyük észre, hogy
	\(
	A^kA^l=A^{k+l}.
	\)
	Így, amikor összegyűjtjük, hogy a $q\left( A \right)p\left( A \right)$
	kompozícióban, mi lesz $A^j$ együthatója,
	akkor azt kapjuk,
	hogy
	\begin{displaymath}
		\left( \sum_{\substack{k,l\\k+l=j}}\alpha_k\beta_l \right)A^j
		=
		\left( \sum_{k=0}^j\alpha_k\beta_{j-k} \right)A^j.
	\end{displaymath}
	Vegyük észre, hogy \aref{def:polmuveletek}. definíció szerint az
	$r$ szorzat polinomban is a fenti zárójelben lévő szám a
	$t^j$ tag együtthatója.
\end{proof}
Tudjuk, hogy lineáris transzformációk szorzata függ azok sorrendjétől.
Ugyanúgy mint mátrixokra, két lineáris transzformációt \emph{kommutálónak}\index{kommutál}
mondunk, ha szorzatuk a szorzás sorrendjétől független.
Például az $I$ identitás minden transzformációval kommutál.
Azt is láttuk, hogy ha $AB=I$, akkor $BA=I$ is fennáll, azaz $A$ és $B$ kommutálnak.
Nagyon fontos, de nyilvánvaló következménye a fenti számolási szabálynak,
hogy ha $p,q$ tetszőleges polinomok, akkor a $p\left( A \right)$ és $q\left( A \right)$ egymással kommutáló lineáris transzformációk lesznek, hiszen az
$\mathbb{F}\left[ t \right]$ egy kommutatív gyűrű,
azért ha $r=pq$, akkor $qp=r$,
ergo
\[
	p\left( A \right)q\left( A \right)=r\left( A \right)=q\left( A \right)p\left( A \right).
\]
Tehát meggondoltuk, hogy
\begin{proposition}\index{kommutáló transzformációk}
	Lineáris transzformáció polinomjai egymással kommutálnak.\index{kommutál}
\end{proposition}
Polinomok segítségével sok-sok új invariáns alteret kapunk.
\begin{proposition}
	Tetszőleges $p$ polinomra és $A\in L\left( V \right)$
	lineáris transzformáció mellett
	$\ker p\left( A \right)$ és $\im p\left( A \right)$ is invariáns alterek.
\end{proposition}
\begin{proof}
	Legyen előszőr $v\in\ker p\left( A \right)$.
	Ekkor, mivel $A$ és $p\left( A \right)$ kommutálnak
	\[
		p\left( A \right)Av=Ap\left( A \right)v=A0=0,
	\]
	ami pont azt jelenti, hogy $Av\in\ker p\left( A \right)$.

	Legyen most $v\in\im p\left( A \right)$,
	azaz
	$v=p\left( A \right)x$ valamely $x\in V$ mellett.
	Ekkor, mivel $A$ és $p\left( A \right)$ kommutálnak
	\[
		Av=Ap\left( A \right)x=p\left( A \right)\left( Ax \right),
	\]
	amiből már látszik, hogy $Av\in\im p\left( A \right)$.
\end{proof}
Speciálisan ez igaz az $t-\lambda$ polinomra is,
amikor $\lambda$
egy sajátértéke $A$-nak. Tehát a $\lambda$ sajátértékhez tartozó
$\ker \left( A-\lambda I \right)$ sajátaltér egy legalább 1 dimenziós invariáns altere
$A$-nak.\index{sajátaltér}
\begin{definition}
	Azt mondjuk, hogy az $A$ lineáris transzformáció a $p$ polinom gyöke,
	ha $p\left( A \right)=0$.
\end{definition}
Mielőtt tovább lépünk érdemes visszagondolnunk arra,
hogy egy test feletti polinomgyűrű egy főideálgyűrű.\index{főideál-gyűrű}\index{főideál}
Láttuk ugyanis
-- \ref{pr:pgyurufoidealgyuru}.~állítás --,
hogy minden nem csak a $\left\{ 0 \right\}$ elemet tartalmazó ideálnak
van egyetlen legkisebb fokú és normált eleme,
ami egyben az ideál generáló eleme is.

\section{Kis minimálpolinom}
\begin{proposition}
	Legyen $A\in L\left( V \right)$ a $V$ véges dimenziós vektortér egy lineáris transzformációja,
	és legyen $v\in V$ egy rögzített vektor.
	Tekintsük az
	$\mathbb{F}\left[ t \right]$ polinomgyűrű következő részhalmazát:
	\[
		J_{A,v}
		=
		\left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)v=0 \right\}.
	\]
	Ez a halmaz egy ideálja $\mathbb{F}\left[ t \right]$-nek,
	amelynek van legfeljebb $\dim(V)$-ed fokú, de nem konstans zérus polinomja.
\end{proposition}
\begin{proof}
	Ha $p,q\in J_{A,v}$,
	akkor
	$\left( p+q \right)(A)v=p\left( A \right)v+q\left( A \right)v=0+0=0$,
	azaz $p+q\in J_{A,v}$.
	Ha most $p\in J_{A,v}$ és $h$ egy tetszőleges polinom,
	akkor
	$
		\left( hp \right)(A)v=h\left( A \right)p\left( A \right)v=h\left( A \right)0=0,
	$
	azaz $hp\in J_{A,v}$.
	Megmutattuk tehát, hogy $J_{A,v}$ egy ideálja a polinomgyűrűnek.

	Jelölje $n=\dim(V)$ és tekintsük az $n+1$ elemű
	$\left\{ v,Av,\ldots,A^nv \right\}$ vektorrendszert.
	Mivel a Steinitz-lemma szerint \index{Steinitz-lemma}
	$n+1$ vektor egy $n$-dimenziós vektortérben lineárisan összefüggő,
	ezért van
	$\alpha_0,\ldots,\alpha_n\in\mathbb{F}$ nem mind zérus szám, hogy
	$\sum_{j=0}^n\alpha_jA^jv=0$.
	Ha tehát $p$ jelöli a $p\left( t \right)=\sum_{j=0}^n\alpha_jt^j$ polinomot,
	akkor
	$p\left( A \right)v=0$, azaz $p\in J_{A,v}$ és $-\infty<\deg p\leq n$.
\end{proof}
Ha például $v=0$, akkor $J_{A,0}=\mathbb{F}\left[ t \right]$, azaz minden polinom
az ideálhoz tartozik.
Ha $v\neq 0$,
akkor a $J_{A,v}$ ideálnak nincs nulladfokú polinomja,
hiszen $p(t)=c, (c\neq 0)$ mellett
$p\left( A \right)v=(cI)v=cv\neq 0$.

\begin{definition}
	Legyen $A\in L\left( V \right)$ a $V$ véges dimenziós vektortér egy lineáris transzformációja,
	és legyen $v\in V$ egy rögzített vektor.
	Láttuk, hogy
	\[
		J_{A,v}
		=
		\left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)v=0 \right\}
	\]
	az $\mathbb{F}\left[ t \right]$ gyűrű egy ideálja, amely nem csak a konstans zéruspolinomot tartalmazza.
	Tudjuk, hogy az $\mathbb{F}\left[ t \right]$ polinomgyűrű egy főideál-gyűrű,\index{főideál}\index{főideál-gyűrű}
	van tehát egyetlen normált polinomja $J_{A,v}$-nek, amely generálja $J_{A,v}$-t.
	Ez a $J_{A,v}$ legkisebb fokú, normált polinomja.
	Ezt a polinomot nevezzük az $A$ transzformáció, 
    \emph{kis minimálpolinomjának}\index{kis minimálpolinom}.\label{def:kisminimal}
    a $v$ pontban.
\end{definition}

\begin{proposition}
	Az  $n$  polinom pontosan akkor az $A$ transzformáció $v$ vektorhoz tartozó kis minimálpolinomja, ha
	\begin{enumerate}
		\item $n$ normált polinom,
		\item $n\left( A \right)v=0$,
		\item ha $p\in\mathbb{F}\left[ t \right], p\neq 0$, amelyre
		      $p\left( A \right)v=0$, akkor $n|p$.
	\end{enumerate}
	A kis minimálpolinom foka legfeljebb a tér dimenziója.
\end{proposition}
\begin{proof}
	Definíció szerint $n$ azon $p$ polinomok közül,
	amelyek normáltak, és $p\left( A \right)v=0$, a legalacsonyabb fokú.
	Láttuk hogy ilyen polinom csak egy van, és erre a polinomra
	\[
		J_{A,v}=J\left( n \right)=\left\{ hn:h\in\mathbb{F}\left[ t \right] \right\}.
	\]
	Ezt kellett belátni.
\end{proof}
\begin{proposition}
	Legyen $v\neq 0$ és tegyük fel, hogy $p\left( A \right)v=0$ valamely normált,
	irreducibilis $p$ polinomra.
	Ekkor $p$ a $v$ vektorhoz tartozó kis minimálpolinom.
\end{proposition}
\begin{proof}
	Ha maga $\deg p=1$, akkor készen vagyunk, hiszen nem zérus vektornak kis minimálpolinomja legalább első fokú.
	Ha $\deg p>1$ és $p$ nem egyezne az $n$ kis minimálpolinommal,
	akkor $\deg n<\deg p$ lenne, és mivel $n$ generálja a $J_{A,v}$ ideált,
	ezért $p=nh$ alakú lenne, ahol $h$ is legalább első fokú.
	Így $p$ két legalább első fokú polinom szorzata, azaz reducibilis lenne.
\end{proof}

A következő állítás módszert ad a kis minimálpolinom meghatározására,
amely mindig használható.
\begin{proposition}
	Legyen $A\in L\left( V \right)$ és $v\neq 0.$
	Feltéve, hogy $V$ egy véges dimenziós vektortér,
	létezik $1\leq k \leq \dim(V)$,
	hogy
	$\left\{ v,Av,\ldots,A^{k-1}v \right\}$ lineáris független,
	de $\left\{ v,Av,\ldots,A^{k-1}v,A^{k}v \right\}$ lineárisan összefüggő.
	Léteznek tehát $\alpha_0,\ldots,\alpha_{k-1}\in\mathbb{F}$
	számok, amelyekre
	\[
		A^{k}v+\sum_{j=0}^{k-1}\alpha_jA^{j}v=0.
	\]
    Ekkor
    \begin{enumerate}
        \item 
	    Az $A$ operátor ezen $v$ vektorhoz tartozó kis minimálpolinomja
    	\[
	    	n\left( t \right)=
            t^k+\alpha_{k-1}t^{k-1}+\cdots+\alpha_1t+\alpha_0.
    	\]
        \item
        A $v$-hez tartozó kis minimálpolinom foka megegyezik a $v$-t tartalmazó legszűkebb invariáns altér dimenziójával, azaz 
        \[
            \deg n=\dim\left( \lin\left( v;A \right) \right).
        \]
        \item
        Sőt, minden $x\in\lin\left( v;A \right)$ mellett $n\left( A \right)x=0$ is fennáll, azaz
        \[
            n(A)|_{\lin\left( v;A \right)}=0|_{\lin\left( v;A \right)}.\qedhere
        \]
    \end{enumerate}
\end{proposition}
\begin{proof}
	Láttuk, hogy $\left\{ v,Av,\ldots,A^{k-1}v \right\}$ bázisa $\lin\left( v;A \right)$ altérnek.
	Világos, hogy $A^kv\in\lin\left( v;A \right)$, emiatt a kívánt előállítás valóban létezik.
	Ezt átrendezve kapjuk, hogy $n$ valóban olyan normált polinom, amelyre $n\left( A \right)v=0$ fennáll.
	No de, $k$-nál alacsonyabb fokú ilyen polinom csak a konstans zérus polinom lehet,
	hiszen $\left\{ v,Av,\ldots,A^{k-1}v \right\}$ lineárisan független.
	Azt láttuk tehát, hogy $n$ a legalacsonyabb fokú nem zérus eleme $J_{A,v}$-nek,
	tehát $n$ generálja a főideált.

    A fentiek szerint $\dim\left( \lin\left( v;A \right) \right)=k=\deg n$.

    Azt már láttuk, hogy $n\left( A \right)v=0$. 
    A $\lin\left( v;A \right)$ egy bázisa $\left\{ v,Av,\ldots,A^{k-1}v \right\}$ és minden bázis elemre
    \[
        n\left( A \right)\left( A^jv \right)=A^j\left( n\left( A \right)v \right)=A^j\left( 0 \right)=0,
    \]
    hiszen $n\left( A \right)$ és $A^j$ kommutáló transzformációk.
    Azt kaptuk tehát, hogy az $n\left( A \right)$ transzformáció a $\lin\left( v;A \right)$ egy bázisát zérusra viszi,
    ezért maga $n\left( A \right)$ a $\lin\left( v;A \right)$ altér zérus transzformációja.
\end{proof}
\subsubsection{Illusztráció}\label{se:kisminimalillusztracio}
Legyen $\left\{ u_1,u_2,u_3 \right\}$ bázisa a $\mathbb{C}$ komplex számtest feletti $V$ vektortérnek.
Az $A\in L\left( V \right)$ lineáris transzformációra
\[
    A\left( \alpha u_1+\beta u_2 +\gamma u_3 \right)
    =
    \left( -3\beta -2\gamma \right)u_1 +
    \left( \alpha-\beta+\gamma \right)u_2+
    \left( -\alpha+3\beta+\gamma \right)u_3.
\]
Írjuk fel az $u_3$ vektorhoz tartozó kis minimálpolinomot.

Világos, hogy $A$ mátrixa a megadott bázisban
\[
    \begin{pmatrix}
        0&-3&-2\\
        1&-1&1\\
        -1&3&1
    \end{pmatrix}.
\]
Az előző állításban megadott algoritmust használjuk:
\[
\begin{array}{r|ccc}
	     & Au_3        & A^2u_3 & A^3u_3\\
	\hline
	     & -2          &     -5 &     -6\\
         &\framebox{1} &     -2 &      3\\
	u_3  &  1          &      6 &      5\\
	\hline
	     &\delta       &     -2 &      3
\end{array}
\quad
\begin{array}{r|cc}
	     &  A^2u_3      & A^3u_3\\
	\hline
        &\framebox{-9} &     0 \\
	Au_3 &      -2      &     3 \\
	u_3  &       8      &     2 \\
	\hline
	     &\delta        &     0
\end{array}
\quad
\begin{array}{r|l}
	     &A^3u_3\\
	\hline
   A^2u_3&    0 \phantom{\framebox{0}}\\
	Au_3 &    3 \\
	u_3  &    2 \\
	\hline
         &\phantom{\delta}
\end{array}
\]
Azt kapjuk tehát, hogy
\(
	\left\{ u_3,Au_3,A^2u_3\right\}
    \)
még lineárisan független, de
\(
A^3u_3-3Au_3-2u_3=0.
\)
Ezek szerint az $u_3$ vektorhoz tartozó kis minimálpolinom:
\(
n\left( t \right)=t^3-3t-2,
\)
amivel a feladatot meg is oldottuk.

Érdemes még látni, hogy mit mond az előző állítás a fenti transzformációról.
Mivel $\lin\left( u_3;A \right)$ egy 3 dimenziós altere a $V$ három dimenziós vektortérnek,
ezért $\lin\left( u_3;A \right)=V$, így $A^{3}-3A-2I=0$ nem csak $u_3$ vektort, de $V$ minden vektorát zérusra viszi,
azaz $n\left( A \right)=A^3-3A-2I=0$.
Alkalmazhatjuk \apageref{ar:elsomegjelenes}.~lapon felírt mátrixot erre a transzformációra.
Így azt kapjuk, hogy $A$-nak az $\left\{A^2u_3,Au_3,u_3\right\}$ bázisban felírt mátrixa a következő.
\[
    \begin{pmatrix}
        0&1&0\\
        3&0&1\\
        2&0&0
    \end{pmatrix}.
\]
Azt érdemes látni, hogy ebben a mátrixban az első oszlopban találjuk a $v_3$ vektorhoz tartozó kis minimálpolinom együtthatóinak ellentetjét
a $k-1$-ediktől kezdve sorban lefelé (a $k$ adik a legmagasabb fokú együttható mindig 1.)
A mátrix szuper diagonálisában csupa 1-es szerepel, és az összes többi tag zérus. 
Az ilyen mátrixoknak központi szerepe van, amint azt később látni fogjuk.

Első alkalmazásként megmutatjuk, hogy minden normált polinom lehet kis minimálpolinom.
\begin{proposition}\label{pr:polinom-kis-minimalpolinom}
    Legyen $p\left( t \right)\in\mathbb{F}\left[ t \right]$ egy tetszőleges normált $k$-ad fokú polinom, ahol $k\geq 1$.
    Ekkor tetszőleges $V$ éppen $k$ dimenziós vektortér tetszőleges $v\in V$, $v\neq 0$ vektorához van olyan $A\in L\left( V \right)$
    lineáris transzformáció, amelyre a $v$-hez tartozó kis minimálpolinom éppen $p$.
\end{proposition}
\begin{proof}
    Jelölje 
    \(
    p\left( t \right)=t^k+\alpha_{k-1}t^{k-1}+\dots+\alpha_1t+\alpha_0,
    \)
    $v_1=v.$ 
    A $v_1\neq 0$ vektor mint egy elemű vektorrendszer lineárisan független, ezért kiegészíthető a tér
    $\left\{ v_k,\ldots,v_2,v_1 \right\}$ bázisává.
    Legyen $A\in L\left( V \right)$ az a lineáris transzformáció, 
    amelynek ebben a bázisban felírt mátrixa\footnote{Az első oszlopban sorban lefelé $p\left( t \right)$ együtthatóinak ellentetje, a szuper diagonálisban 1, mindenütt másutt zérus.}
    \[
        \begin{pmatrix}
            -\alpha_{k-1}&1&0&\dots&0\\
            -\alpha_{k-2}&0&1&\dots&0\\
            \vdots       &\vdots &\ddots & \ddots    &\vdots\\
            -\alpha_1    &0&0&\ddots&1\\
            -\alpha_0    &0&0&\dots&0
        \end{pmatrix}.
    \]
    Ez persze azonos avval, mintha $A$-t a $\left\{ v_k,\ldots,v_1 \right\}$
    bázison definiálnánk az alábbi módon:
    \[
        A\left( v_j \right)=v_{j+1}, j=1,\ldots,k-1\mbox{ esetén, és } 
        A\left( v_k \right)=\sum_{j=0}^{k-1}-\alpha_jv_{j+1}.
    \]
    Ekkor minden $j=0,\ldots,k-1$ mellett $v_{j+1}=A^jv_1$, 
    ezért $\left\{ v_1,Av_1,\ldots,A^{k-1}v_1 \right\}=\left\{ v_1,v_2,\ldots,v_{k} \right\}$ lineárisan független rendszer.
    A mátrix első oszlopa szerint 
    \[
        A^kv_1=
        A\left( A^{k-1}v_1 \right)=
        A\left( v_k \right)=
        \sum_{j=0}^{k-1}-\alpha_jv_{j+1}=
        \sum_{j=0}^{k-1}-\alpha_jA^jv_1,
    \]
    ami pont azt jelenti, hogy $p(A)v_1=0$, 
    ezért valóban $p\left( t \right)$ a $v_1$-hez tartozó kis minimálpolinom.
\end{proof}
Vegyük észre, hogy a fenti bizonyításban
$V$ éppen $k$ dimenziós, így csak $\lin\left( v_1;A \right)=V$ lehetséges.
Ez azt jelenti, hogy $p\left( A \right)$ nem csak $v_1$-et, de $V$ valamennyi más vektorát is zérusra viszi,
ergo $p\left( A \right)=0$, sőt $p$ a legalacsonyabb fokú polinom, amire ez teljesül,
hiszen $\left\{ v_1,Av_1,\ldots,A^{k-1}v_1 \right\}$ vektorrendszer még lineárisan független.

Ez vezet a minimálpolinom fogalmához.

\section{Minimálpolinom}
\begin{proposition}
	Legyen $A\in L\left( V \right)$ a $V$ véges dimenziós vektortér egy lineáris transzformációja.
	Tekintsük az
	$\mathbb{F}\left[ t \right]$ polinomgyűrű következő részhalmazát.
	\[
		J_{A}
		=
		\left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)=0 \right\}
	\]
	Ez a halmaz egy ideálja $\mathbb{F}\left[ t \right]$-nek,
	amelynek van legfeljebb $\left( \dim(V) \right)^2$-ed fokú,
	de nem konstans zérus polinomja.
\end{proposition}
\begin{proof}
	Ha $p,q\in J_{A}$,
	akkor
	$\left( p+q \right)(A)=p\left( A \right)+q\left( A \right)=0+0=0$,
	azaz $p+q\in J_{A}$.
	Ha most $p\in J_{A}$ és $h$ egy tetszőleges polinom,
	akkor
	$
		\left( hp \right)(A)=h\left( A \right)p\left( A \right)=h\left( A \right)0=0,
	$
	azaz $hp\in J_{A}$.
	Megmutattuk tehát, hogy $J_{A}$ egy ideálja a polinomgyűrűnek.

	Jelölje $n=\dim(V)$ és tekintsük az $n^2+1$ elemű
	$\left\{ I,A,\ldots,A^{n^2} \right\}$ rendszerét az $L\left( V \right)$ vektortérnek.
	Mivel a Steinitz-lemma szerint \index{Steinitz-lemma}
	$n^2+1$ vektor egy $n^2$-dimenziós vektortérben lineárisan összefüggő,
	ezért van
	$\alpha_0,\ldots,\alpha_{n^2}\in\mathbb{F}$ nem mind zérus szám, hogy
	$\sum_{j=0}^{n^2}\alpha_jA^j=0$.
	Ha tehát $p$ jelöli a $p\left( t \right)=\sum_{j=0}^{n^2}\alpha_jt^j$ polinomot,
	akkor
	$p\left( A \right)=0$, azaz $p\in J_{A}$ és $-\infty<\deg p\leq n^2$.
\end{proof}
\begin{definition}[minimálpolinom]\index{minimálpolinom}
	Legyen $A\in L\left( V \right)$ a $V$ véges dimenziós vektortér egy lineáris transzformációja.
	Láttuk, hogy
	\[
		J_{A}
		=
		\left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)=0 \right\}
	\]
	az $\mathbb{F}\left[ t \right]$ gyűrű egy ideálja, amely nem csak a konstans zéruspolinomot tartalmazza.
	Tudjuk, hogy az $\mathbb{F}\left[ t \right]$ polinomgyűrű egy főideál-gyűrű,\index{főideál}\index{főideál-gyűrű}
	van tehát egyetlen normált polinomja $J_{A}$-nek, amely generálja $J_{A}$.
	Ez a $J_{A}$ legkisebb fokú, normált polinomja,
	amit $A$ transzformáció \emph{minimálpolinomjának} nevezünk.
\end{definition}
Ha $V\neq\left\{ 0 \right\}$, ergo $\dim(V)\geq 1$,
akkor a $J_{A}$ ideálnak nincs nulladfokú polinomja,
hiszen $p(t)=c, (c\neq 0)$ mellett
$p\left( A \right)=cI\neq 0$,
tehát legalább egy dimenziós tér egy lineáris transzformációjának a minimálpolinomja legalább első fokú.
\begin{proposition}
	Az  $m\in\mathbb{F}\left[ t \right]$ polinom pontosan akkor
	az $A\in L\left( V \right)$ transzformáció minimálpolinomja,
	ha
	\begin{enumerate}
		\item $m$ normált polinom,
		\item $m\left( A \right)=0$,
		\item ha $p\in\mathbb{F}\left[ t \right], p\neq 0$, amelyre
		      $p\left( A \right)=0$, akkor $m|p$.
	\end{enumerate}
	A minimálpolinom fokszámára:%
	\footnote{Kisvártatva kiderül, hogy $\deg m\leq \dim(V)$ is igaz.}%
	$\deg m\leq\dim(V)^2$.
\end{proposition}
\begin{proof}
	Definíció szerint $m$ azon $p$ polinomok közül,
	amelyek normáltak, és $p\left( A \right)=0$, a legalacsonyabb fokú.
	Láttuk hogy ilyen polinom csak egy van, és erre a polinomra
	\[
		J_{A}=J\left( m \right)=\left\{ hm:h\in\mathbb{F}\left[ t \right] \right\}.
	\]
	Ezt kellett belátni.
\end{proof}
\begin{proposition}
	Legyen $V$ legalább 1 dimenziós,
	és tegyük fel, hogy $p\left( A \right)=0$ valamely normált,
	irreducibilis $p$ polinomra.
	Ekkor $p$ az $A$ minimálpolinomja.
\end{proposition}
\begin{proof}
	Ha maga $\deg p=1$, akkor készen vagyunk, hiszen a minimálpolinom legalább első fokú.
	Ha $\deg p>1$ és $p$ nem egyezne az $m$ minimálpolinommal,
	akkor $\deg m<\deg p$ lenne, és mivel $m$ generálja a $J_{A}$ ideált,
	ezért $p=mh$ alakú lenne, ahol $h$ is legalább első fokú.
	Így $p$ két legalább első fokú polinom szorzata, azaz reducibilis lenne.
\end{proof}

A következő állítás módszert ad a minimálpolinom meghatározására.
\begin{proposition}
	Legyen $A\in L\left( V \right)$, ahol $\dim(V)\geq 1$.
	Mivel $\dim(L)\left( V \right)=\left( \dim(V) \right)^2$ egy véges dimenziós vektortér,
	ezért létezik $1\leq k \leq (\dim(V))^2$,
	hogy
	$\left\{ I,A,\ldots,A^{k-1} \right\}$ lineárisan független,
	de $\left\{ I,A,\ldots,A^{k-1},A^{k} \right\}$ lineárisan összefüggő.
	Ekkor léteznek $\alpha_0,\ldots,\alpha_{k-1}\in\mathbb{F}$
	számok, amelyekre
	\(
		A^{k}=\sum_{j=0}^{k-1}\alpha_jA^{j},
	\)
	így az $A$ transzformáció minimálpolinomja a következő alakú:
	\[
		m\left( t \right)=
		t^k-\alpha_{k-1}t^{k-1}-\dots-\alpha_1t-\alpha_0.\qedhere
	\]
\end{proposition}
\begin{proof}
	Mivel a $k+1$ elemű rendszer lineárisan összefüggő és a $k$ elemű rendszer lineárisan független,
	ezért a kívánt előállítás valóban létezik.
	Ezt átrendezve kapjuk, hogy $m$ valóban olyan normált polinom, amelyre $m\left( A \right)=0$ fennáll.
	No de, $k$-nál alacsonyabb fokú ilyen polinom csak a konstans zérus polinom lehet,
	hiszen $\left\{ I,A,\ldots,A^{k-1} \right\}$ lineárisan független.
	Azt láttuk tehát, hogy $m$ a legalcsonyabb fokú nem zérus eleme $J_{A}$-nek,
	tehát az $m$ polinom generálja a főideált.\index{főideál}
\end{proof}
A minimálpolinom algoritmikus meghatározásához a kis minimálpolinomok is használhatók.
\begin{proposition}
    \index{kis minimálpolinom}\label{pr:lkkt-minimalpolinom}
	Legyen az $A\in L\left( V \right)$ lineáris transzformáció minimálpolinomja $m$.
	Tegyük fel, hogy az $\left\{ e_1,\ldots,e_n \right\}$ egy bázisa $V$-nek,
	és $p_1,\ldots,p_n$ rendre a bázis elemekhez tartozó kis minimálpolinomok.
	Ekkor a $p_1,\ldots,p_n$ polinomok legkisebb közös többszöröse az $m$ minimálpolinom.
	\index{legkisebb közös többszörös}
\end{proposition}
\begin{proof}
	Tetszőleges $v\in V$ mellett, ha $p_v$ a kis minimálpolinom,
	akkor $p_v|m$, hiszen $m\left( A \right)$ a konstans zérus transzformáció.
	Emiatt $m$ egy közös többszöröse a $p_j$ kis minimálpolinomoknak.
	Most tegyük fel, hogy egy $p$ polinom többszöröse a $p_j$ kis minimálpolinomoknak.
	Világos, hogy minden $j$ mellett
	\[
		p\left( A \right)e_j
		=
		h_j\left( A \right)\left( p_j\left( A \right)e_j \right)
		=
		h_j\left( A \right)0
		=
		0.
	\]
	Mivel egy lineáris transzformáció egy bázison egyértelműen meghatározott, ezért
	$p\left( A \right)=0$.
	Ekkor persze $m|p$,
	azaz $m$ valóban  a kis minimálpolinomok legkisebb közös többszöröse.
\end{proof}
\begin{proof}[Egy másik bizonyítás]\index{főideál}
	Mivel a $p\left( A \right)$ lineáris transzformáció a bázison egyértelműen meghatározott,
	ezért $p\in J_A$ pontosan akkor teljesül,
	ha $p\in J_{A,e_i}$ a bázis minden $e_i$ elemére.
	Így
	\[
		\cap_{i=1}^nJ(p_i)
		=
		\cap_{i=1}^nJ_{A,e_i}
		=
		J_A
		=
		J(m).
	\]
	No de, a baloldali ideál is főideál, aminek a normált generáló eleme
	-- \aref{pr:lkkt}. állítás szerint --
	a $p_1,\ldots,p_n$ polinomok legkisebb közös többszöröse.
	Mivel egy főidálnak csak egy normált generáló eleme van,
	ezért a legkisebb közös többszörös azonos az $m$ minimálpolinommal.
\end{proof}
\subsubsection{Illusztráció}\label{se:minimalillusztracio}
Újra vegyük elő \apageref{se:kisminimalillusztracio}.~oldalon vett mátrixot.
\[
    A=
    \begin{pmatrix}
        0&-3&-2\\
        1&-1&1\\
        -1&3&1
    \end{pmatrix}.
\]
Láttuk, hogy az $u_3$ bázisvektorhoz tartozó kis minimálpolinom $t^3-3t-2$.
Az is világos, hogy az $u_3$ vektort tartalmazó legszűkebb invariáns altér dimenziója a kis minimálpolinom foka, 
ergo 3.
Így $\lin\left( u_3;A \right)=V$, tehát $A^3-3A-2I=0$.
Azt is meggondoltuk, hogy $3$-nál alacsonyabb fokú normált polinom nincs, ami $u_3$ vektort zérusra viszi,
azaz $m\left( t \right)=t^{3}-3t-2$ az $A$ minimálpolinomja.

Persze itt a szerencsénk, hogy $u_3$ tartalmazó legszűkebb invariáns altér az egész vektortér.
Ha ezt nem használjuk, 
akkor \aref{pr:lkkt-minimalpolinom}.~állítás eliminációs algoritmusát alkalmazhatjuk a bázis elemekhez tartozó kis minimálpolinomok felírására, 
majd ezek legkisebb közös többszöröseként kapjuk a transzformáció minimálpolinomját.
\[
		\begin{array}{r|cc}
			    & Au_1          & A^2u_1 \\
			\hline
			u_1 &  0            &-1      \\
			    & \framebox{1}  &-2      \\
			    & -1            & 2      \\
			\hline
			    & \delta        &-2
		\end{array}
		\qquad
		\begin{array}{r|c}
			     & A^2u_1 \\
			\hline
			u_1  & -1     \\
			Au_1 & -2     \\
			     & 0      \\
			\hline
			     &
		\end{array}
		\qquad\qquad
		\begin{array}{r}
			\left\{ u_1,Au_1 \right\}\mbox{ lineárisan független, de} \\
			A^2u_1+2Au_1+u_1=0,                                        \\
			\mbox{ezért } p_1\left( t \right)=t^2+2t+1=\left( t+1 \right)^2.
		\end{array}
\]
Tehát itt nem volt olyan szerencsénk mint $u_3$ esetében.
Most $\lin\left( u_1;A \right)$ egy 2 dimenziós altér $V$-ben.
Csak $x\in\lin\left( u_1;A \right)$ vektoraira tudjuk, hogy $A^2x+2Ax+x=0$.
Folytatnunk kell a második bázisvektorra:
\[
		\begin{array}{r|cc}
			    & Au_2          & A^2u_2 \\
			\hline
			    & -3            &-3      \\
		   u_2  & -1            & 1      \\
                & \framebox{3}  & 3      \\
			\hline
			    & \delta        & 1
		\end{array}
		\qquad
		\begin{array}{r|c}
			     & A^2u_2 \\
			\hline
			     &  0     \\
			 u_2 &  2     \\
			Au_2 &  1     \\
			\hline
			     &
		\end{array}
		\qquad\qquad
		\begin{array}{r}
			\left\{ u_2,Au_2 \right\}\mbox{ lineárisan független, de} \\
			A^2u_2-Au_2-2u_2=0,                                        \\
			\mbox{ezért } p_2\left( t \right)=t^2-t-2=\left( t+1 \right)\left( t-2 \right).
		\end{array}
\]
Ugyanúgy mint az imént az $u_2$ vektort tartalmazó legszűkebb invariáns altér csak két dimenziós.
Látni fogjuk, hogy a minimálpolinom foka legfeljebb a tér dimenziója. 
Ha ezt most felhasználjuk, akkor nem is kell tovább lépnünk, hiszen egy olyan legfeljebb 3-ad fokú normált polinomot keresünk, amely mind $p_1\left( t \right)=\left( t+1 \right)^2$ polinomnak,
mind $p_2\left( t \right)=\left( t+1 \right)\left( t-2 \right)$ polinomnak többszöröse.
Persze ilyen polinom csak egy van az
\[
    m\left( t \right)=\left( t+1 \right)^2\left( t-2 \right).
\]
Egy kicsi szerencsénk mégis volt a fenti eljárásban, 
hiszen már a második bázisvektor után kiderült a minimálpolinom. 
Általában csak azt tudjuk, hogy a kis minimálpolinomok legkisebb közös többszöröse a minimálpolinom.
Ez a mi konkrét esetünkben azt jelenti, 
hogy az $m(t)$ minimálpolinom a $p_1(t)=\left( t+1 \right)^2$, 
a $p_2\left( t \right)=(t+1)(t-2)$ és a $p_3(t)=t^3-3t-2=\left( t+1 \right)^2\left( t-2 \right)$ polinomok legkisebb közös többszöröse, ami most nyilvánvaló módon teljesül.

Hasonlóan mint a kis minimálpolinom esetében, most is igaz, hogy minden normált polinom egy minimálpolinom.
Ez \aref{pr:polinom-kis-minimalpolinom}.~állítás kiterjesztése. 
Csak azt kell még észrevennünk $A|_{\lin\left( v;A \right)}$ transzformáció minimálpolinomja éppen a $v$-hez tartozó
kis minimálpolinom.
\begin{proposition}\label{pr:polinom-minimalpolinom}
    Legyen $m\left( t \right)\in\mathbb{F}\left[ t \right]$ egy tetszőleges normált $k$-ad fokú polinom, ahol $k\geq 1$.
    Ekkor tetszőleges $V$ éppen $k$ dimenziós vektortér tetszőleges $v\in V$, $v\neq 0$ vektorához van olyan $A\in L\left( V \right)$
    lineáris transzformáció, 
    amelyre a $v$-hez tartozó kis minimálpolinom éppen $m$.
    Így $\lin\left( v;A \right)$ is $k$-dimenziós, ezért $m$ a minimálpolinomja is az $A$-nak.
\end{proposition}
\section{Sajátvektorok és diagonalizálhatóság}\index{sajátvektor}\index{diagonalizálhatóság}

\begin{proposition}
	Tegyük fel, hogy $p$ legalább elsőfokú osztója az $A$ transzformáció minimálpolinomjának.
	Ekkor $p\left( A \right)$ szinguláris\index{szinguláris}.
\end{proposition}
\begin{proof}
	Jelölje $m$ a minimálpolinomot,
	és $m=pq$.
	Így $\deg q<\deg m$. Persze $m\left( A \right)=p\left( A \right)q\left( A \right)$,
	így ha $p\left( A \right)$ reguláris lenne,
	akkor
	\[
		q\left( A \right)=p\left( A \right)^{-1}m\left( A \right)=0
	\]
	Ebből persze $m|q$, így $\deg m\leq \deg q$, ami ellentmondás.
\end{proof}
\begin{proposition}[sajátérték és minimálpolinom]\index{sajátérték}
	Legyen $A\in L\left( V \right)$ lineáris transzformációja a $V$ véges dimenziós
	vektortérnek, és $\lambda\in\mathbb{F}$ egy szám, valamint $m$ az $A$ minimálpolinomja.
	Az alábbi feltevések ekvivalensek:
	\index{kis minimálpolinom}
	\begin{enumerate}
		\item $\lambda$ sajátértéke $A$-nak,
		\item $\ker (A-\lambda I)\neq \left\{ 0 \right\}$,
		\item $A-\lambda I$ szinguláris,
		\item létezik $v\in V$,
		      amelyre a $v$-hez tartozó kis minimálpolinomja $A$-nak $p_v(t)=t-\lambda$.
		\item $t-\lambda|m\left( t \right)$,
		\item $m\left( \lambda \right)=0.$\qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Az első három pont ekvivalenciája nyilvánvaló,
	majd a $3\implies 4\implies 5\implies 6\implies 5\implies 3$
	utat érdemes követni.
	Az utolsó lépéshez mutattuk meg az előző állítást.
\end{proof}
Az egyik legfontosabb definícióhoz érkeztünk.
\begin{definition}[diagonalizálható transzformáció]
	Az $A\in L\left( V \right)$ lineáris transzformációt \emph{diagonalizálhatónak}\index{diagonalizálhatóság}
	mondjuk,
	ha van a térnek olyan bázisa,
	amelyben a transzformáció mátrixa diagonális alakú,\index{diagonális alakú mátrix}
	azaz a fődiagonálisán kívül minden elem zérus.
\end{definition}
Az $\left[ A \right]$ négyzetes mátrix tehát pontosan akkor diagonális alakú,
ha minden $i\neq j$ mellett $\left[ A \right]_{i,j}=0.$
Ez azt jelenti hogy a $j$-edik bázis elem képének a nem $j$-edik koordinátája zérus,
azaz az $e_j$ bázisvektorra $Ae_j=\lambda e_j$ áll fenn,
valamely $\lambda\in\mathbb{F}$ számmal.
Ez éppen azt jelenti,
hogy az $e_j$ bázisvektor egy sajátvektor.
Nyilvánvaló tehát,
hogy diagonalizálhatóság szükséges és elegendő módon megragadható a sajátvektor fogalmának segítségével.
\begin{proposition}[diagonalizálhatóság]
	Az $A\in L\left( V \right)$ egy lineáris transzformáció
	pontosan akkor diagonalizálható,
	ha van térnek csupa sajátvektorokból álló bázisa.

	Ebben az esetben a transzformáció $\left\{ v_1,\ldots,v_n \right\}$ sajátvektorokban, mint bázisban,
    felírt mátrixának a $j$-edik diagonális eleme,
    éppen az a $\lambda_j$ sajátértéke $A$-nak,
    amelyre $Av_j=\lambda_jv_j$.
\end{proposition}
Mivel a
\begin{math}
	\begin{pmatrix}
		-t & 1 \\-1&-t
	\end{pmatrix}
\end{math}
valós test feletti mátrix minden valós $t$ mellett reguláris,
ezért a
\begin{math}
	\begin{pmatrix}
		0 & 1 \\-1&0
	\end{pmatrix}
\end{math}
mátrix egy nagyon egyszerű példa olyan mátrixra, amelynek spektruma üres,
így persze nem diagonalizálható.

Csak a játék kedvéért, ha az
\begin{math}
	\begin{pmatrix}
		0 & 1 \\0&0
	\end{pmatrix}
\end{math}
mátrixot tekintjük $\mathbb{R}$ felett,
azt kapjuk, hogy ez sem diagonalizálható.
Van ugyan egyetlen sajátértéke $\lambda=0$,
de az ehhez a sajátértékhez tartozó $\ker A$ sajátaltér egydimenziós,
emiatt nincs a térben két lineárisan független sajátvektor.

A pozitív példa kedvéért nézzük az
\begin{math}
	A
	=
	\begin{pmatrix}
		1 & 1 \\1&1
	\end{pmatrix}
\end{math}
mátrixot.
Mind az $\mathbb{R}$,
mind a $\mathbb{C}$ test felett diagonalizálható,
hiszen $\sigma\left( A \right)=\left\{ 0,2 \right\}$, továbbá
a $\lambda=0$-hoz tartozó sajátaltérre
$\ker A=\lin\left\{
	\begin{pmatrix}
		1 \\-1
	\end{pmatrix}
	\right\},$
míg a $\lambda=2$ sajátértékhez tartozó sajátaltérre
\begin{math}
	\ker\left( A-2I \right)
	=
	\lin\left\{
	\begin{pmatrix}
		1 \\1
	\end{pmatrix}
	\right\}.
\end{math}
Világos, hogy a
\begin{math}
	B
	=
	\left\{
	\begin{pmatrix}
		1 \\-1
	\end{pmatrix},
	\begin{pmatrix}
		1 \\1
	\end{pmatrix}
	\right\}
\end{math}
vektorrendszer egy sajátvektorokból álló bázisa a két dimenziós vektortérnek.
A fenti bázisban a transzformáció mátrixa
\begin{math}
	\begin{pmatrix}
		0 & 0 \\
		0 & 2
	\end{pmatrix}.
\end{math}
Mivel az erre a bázisra való áttérés mátrixa
\begin{math}
	B=
	\begin{pmatrix}
		1  & 1 \\
		-1 & 1
	\end{pmatrix},
\end{math}
ezért az új bázisra való áttérést formuláját használva
\[
	[B]^{-1}[A]_{\rgi}[B]=
	\frac{1}{2}
	\begin{pmatrix}
		1 & -1 \\
		1 & 1
	\end{pmatrix}
	\begin{pmatrix}
		1 & 1 \\1&1
	\end{pmatrix}
	\begin{pmatrix}
		1  & 1 \\
		-1 & 1
	\end{pmatrix}
	=
	\begin{pmatrix}
		0 & 0 \\
		0 & 2
	\end{pmatrix}
	=[A]_{\uj}.
\]
Ezt úgy fejezzük ki, hogy az
\begin{math}
	\begin{pmatrix}
		1  & 1 \\
		-1 & 1
	\end{pmatrix}
\end{math}
izomorfizmus diagonalizálja az
\begin{math}
	\begin{pmatrix}
		1 & 1 \\1&1
	\end{pmatrix}
\end{math}
mátrixot.
\begin{proposition}\label{pr:svlinfgtlen}
	Különböző sajátértékekhez tartozó sajátvektorok rendszere lineárisan független.

	Formálisabban:
	Legyen $\left\{ \lambda_1,\ldots,\lambda_s \right\}\subseteq \sigma\left( A \right)$
	a sajátérékek páronként különböző elemekből álló rendszere,
	és legyen $\left\{ v_1,\ldots,v_s \right\}\subseteq V$ sajátvektorok olyan rendszere,
	amelyre $v_j\in\ker\left( A-\lambda_jI \right)$ minden $j=1,\ldots,s$ mellett.
	Ekkor a sajátvektorok rendszere lineárisan független.
\end{proposition}
\begin{proof}
	A sajátvektorok száma, azaz $s$ szerinti indukció.
	Ha $s=1$, akkor készen is vagyunk,
	hiszen egy sajátvektor egy nem zérus vektor.

	Most tegyük fel, hogy igaz az állítás sajátvektorok $s$-nél kevesebb elemből álló rendszerére,
	és lássuk be sajátvektorok olyan $s$ elemű rendszerére, amelyek különböző sajátértékhez tartoznak.
	Az indukciós feltevés szerint tehát $\left\{ v_1,\ldots,v_{s-1} \right\}$ lineárisan független.
	Ha $\left\{ v_1,\ldots,v_{s-1},v_s \right\}$ lineárisan összefüggő lenne,
	akkor valamely $\alpha_1,\ldots,\alpha_{s-1}$ számokkal
	\[
		v_s=\sum_{j=1}^{s-1}\alpha_jv_j
	\]
	lenne.
	No de,
	\begin{equation*}
		\sum_{j=1}^{s-1}\lambda_s\alpha_jv_j
		=
		\lambda_sv_s
		=
		Av_s
		=
		\sum_{j=1}^{s-1}\alpha_jAv_j
		=
		\sum_{j=1}^{s-1}\alpha_j\lambda_jv_j,
	\end{equation*}
	ami az első $s-1$ elem lineárisan függetlensége szerint csak úgy lehetséges,
	ha minden $j=1,\dots s-1$ mellett
	\begin{math}
		\lambda_s\alpha_j
		=
		\alpha_j\lambda_j
	\end{math},
	ergo
	\begin{math}
		\alpha_j\left( \lambda_s-\lambda_j \right)
		=
		0.
	\end{math}
	Mivel itt különböző sajátértékekről van szó,
	ezért minden szóba jövő $j$ mellett $\alpha_j=0$.
	Ebből $v_s=0$ következik,
	ami ellentmond annak, hogy $v_s$ egy sajátvektor.
\end{proof}
Egy $n$-dimenziós térben $n$-elemű lineárisan független rendszer generátorrendszer is, így
azonnali következmény a diagonalizálhatóság egy elegendő feltétele:
\begin{proposition}[diagonalizálhatóság elegendő feltétele]
	Tegyük fel, hogy az $A\in L\left( V \right)$ lineáris transzformációnak annyi különböző sajátértéke van,
	mint a $V$ vektortér dimenziója.
	Ekkor $A$ diagonalizálható.
\end{proposition}
Az identitás mátrix példája mutatja, hogy a feltétel elegendő, de nem szükséges.
Mivel $n$-dimenziós térben legfeljebb $n$ elemű linárisan független rendszer van,
ezért kapjuk, hogy a spektrumnak több eleme nem lehet, mint a tér dimenziója:
\begin{proposition}
	Legyen $A\in L\left( V \right)$ lineáris transzformáció.
	Ekkor $A$-nak legfeljebb $\dim(V)$ darab különböző sajátértéke lehet.
\end{proposition}

\begin{definition}[geometriai multiplicitás]
	Ha $A\in L\left( V \right)$ egy lineáris transzformáció,
	és $\lambda\in\sigma\left( A \right)$ annak egy sajátértéke,
	akkor az $A-\lambda I$ sajátaltér defektusát,
	tehát a $\ker\left( A-\lambda I \right)$ altér dimenzióját,
	a $\lambda$ sajátérték \emph{geometriai multiplicitásának}\index{geometriai multiplicitás}
	mondjuk.
\end{definition}
\begin{proposition}
	Legyen $A\in L\left( V \right)$ transzformáció, és
	$\left\{ \lambda_1,\ldots,\lambda_s \right\}\subseteq\sigma\left( A \right)$,
	a spektrum különböző elemei.
	Jelölje $M_j=\ker\left( A-\lambda_j I \right)$.
	Ekkor értelmes az $M_1\oplus\dots\oplus M_s$ direkt összeg.
\end{proposition}
\begin{proof}
	Megmutatjuk, hogy a $\sum_{j=1}^sM_j$ összegben minden elem előállítása egyértelmű.
	Ehhez elég azt belátni, hogy $\sum_{j=1}^sv_j=0$, $v_j\in M_j$ csak úgy lehetséges, ha
	minden $j=1,\ldots,s$ mellett $v_j=0$.
	Tegyük fel tehát, hogy valamely $v_j\in M_j$ vektorokra
	\[
		\sum_{j=1}^sv_j=0.
	\]
	Ez egy olyan lineáris kombináció, amelyben minden vektor együtthatója 1.
	Emiatt a $\left\{ v_1,\ldots,v_s \right\}$ vektorrendszer nem zérus vektorai is összefüggő rendszert alkotnak,
	feltéve hogy vannak ilyenek.
	No de, egy $v_j\in M_j$ vektor ha nem zérus, akkor egy sajátvektor.
	Tehát ha a vektorrendszerben lenne nem zérus elem,
	akkor találnánk különböző sajátértékekhez tartozó sajátvektorok egy lineárisan összefüggő rendszerét,
	ami \aref{pr:svlinfgtlen}.~állítás szerint nem lehetséges.
\end{proof}
Meggondoltuk tehát,
hogy ha páronként különböző $\left\{ \lambda_1,\ldots,\lambda_s \right\}\subseteq\sigma\left( A \right)$ sajátértékekből indulunk ki,
és egyesítjük a $\ker\left( A-\lambda_j I \right)$ sajátalterek egy-egy bázisait,
akkor az így összetett vektorrendszer az
\[
	\ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)
\]
altér egy -- sajátvektorokból álló -- bázisa.
Rögzítsük is ezt a fontos gondolatot,
amelyet a feladatok megoldása során sokszor használjuk majd.
\begin{proposition}
	A különböző sajátértékekhez tartozó sajátalterek bázisainak egyesítése a sajátalterek direkt összegének egy bázisa.
\end{proposition}
\begin{proposition}[diagonalizálhatóság]\label{pr:diagkar}
	Legyen $A\in L\left( V \right)$ lineáris transzformáció.
	Jelölje $\left\{ \lambda_1,\ldots,\lambda_s \right\}=\sigma\left( A \right)$
	az $A$ spektrumát,
	azaz valamennyi különböző sajátértékét.
	Az alábbi feltevések ekvivalensek.
	\begin{enumerate}
		\item Az $A$ sajátértékei geometriai multiplicitásának összege $\dim(V)$,
		\item
		      \begin{math}
			      \ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)=V,
		      \end{math}
		\item Minden vektor előáll mint sajátvektorok összege,
		\item Az $A$ diagonalizálható lineáris transzformáció.\qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Mivel a $\ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)\subseteq V$
	tartalmazás mindig fennáll,
	ezért a 2. feltétel ekvivalens avval, hogy
	\[
		\sum_{j=1}^s\dim\left( \ker\left( A-\lambda_j I \right) \right)
		=
		\dim\left(
		\ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)
		\right)
		=\dim(V),
	\]
	ami éppen a geometriai multiplicitásra vonatkozó feltétel.
	Így az első két feltevés ekvivalenciáját megértettük.


	Mivel a $\ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)\subseteq V$
	tartalmazás mindig fennáll,
	ezért a 2. feltétel ekvivalens avval, hogy
	\[
		\sum_{j=1}^s\ker\left( A-\lambda_j I \right)=V,
	\]
	ami éppen a 3. feltétel.
	Így a 2. és a 3. feltételek ekvivalenciáját is megértettük.

	Ha 3., ezért 2. fennáll, akkor az egyes $\ker\left( A-\lambda_j I \right)$ sajátalterek bázisait egyesítve a
	$V$ tér egy sajátvektorokból álló bázisát kapjuk,
	ergo az $A$ diagonalizálható transzformáció.
	Megfordítva,
	ha $A$ diagonalizálható,
	akkor van sajátvektorokból álló bázisa,
	így minden vektor előáll mint sajátvektorok összege.
	Evvel a 3. és a 4. feltételek ekvivalenciáját is igazoltuk.
\end{proof}
\chapter{Transzformációk redukálása}
\begin{proposition}
	Tekintsünk egy $A\in L\left( V \right)$ lineáris transzformációt,
	amelynek minimálpolinomja $m\in\mathbb{F}\left[ t \right]$.
	Tegyük fel, hogy $m$ előáll mint a $p,q$ egymással relatív prím, normált
	polinomok
	\[
		m=pq
	\]
	szorzata.
	Ekkor a tér szétesik a
	$\ker p\left( A \right)$ és a
	$\ker q\left( A \right)$ invariáns alterei direkt összegére, azaz
	\[
		\ker p\left( A \right)\oplus
		\ker q\left( A \right)=
		V.
	\]
	Ha $A_1=A_{|\ker p\left( A \right)}$ és
	$A_2=A_{|\ker q\left( A \right)}$, akkor $A_1$ minimálpolinomja $p$ és $A_2$ minimálpolinomja $q$.
\end{proposition}
\begin{proof}
	Mivel $p$ és $q$ relatív prímek, ezért a Bezout-azonosság szerint van $f,g\in\mathbb{F}\left[ t \right]$ polinom, amelyekre
	\[
		fp+gq=1.
	\]
	Emiatt persze minden $x\in V$ mellett
	\[
		f\left( A \right)p\left( A \right)x
		+
		g\left( A \right)q\left( A \right)x=Ix=x.\tag{\dag}
	\]

	Ha $x\in\ker p\left( A \right)\cap\ker q\left( A \right)$,
	akkor
	$p\left( A \right)x=0=q\left( A \right)x$, tehát $x=0$, ami azt jelenti, hogy
	$\ker p\left( A \right)$ és
	$\ker q\left( A \right)$ diszjunkt alterek.
	\\
	Vegyük észre, hogy a
	$f\left( A \right)p\left( A \right)x\in\ker q\left( A \right)$,
	és hasonlóan
	$g\left( A \right)q\left( A \right)x\in\ker p\left( A \right)$.
	Ez azt jelenti, hogy $\ker q\left( A \right)+\ker p\left( A \right)=V$ azonosság is fennáll.
	Megmutattuk tehát, hogy $V$ előáll mint a $\ker p\left( A \right)$ és a
	$\ker q\left( A \right)$ alterek direkt összege.

	Világos, hogy minden $u\in\ker p\left( A \right)$ mellett
	$p\left( A_1 \right)u
		=
		p\left( A \right)u=0$.

	Ha $h$ egy másik olyan polinom, amelyre $h\left( A_1 \right)=0\in L\left( \ker p\left( A \right) \right)$,
	akkor mivel a direkt összegre vonatkozó állítást már igazoltuk
	\[
		(hq)\left( A \right)x=
		h\left( A \right)q\left( A \right)\left( x_1+x_2 \right)
		=
		q\left( A \right)h\left( A \right)x_1
		+
		h\left( A \right)q\left( A \right)x_2
		=0+0
		=0,
	\]
	ahol $x=x_1+x_2$, $x_1\in \ker p\left( A \right)$ és
	$x_2\in\ker q\left( A \right)$.
	Látjuk tehát, hogy az $A$ transzformáció a $hq$ polinomnak is gyöke,
	emiatt $m|hq$.
	Mivel $p|m$, ezért
	\[p|hq\] is fennáll.
	Most újra használjuk, hogy a $p$ és a $q$ polinomok relatív prímek,
	így azt kapjuk, hogy $p|h.$
	Megmutattuk, hogy a $J_{A_1}$ ideált a $p$ normált polinom generálja,
	ami éppen azt jelenti, hogy $p$ az $A_1$ transzformáció minimálpolinomja.
	Az $A_2$ minimálpolinomja $q$ állitás igazolása a fentivel analóg.
\end{proof}
\begin{proposition}\label{pr:redukcio-primszorzat}
	Tekintsünk egy $A\in L\left( V \right)$ lineáris transzformációt,
	amelynek minimálpolinomja $m\in\mathbb{F}\left[ t \right]$.
	Tegyük fel, hogy $m$ előáll mint a páronként relatív prím
	normált polinomok
	\[
		m=p_1p_2\cdots p_n
	\]
	szorzata.
	Jelölje minden $i=1,\ldots,n$ mellett $V_i=\ker p_i\left( A \right)$
	invariáns alteret, és $A_i=A|V_i$ megszorítást.
	Világos, hogy $A_i\in L\left( V_i \right)$.
	Ekkor
	\begin{enumerate}
		\item $V=V_1\oplus\dots\oplus V_n$;
		\item $p_i$ az $A_i$ transzformáció minimálpolinomja minden szóba jövő
		      $i=1,\ldots,n$ mellett.\qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	A polinomok $n$ száma szerinti teljes indukcióval igazolunk.
	Az $n=1$ eset triviális, de $n=2$ éppen az előző állítás.

	Most tegyük fel, hogy az állítás $n$-nél kevesebb polinom szorzatára igaz,
	és lássuk be $n$-re.
	Feltehető tehát, hogy $n\geq 3$.
	Legyen $p=p_1\cdots p_{n-1}$.
	Világos, hogy $p$ és $p_n$ relatív prímek, hiszen ha $d$ irreducibilis osztója
	$p$-nek és $p_n$-nek,
	akkor $d$ prím tulajdonsága szerint $d|p_i$ valamely $i<n$-re,
	tehát $d|p_i$ és $d|p_n$.
	A feltevés szerint ilyen csak a konstans polinom lehetséges,
	ami valóban igazolja, hogy $p$ és $p_n$ relatív prímek.
	Persze
	\[
		m=pp_n.
	\]
	Alkalmazhatjuk tehát az előző állítást, azaz
	\[
		V=\ker p\left( A \right)\oplus V_n,
	\]
	továbbá $p$ a minimálpolinomja az $A|\ker p\left( A \right)$-nak
	és persze $p_n$ minimálpolinomja $A_n$-nek.

	Alkalmazzuk most az indukciós feltevést a $\ker p\left( A \right)$ vektortérre.
	Ott az $A_{|\ker p\left( A \right)}$ lineáris transzformáció $p$ minimálpolinomja
	előáll mint $n-1$ páronként relatív prím polinom szorzata:
	\[
		p=p_1\cdots p_{n-1}.
	\]
	Világos tehát, hogy
	\begin{enumerate}
		\item
		      $\ker p\left( A \right)=V_1\oplus\dots\oplus V_{n-1}$ és
		\item $p_i$ az $A_i$ minimálpolinomja minden $i=1,\ldots,n-1$ mellett.
	\end{enumerate}
	Teljesül tehát a $V_1,\ldots,V_{n-1},V_n$ alterekre,
	hogy mind diszjunkt -- az itt adott sorrendben -- az előzőek összegétől,
	és a Minkowski-összegük az egész $V$ vektortér.\index{Minkowski-összeg}
	\Aref{pr:direktosszegsorrend}. állítás szerint tehát $V=V_1\oplus\dots\oplus V_n$.
\end{proof}
\section{Minimálpolinom és diagonalizálhatóság}
Alkalom nyílik,
hogy a lineáris transzformáció diagonalizálhatóságát karakterizáló \aref{pr:diagkar}.~állítást
tovább bővítsük, a minimálpolinom szerepének hangsúlyozásával.
\begin{proposition}[diagonalizálhatóság]\label{pr:diagonalizalhatosag}
	Legyen $A\in L\left( V \right)$ lineáris transzformáció.
	Jelölje $\left\{ \lambda_1,\ldots,\lambda_s \right\}=\sigma\left( A \right)$
	az $A$ spektrumát,
	azaz valamennyi különböző sajátértékét.
	Az alábbi feltevések ekvivalensek.
	\begin{enumerate}
		\item Az $A$ sajátértékei geometriai multiplicitásának összege $\dim(V)$;
		\item
		      \begin{math}
			      \ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)=V
		      \end{math};
		\item Minden vektor előáll mint sajátvektorok összege;
		\item Az $A$ diagonalizálható lineáris transzformáció;
		\item Az $A$ transzformáció minimálpolinomja
		      \[
			      m\left( t \right)=
			      \prod_{j=1}^s\left( t-\lambda_j \right).
			      \qedhere
		      \]
	\end{enumerate}
\end{proposition}
\begin{proof}
	Ha az $A$ transzformáció diagonalizálható,
	akkor a diagonálisában a sajátértékei vannak.
	Írjuk fel tehát a
	\begin{math}
		\prod_{j=1}^s\left( A-\lambda_j I \right)
	\end{math}
	transzformáció mátrixát abban a bázisban,
	amelyben $A$ mátrixa is diagonális.
	Az eredmény egy diagonális mátrix,
	és ha felírjuk ezt mint az $[A-\lambda_j I]$ mátrixok szorzatát,
	akkor minden diagonális pozició az egyik szorzó mátrixban zérus,
	ergo a szorzat mátrix is a zéró mátrix.
	A fenti polinomnak tehát az $A$ transzformáció gyöke.
	Mivel minden sajátérték a minimálpolinom gyöke,
	ezért a fenti polinom a legalacsonyabb fokú normált polinom,
	amelynek gyöke $A$.
	\footnote{
		Egy másik érv: $t-\lambda_j$ a $\lambda_j$ sajátértékhez tartozó sajátvektor kis minimálpolinomja.
		Mivel a sajátvektorok egy bázist alkotnak,
		ezért ezen polinomok legkisebb közös többszöröse a minimálpolinom.
		Persze $\dim(V)$ darab elsőfokú normált polinom legkisebb közös többszöröse,
		ezek közül a különbözők szorzata.
		\index{kis minimálpolinom}
	}

	Megfordítva,
	legyen $p_j\left( t \right)=t-\lambda_j$ minden $j=1,\ldots,s.$
	Ekkor $m=p_1\cdots p_s$ páronként relatív prím, normált polinomok szorzata,
	ezért az éppen igazolt \ref{pr:redukcio-primszorzat}.~állítás szerint
	\begin{equation*}
		V=
		\ker p_1\left( A \right)\oplus\dots\oplus\ker p_s\left( A \right)
		=
		\ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)
	\end{equation*}
	Ezt kellett belátni.
\end{proof}
\section{Redukálás: az általános eset}
Mivel minden polinom előáll mint néhány irreducibilis polinom szorzata,
ezért \ref{pr:redukcio-primszorzat}.~állítás így is fogalmazható.
\begin{proposition}\label{pr:redukcio-primfelbontas}
	Legyen $A\in L\left( V \right)$ lineáris transzformáció minimálpolinomja
	$m$.
	Tudjuk, hogy $m$ egyértelműen áll elő
	\[
		m
		=
		p_1^{m_1}p_2^{m_2}\cdots p_r^{m_r},
	\]
	alakban,
	ahol $p_1,\ldots,p_r$ egymástól páronként különböző normált, irreducibilis polinomok.
	Jelölje $V_i=\ker p_i^{m_i}\left( A \right)$
	jelölje $A_i=A_{|V_i}$ minden $i=1,\ldots,r$ mellett.
	Ekkor
	\begin{enumerate}
		\item $V=V_1\oplus\dots\oplus V_r$ és,
		\item $A_i$ minimálpolinomja $p_i^{m_i}$ minden $i=1,\ldots,r$ mellett.\qedhere
	\end{enumerate}
\end{proposition}
A konklúzió tehát az,
hogy elég olyan transzformációkkal foglalkoznunk,
amelyek minimálpolinomja egy irreducibilis polinom valamely egész kitevős hatványa.
Az algebra alaptétele szerint a komplex számtest feletti irreducibilis polinom csak elsőfokú polinom lehet.
Abban a speciális esetben tehát,
amikor a $\mathbb{C}$ komplex számtest feletti vektortereket\index{komplex számtest feletti vektortér} vizsgálunk,
ez azt jelenti,
hogy elég ha olyan $A\in L\left( V \right)$ transzformációval foglalkozunk,
amelynek $m$ minimálpolinomjára
\[
	m\left( t \right)=(t-\alpha)^n
\]
teljesül valamely $\alpha\in\mathbb{C}$ komplex szám és $n\in\mathbb{N}$ egész mellett.
Ilyen $A$ transzformációra, ha $B$ jelöli a $B=A-\alpha I$ lineáris transzformációt,
akkor a $B$ olyan,
hogy valamely egész kitevős hatványa a konstans zérus transzformáció.
Ez vezet majd a \emph{nilpotens}\index{nilpotens transzformáció} fogalmához.
Ha felírjuk valamely bázisban egy ilyen $B$ nilpotens transzformáció mátrixát,
akkor $A$ mátrixa is könnyen adódik $B$ mátrixából.
Ehhez csak az $\alpha [I]$ mátrixot kell $\left[ B \right]$-hez adni,
ami praktikusan nem jelent többet,
mint hogy a $\left[ B \right]$ diagonális elemeket kell az $\alpha$ komplex számmal megemelni.
A fenti gondaloton alapul a \emph{Jordan-normálak}\index{Jordan-normálalak} fogalma.

\chapter{Redukálás irreducibilis minimálpolinom esetén}
\scwords A legegyszerűbb eset, mikor a minimálpolinom elsőfokú irreducibilis polinomok szorzata.

\begin{proposition}\label{pr:irred_redukcio}
	Legyen $A\in L\left( V \right)$ egy lineáris transzformációja a $V$ véges dimenziós vektortérnek,
	az $m\in\mathbb{F}\left[ t \right]$ egy $k$-adfokú, irreducibilis polinom, amelyre $m\left( A \right)=0$.
	Ekkor
	\begin{enumerate}
		\item minden $v\neq 0$ mellett
		      \begin{math}
			      \dim\lin\left( v;A \right)=k;
		      \end{math}
		\item
		      minden $v\in V$ vektor és minden $K\subseteq V$ invariáns altér mellett
		      $\lin\left( v;A \right)\cap K=\left\{ 0 \right\}$ vagy
		      $\lin\left( v;A \right)\subseteq K$;
		\item
		      a $V$ altér nulla dimenziós,
		      vagy $k$ dimenziós,
		      vagy $k$ dimenziós $A$ invariáns alterek direkt összege.
		      Pontosabban,
		      ha $\dim(V)>0$, akkor létezik $r\geq 1$ szám,
		      és léteznek $v_1,\ldots,v_r$ vektorok, amelyekre
		      \[
			      \lin\left( v_1;A \right)\oplus\dots\oplus \lin\left( v_r;A \right)=V.
			      \qedhere
		      \]
	\end{enumerate}
\end{proposition}
\begin{proof}[Bizonyítás (1.)]
	\index{kis minimálpolinom}
	Legyen adott $v\neq 0$ vektor mellett $p_v$ a $v$-hez tartozó kis minimálpolinom.
	Mivel $m\left( A \right)v=0$, ezért $p_v|m$.
	No de, $m$ irreducibilis, ezért $m=p_v$.
	Ekkor viszont
	\[
		k=\deg m=\deg p_v
		=
		\dim\lin\left( v;A \right).\qedhere
	\]
\end{proof}
\begin{proof}[Bizonyítás (2.)]
	Ha $v=0$, akkor az állítás nyilvánvaló.
	A továbbiakban emiatt $v\neq 0$.
	Most tegyük fel, hogy $x\in\lin\left( v;A \right)\cap K$ és $x\neq 0$.
	Ekkor
	\[
		\lin\left( x;A \right)
		\subseteq
		\lin\left( v;A \right)\cap K
		\subseteq
		\lin\left( v;A \right)
	\]
	No de,
	a bal- és a jobboldali altér azonos dimenziós alterek,
	emiatt fent mindenütt egyenlőség van.
	Speciálisan
	$\lin\left( v;A \right)=\lin\left( v;A \right)\cap K$,
	ami éppen azt jelenti,
	hogy
	$\lin\left( v;A \right)\subseteq K.$
\end{proof}
\begin{proof}[Bizonyítás (3.)]
	Először is gondoljuk meg, hogy invariáns alterek Minkowski--összege is invariáns altér marad.
	Ha $V$ nem tartalmaz nem zérus vektort, akkor $\dim(V)= 0$.

	Ha $v_1\in V$ egy nem zérus vektor,
	akkor jelölje $V_1=\lin\left( v_1;A \right)$.
	Ha $V=V_1$,
	akkor $V$ egy $k$-dimenziós vektortér.

	Ha $V\neq V_1$, akkor van $v_2\in V\smallsetminus V_1$, $v_2\neq 0$ vektor.
	Mivel $V_1$ egy invariáns altér,
	ezért a már igazolt állítás szerint bevezetve a $V_2=\lin\left( v_2;A \right)$
	jelölést $V_2\cap V_1=\left\{ 0 \right\}$.
	Értelmes tehát venni e két invariáns altér direkt összegét.
	Ha $V=V_1\oplus V_2$, akkor $V$ előállt két $k$ dimenziós invariáns alterének direkt összegeként.

	Ha $V\neq V_1\oplus V_2$, akkor van $v_3\in V\smallsetminus (V_1\oplus V_2)$, $v_3\neq 0$ vektor.
	Mivel $V_1\oplus V_2$ egy invariáns altér,
	ezért a már igazolt állítás szerint bevezetve a $V_3=\lin\left( v_3;A \right)$
	jelölést $V_3\cap (V_1\oplus V_2)=\left\{ 0 \right\}$.
	Értelmes tehát venni e három invariáns altér direkt összegét, hiszen
	a $V_1,V_2,V_3$ alterek ebben a sorrendben véve olyanok,
	hogy mind diszjunkt az előzőek összegétől.
	Ha $V=V_1\oplus V_2\oplus V_3$,
	akkor $V$ előállt három $k$ dimenziós invariáns alterének direkt összegeként.

	Ha $V\neq V_1\oplus\dots\oplus V_t$, valamely $t\geq 2$ mellett,
    akkor van $v_{t+1}\in V\smallsetminus (V_1\oplus\dots\oplus V_t)$, $v_{t+1}\neq 0$ vektor.
	Mivel $V_1\oplus\dots\oplus V_t$ egy invariáns altér,
	ezért a már igazolt állítás szerint bevezetve a $V_{t+1}=\lin\left( v_{t+1};A \right)$
	jelölést $V_{t+1}\cap (V_1\oplus\dots\oplus V_t)=\left\{ 0 \right\}$.
	Értelmes tehát venni ezen $t+1$ invariáns altér direkt összegét, hiszen
	a $V_1,V_2,V_3,\ldots,V_t,V_{t+1}$ alterek ebben a sorrendben véve olyanok,
	hogy mind diszjunkt az előzőek összegétől.
	Ha $V=V_1\oplus\dots\oplus V_{t+1}$,
	akkor $V$ előállt $t+1$ darab $k$ dimenziós invariáns alterének direkt összegeként.

	Az eljárás előbb utóbb a $V$ vektortér véges dimenziós volta miatt megáll.
\end{proof}
\subsection{Következmény}
Világos, hogy
\(
\left\{
A^{k-1}v_1, A^{k-2}v_1,\ldots,Av_1,v_1
\right\}
\)
bázisa a $\lin( v_1;A)$
invariáns altérnek.
Ha ebben a bázisban felírjuk a transzformáció mátrixát,
akkor az első oszlopban vannak a minimálpolinom együtthatóinak ellentettjei,
a diagonális felett $1$-ek vannak és minden más elem zérus:
\[
	\begin{pmatrix}
		-\alpha_{k-1} & 1      & 0      & \dots  & 0      \\
		-\alpha_{k-2} & 0      & 1      & \dots  & 0      \\
		\vdots        & \vdots & \ddots & \ddots & \vdots \\
		-\alpha_{1}   & 0      & 0      & \dots  & 1      \\
		-\alpha_{0}   & 0      & 0      & \dots  & 0
	\end{pmatrix}
\]
Meggondoltuk tehát, hogy ha a transzformáció $m$ minimálpolinomja irreducibilis és
\[m\left( t \right)
	=\alpha_0+\alpha_1t+\dots+\alpha_{k-1}t^{k-1}+t^k
\]
alakú, akkor a térnek van olyan bázisa, amelyben a transzformáció mátrixa a fenti mátrix
diagonális elrendezésű $r$ darab másolatából áll, ahol $rk=\dim(V)$.

Korábban -- lásd \aref{pr:polinom-minimalpolinom}.~állítást -- láttuk, 
hogy ha $m\left( t \right)$ egy előre adott $k$-adfokú normált polinom,
akkor például a fenti mátrix által reprezentált,
$k$-dimenziós téren értelmezett lineáris transzformációnak $m\left( t \right)$
a minimálpolinomja. 
Most megmutattuk azt, hogy amennyiben $m\left( t \right)$ még irreducibilis is, 
akkor egyedül a fenti transzformációnak lehet $m\left( t \right)$ a minimálpolinomja,
amennyiben a $k$-dimenziós vektortereken értelmezett lineáris transzformációkra hagyatkozunk.
Ha feloldjuk ezt a dimenzió korlátot, akkor még az $rk$ dimenziós téren értelmezett transzformációk is szóba jönnek
fent megértett módon, tehát pl az $5k$ dimenziós téren annak és csak annak a transzformációnak a minimálpolinomja $m$,
amelynek alkalmas bázisban felírt mátrixa 5 darab fenti típusú mátrix diagonális elrendezésével keletkezik.

%Itt nagyon fontos volt, hogy $m\left( t \right)$ irreducibilis. 
%Kérdés:
%Igaz-e, hogy nem irreducibilis polinom esetén több egymástól különböző transzformáció is definiálható egy akkora dimenziós téren mint a polinom foka, 
%úgy hogy azok minimálpolinomja az előre megadott polinom legyen. 
%Ezt nem tudom, hogy igaz-e????

\section{Irreducibilis polinommal képzett magtér redukálása}
Legyen most $p$ egy tetszőleges $k$-ad fokú irreducibilis polinom,
és $A\in L(V)$ egy lineáris transzformáció.
Tekintsük a $V_1=\ker p\left( A \right)$ invariáns alteret, amelyre szorítsuk meg az $A$
transzformációt, azaz $A_1=A|V_1$.
Világos, hogy $p\left( A_1 \right)=0\in L\left( V_1 \right)$,
ezért alkalmazhatjuk a fent igazolt \ref{pr:irred_redukcio}.~állítást
a $V_1$ altérre és az $A_1$ transzformációra.
\begin{proposition}\label{pr:csaba}
	Legyen $V$ egy véges dimenziós vektortér, $A\in L\left( V \right)$ egy lineáris transzformáció,
	és $p\in\mathbb{F}\left[ t \right]$ egy irreducibilis polinom.
	Ekkor:
	\begin{enumerate}
		\item Minden $v\neq 0$, $v\in\ker p\left( A \right)$ mellett
		      \(
		      \dim\left( \lin(v;A) \right)=\deg p;
		      \)
		\item A $\ker p\left( A \right)$ altér nulla dimenziós,
		      vagy $k$ dimenziós, vagy $k$ dimenziós invariáns alterek direkt összege.
		      Pontosabban fogalmazva
		      ha $\nu\left( A \right)>0$, akkor létezik $r\geq 1$ szám,
		      és léteznek $v_1,\ldots,v_r$ vektorok, amelyekre
		      \[
			      \lin\left( v_1;A \right)\oplus\dots\oplus \lin\left( v_r;A \right)
			      =
			      \ker p\left( A \right).
			      \qedhere
		      \]
	\end{enumerate}
\end{proposition}
Összefoglalhatjuk azt az esetet,
mikor a minimálpolinom különböző irreducibilis polinomok szorzata.
Ez éppen \aref{pr:redukcio-primszorzat}.~állítás esete.
\begin{proposition}\label{pr:minpolelsofokufelbontas}
	Tekintsünk egy $A\in L\left( V \right)$ lineáris transzformációt,
	amelynek minimálpolinomja $m\in\mathbb{F}\left[ t \right]$.
	Tegyük fel,
	hogy $m$ előáll mint a különböző normált, irreduciblis polinomok elsőfokú hatványainak
	\[
		m=p_1p_2\dots p_s
	\]
	szorzata.
	Ekkor $V$ előáll mint néhány -- de legalább egy-egy darab --
	$\deg p_1,\deg p_2,\ldots,\deg p_s$ dimenziós minimális invariáns alterének direkt összege.
\end{proposition}
\begin{proof}
	\Aref{pr:redukcio-primfelbontas}.~állításban láttuk,
	hogy $V=\ker p_1\left( A \right)\oplus\dots\oplus\ker p_s\left( A \right)$ alakú.
	Minden egyes $p_j$ a minimálpolinom legalább elsőfokú osztója, így $\ker p_j\left( A \right)\neq\left\{ 0 \right\}$.
	Az előző állítás szerint minden $j$ mellett $\ker p_j\left( A \right)$ egy $\deg p_j$
	dimenziós invariáns altér, vagy néhány ilyen direkt összege.
	Mivel ez minden $j$ mellett igaz, ezért $\ker p_j\left( A \right)$ felbontását a $V$ felbontásába helyettesítve készen is vagyunk.
\end{proof}
\subsection{A komplex és a valós eset}
Nézzük meg mit jelent \aref{pr:minpolelsofokufelbontas}.~állítás abban a speciális esetben,
mikor az $\mathbb{F}$ test a komplex számok, vagy a valós számok teste.

Elsőként tegyük fel, hogy a $V$ vektortér a $\mathbb{C}$ komplex számtest feletti vektortér.
Ekkor az $A\in L\left( V \right)$ lineáris transzformáció minimálpolinomja egy komplex együtthatós polinom.
Az algebra alaptétele szerint irreducibilis normált polinom csak elsőfokú, azaz $t-\lambda$ alakú lehet,
tehát az előző, \aref{pr:minpolelsofokufelbontas}.~állítás feltétele most abba megy át, hogy $A$ minimálpolinomja
\[
	m\left( t \right)=\left( t-\lambda_1 \right)\left( t-\lambda_2 \right)\dots\left( t-\lambda_s \right)
\]
alakú, ahol $\lambda_1,\ldots,\lambda_s$ az $m$ különböző gyökei.
Az állítás implikációja, hogy ekkor a vektortér előáll mint egy dimenziós $A$-invariáns
altereinek direkt összege, ergo az $A$ transzformáció diagonalizálható.
Kiderült tehát, hogy nincs új a nap alatt, hiszen azt már korábban is tudtuk, -- lásd:\ref{pr:diagonalizalhatosag} -- hogy $A$ pontosan akkor diagonalizálható, ha a minimálpolinomja a fenti kiemelt alakú.%
\footnote{
    Mi több, ez utóbbi tetszőleges test feletti vektortéren is igaz nem csak $\mathbb{C}$ felett.
}

Ha most a valós vektortér esetére szorítkozunk, akkor arra kell emlékeznünk, hogy
egy a valós test feletti irreducibilis polinomom első- vagy másodfokú lehet csak.
Ezért állításunk feltétele most azt jelenti, 
hogy a minimálpolinom előáll mint páronként relatív prím, első vagy másodfokú polinomok szorzata.
A konklúzió szerint ekkor a $V$ vektortér is előáll mint egy vagy két dimenziós invariáns altereinek direkt összege. Bebizonyítottuk tehát a következő állítást.
\begin{proposition}
	Tegyük fel, hogy $V$ egy $\mathbb{R}$ feletti vektortér és $A\in L\left( V \right)$,
    az $A$ minimálpolinomjának irreducibilis polinomokra felbontásában minden polinom annak első fokú hatványával szerepel.
    Ekkor a tér előáll mint egy vagy két dimenziós invariáns altereinek direkt összege,
    ergo van a térnek olyan bázisa, amelyben felírt mátrix diagonálisában csak 1-szer 1-es vagy 2-szer 2-es blokkok szerepelnek, mindenütt másutt a mátrixban zérus szerepel.
\end{proposition}
%Emiatt csak a diagonálisban, a diagonális felett és a diagonális alatt lehetnek nem zérus számok.

\subsubsection{Illusztráció}
\textit{%
	Bontsuk lehető legalacsonyabb invariáns alterek direkt összegére az alábbi $\mathbb{R}$
	feletti vektortéren értelmezett lineáris transzformáció értelmezési tartományát,
	és írjuk fel $A$ mátrixát a lehető legegyszerűbb módon.
	A transzformáció definíciója egy
	$\left\{
		u_1,u_2,u_3,u_4
		\right\}
	$
	bázis felett a következő:
	\[
		A
		\left(
		\alpha u_1
		+
		\beta u_2
		+
		\gamma u_3
		+
		\delta u_4
		\right)
		=
		\left( -2\alpha + 3\gamma \right)u_1
		+
		\left( -2\alpha-\beta+3\gamma+\delta \right)u_2
		+
		\left( -\alpha+\gamma \right)u_3
		+
		\left( -\alpha-\beta+3\gamma \right)u_4.
	\]
}
\begin{proof}[Megoldás]
	Írjuk fel az operátor mátrixát:
	\(
	A=
	\begin{pmatrix}
		-2 & 0  & 3 & 0 \\
		-2 & -1 & 3 & 1 \\
		-1 & 0  & 1 & 0 \\
		-1 & -1 & 3 & 0
	\end{pmatrix}.
	\)
	Ha a sajátvektorokat keressük látjuk, hogy nincs valós sajátérték.
	Keressük tehát a minimálpolinomot a bázis egyes elemeihez tartozó kis minimálpolinomok
	meghatározásával.
	\[
		\begin{array}{r|cc}
			    & Au_1          & A^2u_1 \\
			\hline
			u_1 & -2            & 1      \\
			    & \framebox{-2} & 2      \\
			    & -1            & 1      \\
			    & -1            & 1      \\
			\hline
			    & \delta        & -1
		\end{array}
		\qquad
		\begin{array}{r|c}
			     & A^2u_1 \\
			\hline
			u_1  & -1     \\
			Au_1 & -1     \\
			     & 0      \\
			     & 0      \\
			\hline
			     &
		\end{array}
		\qquad\qquad
		\begin{array}{r}
			\left\{ u_1,Au_1 \right\}\mbox{ lineárisan független, de} \\
			A^2u_1+Au_1+u_1=0,                                        \\
			\mbox{ezért } p_1\left( t \right)=t^2+t+1.
		\end{array}
	\]
	\[
		\begin{array}{r|cc}
			    & Au_2          & A^2u_2 \\
			\hline
			    & 0             & 0      \\
			u_2 & -1            & 0      \\
			    & 0             & 0      \\
			    & \framebox{-1} & 1      \\
			\hline
			    & \delta        & -1
		\end{array}
		\qquad
		\begin{array}{r|c}
			     & A^2u_2 \\
			\hline
			     & 0      \\
			u_2  & -1     \\
			     & 0      \\
			Au_2 & -1     \\
			\hline
			     &
		\end{array}
		\qquad\qquad
		\begin{array}{r}
			\left\{ u_2,Au_2 \right\}\mbox{ lineárisan független, de} \\
			A^2u_2+Au_2+u_2=0,                                        \\
			\mbox{ezért } p_2\left( t \right)=t^2+t+1.
		\end{array}
	\]
	\[
		\begin{array}{r|cc}
			    & Au_3         & A^2u_3 \\
			\hline
			    & \framebox{3} & -3     \\
			    & 3            & -3     \\
			u_3 & 1            & -2     \\
			    & 3            & -3     \\
			\hline
			    & \delta       & -1
		\end{array}
		\qquad
		\begin{array}{r|c}
			     & A^2u_3 \\
			\hline
			Au_3 & -1     \\
			     & 0      \\
			u_3  & -1     \\
			     & 0      \\
			\hline
			     &
		\end{array}
		\qquad\qquad
		\begin{array}{r}
			\left\{ u_3,Au_3 \right\}\mbox{ lineárisan független, de} \\
			A^2u_3+Au_2+u_3=0,                                        \\
			\mbox{ezért } p_3\left( t \right)=t^2+t+1.
		\end{array}
	\]
	\[
		\begin{array}{r|cc}
			    & Au_4         & A^2u_4 \\
			\hline
			    & 0            & 0      \\
			    & \framebox{1} & -1     \\
			    & 0            & 0      \\
			u_4 & 0            & -1     \\
			\hline
			    & \delta       & -1
		\end{array}
		\qquad
		\begin{array}{r|c}
			     & A^2u_4 \\
			\hline
			     & 0      \\
			Au_4 & -1     \\
			     & 0      \\
			u_4  & -1     \\
			\hline
			     &
		\end{array}
		\qquad\qquad
		\begin{array}{r}
			\left\{ u_4,Au_4 \right\}\mbox{ lineárisan független, de} \\
			A^2u_4+Au_4+u_4=0,                                        \\
			\mbox{ezért } p_4\left( t \right)=t^2+t+1.
		\end{array}
	\]
	Ez azt jelenti, hogy a minimálpolinom az
	$m\left( t \right)=t^2+t+1$ másodfokú, az $\mathbb{R}$ test felett irreducibilis polinom.
	A mátrix, tehát két darab
	\(
	\begin{pmatrix}
		-1&1 \\
		-1&0
	\end{pmatrix}
	\)
	mátrix diagonális elrendezésű partíciója.
	Mivel $Au_4=u_2$ az első invariáns altér bázisa lehet például
	$
		\left\{
		u_2,u_4
		\right\}
	$. Minden olyan nem zérus vektorra, amely nincs e két vektor lineáris burkában,
	az
	$
		\left\{ Av,v \right\}$
	rendszer egy invariáns direktkiegészítőt definiál.
	Pont erről szól \aref{pr:irred_redukcio}.~állítás.
	Ilyen módon például az
	\(
	\left\{
	u_2,u_4,Au_1,u_1
	\right\}
	\)
	bázisban a transzformáció mátrixa
	\(
	\begin{pmatrix}
		-1 & 1 & 0  & 0 \\
		-1 & 0 & 0  & 0 \\
		0  & 0 & -1 & 1 \\
		0  & 0 & -1 & 0
	\end{pmatrix}
	\)
	alakú.
	Emlékezve az általános bázistranszformációra azt kaptuk, hogy
	\[
		\begin{pmatrix}
			-1 & 1 & 0  & 0 \\
			-1 & 0 & 0  & 0 \\
			0  & 0 & -1 & 1 \\
			0  & 0 & -1 & 0
		\end{pmatrix}
		=
		\begin{pmatrix}
			0 & 0 & -2 & 1 \\
			1 & 0 & -2 & 0 \\
			0 & 0 & -1 & 0 \\
			0 & 1 & -1 & 0
		\end{pmatrix}^{-1}\cdot
		\begin{pmatrix}
			-2 & 0  & 3 & 0 \\
			-2 & -1 & 3 & 1 \\
			-1 & 0  & 1 & 0 \\
			-1 & -1 & 3 & 0
		\end{pmatrix}\cdot
		\begin{pmatrix}
			0 & 0 & -2 & 1 \\
			1 & 0 & -2 & 0 \\
			0 & 0 & -1 & 0 \\
			0 & 1 & -1 & 0
		\end{pmatrix}.\qedhere
	\]
\end{proof}
\chapter{A minimálpolinom fokszámáról}
\scwords A minimálpolinom definiálásakor csak annyit láttunk,
hogy legfeljebb $n^2$ fokú polionom mindig konstruálható,
amelynek a transzformáció gyöke, ahol $n$ a tér dimenziója.
Ebben a fejezetben látni fogjuk, hogy a fenti gondolat nagyon lényegesen erősíthető.
Azt mutatjuk meg,
hogy a minimálpolinom fokszáma nem lehet a tér dimenziójánál magasabb.
\begin{lemma}
	Legyen $B\in L\left( V \right)$ lineáris transzformáció,
	tegyük fel, hogy a $v_1,\ldots,v_r$ vektorok mindegyikére $B^mv_j=0$,
	de a
	\[
		\left\{ B^{m-1}v_1,B^{m-1}v_2,B^{m-1}v_3,\ldots,B^{m-1}v_r \right\}
	\]
	vektorrendszer lineárisan független.
	Ekkor a
	\[
		\left\{
		\begin{matrix}
			v_1        & v_2        & v_3        & \dots  & v_r        \\
			Bv_1       & Bv_2       & Bv_3       & \dots  & Bv_{r}     \\
			B^2v_1     & B^2v_2     & B^2v_3     & \dots  & B^2v_r     \\
			\vdots     & \vdots     & \vdots     & \ddots & \vdots     \\
			B^{t}v_1   & B^{t}v_2   & B^{t}v_3   & \dots  & B^{t}v_r   \\
			\vdots     & \vdots     & \vdots     & \ddots & \vdots     \\
			B^{m-1}v_1 & B^{m-1}v_2 & B^{m-1}v_3 & \dots  & B^{m-1}v_r
		\end{matrix}
		\right\}
	\]
	vektorrendszer is lineárisan független.
	\label{le:nilp-fgtlen}
\end{lemma}
\begin{proof}
	Tekintsük ezen vektorok egy
	\[
		\sum_{j=0}^{m-1}\sum_{k=1}^r\alpha_{j,k}B^jv_k=0
	\]
	lineáris kombinációját.
	Meg kell mutatnunk, hogy az összes $\alpha_{j,k}=0$.
	Ha $m-1\geq t\geq 0$, az első olyan index, amelyre van $\alpha_{t,k}\neq 0$
	együttható, akkor a $t$-edik index előtt,
	minden együttható zérus, ergo
	\[
		\sum_{j=t}^{m-1}\sum_{k=1}^r\alpha_{j,k}B^jv_k=0
	\]
	Erre alkalmazva a $B^{m-t-1}$ transzformációt azt kapjuk, hogy
	\[
		0
		=
		B^{m-t-1}0
		=
		\sum_{j=t}^{m-1}\sum_{k=1}^r\alpha_{j,k}B^{j+m-t-1}v_k
		=
		\sum_{k=1}^r\alpha_{t,k}B^{t+m-t-1}v_k
		=
		\sum_{k=1}^r\alpha_{t,k}B^{m-1}v_k.
	\]
	No de, az $\left\{ B^{m-1}v_1,\ldots,B^{m-1}v_k \right\}$ egy lineárisan független rendszer,
	amiből már következik,
	hogy minden $\alpha_{t,k}=0$,
	ami ellentmondásban van a $t$ index definíciójával.
\end{proof}
\begin{proposition}
	Legyen $B\in L\left( V \right)$ lineáris transzformációra $B^m=0$.
	Ekkor $\dim(V)\geq m\rho\left( B^{m-1} \right)$.
	\label{th:nildim}
\end{proposition}
\begin{proof}
	Létezik tehát olyan $\left\{ B^{m-1}v_1,B^{m-1}v_2,\ldots,B^{m-1}v_r \right\}$
	lineárisan független vektorrendszer, ahol $r=\rho(B^{m-1})$ a $B^{m-1}$ transzformáció képterének dimenziója.
	\Aref{le:nilp-fgtlen}.~lemma szerint a térben van egy $m\cdot r$ elemű lineárisan független vektorrendszer,
	ergo a tér legalább $mr=m\rho\left( B^{m-1} \right)$ dimenziós.
\end{proof}
\begin{proposition}
	Legyen $A\in L\left( V \right)$ minimálpolinomja
	$
		p^{m}\left( t \right)
	$
	alakú,
	ahol $p$ egy irreducibilis polinom, melynek foka $k$.
	Ekkor a tér legalább $m\cdot k$ dimenziós
	(, azaz a minimálpolinom foka legfeljebb a tér dimenziója).
	\label{th:minpol1}
\end{proposition}
\begin{proof}
	Először nézzük a trivialitásokat.
	Ha $p$ a konstans 1 polinom, akkor a tér csak a 0 vektort tartalmazza,
	tehát mind a minimálpolinom fokszáma, mind a tér dimenziója zérus.
	Ha $\deg p\geq 1$, és $m=1$, akkor a minimálpolinom irreducibilis,
	de van $v\in V$ nem zérus vektor.
	Láttuk, hogy ilyenkor
	\[
		\dim\left( \lin\left( v;A \right) \right)=\deg p=k,
	\]
	amiből persze következik, hogy a tér legalább $k$ dimenziós.
	\footnote{Sőt még azt is láttuk, hogy néhány -- lehet, hogy csak egy -- $k$ dimenziós invariáns altér direkt összege.}

	Most nézzük az érdekes esetet,
	mikor $\deg p\geq 1$ és $m\geq 2$.
	Mivel a
	\begin{math}
		p^{m-1}\left( t \right)
	\end{math}
	a minimálpolinomnál alacsonyab fokú de nem zérus polinom,
	ezért létezik $v\in V$, melyre $B=p\left( A \right)$ jelöléssel
	\(
	B^{m-1}v\neq 0.
	\)
	Persze e vektor a $\ker B=\ker p\left( A \right)$
	egy eleme,
	és $\ker p\left( A \right)$-ban minden nem zérus elem generálta invariáns altér éppen $k$-dimenziós,
	ezért
	\[
		\lin(B^{m-1}v;A)=
		\lin\left\{
		B^{m-1}v,
		AB^{m-1}v,
		A^2B^{m-1}v,
		\ldots,
		A^{k-1}B^{m-1}v
		\right\}
	\]
	pontosan $k$ dimenziós.
	Persze $\left\{ B^{m-1}v,B^{m-1}Av,B^{m-1}A^2v,\ldots,B^{m-1}A^{k-1}v \right\}\subseteq \im B^{m-1}$,
	ergo
	\[
		\rho\left( B^{m-1}\right)\geq k.
	\]
	Alkalmazva $B$-re az imént igazolt \ref{th:nildim} tételt kapjuk a kívánt
	\(
	\dim(V)\geq m\rho\left( B^{m-1} \right)\geq m\cdot k
	\)
	becslést.
\end{proof}
\begin{proposition}
	Tetszőleges lineáris transzformáció minimálpolinomjának foka legfeljebb a tér dimenziója.
	\label{th:minpol}
\end{proposition}
\begin{proof}
	Legyen
	$
		m\left( t \right)
		=
		p_1^{m_1}\left( t \right)\cdot
		p_2^{m_2}\left( t \right)\cdots
		p_r^{m_r}\left( t \right)
	$
	a minimálpolinom relativ prím, irreducibilis polinomok hatványaiként való faktorizációja.
	Tudjuk, hogy ekkor
	\[
		V
		=
		\ker p_1^{m_1}\left( A \right)
		\oplus
		\ker p_2^{m_2}\left( A \right)
		\oplus\dots\oplus
		\ker p_r^{m_r}\left( A \right).
	\]
	Az $A|_{\ker p_i^{m_i}\left( A \right)}$ minimálpolinomja $p_i^{m_i}\left( t \right)$, így az előző tétel szerint minden egyes $i$ index mellett
	$
		m_i\cdot k_i\leq \dim(\ker p_i^{m_i}\left( A \right)).
	$
	Világos, hogy
	%    \begin{IEEEeqnarray}{+c+c*}
	\[
		\deg m
		=
		\sum_{i=1}^rk_i\cdot m_i
		\leq
		\sum_{i=1}^r\dim(\ker p_i^{m_i}\left( A \right))
		=
		\dim(V).\qedhere
	\]
	%    \end{IEEEeqnarray}
\end{proof}
Később ki fog derülni,
hogy a transzformáció karakterisztikus polinomjának, amely pontosan $\dim(V)$-ed fokú,
is mindig gyöke a transzformáció.
Ez az úgynevezett Cayley\,--\,Hamilton-tétel.\index{Cayley\,--\,Hamilton}
\chapter{Nilpotens transzformációk}
\section{Hatvány függvény alakú minimálpolinom}
\begin{definition}[nilpotens transzformáció]
	Egy $A\in L\left( V \right)$ lineáris transzformációt
	\emph{nilpotensnek} mondjuk, ha
	létezik $k\in\mathbb{N}$, melyre $A^k=0$.
	Ha $A$ egy nilpotens transzformáció, akkor azt a legkisebb $m$ számot, melyre $A^m=0$ a \emph{nilpotencia rendjének} nevezzük.
\end{definition}

Például egy 5 dimenziós téren könnyen definiálhatunk első-, másod-, harmad-, negyed, és ötöd-rendű nilpotens transzformációkat.
De van-e mondjuk hatod-rendű nilpotens transzformáció ezen öt dimenziós vektortéren?
Az első észrevétel adja a negatív választ.
\begin{proposition}
	Legyen $B$ egy $m$-ed rendben nilpotens operátor.
	Ekkor létezik $v\in V$ vektor,
	melyre $B^{m-1}v\neq 0$.
	Minden ilyen $v$ vektorra a
	\[
		\left\{ v,B,B^2v,\cdots,B^{m-1}v \right\}
	\]
	$m$ elemű vektorrendszer lineárisan független.

	Emiatt ha a $V$ vektortérnek van $m$-edrendben nilpotens lineáris transzformációja, akkor $\dim(V)\geq m$,
	azaz a nilpotencia rendje legfeljebb a tér dimenziója.
	\label{th:fgtlen}
\end{proposition}
\begin{proof}
	Mivel $B^{m-1}\neq 0$, ezért valóban létezik $v\in V$ vektor,
	melyre $B^{m-1}v\neq 0$.
	Persze ekkor a $\left\{ B^{m-1}v \right\}$ egy elemet tartalmazó rendszer lineárisan független,
	ezért \aref{le:nilp-fgtlen}.~lemma szerint a tételbéli rendszer is lineárisan független.
\end{proof}
\begin{proposition}
	Egy lineáris transzformáció pontosan akkor nilpotens,
	ha valamely $m\leq\dim(V)$ mellett a minimálpolinomja
	$m\left( t \right)=t^m$ alakú.
\end{proposition}
\begin{proof}
	Tegyük fel először, hogy $B$ lineáris transzformáció $m$-ed rendben
	nilpotens.
	Legyen $p\left( t \right)=t^m$.
	Ekkor $p\in J_A$, így a minimálpolinom $p$ osztója.
	Másrészt \aref{th:fgtlen}.~állítás szerint van a térnek olyan vektora,
	amelyhez tartalmazó kis minimálpolinom is $p$.
	Így $p$ osztója a minimálpolinomnak, ergo azonos vele.

	Megfordítva, ha $m\left( t \right)=t^m$ a minimálpolinomja $B$-nek,
	akkor $B^m=0$ és ez $m$-nél kisebb kitevőre nem teljesülhet.
	Ez éppen azt jelenti, hogy $B$ transzformáció $m$-ed rendben nilpotens.
	\index{kis minimálpolinom}
\end{proof}

\section{Nilpotens operátorok redukálása}
Az alábbi lemmának nincs köze a transzformációk redukálásához.
Arra kell emlékeznünk, hogy egy véges dimenziós vektortérben minden altérnek van direkt kiegészítője.
\begin{lemma}
	Legyenek $V_1$ és $V_2$ diszjunkt alterei a véges dimenziós $W$ vektortérnek.
	Ekkor $V_1$-nek létezik $V_2$ alteret tartalmazó direkkiegészítője,
	azaz
	létezik $K\subset W$ altér, amelyre $V_2\subseteq K$ és $V_1\oplus K=W.$
	\label{felb0}
\end{lemma}
\begin{proof}
	Jelölje $V=V_1+V_2$.
	Világos, hogy $V$ altér $W$-ben.
	Legyen $L$ a direktkiegészítője, azaz $V\oplus L=W$.
	Ha $K=V_2+L$, akkor $K$ olyan altér $W$-ben,
	amelyre $V_2\subseteq K$, $K\cap V_1=\left\{ 0 \right\}$, valamint
	$V_1+K=W$.
\end{proof}
A következő lemmának nagyon fontos szerepe lesz a fejezet leglényegesebb állításában
a nilpotens operátorok redukcióját biztosító állításban.
\begin{lemma}\label{lem:felb}
	Legyen a $W$ véges dimenziós vektortérnek $H,K_0,\bar{K}$ altere.
	Tegyük fel, hogy
	\begin{enumerate}\firmlist
		\item $H\cap K_0=\left\{ 0 \right\}$;
		\item $H+\bar{K}=W$;
		\item $K_0\subseteq\bar{K}$.
	\end{enumerate}
	Ekkor létezik $K$ altere $W$-nek, melyre
	\begin{enumerate}\firmlist
		\item $K_0\subseteq K\subseteq\bar{K}$ és
		\item $K$ direkt kiegészítője $H$-nak, azaz
		      $H\oplus K=W$.\qedhere
	\end{enumerate}
\end{lemma}
\begin{proof}
	Először is a $K$ altér konstrukciója következik.
	Világos, hogy $H\cap\bar{K}\subseteq\bar{K}$ és $K_0\subseteq\bar{K}$ diszjunkt alterek
	$\bar{K}$-ban, hiszen
	\(
	\left( H\cap\bar{K} \right)\cap K_0\subseteq
	H\cap K_0=\left\{ 0 \right\}.
	\)
	Alkalmazzuk az előző \ref{felb0}.~lemmát a $\bar{K}$ altérben.
	Létezik tehát $K_0\subseteq K\subseteq \bar{K}$ altér $\bar{K}$-ban,
	amelyre
	\(
	\left( H\cap\bar{K} \right)\oplus K=\bar{K}.
	\)
	\\
	A $H$ és a $K$ diszjunkt alterek:
	\(
	H\cap K=H\cap\left( K\cap \bar{K} \right)=\left( H\cap\bar{K} \right)\cap K=\left\{ 0 \right\}.
	\)
	\\
	A $H$ és a $K$ Minkowski-összege az egész tér:
	\(
	W=H+\bar{K}
	=
	H+\left( (H\cap\bar{K})+K \right)
	\subseteq
	H+(H+K)
	=
	H+K.
	\)
	Ez éppen azt jelenti, hogy $K$ direkt kiegészítője $H$-nak.
\end{proof}
\begin{proposition}[nilpotens operátorok felbontása]
	Legyen $B\in L\left( W \right)$ egy $m$-ed rendben nilpotens lineáris transzformáció.
	Ekkor minden olyan $v\in V$ vektorhoz,
	amelyre $B^{m-1}v\neq 0$, a $\lin(v;B)$ invariáns altérnek van invariáns altér
	direkt kiegészítője.

	Formálisabban: létezik $K\subseteq W$ invariáns altér, amelyre
	\(
	\lin\left\{ v,Bv,\ldots,B^{m-1}v \right\}\oplus K = W.
	\)
	\label{pr:nilpfelb}
\end{proposition}
\begin{proof}
	A nilpotens transzformáció rendje szerinti teljes indukció.
	Ha $m=1$, akkor $B=0$, de a konstans zérus operátorra nézve minden altér invariáns,
	így a tétel összesen annyit állít, hogy egy nem zérus $v$ vektor generálta egydimenziós altérnek van direkt kiegészítője.

	Tegyük fel, hogy igaz az állítás minden vektortér legfeljebb $m-1$-ed rendben nilpotens transzformációjára.
	Legyen tehát $m>1$ és $B$ egy a $W$ vektortéren értelmezett $m$-ed rendben nilpotens lineáris transzformáció.
	Rögzítsünk egy $v\in W$ elemet, amelyre $B^{m-1}v\neq 0$.
	Tekintsük az $\im B$ invariáns alteret.
	Világos, hogy $B|_{\im B}$ egy lineáris transzformáció az $\im B$ vektortéren.
	Az is világos, hogy $B|_{\im B}$ egy $m-1$-rendben nilpotens lineáris transzformáció, hiszen
	minden $u\in\im B$ mellett $B^{m-1}u=0$.
	Azt is vegyük észre, hogy ezek szerint $v\notin\im B$.

	Alkalmazhatjuk tehát az indukciós feltevést $B|_{\im B}\in L\left( \im B \right)$ mellett a
	$Bv$ vektorra.
	Persze $B^{m-2}Bv=B^{m-1}v\neq 0$.
	Létezik tehát $K_0\subseteq\im B$ a $B$-re is invariáns altér, amelyre
	\[
		\lin\left\{ Bv,B^2v,\ldots,B^{m-1}v \right\}\oplus K_0 = \im B.
	\]

	Most megmutatjuk, hogy $\lin(v;B)\cap K_0=\left\{ 0 \right\}.$
	Ugyanis, ha
	$
		x=\sum_{k=0}^{m-1}\alpha_kB^kv\in K_0\subseteq\im B
	$, akkor $\alpha_0v\in\im B$,
	ami csak úgy lehetséges, hogy $\alpha_0=0$.
	Ezek szerint $x\in\lin(Bv;B)$ altérnek melynek direkt kiegészítője $K_0$.
	Ez persze csak úgy lehetséges, hogy $x=0$.

	Definiálja
	\[
		\bar{K}=\left\{ x\in W:Bx\in K_0 \right\}.
	\]
	Mivel $K_0$ egy altér, ezért $\bar{K}$ is az.
	Mivel a $K_0$ altér $B$-invariáns, azért teljesül a $K_0\subseteq \bar{K}$ tartalmazás.

	Most megmutatjuk, hogy $\lin(v;B)+\bar{K}=W$.
	Válasszunk egy $u\in W$ vektort.
	Persze $Bu\in\im B$, ezért előáll
	\[
		Bu
		=
		\sum_{k=1}^{m-1}\alpha_kB^kv +k_0
		=
		B\left(
		\sum_{k=0}^{m-2}\alpha_{k+1}B^kv
		\right)
		+ k_0
	\]
	alakban, ahol $k_0\in K_0$.
	Ebből azt látjuk, hogy
	$
		B
		\left(
		u-
		\sum_{k=0}^{m-2}\alpha_{k+1}B^kv
		\right)
		\in
		K_0,
	$
	ami persze $\bar{K}$ definícióját figyelembe véve azt jelenti, hogy
	\(
	u-
	\sum_{k=0}^{m-2}\alpha_{k+1}B^kv
	\in
	\bar{K}.
	\)
	Előállítottuk tehát az $u$ vektort egy $\lin(v;B)$-beli és egy
	$\bar{K}$-beli vektor összegeként.

	Alkalmazhatjuk tehát \aref{lem:felb}.~lemmát.
	Így létezik $K_0\subseteq K\subseteq\bar{K}$ altér, melyre $\lin(v;B)\oplus K=W.$
	Persze ha $u\in K\subseteq\bar{K}$, akkor $Bu\in K_0\subseteq K$, ergo $K$ egy $B$-invariáns direkt kiegészítője a
	$v$-t tartalmazó legszűkebb $B$ invariáns altérnek.
	Ezt kellett belátni.
\end{proof}
Ahhoz, hogy megkapjuk az egész vektorteret $v$-invariáns altérként vagy ilyenek direkt összegeként, az előző tételt
kell rekurzívan alkalmaznunk.
A $W$ vektortér véges dimenziós volta garantálja, hogy a rekurzió véget ér.

Persze $B|_K$ a $K$ altér lineáris transzformációja,
ami $m\geq n_2\geq 1$ rendben nilpotens.
Ha alkalmazzuk a fenti tételt, akkor kapjuk,
hogy létezik $v_2\in K, v_2\neq 0$ elem és létezik $K_2\subseteq K$ a $B$-re nézve invariáns altér,
amelyre
\[
	\lin\left\{ v_2,Bv_2,\ldots,B^{n_2-1}v_2 \right\}
	\oplus
	K_2=K.
\]
Itt persze $\dim(K_2)<\dim(K)$, hiszen a baloldali első invariáns altér legalább egydimenziós.
Az első két lépést összefoglalva:
\[
	\lin(v_1;B)
	\oplus
	\lin(v_2;B)
	\oplus
	K_2=W,
\]
Az eljárást folytatva minden lépésben legalább eggyel csökken a kiegészítő invariáns altér dimenziója.
Végül a vektortér előáll néhány, mondjuk $r$ darab $B$-re invariáns altér direkt összegeként:
\[
	\lin(v_1;B)
	\oplus
	\lin(v_2;B)
	\oplus
	\ldots
    \oplus
	\lin(v_r;B)
	=W.
\]
\begin{proposition}[nilpotens transzformáció redukálása]
	Legyen $B\in L\left( W \right)$ egy $m$-ed rendben nilpotens transzformáció.
	Ekkor léteznek olyan $v_1,v_2\dots,v_r\in W$ vektorok és
	léteznek olyan $m=n_1\geq n_2\geq \dots\geq n_r\geq 1$ pozitív egészek,
	amelyekre
	\[
		\lin\left\{ v_1,Bv_1,\ldots,B^{n_1-1}v_1 \right\}
		\oplus
		\lin\left\{ v_2,Bv_2,\ldots,B^{n_2-1}v_2 \right\}
		\oplus
		\ldots
		\oplus
		\lin\left\{ v_r,Bv_r,\ldots,B^{n_r-1}v_r \right\}
		=
		W.
	\]
	Emiatt az egyes invariáns alterek bázisainak egyesítésével kapott vektorrendszer bázisa $W$-nek:
	\[
		\left\{ B^{n_1-1}v_1,\ldots,Bv_1,v_1 \right\}
		\cup
		\left\{ B^{n_2-1}v_2,\ldots,Bv_2,v_2 \right\}
		\cup
		\ldots
		\cup
		\left\{ B^{n_r-1}v_r,\ldots,Bv_r,v_r \right\}
		\qedhere
	\]
\end{proposition}
\section{Egyértelműség}
Azt mutatjuk meg, hogy a normálalakban $r=\nu\left( B \right)$,
és az $n_1,n_2,\ldots,n_r$ számok is a $B$ nilpotens transzformáció által egyértelműen meghatározottak.
Ez azt jelenti, hogy minden nilpotens transzformációnak csak egyetlen normálalakja van.
\begin{lemma}
	Legyen $A\in L\left( V \right)$ lineáris transzformáció, és $v\in V$ olyan vektor, amelyre
	$A^mv=0$, de $A^{m-1}v\neq 0$.
	Ekkor
	\begin{enumerate}
		\item
		      $\left\{ v,Av,\ldots,A^{m-1}v \right\}$ lineárisan független,
		      így $\lin(v;A )=\lin\left\{ v,Av,\ldots,A^{m-1}v \right\}$;
		\item
		      Minden $0\leq l\leq m$ mellett
		      \begin{math}
			      \nu\left( A^l|\lin(v;A) \right)=l
		      \end{math}
		      és
		      \begin{math}
			      \rho\left( A^l|\lin(v;A) \right)=m-l.
		      \end{math}\qedhere
	\end{enumerate}
\end{lemma}
\begin{proof}
	Ha $l=m$, akkor mindkét állítás triviálisan teljesül. Legyen tehát $0\leq l<m$.
	Az $A^l$ transzformáció az $\left\{ A^{m-l}v,\ldots,A^{m-1}v \right\}$
	lineárisan független vektorrendszert nullára viszi,
	így $l\leq\nu\left( A^l|\lin(v;A)\right)$.
	A maradékot, a
	$\left\{ v,\ldots,A^{m-l-1}v \right\}$ vektorrendszert pedig az
	$\left\{ A^lv,\ldots,A^{m-1}v \right\}$ lineárisan független rendszerre képezi.
	Így
	$m-l\leq \rho\left( A^l |\lin(v;A)\right)$,
	amiből
	\[
		l\leq\nu\left( A^l|\lin(v;A) \right)=m-\rho\left( A^l|\lin(v;A) \right)\leq l.\qedhere
	\]
\end{proof}
\begin{lemma}
	Legyen $A\in L\left( V \right)$ és tegyük fel, hogy a $V$ vektortér előáll a $K_1,K_2$ invariáns alterei
	direkt összegeként, azaz
	\(
	V=K_1\oplus K_2.
	\)
	Ekkor
	\begin{math}
		\rho\left( A \right)=\rho\left( A|K_1 \right)+\rho\left( A|K_2 \right)
	\end{math}
	és
	\begin{math}
		\nu\left( A \right)=\nu\left( A|K_1 \right)+\nu\left( A|K_2 \right).
	\end{math}
	\qedhere
\end{lemma}
\begin{proof}
	Világos, hogy $A\left( V \right)=A\left( K_1 \right)+A\left( K_2 \right)$, és az
	invariancia szerint $A\left( K_1 \right)\cap A\left( K_2 \right)\subseteq K_1\cap K_2=\left\{ 0 \right\}$.
	Így persze
	\(
	A\left( V \right)=A\left( K_1 \right)\oplus A\left( K_2 \right),
	\)
	és
	\[
		\rho\left( A \right)=
		\dim\left( A\left( V \right) \right)
		=
		\dim \left( A\left( K_1 \right) \right)+
		\dim \left( A\left( K_2 \right) \right)
		=
		\rho\left( A|K_1 \right)+\rho\left( A|K_2 \right).
	\]
	Ebből már
	\[
		\nu\left( A \right)
		=
		\dim\left( V \right)-\rho\left( A \right)
		=\dim\left( K_1 \right)+\dim\left( K_2 \right)-\rho\left( A|K_1 \right)-\rho\left( A|K_2 \right)
		=
		\nu\left( A|K_1 \right)+\nu\left( A|K_2 \right)
	\]
	könnyen adódik.
\end{proof}

\begin{proposition}[A nilpotens felbontás egyértelműsége]
	Legyen $A\in L\left( V \right)$ egy lineáris transzformáció.
	Tegyük fel, hogy valamely $\left\{ v_1,\ldots,v_r \right\}$ vektorrendszerre és valamilyen
	$m_1\geq m_2\geq \dots\geq m_r$ pozitív számokra
	$A^{m_k-1}v_k\neq 0$, de $A^{m_k}v_k=0$ fennáll minden $k=1,\ldots,r$, továbbá
	\[
		V=\lin(v_1;A)\oplus \dots \oplus\lin(v_r;A).
	\]
	Tegyük fel még azt is,
	hogy valamely másik $\left\{ w_1,\ldots,w_s \right\}$ vektorrendszerre és valamely más
	$n_1\geq n_2\geq \dots\geq n_s$ pozitív számokra
	$A^{n_k-1}w_k\neq 0$, de $A^{n_k}w_k=0$ fennáll minden $k=1,\ldots,s$ mellett, és
	\[
		V=\lin(w_1;A)\oplus \dots \oplus\lin(w_s;A).
	\]
	Ekkor
	\begin{enumerate}
		\item $A$ nilpotens lineáris transzformációja $V$-nek;
		\item Ha $m$ jelöli a nilpotencia rendjét, akkor
		      \(
		      m_1=m=n_1
		      \);
		\item A transzformáció $\nu\left( A \right)$ defektusára
		      \(
		      r=\nu\left( A \right)=s
		      \);
		\item Valamennyi $k=1,\ldots,r$ esetén
		      \(
		      m_k=n_k
		      \).\qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Az első két állítás nyilvánvaló, mivel a rendezettség szerint
    $A^{m_1}|_{\lin(v_k;A)}=0$,
	minden $k=1,\ldots,r$ mellett

	A harmadik állításhoz:
	\[
		\nu\left( A \right)=\sum_{k=1}^r\nu\left( A|\lin(v_k;A) \right)=
		\sum_{k=1}^r1=r.
	\]
	Ugyanígy a másik direkt összeg felbontásból kapjuk, hogy $\nu\left( A \right)=s$.

	A negyedik állítás.
	Tegyük fel -- indirekt --, hogy $m_k=n_k$ nem teljesül minden $k=1,\ldots,r$ számra.
	Legyen $1<t\leq r$ az a legkisebb szám, amelyre $m_t\neq n_t$.
	Ezek szerint $k=1,\ldots,t-1$ mellett $m_k=n_k$, de $m_t\neq n_t$.
	Feltehető, hogy $m_t>n_t$.
	Ekkor tehát
	\[
		m_1=n_1\geq m_2=n_2\geq\dots\geq m_{t-1}=n_{t-1}\geq m_t>n_t.
	\]
	Ekkor az első direkt összeg felbontásban a $t$-nél magasabb indexű tagokat elhagyva
	\[
		\rho\left( A^{n_t} \right)
		\geq
		\left( \sum_{k=1}^{t-1}\rho\left( A^{n_t}|\lin(v_k;A) \right) \right)
		+\rho\left( A^{n_t}|\lin(v_t;A) \right)
		=
		\left( \sum_{k=1}^{t-1}(m_k-n_t) \right)+m_t-n_t.
	\]
	Hasonlóan, a második direkt összeg felbontásban a $t$-edik, és a $t$-nél magasabb indexű rangok zérók, így
	\[
		\rho\left( A^{n_t} \right)
		=
		\sum_{k=1}^{t-1}\rho\left( A^{n_t}|\lin(w_k;A) \right)
		=
		\sum_{k=1}^{t-1}(n_k-n_t)
		=
		\sum_{k=1}^{t-1}(m_k-n_t),
	\]
	ami ellentmond $m_t>n_t$ feltételnek.
\end{proof}
Az alábbiakban a nilpotens felbontási tételt annak egyértelműségével együtt foglaljuk össze.
\begin{proposition}[nilpotens transzformáció normálakja]\label{pr:nilpnormal}
	Legyen $B\in L\left( W \right)$ egy $m$-ed rendben nilpotens transzformáció.
	Jelölje $r=\nu\left( B \right)$ a $B$ defektusát.
	Ekkor léteznek olyan $v_1,v_2\dots,v_r\in W$ vektorok és
	létezik pozitív egészek egyetlen olyan $m=n_1\geq n_2\geq \dots\geq n_r\geq 1$ véges sorozata,
	amelyekre
	\[
		\lin\left\{ v_1,Bv_1,\ldots,B^{n_1-1}v_1 \right\}
		\oplus
		\lin\left\{ v_2,Bv_2,\ldots,B^{n_2-1}v_2 \right\}
		\oplus
		\ldots
		\oplus
		\lin\left\{ v_r,Bv_r,\ldots,B^{n_r-1}v_r \right\}
		=
		W.
	\]
	Emiatt a
	\[
		\left\{ B^{n_1-1}v_1,\ldots,Bv_1,v_1 \right\}
		\cup
		\left\{ B^{n_2-1}v_2,\ldots,Bv_2,v_2 \right\}
		\cup
		\ldots
		\cup
		\left\{ B^{n_r-1}v_r,\ldots,Bv_r,v_r \right\}
		\tag{\dag}
	\]
	vektorrendszer bázisa $W$-nek.

	Ha ebben a bázisban felírjuk $B$ mátrixát,
	akkor $r$ darab diagonálisan elhelyezkedő részmátrixból álló mátrixot kapunk.
	Az első $n_1\times n_1$ méretű,
	a második $n_2\times n_2$ méretű, \dots, az utolsó $n_r\times n_r$ méretű.
	Minden ilyen blokkban csak a (felső) mellék diagonális elemei nem nullák.
	A mellék diagonális elemei 1-esek.
	Minden más elem zérus.
	Ezt a mátrixot nevezzük a $B$ nilpotens transzformáció \emph{normálalakjának}.
	\index{nilpotens transzformáció}
\end{proposition}
Adott $1\leq j\leq r$ mellett tehát, $\mathbf{B}_j\in \mathbb{F}^{n_j\times n_j}$
a fenti $j$-edik invariáns altérre leszorított $B$ leképezésnek a
\begin{math}
	\left\{ B^{n_j-1}v_j,\ldots,Bv_j,v_j \right\}
\end{math}
bázison felírt mátrixa:
\[
	\mathbf{B}_j
	=
	\begin{pmatrix}
		0      & 1      & 0      & \dots  & 0      & 0      & 0      \\
		0      & 0      & 1      & \dots  & 0      & 0      & 0      \\
		\vdots & \vdots & \ddots & \ddots & \vdots & \vdots & \vdots \\
		0      & 0      & \dots  & 0      & 1      & 0      & 0      \\
		0      & 0      & \dots  & 0      & 0      & 1      & 0      \\
		0      & 0      & \dots  & \dots  & 0      & 0      & 1      \\
		0      & 0      & \dots  & \dots  & 0      & 0      & 0
	\end{pmatrix}.
\]
Így az egész $V$ vektortéren értelmezett $B$ lineáris transzformáció normálakja
--
azaz a $B$-nek (\dag) bázisban felírt mátrixa
--
a fenti tipusú mátrixok diagonális alakú elrendezésével adódik:
\begin{displaymath}
	[B]=
	\begin{pmatrix}
		\begin{matrix}
			0      & 1      & 0      & \dots  & 0      & 0      & 0      \\
			0      & 0      & 1      & \dots  & 0      & 0      & 0      \\
			\vdots & \vdots & \ddots & \ddots & \vdots & \vdots & \vdots \\
			0      & 0      & \dots  & 0      & 1      & 0      & 0      \\
			0      & 0      & \dots  & 0      & 0      & 1      & 0      \\
			0      & 0      & \dots  & \dots  & 0      & 0      & 1      \\
			0      & 0      & \dots  & \dots  & 0      & 0      & 0
		\end{matrix} &   &        & \\
		                                &
		\begin{matrix}
			0      & 1      & 0      & \dots  & 0      & 0      & 0      \\
			0      & 0      & 1      & \dots  & 0      & 0      & 0      \\
			\vdots & \vdots & \ddots & \ddots & \vdots & \vdots & \vdots \\
			0      & 0      & \dots  & 0      & 1      & 0      & 0      \\
			0      & 0      & \dots  & 0      & 0      & 1      & 0      \\
			0      & 0      & \dots  & \dots  & 0      & 0      & 1      \\
			0      & 0      & \dots  & \dots  & 0      & 0      & 0
		\end{matrix}
		                                &   &          \\
		                                &   & \ddots & \\
		                                &   &        &
		\begin{matrix}
			0      & 1      & 0      & \dots  & 0      & 0      & 0      \\
			0      & 0      & 1      & \dots  & 0      & 0      & 0      \\
			\vdots & \vdots & \ddots & \ddots & \vdots & \vdots & \vdots \\
			0      & 0      & \dots  & 0      & 1      & 0      & 0      \\
			0      & 0      & \dots  & 0      & 0      & 1      & 0      \\
			0      & 0      & \dots  & \dots  & 0      & 0      & 1      \\
			0      & 0      & \dots  & \dots  & 0      & 0      & 0
		\end{matrix}
	\end{pmatrix}.
\end{displaymath}
Hangsúlyozni szeretném, hogy az egyértelműség szerint a $\mathbf{B}_j$ blokkok száma, és azok mérete is csak
a nilpotens transzformációtól függ.
Több bázis is lehetséges, amelyben a nilpotens transzformáció normálalakú,
de nem csak hogy minden normálalakban azonos számú blokk van ($B$ defektusa),
de a blokkok mérete is azonos.
A nilpotens felbontásban szereplő direkt összeadandó alterek páronként izomorfak egymással.
\section{Illusztrációk}
\subsection{Egyetlen invariáns altér}
Írjuk fel a $W$ vektortéren értelmezett lineáris transzformáció normál alakját,
ahol az $\left\{ u_1,u_2,u_3,u_4 \right\}$ bázisban
\[
	B\left( \alpha u_1+\beta u_2+\gamma u_3+\delta u_4 \right)=
	-\left( \gamma + \delta \right)u_1
	+\gamma u_2
	-\left( \alpha+\beta+\gamma \right)u_3
	+\left( \alpha+\beta+\gamma+\delta \right)u_4.
\]
A $B$ mátrixa a fent rögzített bázisban:
\[
	\begin{pmatrix}
		0  & 0  & -1 & -1 \\
		0  & 0  & 1  & 0  \\
		-1 & -1 & -1 & 0  \\
		1  & 1  & 1  & 1
	\end{pmatrix}.
\]
Mivel két azonos oszlop is van, ezért a defektus legalább egy.
Számoljuk ki a minimálpolinomot!
Az $u_1$ bázis elemhez
\[
	\begin{array}{c|cccccc}
		    & u_1 & Bu_1 & B^2u_1 & B^3u_1 & B^4u_1 \\
		\hline
		u_1 & 1   & 0    & 0      & -1     & 0      \\
		u_2 & 0   & 0    & -1     & 1      & 0      \\
		u_3 & 0   & -1   & 1      & 0      & 0      \\
		u_4 & 0   & 1    & 0      & 0      & 0
	\end{array}.
\]
Mivel az első négy oszlop lineárisan független,
ezért az $u_1$-hez tartozó kis minimálpolinom $p_1=t^4.$
Mivel tudjuk, hogy a minimálpolinom legfeljebb 4-ed fokú, és
$p_1$ osztja, ezért csak $m\left( t \right)=t^4$ lehetséges,
ezért $B$ transzformáció 4-ed rendben nilpotens.
Éppen most számoltuk ki, hogy $B^3u_1\neq 0$, ezért
az $u_1$-et tartalmazó legszűkebb $B$ invariáns altér az egész $W$,
tehát
\[
	\lin\left\{ u_1,Bu_1,B^2u_1,B^3u_1 \right\}=W
\]
és a $\left\{ B^3u_1,B^2u_1,Bu_1,u_1 \right\}$ sorrendű bázisban $B$ mátrixa
\[
	\begin{pmatrix}
		0 & 1 & 0 & 0 \\
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1 \\
		0 & 0 & 0 & 0
	\end{pmatrix}
\]
alakú.
Emlékezzünk, hogy az új bázisra való áttérés mátrixa egyszerűen az új bázis
elemeiből mint oszlopokból alkotott mátrix, ami azt jelenti, hogy
\[
	\begin{pmatrix}
		-1 & 0  & 0  & 1 \\
		1  & -1 & 0  & 0 \\
		0  & 1  & -1 & 0 \\
		0  & 0  & 1  & 0
	\end{pmatrix}^{-1}
	\begin{pmatrix}
		0  & 0  & -1 & -1 \\
		0  & 0  & 1  & 0  \\
		-1 & -1 & -1 & 0  \\
		1  & 1  & 1  & 1
	\end{pmatrix}
	\begin{pmatrix}
		-1 & 0  & 0  & 1 \\
		1  & -1 & 0  & 0 \\
		0  & 1  & -1 & 0 \\
		0  & 0  & 1  & 0
	\end{pmatrix}
	=
	\begin{pmatrix}
		0 & 1 & 0 & 0 \\
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1 \\
		0 & 0 & 0 & 0
	\end{pmatrix}.
\]
\subsection{Két invariáns altér (3- és 1 dimenziós)}
Most a játék kedvéért, keressük meg a fenti  felbontást és bázist, egy konkrét esetben.
Legyen $W$ egy négy dimenziós vektortér az $\left\{ u_1,u_2,u_3,u_4 \right\}$ rögzült bázissal.
A $B\in L\left( W \right)$ lineáris transzformáció definíciója a báziselemek segítségével:
\[
	B\left( \alpha u_1+\beta u_2 +\gamma u_3+\delta u_4 \right)
	=
	\left( \gamma-\delta \right)u_1+
	\gamma u_2+
	\delta u_3.
\]

Első lépésként keressük meg a minimálpolinomot.
A transzformáció mátrixa
\[
	B=
	\begin{pmatrix}
		0 & 0 & 1 & -1 \\
		0 & 0 & 1 & 0  \\
		0 & 0 & 0 & 1  \\
		0 & 0 & 0 & 0
	\end{pmatrix}.
\]
Az $u_1$-minimálpolinom $p_1\left( t \right)=t$,
az $u_2$-minimálpolinom $p_2\left( t \right)=t$.
A harmadik kis minimálpolinomhoz írjuk fel $u_3$ hatványait:
\[
	\begin{array}{c|cccc}
		    & u_3 & Bu_3 & B^2u_3 \\
		\hline
		u_1 & 0   & 1    & 0      \\
		u_2 & 0   & 1    & 0      \\
		u_3 & 1   & 0    & 0      \\
		u_4 & 0   & 0    & 0
	\end{array}.
\]
Persze az első két oszlop lineárisan független, így $p_3\left( t \right)=t^2$.
Hasonlóan az $u_4$ vektor $B$ hatványait felírva:
\[
	\begin{array}{c|ccccc}
		    & u_4 & Bu_4 & B^2u_4 & B^3u_4 \\
		\hline
		u_1 & 0   & -1   & 1      & 0      \\
		u_2 & 0   & 0    & 1      & 0      \\
		u_3 & 0   & 1    & 0      & 0      \\
		u_4 & 1   & 0    & 0      & 0
	\end{array}.
\]
Az első három oszlop ránézésre független, emiatt a minimálpoliniom $p_4\left( t \right)=t^3$.
\\
Emlékszünk, hogy a minimálpolinom a $p_1,p_2,p_3,p_4$ legkisebb közös többszöröse,
ergo $m\left( t \right)=t^3$, és $B$ egy harmadrendben nilpotens transzformáció.

Írjuk fel a fenti tételben szereplő invariáns direktfelbontást.
Mivel a $B$ rangja ránézésre 2, így a magtere 2 dimenziós, ergo két invariáns altér direkt összege $W$, amiből
az egyik 3 dimenziós, így a másik csak egydimenziós lehet, ezért azt csak a magtér egyik eleme generálhatja!
A három dimenziós altér lehet például a $\lin(u_4;B)$,
hiszen éppen az imént láttuk, hogy $B^2u_4\neq 0$.
A fenti \ref{pr:nilpfelb}.~állítás éppen azt mondja, hogy ennek az altérnek van olyan $K$ invariáns altér direkt kiegészítője,
aminek van olyan bázisa, amely annyi magtérbeli elemet tartalmaz, ami a $B|_K$ nilpotens operátor rendje, tehát legalább 1.
Így most nyilvánvaló, hogy az
egydimenziós invariáns alteret generálhatja a $\ker A$ magtér bármely olyan eleme,
amely lineárisan független $B^2u_4$-től.
Például $u_1$.\footnote{Vagy bármi, ami $\alpha u_1+\beta u_2$ alakú, ahol $\alpha\neq\beta$.}
Ilyen módon
\[
	\lin\left\{ u_4,Bu_4,B^2u_4 \right\}\oplus
	\lin\left\{ u_1 \right\}
	=
	W,
\]
és az
\(
\left\{
B^2u_4,Bu_4,u_4,u_1
\right\}
\)
vektorok olyan bázisát adják a térnek, melyben $B$ mátrixa az alábbi alakú
\[
	\begin{pmatrix}
		0 & 1 & 0 & 0 \\
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0
	\end{pmatrix}.
\]
\section{A nilpotens felbontási tétel nélkül?}
Legyen $B\in L\left( V \right)$ egy $m$-edrendben nilpotens operátor.
Világos, hogy $\nu\left( B \right)\geq 1$,
hiszen egyébként $B$ valamennyi hatványa is reguláris maradna.

Válasszuk ki a $\ker B$ egy
$\left\{ e_1,\ldots,e_r \right\}$ bázisát,
ahol a rövidség kedvéért $r=\nu\left( B \right)$.
Most minden $i=1,\ldots,r$ mellett legyen $m_i$ a legnagyobb olyan $k$ egész, amelyre a
$B^{k-1}x=e_i$ egyenletnek még van megoldása.
Világos, hogy $1\leq m_i\leq m$.
Az általánosság elvesztése nélkül feltehető, hogy
$m\geq m_1\geq m_2\geq \dots \geq m_r$, hiszen a bázis elemeket az $m_i$ számok
csökkenő sorrendjében átindexelhetjük.
Jelölje $v_i\in V$ egy tetszőleges megoldását $B^{m_i-1}x=e_i$-nek.
Világos tehát, hogy minden $i=1,\ldots,r$ mellett
\[
	B^{m_i-1}v_i=e_i \quad\text{ és }\quad B^{m_i}v_i=0
\]
Írjuk egy táblázat legalsó sorába a $\ker B$ kiválaszott bázis elemeit,
majd föléjük a megfelelő csökkenő $B$ hatványokat.%
\footnote{Tipográfiai probléma e táblázat áttekinthető leírása.
	A lényeg, hogy a legalsó sorban lévő vektorok vannak csak biztosan egy sorban. A 2. oszlopnak $m_2\leq m_1$
	eleme van, tehát a $v_1$ akkor és csak akkor esik egy sorba $v_2$ vel, ha $m_1=m_2$. Egyébként $v_2$ lejjebb van mint $v_1$. Képzeljük úgy a táblázatot, mint monoton fogyó elemszámú oszlopok, alulra zárt összeségét, úgy hogy a legnagyobbal kezdem, stb.}
\[
	\begin{matrix}
		v_1          & \vdots       & \vdots       & \dots  & \vdots       \\
		Bv_1         & v_2          & \vdots       & \dots  & \vdots       \\
		B^2v_1       & Bv_2         & v_3          & \dots  & v_r          \\
		\vdots       & \vdots       & \vdots       & \ddots & \vdots       \\
		B^{m_1-k}v_1 & B^{m_2-k}v_2 & B^{m_3-k}v_3 & \dots  & B^{m_r-k}v_r \\
		\vdots       & \vdots       & \vdots       & \ddots & \vdots       \\
		B^{m_1-1}v_1 & B^{m_2-1}v_2 & B^{m_3-1}v_3 & \dots  & B^{m_r-1}v_r
	\end{matrix}
\]
Felmerül, hogy a fenti vektorrendszer bázis, evvel a nilpotens felbontási tételt megkerülve
kapnánk a nilpotens normálalak igazolását.

Sajnos nem feltétlen bázis a fenti vektorrendszer.
A lineárisan függetlenség a korábbi technikával igazolható -- tegyük ezt meg! --, de a rendszer nem mindig
generátorrendszer.

Tekintsük például a
\[
	B=
	\begin{pmatrix}
		0 & 1 & 0 & 0 \\
		0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0
	\end{pmatrix}
\]
mátrixát.
Jelölje most $\left\{ e_1,e_2,e_3,e_4 \right\}$ a térnek azt  a bázisát, amelyben
a fenti mátrixot felírtuk.
Világos, hogy $\left\{ e_4,e_3+e_4,e_1+e_3+e_4 \right\}$ vektorrendszer a $\ker B$ egy olyan bázisa,
amelynek egyik eleme sem esik $B$ képterébe, hiszen egyik elem sem az $e_1$ skalárszorosa.
Ilyen módon a fent konstruált vektorrendszer csak három elemű lesz, ergo nem bázisa a négy
dimenziós térnek.

A nilpotens felbontási tétel éppen azt mondja, hogy a $\ker B$-nek van olyan alkalmasan megválasztott bázisa, amelyre a fenti konstrukcióban kapott vektorrendszer a térnek generátorrendszere, ergo bázisa.
Persze amikor konkrétan a normálalakot konstruáljuk, akkor elegendő olyan bázisát keresni $\ker B$-nek,
amelyből kiindulva a fenti vektorrendszer elemeinek száma a tér dimenziójával egyezik meg.
Az így kapott rendszer persze bázis lesz.
\chapter{A Jordan-normálalak}
\scwords A legfontosabb gondolathoz érkeztünk,
de igazán nincs új a fejezetben. Az előző fejezeteket foglaljuk össze.
Először azt az esetet vizsgáljuk, mikor egy transzformáció minimálpolinomja
egyetlen elsőfokú polinom valamilyen hatványa,
majd ezt felhasználva kapjuk az úgynevezett kanonikus alakot abban az esetben is,
mikor a minimálpolinom különböző elsőfokú polinomok hatványainak szorzataiként áll elő.
Mivel egy komplex együtthatós polinom mindig ilyen alakú,
ezért az eredményeinket komplex test feletti vektorterek lineáris transzformációira alkalmazhatjuk.

\section{Egy gyöktényezős minimálpolinom}
\begin{definition}[Jordan-normálalak]
	Tegyük fel, hogy az $A\in L\left( V \right)$ lineáris transzformáció
	minimálpolinomja
	\[
		\left( t-\lambda \right)^m
	\]
	alakú, valamely $\lambda\in\mathbb{F}$ és $m$ pozitív egész mellett.
	Világos, hogy $B=A-\lambda I$ éppen $m$-edrendben nilpotens.
	Alkalmazaható tehát $B$-re \aref{pr:nilpnormal}.~állítás.
	Jelölje tehát $r=\nu\left( B \right)$ a $B$ defektusát.
	Így léteznek olyan $v_1,v_2\dots,v_r\in V$ vektorok és
	létezik pozitív egészek egyetlen olyan $m=n_1\geq n_2\geq \dots\geq n_r\geq 1$ véges sorozata,
	amelyekre a
	\[
		\left\{ B^{n_1-1}v_1,\ldots,Bv_1,v_1 \right\}
		\cup
		\left\{ B^{n_2-1}v_2,\ldots,Bv_2,v_2 \right\}
		\cup
		\ldots
		\cup
		\left\{ B^{n_r-1}v_r,\ldots,Bv_r,v_r \right\}
		\tag{\dag}
	\]
	vektorrendszer bázisa $V$-nek.

	A $B$ transzformációnak ebben a bázisban felírt mátrixa olyan, hogy
	a szuper diagonálisán kívül minden elem zérus,
	a szuper diagonálisban $n_1-1$ db $1$-es, aztán egy zérus, majd $n_2-1$ db $1$-es aztán egy zérus, stb.
	Adjuk a $B=A-\lambda I$ mátrixához a $\lambda I$ diagonális mátrixot!
	Ekkor kapjuk az $A$-nak a $(\dag)$ bázisban felírt mátrixát.
	Ezt a mátrixot nevezzük az $A$ transzformáció \emph{Jordan-normálalakjának}\index{Jordan-normálalak}.
\end{definition}
Adott $1\leq j\leq r$ mellett,
a $j$-edik invariáns altérre leszorított $A$ leképezésnek a
\begin{math}
	\left\{ B^{n_j-1}v_j,\ldots,Bv_j,v_j \right\}
\end{math}
bázison felírt mátrixát \emph{Jordan-blokknak}\index{Jordan-blokk} mondjuk.
\[
	\begin{pmatrix}
		\lambda & 1       & 0      & \dots   & 0       & 0       & 0       \\
		0       & \lambda & 1      & \dots   & 0       & 0       & 0       \\
		\vdots  & \vdots  & \ddots & \ddots  & \vdots  & \vdots  & \vdots  \\
		0       & 0       & \dots  & \lambda & 1       & 0       & 0       \\
		0       & 0       & \dots  & 0       & \lambda & 1       & 0       \\
		0       & 0       & \dots  & \dots   & 0       & \lambda & 1       \\
		0       & 0       & \dots  & \dots   & 0       & 0       & \lambda
	\end{pmatrix}.
\]
Így az egész $V$ vektortéren értelmezett $A$ lineáris transzformáció normálakja
--
azaz a $A$-nak (\dag) bázisban felírt mátrixa
--
a fenti tipusú Jordan-blokkok, diagonális alakú elrendezésével adódik:
\begin{displaymath}
	[A]=
	\begin{pmatrix}
		\begin{matrix}
			\lambda & 1       & 0      & \dots   & 0       & 0       & 0       \\
			0       & \lambda & 1      & \dots   & 0       & 0       & 0       \\
			\vdots  & \vdots  & \ddots & \ddots  & \vdots  & \vdots  & \vdots  \\
			0       & 0       & \dots  & \lambda & 1       & 0       & 0       \\
			0       & 0       & \dots  & 0       & \lambda & 1       & 0       \\
			0       & 0       & \dots  & \dots   & 0       & \lambda & 1       \\
			0       & 0       & \dots  & \dots   & 0       & 0       & \lambda
		\end{matrix} &        &        & \\
		                                & \!\!\!
		\begin{matrix}
			\lambda & 1       & 0      & \dots   & 0       & 0       & 0       \\
			0       & \lambda & 1      & \dots   & 0       & 0       & 0       \\
			\vdots  & \vdots  & \ddots & \ddots  & \vdots  & \vdots  & \vdots  \\
			0       & 0       & \dots  & \lambda & 1       & 0       & 0       \\
			0       & 0       & \dots  & 0       & \lambda & 1       & 0       \\
			0       & 0       & \dots  & \dots   & 0       & \lambda & 1       \\
			0       & 0       & \dots  & \dots   & 0       & 0       & \lambda
		\end{matrix}
		                                &        &          \\
		                                &        & \ddots & \\
		                                &        &        &
		\begin{matrix}
			\lambda & 1       & 0      & \dots   & 0       & 0       & 0       \\
			0       & \lambda & 1      & \dots   & 0       & 0       & 0       \\
			\vdots  & \vdots  & \ddots & \ddots  & \vdots  & \vdots  & \vdots  \\
			0       & 0       & \dots  & \lambda & 1       & 0       & 0       \\
			0       & 0       & \dots  & 0       & \lambda & 1       & 0       \\
			0       & 0       & \dots  & \dots   & 0       & \lambda & 1       \\
			0       & 0       & \dots  & \dots   & 0       & 0       & \lambda
		\end{matrix}
	\end{pmatrix}
\end{displaymath}
Hangsúlyozni szeretném, hogy az egyértelműség szerint a Jordan blokkok száma, és azok mérete is csak
az $A$ transzformációtól függ.
Több bázis is lehetséges, amelyben a transzformáció Jordan-normálalakú,
de nem csak hogy minden normálalakban azonos számú Jordan-blokk van,
\footnote{A Jordan-blokkok száma a $B=A-\lambda I$ defektusa, azaz a $\lambda$ sajátérték
	geometriai dimenziója.\index{sajátérték geometriai dimenziója}}
de a blokkok mérete is azonos.
A felbontásban szereplő direkt összeadandó alterek páronként izomorfak egymással.
\section{Jordan-normálalak: az általános eset}
\begin{definition}
	Legyen $A\in L\left( V \right)$ lineáris transzformáció valamely $\mathbb{F}$ test
	feletti vektortéren.
	Tegyük fel, hogy $A$ minimálpolinomja
	\[
		\left( t-\lambda_1 \right)^{m_1}\left( t-\lambda_2 \right)^{m_2}\dots\left( t-\lambda_s \right)^{m_s}
	\]
	alakú, ahol $\lambda_1,\ldots,\lambda_s$ az $A$ különböző sajátértékei,
	és $m_1,\ldots,m_s$ pozitív egészek.
	Jelölje $V_j=\ker\left( A-\lambda_j I \right)^{m_j}$,
	és $A_j=A|V_j$ minden $j=1,\ldots,s$ mellett.
	\Aref{pr:redukcio-primfelbontas}.~állításban rögzítettük, hogy
	\[
		V=V_1\oplus\dots\oplus V_s
	\]
	továbbá a $V_j$ vektortér $A_j$ lineáris transzformációjának minimálpolinomja
	\[
		\left( t-\lambda_j \right)^{m_j}.
	\]
	Minden egyes $j$ mellett írjuk fel az $A_j\in L\left( V_j \right)$ transzformáció
	Jordan-normálakját,
	majd egyesítsük a $V_j$ alterek Jordan-bázisait a $V$ tér bázisává.
	Jelölje $B_j=A-\lambda_j I$,
	így kapjuk minden $j=1,\ldots,s$ mellett
	az
	\[
		m_j=n_1^{(j)}\geq n_2^{(j)}\geq\dots\geq n_{r_j}^{(j)}\geq 1
		\text{ pozitív egészeket és a }
		v_1^{(j)},v_2^{(j)},\ldots,v_{r_j}^{(j)}\in V
	\]
	vektorokat, amelyekre a
	\[
		\begin{Bmatrix}
			B_1^{n_1^{(1)}-1}v_1^{(1)},\ldots,B_1v_1^{(1)},v_1^{(1)};
			       &
			B_1^{n_2^{(1)}-1}v_2^{(1)},\ldots,B_1v_2^{(1)},v_2^{(1)};
			       &
			\dots ;
			       &
			B_1^{n_{r_1}^{(1)}-1}v_{r_1}^{(1)},\ldots,B_1v_{r_1}^{(1)},v_{r_1}^{(1)};
			\\
			B_2^{n_1^{(2)}-1}v_1^{(2)},\ldots,B_2v_1^{(2)},v_1^{(2)};
			       &
			B_2^{n_2^{(2)}-1}v_2^{(2)},\ldots,B_2v_2^{(2)},v_2^{(2)};
			       &
			\dots ;
			       &
			B_2^{n_{r_2}^{(2)}-1}v_{r_2}^{(2)},\ldots,B_2v_{r_2}^{(2)},v_{r_2}^{(2)};
			\\
			\vdots & \vdots & \ddots & \vdots
			\\
			B_s^{n_1^{(s)}-1}v_1^{(s)},\ldots,B_sv_1^{(s)},v_1^{(s)};
			       &
			B_s^{n_2^{(s)}-1}v_2^{(s)},\ldots,B_sv_2^{(s)},v_2^{(s)};
			       &
			\dots ;
			       &
			B_s^{n_{r_s}^{(s)}-1}v_{r_s}^{(s)},\ldots,B_sv_{r_s}^{(s)},v_{r_s}^{(s)}.
		\end{Bmatrix}
	\]
	vektorrendszer bázisa $V$-nek, amely bázist az $A$ transzformáció \emph{Jordan-bázisának}\index{Jordan-bázis} nevezünk.
	Az $A$ transzformációnak ebben a bázisban felírt mátrixát nevezzük az $A$ \emph{Jordan-normálalakjának.}
\end{definition}
Az $A$ lineáris transzformáció Jordan-normálalakját tehát az egyes $\lambda_j$ sajátértékekhez tartozó $A_j\in L\left( V_j \right)$
transzformációk Jordan-normálalakú mátrixainak diagonális alakú elrendezésével kapjuk.
Tehát
\[
	[A]
	=
	\begin{pmatrix}
		[A_1] &             &        &                 &             \\
		      & \!\!\![A_2] &        &                 &             \\
		      & \!\!\!      & \ddots &                 &             \\
		      & \!\!\!      &        & \!\!\![A_{s-1}] &             \\
		      & \!\!\!      &        & \!\!\!          & \!\!\![A_s]
	\end{pmatrix}.
\]
Itt minden egyes $[A_j]$ mátrix annyi Jordan-blokkból áll,
amennyi a $\lambda_j$ sajátérték geometriai dimenziója.
Minden egyes $A_j$ mátrix első Jordan-blokkja
egy $m_j\times m_j$ méretű részmátrix,
majd rendre kisebb vagy egyenlő méretű blokkok következnek.
\begin{proposition}
	Legyen $V$ egy $\mathbb{C}$ komplex számtest feletti vektortér,
	és $A\in L\left( V \right)$ egy lineáris transzformáció.
	Ekkor $A$-hoz létezik Jordan-bázis, azaz $A$ felírható Jordan-normálalakban.
	A Jordan-normálalak egyértelmű abban az értelemben,
	hogy minden $\lambda_j$ sajátértékhez annyi Jordan-blokk tartozik, mint
	a sajátérték geometriai multiplicitása, és a Jordan-blokkok mérete csak az $A$ transzformációtól függ.
\end{proposition}
\begin{proof}
	Mivel a komplex számtest felett igaz az algebra alaptétele,
	ezért minden polinom, így $A$ minimálpolinomja is előáll
	$\left( t-\lambda_1 \right)^m_1\left( t-\lambda_2 \right)^{m_2}\dots\left( t-\lambda_s \right)^{m_s}$
	alakban.
	Alkalmazhatjuk tehát a fent leírt eljárast $A$ Jordan-bázisának konstrukciójához.
\end{proof}

\subsection{Következmények}
Érdemes látni,
hogy a fenti Jordan normálakról szóló tétel speciális esetként tartalmaz néhány korábbi,
de későbbi fontos eredményünket feltéve, hogy az alaptest a komplex számtest.

Az egyes sajátértékekhez tartozó első Jordan blokk mérete $m_j$.
Ezért a minimálpolinom fokszámánál, a $\sum_{j=1}^sm_j$ számnál a tér dimenziója nem lehet kisebb.
Meggondoltuk tehát, hogy \emph{a minimálpolinom foka legfeljebb  a tér dimenziója.}

Az is nyilvánvaló, hogy a Jordan-normálalak pontosan akkor diagonális, ha minden Jordan-blokk diagonális.
Mivel a legnagyobb méretű Jordan-blokkok az $A_j$ transzformációk első blokkja, aminek mérete $m_j$, ezért a \emph{Jordan-normálak pontosan akkor diagonális mátrix, ha} minden $j$ mellett $m_j=1$,
ami éppen azt jelenti, hogy \emph{a minimálpolinom gyökei egyszeresek.}
Persze ebben az esetben a Jordan-bázis minden eleme az $A$ egy sajátvektora.

Később teljesen nyilvánvaló lesz, hogy egy Jordan-normálalakú mátrix
úgynevezett \emph{karakterisztikus polinomja}\index{karakterisztikus polinom}
\[
	\left( t-\lambda_1 \right)^{k_1}\left( t-\lambda_2 \right)^{k_2}\dots\left( t-\lambda_s \right)^{k_s}
\]
alakú,
ahol a $\lambda_j$ sajátérték pontosan $k_j$-szer szerepel a Jordan-normálalak diagonálisában.
Mivel minden $\lambda_j$-re a legnagyobb Jordan-blokk éppen $m_j\times m_j$ méretű,
ezért $m_j\leq k_j$ minden $j$-mellett.
Ebből persze következik,
hogy a minimálpolinom osztója a karakterisztikus polinomnak, ami
avval ekvivalens,
hogy \emph{az $A$ transzformáció karakterisztikus polinomjának is gyöke maga az $A$ transzformáció}.
Ezt az eredményt gondoltuk meg, mikor a vektortér olyan test felett van értelmezve,
ahol minden polinomnak van gyöke.
A gondolat általánosabban is igaz, tetszőleges test feletti vektorterek esetére.
Ez a \emph{Cayley\,--\,Hamilton-tétel}\index{Cayley\,--\,Hamilton}.

Mivel a karakterisztikus polinom mindig pontosan egy $\dim(V)$-ed fokú normált polinom,
ezért a Cayley--Hamilton-tételből is nevetve következik,
hogy a minimálpolinom mindig legfeljebb $\dim(V)$-ed fokú.

Szokás azt mondani, hogy a $\lambda\in\sigma\left( A \right)$ sajátérték
\emph{algebrai multiplicitása}\index{sajátérték algebrai multiplicitása} $k$,
ha a Jordan-normálalakban $\lambda_j$ éppen $k$-szor szerepel.
Világos, hogy ez azonos avval, hogy a transzformáció karakterisztikus polinomjának
$\lambda$ egy $k$-szoros multiplicitású gyöke.
Mivel minden egyes Jordan-blokk legalább $1\times 1$ méretű,
ezért a $\lambda_j$ geometriai multiplicitása,
azaz a $\lambda_j$-hez tartozó Jordan-blokkok száma legfeljebb ezen blokkokban lévő diagonális elemek száma,
ergo a $\lambda_j$ algebrai multiplicitása.
Az is világos, hogy e két multiplicitás pontosan akkor egyezik meg,
ha minden Jordan-blokk $1\times 1$ méretű, tehát ha diagonalizálható.
Arra jutottunk tehát,
hogy egy komplex számtest feletti vektortér esetén az alábbi feltételek ekvivalensek.
\begin{itemize}\tightlist
	\item[\textendash] $A$ diagonalizálható;
	\item[\textendash] $A$ minimálpolinomjának gyökei egyszeresek;
	\item[\textendash] $A$ minden sajátértékének a geometriai és algebrai multiplicitása azonos.
\end{itemize}
\chapter{Determináns}
\scwords Súlyos önbizalom hiányhoz vezet tapasztalataim szerint a determináns fogalmának mellőzése
a lineáris algebra körében.
E jelenség okait inkább a megszokások, a régi beidegződések közt kell keresnünk,
mintsem a logikai szükségszerűségek közt.
Tény ugyanakkor,
hogy egyszerű egy mátrix szingularitására következtetni,
ha egy konkrét függvény argumentumába helyettesítve a mátrixunkat zérust kapunk.
Hasonlóan egyszerű,
ha egy explicit formulánk van, egy képlet amibe csak be kell helyettesíteni,
egy reguláris mátrixú inhomogén lineáris egyenletrendszer megoldására,
vagy egy mátrix inverzének felírására.
A determináns fogalmát valóban nagyon sokszor csak arra használják,
hogy a lineáris algebra alap algoritmusát, a Gauss\,--\,Jordan-eliminációt, egy explicit formulára cseréljék.
Azt szeretném itt hangsúlyozni,
hogy pusztán emiatt nem érdemes előnyben részesíteni a determináns használatát.
Az algoritmus éppen olyan jól,
sőt látni fogjuk: sokkal de sokkal hatékonyabban működik,
a mint a képlet.

Amiért mégis fontos ez a fejezet az kisebb részt az,
hogy a lineáris algebra tanulásának az is a célja,
hogy a későbbi standard felhasználásokat megértsük.
Sok-sok közgazdasági és műszaki szakkönyv előszeretettel sűríti a mátrixokkal kapcsolatos tudnivalókat a determináns fogalmának segítségével.
Az igazi ok viszont nem ez.
Egyetlen helyen van megkerülhetetlen szerepe a transzformáció determinánsának.
Ez az, amikor az válik valamiért fontossá,
hogy egy lineáris transzformáció, a tér egység kockáját,
amelynek a ,,térfogata'' nyilván 1, mekkora térfogatú paralelepipedonba transzformálja?
\footnote{A válasz érzelmi része az lesz, hogy a transzformáció determinánsa ennek a ,,mértéke''.}
Ez a kérdés a valós analízis fontos és érdemi pontjain merül fel.
Például, amikor egy többváltozós függvény integrálját számoljuk valamilyen helyettesítéssel. Ezen
a ponton válik majd a determináns fogalma igazából élővé.
A fejezet tehát az első lépés ebben az irányban.
\footnote{Persze fontos az olvasó szociális érzelmi állapota,
ami pozitív mellékterméke e fejezet pontos megértésének.
{\DejaSans😍😱 }
}


\section{Permutációk}

Emlékezzünk arra,
hogy egy $n$ elemű halmaz permutációi $n!$ elemszámú csoportot alkotnak a kompozíció műveletével.
Szokásos elnevezés, hogy ezt a kompozíció műveletet szorzásnak nevezzük.
Ezt a csoportot a továbbiakban $S_{n}$-el jelöljük.\index{permutáció csoport}
A determináns definíciójában majd olyan függvény szerepel,
amely egy permutációhoz egy számot rendel,
tehát egy $\varphi:S_n\to\mathbb{F}$ függvényről van szó.
Mivel a $\pi\mapsto \pi^{-1}$ hozzárendelés egy $S_n\to S_n$ bijekció,
ezért tetszőleges ilyen függvény mellett
\[
	\sum_{\pi\in S_n}\varphi\left( \pi \right)=
	\sum_{\pi\in S_n}\varphi\left( \pi^{-1} \right),
\]
hiszen a két $n!$ összeadandóból álló összeg csak az elemek sorrendjében különbözik.
Így mindkét oldal a $\varphi$ értékkészlete elemeinek összegét jelöli.

A permutáció csoport elemeit,
tehát az egyes permutációkat,
érdemes a sakktábla bástyafelrakásaival azonosítani.
Képzeljünk el egy $n\times n$ méretű sakktáblát.
A feladat, hogy helyezzünk el a táblára $n$ db bástyát olyan módon,
hogy egy bástya se legyen ütésben.
Nyilván ez úgy és csak úgy lehetséges,
ha minden sorba és minden oszlopba egy bábút raktunk le.
A bástyák egy ilyen elhelyezését nevezzük \emph{bástyafelrakásnak.}\index{baa@bástyafelrakás}

Vegyük észre, hogy $n$ darab bábú minden olyan elrendezése,
amelynél egy sorban egy bábú van, egy
\begin{math}
	\pi:\left\{ 1,\ldots,n \right\} \to \left\{ 1,\ldots,n \right\}
\end{math}
függvénnyel azonosítható.
Gondoljunk a sorok számozására mint az értelmezési tartomány elemeire.
Mivel így minden sorban van bábú,
és minden sorban csak egy bábú van,
ezért értelmes az a definíció,
amely szerint  $\pi\left( j \right)$ értéke legyen a $j$-edik sorban lévő bábú oszlopszáma.

Ha a bástyafelrakásra gondolunk,
akkor további feltétel,
hogy egyetlen oszlopban se legyen két bástya,
ami éppen a $\pi$ függvény injektivitását jelenti.
No de, egy véges halmaznak önmagára képező injekciója egyben szürjekció is,
emiatt a bástyafelrakások a bijekciókkal azonosíthatók.

A determináns fogalmának megértését nagyban segíti,
ha $n$ elem permutációjára mint a $n$ darab bástyafelrakására gondolunk.
\footnote{Persze ez ugyanaz mintha olyan $n\times n$ mátrixokról beszélnénk,
	ahol csak 0 és 1-es szerepel, mégpedig úgy, hogy minden sorban és minden oszlopban pontosan egy db 1-es van. Az 1-esek játsszák a bástyák szerepét. Az ilyen mátrixokat szoktuk \emph{permutáció mátrixnak} mondani.\index{permutáció mátrix}}
Az $\id$ identikus permutációt például a diagonális bástyafelrakás jelképezi.
Ha adott egy $\pi$ permutációhoz tartozó bástyafelrakás,
akkor a $\pi^{-1}$ inverzhez tartozó bástyafelrakást a diagonálisra való tükrözéssel kapjuk.
\begin{definition}[transzpozíció]
	Egy permutációt \emph{transzpozíciónak} nevezünk,
	ha az csak két elemet cserél fel.
\end{definition}
Egy bástyafelrakásra gondolva,
az akkor transzpozíció,
ha az identitásnak megfelelő diagonális felrakás két sorának felcserélésével keletkezik.
Emiatt transzpozíciók szorzata egy olyan permutáció,
amely a diagonális felrakásból néhány sor felcserélésével keletkezik.

Na most,
gondoljunk bele,
hogy minden bástyafelrakás véges sok sor felcserélésével az identitás permutációt reprezentáló diagonális felrakásba vihető át.
Az algoritmus a következő.
Keressük meg azt a sort,
ahol a bástya az első oszlopban áll.
Cseréljük fel ezt a sort az első sorral.
Most keressük meg azt a sort,
ahol a bástya a második oszlopot foglalja.
Cseréljük fel ezt a sort a második sorral.
Stb.
Az $n-1$ -edik diagonális elem helyre kerülése után, az utolsó sorban,
a bástya az utolsó pozíciót foglalja el,
hiszen a sorok cserélgetésével megmarad a táblázatnak az a tulajdonsága,
hogy minden sorban és minden oszlopban pontosan egy bástya van.

Adott $\pi$ permutációhoz találtunk tehát $\sigma_1,\ldots,\sigma_{n-1}$ transzpozíciókat,
amelyekre $\pi\cdot\sigma_1\cdots\sigma_{n-1}=\id$, amiből persze
$\pi=\sigma_{n-1}^{-1}\cdots\sigma_1^{-1}$ következik.
Egy transzpozíció inverze is transzpozíció, tehát igazoltuk a könyvtárak klasszikus felíratáról
szóló állítást.
\begin{proposition}
	Minden permutáció transzpozíciók szorzata.
\end{proposition}
Persze szó nincs arról, hogy a fenti előállítás egyértelmű lenne.
Még nagyon sok más algoritmust is kitalálhatunk a sorba rendezésre,
olyanokat is,
amelyek minden lépésükben csak két elemet cserélnek fel.%
\footnote{Keressünk például a 'bubble sort' kifejezésre.}
Annyi viszont igaz,
hogy amennyiben egy permutáció páros (páratlan) sok transzpozíció szorzata,
akkor minden más előállításában is páros (páratlan) sok transzpozíció szerepel.
Kisvártatva látjuk majd, hogy ez miért van így.

\subsection{A permutáció paritása}
Egy kényelmes jelölést vezetünk be egy permutációnak és egy előre adott transzpozíciónak a szorzatára.
\begin{definition}
	Legyenek $1\leq i,j\leq n$ különböző egészek,
	és $\pi \in S_{n}$ egy permutáció.
	Definiálja $\pi ^{\ast }\in S_{n}$ a $\pi$ permutációnak és az $\left( i,j \right)$ párt felcserélő transzpozíciónak a szorzatát,
	azaz
	\[
		\pi ^{\ast }\left( k\right) \doteq \left\{
		\begin{array}{ll}
			\pi \left( k\right) &
			\text{, ha }k\neq i\text{ és }k\neq j; \\
			\pi \left( j\right) & \text{, ha }k=i; \\
			\pi \left( i\right) & \text{, ha }k=j.
		\end{array}
		\right.
	\]
	Látható,
	hogy a $\pi$-t reprezentáló bástyafelrakásból az $i$ és $j$ sorok felcserélésével kapjuk a $\pi^{\ast}$ bástyafelrakását.
\end{definition}
Az is világos, hogy rögzített $\left( i,j \right)$ transzpozíció mellett a $\pi\mapsto\pi^\ast$ egy $S_n\to S_n$ bijekció,
emiatt újra elmondható, a fejezet eleji megjegyzés,
tehát ha $\varphi:S_n\to\mathbb{F}$ egy tetszőleges függvény, akkor
\[
	\sum_{\pi\in S_n}\varphi\left( \pi \right)
	=
	\sum_{\pi\in S_n}\varphi\left( \pi^\ast \right).
\]

\begin{definition}[Inverzió]\index{inverzió}
	Legyenek újra $1\leq i,j\leq n$ különböző egészek,
	és $\pi \in S_{n}$ egy permutáció.
	Azt mondjuk,
	hogy az $i$-edik és a $j$-edik sorban álló \emph{bástyák inverzióban vannak},
	ha
	\[
		\left( \pi \left( j\right) -\pi \left( i\right) \right) \left( j-i\right) <0.\qedhere
	\]
\end{definition}
A fenti jelölések mellett az $i$ és $j$-edik sor bástyái pontosan akkor vannak inverzióban,
ha $j>i$ esetén
$\pi \left( j\right)<\pi \left( i\right)$.
Gondoljuk meg, hogy az identitás permutáció az egyetlen olyan a permutációk közt,
melynél nincs inverzióban álló bástyapár.
\begin{proposition}
	Minden $\pi\in S_n$ permutációra,
	a $\pi$-nél és $\pi^{-1}$-nél inverzióban álló bástyapárok száma azonos.
\end{proposition}
\begin{proof}
	Világos, hogy
	\[
		\left( \pi\left( j \right)-\pi\left( i \right) \right)\left( j-i \right)
		=
		\left( j-i \right)\left( \pi\left( j \right)-\pi\left( i \right) \right)
		=
		\left( \pi^{-1}(\pi(j))-\pi^{-1}(\pi(i)) \right)\left( \pi\left( j \right)-\pi\left( i \right) \right).
	\]
	Ez azt jelenti, hogy az $i$ és $j$ sorokat foglaló bástyapár pontosan akkor van inverzióban $\pi$-szerint,
	ha a $\pi\left( i \right)$ és a $\pi\left( j \right)$ sorokat foglaló bástyapár inverzióban van $\pi^{-1}$ szerint.
	%    Tehát ha a diagonálisra való tükrözéssel áttérünk $\pi$ bástyafelrakásából a $\pi^{-1}$ bástyafelrakásába,     akkor ugyanazok a bástyapárok lesznek inverzióban a tükrözés előtt mint után.
	Mivel az $\left( i,j \right)\mapsto \left( \pi(i),\pi(j) \right)$ leképezés az $\left\{ \left( i,j \right):i\neq j:1\leq i,j\leq n \right\}$
	halmaznak önmagára képező bijekciója,
	ezért kölcsönösen egyértelmű leképezést találtunk a $\pi$ és a  $\pi^\ast$ inverzióban álló bástyapárjai közt.
\end{proof}

\begin{definition}[Páros, páratlan permutáció]
	\index{paa@páros permutáció}\index{paa@páratlan permutáció}
	Egy $\pi \in S_{n}$ permutációt \emph{párosnak (páratlannak)} nevezünk,
	ha $\pi $-nél inverzióban lévő párok száma páros (páratlan).
	A permutáció \emph{előjele:}\index{permutáció előjele}
	\[\operatorname{sgn}\pi \doteq
		\left\{
		\begin{tabular}{ll}$+1$ & , ha $\pi $ páros; \\ $-1$ & , ha $\pi $ páratlan.
		\end{tabular}
		\right. \qedhere
	\]
\end{definition}
Egy permutáció előjelét meghatározó algoritmus a definíció szerint a következő:
\emph{
	Tekintsük a $\pi$ permutáció bástyafelrakását.
	Hasonlítsuk össze az összes különböző -- tehát $\binom{n}{2}$ darab --
	bástyapárnak az oszlopszámait a sorszámaival.
	Ha a nagyobb sorindexű bástya oszlopszáma a kisebb, akkor ők inverzióban vannak.
	Ha $k$ jelöli az inverzióban álló bástyapárok számát,
	akkor $\sgn(\pi)=\left( -1 \right)^{k}$.
}

Az identitás permutációban zérus az inverziók száma,
tehát $\sgn(\id)=+1$.
Mivel egy permutációnak és az inverzének azonos számú inverziója van,
ezért az előjelük is azonosak:
\[
	\sgn\left( \pi \right)=\sgn\left( \pi^{-1} \right).
\]

A következő állítás szerint,
egy bástyafelrakás előjele két sorának felcserélésével az ellenkezőjére vált.
\begin{proposition}
	Jelölje $\pi^\ast$ a $\pi$ permutációnak és egy adott transzpozíciónak a szorzatát.
	Ekkor
	\[\sgn \pi=-\sgn\pi^\ast.\qedhere
	\]
\end{proposition}
\begin{proof}
	Tegyük fel,
	hogy $\pi^\ast$ a $\pi$-ből az $i$ és $j$-edik sorok felcserélésével keletkezik,
	ahol  $i<j$.
	Leszámoljuk,
	hogy hány az $u,v$ sorokat foglaló bástyapár van inverzióban $\pi$ és $\pi^\ast$ mellett.
	Az esetek szétválasztása folyamán végig páros számban tér el egymástól  a $\pi ^{\ast }$ és $\pi $ -melletti
	inverziók száma,
	míg az utolsó esetben pontosan egy a különbség.
	Ily módon a $\pi ^{\ast }$ és $\pi$ -melletti inverziók összességében páratlan számban különböznek,
	ami igazolja az állítást.
	Az eset szétválasztás aszerint történik, hogy az egyes esetekben számolandó $u$ és $v$ sorbeli bástyák
	milyen kapcsolatban vannak,
	a felcserélendő $i$ és $j$ sorokkal.
	Lássuk tehát a különböző eseteket:

	\begin{description}
		\item[$1.$]  Az $u$ és $v$ egyike sem egyezik $i$ és $j$ egyikével sem.\\
		      Világos, hogy ebben az esetben a $\pi $ melletti és a $\pi ^{\ast }$ melletti
		      inverziók száma azonos, hiszen egyik itt vizsgált bástyapár sem mozdul a $\pi$-ről $\pi^\ast$-ra
		      való áttérés során.

		\item[$2.$]  Az $u$ megegyezik az $i$ és $j$ közül az egyikkel, de $v>j.$\\
		      Tekintsünk egy $u,v$ sorbeli bástyapárt.
		      Az $i$-edik és a $j$-edik sor felcseréléséve az $u$-adik sorban álló bástya megmozdul,
		      de az oszlop koordinátáját megtartja, és sor koordinátája is $v$-nél kisebb marad.
		      Ez azt jelenti,
		      hogy ugyanazok bástyapárok lesznek inverzióban a $\pi$-nél, mint $\pi^\ast$-nál, ergo a
		      $\pi$-nél és $\pi^\ast$-nál inverzióban álló párok száma azonos.

		\item[$3.$]  Az $u$ megegyezik az $i$ és $j$ közül az egyikkel, de $i<v<j.$\\
		      Különböztessünk meg ezen belül is három esetet.
		      \begin{description}
			      \item[$i.$]  Ha $\pi \left( v\right) >
				            \max \left( \pi \left( j\right) ,\pi\left( i\right) \right)$.\\
			            Ekkor a $v$-edik sor bástyája az $i$-sorbeli bástyával nincs inverzióban,
			            de a $j$-sorbeli bástyával inverzióban van.
			            A $\pi$-nél tehát az inverziók száma 2.
			            Ugyanez a helyzet $\pi^\ast$-nál is,
			            ergo a $\pi $ melletti és a $\pi ^{\ast }$ melletti inverziók száma azonos.

			      \item[$ii.$]  Ha $\pi \left( v\right) <
				            \min \left( \pi \left( j\right) ,\pi \left( i\right) \right) .$\\
			            Analóg [i]-vel.

			      \item[$iii.$]  Ha $\min \left( \pi \left( j\right) ,\pi \left( i\right)\right)
				            <
				            \pi \left( v\right)
				            <
				            \max \left( \pi \left( j\right) ,\pi \left(i\right) \right) .$\\
			            Ha $\pi\left( j \right)<\pi\left( i \right)$, akkor az inverziók száma $\pi$-nél 2,
			            $\pi^\ast$-nál zérus.
			            Ha viszont $\pi\left( j \right)>\pi\left( i \right)$, akkor az inverziók száma $\pi$-nél 0,
			            de $\pi^\ast$-nál 2.
			            Mindkét esetben tehát 2 a különbség,
			            ezért bárhogy is van,
			            de a $\pi $ melletti és a $\pi ^{\ast }$ melletti inverziók száma páros számban különbözik.
		      \end{description}

		\item[$4.$]  Az $u$ megegyezik az $i$ és $j$ közül az egyikkel, de $v<i.$\\
		      Analóg a második esettel.

		\item[$5.$]  Az $u$ is és a $v$ is megegyezik az $i$ és $j$ egyikével.\\
		      Ekkor ha az $u$ és $v$ sorbeli bástyák inverzióban vannak $\pi$-nél,
		      akkor nem lesznek inverzióban $\pi^\ast$-nál,
		      és hasonlóan, ha nincsenek inverzióban $\pi$-nél,
		      akkor inverzióban lesznek $\pi^\ast$ mellett.
		      Bárhogyan is van, de a $\pi$-nél és $\pi^\ast$-nál inverzióban lévő elemek száma 1-el változik.
	\end{description}
	Világos, hogy minden $u,v$ pár a fenti esetek közül pontosan az egyikbe tartozik.
	Összességében a $\pi$-ről $\pi^\ast$-ra való áttéréssel a transzpozíciók száma páratlan számmal változik,
	tehát párosból páratlanná válik, vagy páratlanból párossá.
\end{proof}

Nyilvánvaló tehát, hogy az $S_n$ csoportnak $n!/2$ számú páros és ugyanennyi páratlan eleme van.

Mivel egy transzpozícióval való szorzás a permutáció előjelét ellentettjére állítja,
ezért ha egy permutáció $k$ darab transzpozíció szorzataként áll elő az identikus permutációból,
akkor annak előjele $\left( -1 \right)^k$.
Ebből azonnal következik, hogy egy páros (páratlan) permutáció nem áll elő mint páratlan (páros) sok transzpozíció szorzata.

Meggondoltuk tehát, hogy egy permutációnak
a transzpozíciók szorzatakénti előállítása ugyan nem egyértelmű,
de az előállításban szereplő transzpozíciók számának paritása mindig azonos.
Ez egyben egy alternatív algoritmust eredményez egy permutáció előjelének meghatározására:
\emph{
	Tudjuk, hogy minden bástyafelrakás átrendezhető a diagonális felrakásba pusztán a sorok felcserélésével.
	Ha a $\pi$ permutáció esetén $k$ darab sorcserére van ehhez szükség,
	akkor $\sgn \pi=\left( -1 \right)^k$.
}

\section{Mértékek}
A determinánsról szóló minden további gondolatban feltesszük,
hogy az $\mathbb{F}$ testben $1+1=0$ nem teljesül.
Ezt úgy fejezzük ki, hogy az $\mathbb{F}$ test nem \emph{2 karakaterisztikájú}.\index{2 karakterisztikájú test}
\begin{definition}[mérték]
	\index{n-lineáris operátor}\index{anti-szimmetrikus operátor}\index{alternáló operátor}
	Legyenek $V_{1}$, $V_{2}$ és $V$ ugyanazon test feletti vektorterek.
	Az $A:V_{1}\times V_{2}\rightarrow V$ leképezést \emph{bilineáris operátornak}
	nevezzük, ha:
	\begin{enumerate}
		\item
		      Minden rögzített $x\in V_{1}$ esetén az $A\left( x,\cdot \right):V_{2}\rightarrow V$ lineáris operátor.
		\item
		      Minden rögzített $x\in V_{2}$ esetén az $A\left( \cdot ,x\right):V_{1}\rightarrow V$ lineáris operátor.
	\end{enumerate}

	Ha valamely $A:V\times V\rightarrow V$ bilineáris operátorra minden $u,v\in V$ esetén
	\(
	A\left( u,v\right) =-A\left( v,u\right),
	\)
	akkor $A$-t \emph{anti-szimmetrikus} bilineáris operátornak nevezzük.

	Ha valamely $A:V\times V\rightarrow V$ bilineáris operátorra minden $u\in V$ esetén
	\(
	A\left( u,u\right) = 0,
	\)
	akkor $A$-t \emph{alternáló} bilineáris operátornak nevezzük.

	Legyenek most $V_{1},V_{2},\ldots ,V_{k}$ és $V$ ugyanazon test feletti vektorterek.
	Az $A:V_{1}\times \ldots \times V_{k}\rightarrow V$ leképezést \emph{multi-lineáris operátornak}
	vagy $n$\emph{-lineáris operátornak }nevezzük,
	ha bármely rögzített
	\[
		x_{1},\ldots x_{i-1},x_{i+1},\ldots ,x_{j-1},x_{j+1},\ldots ,x_{k}
	\]
	vektor mellett az
	\[
		A\left( x_{1},\ldots x_{i-1},\,\cdot\, ,x_{i+1},\ldots ,x_{j-1},\,\cdot\, ,x_{j+1},\ldots ,x_{k}\right)
		:
		V_{i}\times V_{j}\rightarrow V
	\]
	függvény bilineáris operátor.
	Ha a fenti függvény anti-szimmetrikus (alternáló) operátor minden $i$-re $j$-re és tetszőlegesen rögzített vektorokra,
	akkor az
	$A:V_{1}\times \ldots \times V_{k}\rightarrow V$
	leképezést \emph{anti-szimmetrikus (alternáló) multi-lineáris operátornak} nevezzük.

	Ha $V$ egy nem 2 karakterisztikájú test feletti $n$-dimenziós vektortér,
	akkor egy $V\times \dots \times V\to V$ $n$-lineáris anti-szimmetrikus függvényt
	a $V$ vektortér feletti \emph{mértéknek}\index{mérték} nevezzünk.
\end{definition}
\begin{proposition}
	Legyen $V$ olyan $\mathbb{F}$ test feletti vektortér, ahol a $1+1\neq 0$.
	Tegyük fel, hogy
	$A:V\times V\rightarrow V$ bilineáris függvény.
	Ekkor az alábbi három feltevés egymással ekvivalens.
	\begin{enumerate}
		\item
		      Az $A$ anti-szimmetrikus;
		\item
		      Az $A$ alternáló;
		\item
		      Minden $u,v\in V$-re valamint tetszőleges $\lambda,\mu\in\mathbb{F}$ skalárokra
		      \[
			      A\left( u,v+\lambda u\right)
			      =
			      A\left( u,v\right)
			      =
			      A\left( u+\mu v,v\right).\qedhere
		      \]
	\end{enumerate}
\end{proposition}

\begin{proof}
	$1.\Rightarrow 2.$ és $2.\Rightarrow 3.$ nyilvánvaló.
	$3.\Rightarrow 1.$: $A\left( u,v\right) =A\left( u,v-u\right)
		=A\left( u+v-u,v-u\right) =A\left( v,v-u\right) =A\left( v,-u\right)
		=-A\left( v,u\right) .$
\end{proof}

Világos, hogy tetszőleges $k$ pozitív egész mellett, $k$-lineáris függvények vektorteret alkotnak.
E vektor tér egy altere a $k$-lineáris anti-szimmetrikus függvények vektortere.
Ez persze fennáll akkor is, mikor $n$-dimenziós vektortérnek $n$-szeres szorzatán értelmezett $n$-lineáris függvényekről van szó,
ergo a mértékek egy vektorteret alkotnak.
A konstans zérus mérték így nyilvánvaló példa mértékre.
A kérdés, hogy van-e más mérték?

\subsection{Példa mértékre}
Egy $n$ dimenziós tér $n$-szeres Descartes-szorzatán értelmezett konstans zérus leképezés egy
triviális példa mértékre.
Az alábbiakban egy olyan mértéket készítünk, ami egészen biztosan nem a konstans zérus függvény.
\begin{definition}
	Legyen $V$ egy vektortér, amelyben rögzített az $\left\{ e_1,\ldots,e_n \right\}$
	bázis.
	Mivel lineáris funkcionál egy bázison tetszőlegesen és egyértelműen előírható,
	ezért tetszőleges $1\leq i\leq n$-hez létezik egyetlen $e_i^\ast:V\to\mathbb{F}$
	lineáris funkcionál, amelyre
	\[
		e_i^\ast(e_j)=\delta_{i,j}
	\]
	fennáll.
	Ezt az $e_i^\ast$ lineáris funkcionált nevezzük az \emph{$i$-edik duális báziselemnek.}\index{duális bázis}%
	\footnote
	{
		Az elnevezés oka,
		hogy $\left\{ e_1^\ast,\ldots,e_n^\ast \right\}$ bázisa az összes $V\to\mathbb{F}$ lineáris funkcionálok vektorterének.
	}
\end{definition}
\begin{proposition}
	Rögzítsük az $\left\{ e_1,\ldots,e_n \right\}\subseteq V$ bázist.
	Ekkor minden $v\in V$ mellett
	\(
	v=\sum_{j=1}^ne_j^\ast(v)e_j,
	\)
	azaz $v$-nek az $\left\{ e_1,\ldots,e_n \right\}$ bázisban felírt $j$-edik koordinátája $e_j^\ast v.$
\end{proposition}
\begin{proof}
	Legyen $v=\sum_{i=1}^n\alpha_ie_i$.
	Ekkor tetszőlegesen rögzített $j$ index mellett
	\[
	e_j^\ast(v)
	=
	e_j^\ast\left( \sum_{i=1}^n\alpha_ie_i \right)
	=
	\sum_{i=1}^n\alpha_ie_j^\ast\left( e_i \right)
	=
	\sum_{i=1}^n\alpha_i\delta_{j,i}
	=
	\alpha_j.
    \qedhere
    \]
\end{proof}
Úgy interpretálhatjuk tehát az $e_j^\ast$ lineáris funkcionált,
hogy az minden $v$ vektorhoz hozzárendeli a vektor $j$-edik koordinátáját.
\begin{proposition}
	Rögzítsünk az $n$-dimenziós $V$ vektortérben egy $\left\{ e_{1},\ldots e_{n}\right\} $ bázist.
	Ekkor minden $v_1,\ldots,v_n\in V$ vektor esetén
	\[
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^{n}e_{i}^{\ast }v_{\pi \left( i\right)}
		=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^{n}e_{\pi \left( i\right)}^{\ast }v_{i}.
	    \qedhere
	\]
\end{proposition}
Képzeljük el, hogy egy $n\times n$ méretű sakktábla $j$-edik oszlopába írjuk a $v_j$ vektor koordinátáit,
minden $j=1,\ldots,n$ mellett.
Tegyünk fel erre a sakktáblára egy $\pi$ permutációt, azaz a $\pi$ bástyafelrakását.
Minden $i$-re az $i$-edik sor $\pi\left( i \right)$-edik elemén áll tehát egy bástya.
No de, a $\pi\left( i \right)$ edik oszlop $i$-edik helyére $v_{\pi\left( i \right)}$ vektor $i$-edik koordinátája,
azaz $e_i^\ast v_{\pi(i)}$ van írva.
Így az $\prod_{i=1}^n e_i^\ast v_{\pi\left( i \right)}$ szám egyszerűen a bástyafelrakásban a bástyák taposta számok szorzata.
A baloldali számhoz ezt kell megszorozni még a szóban forgó permutáció előjelével,
majd ugyanezt az összes permutációra megismételni és a kapott számok összegét képezni.

A jobboldali szám interpretációját kapjuk, ha most a $v_1,\ldots,v_n$ vektorok koordinátáit rendre a sorokba írjuk.
Újra gondoljunk egy $\pi$ permutáció bástyafelrakására.
Minden $i$-re az $i$-edik sor $\pi\left( i \right)$-edik elemén áll tehát egy bástya,
ami most az $i$-edik vektornak tehát $v_i$-nek, a $\pi\left( i \right)$-edik koordinátáját azaz $e_{\pi(i)}^\ast v_i$-t tapossa.
Így az $\prod_{i=1}^n e_{\pi(i)}^\ast v_i$ szám egyszerűen a bástyafelrakásban a bástyák taposta számok szorzata.
A jobboldali számhoz ezt kell megszorozni még a szóban forgó permutáció előjelével,
majd ugyanezt az összes permutációra megismételni és a kapott számok összegét képezni.

Az állítás szerint tehát, mindegy hogy a sorokra vagy az oszlopokra másoljuk a $v_1,\ldots,v_n$ vektorok koordinátáit,
a fenti eljárás mindkét esetben ugyanazt a számot eredményezi.
\footnote{
	E hosszú, de ultra fontos interpretáció után nézzük az egysoros bizonyítást.
}
\begin{proof}
	A számolásban kihasználjuk, hogy minden $\pi\in S_n$ mellett $\sgn\pi=\sgn\pi^{-1}$,
	és azt, hogy a $\pi \mapsto \pi ^{-1}$ hozzárendelés egy bijekció az $S_{n}$ halmazon.
	\[
		\sum_{\pi \in S_{n}}\operatorname{sgn}\pi \prod_{i=1}^{n}e_{i}^{\ast }v_{\pi
		\left( i\right) }=\sum_{\pi \in S_{n}}\operatorname{sgn}\pi
		^{-1}\prod_{k=1}^{n}e_{\pi ^{-1}\left( k\right) }^{\ast }v_{k}=\sum_{\pi \in
			S_{n}}\operatorname{sgn}\pi \prod_{k=1}^{n}e_{\pi \left( k\right) }^{\ast }v_{k}.\qedhere
	\]
\end{proof}

\begin{proposition}[,,Példa'' mértékre]
	Rögzítsünk az $n$-dimenziós $V$ vektortérben egy $\left\{ e_{1},\ldots e_{n}\right\} $ bázist.
	Ekkor
	\[
		d_{\left\{ e_1,\ldots,e_n \right\}}\left( v_1,\ldots,v_n \right)=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^{n}e_{i}^{\ast }v_{\pi \left( i\right)}
		=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^{n}e_{\pi \left( i\right)}^{\ast }v_{i}
	\]
	Ekkor $d_{\left\{ e_1,..,e_n \right\}}:V\times \ldots \times V\rightarrow \mathbb{F}$ egy olyan mérték,
	amelyre $d_{\left\{ e_1,\ldots,e_n \right\}}\left( e_{1},\ldots ,e_{n}\right) =1.$
	\footnote{
		Ha a szövegkörnyezetből világos,
		hogy melyik a rögzített bázis,
		akkor a kicsit nehézkes $d_{\left\{ e_1,\ldots,e_n \right\}}$ jelölés helyett csak $d$-t használunk.
	}
\end{proposition}

\begin{proof}
	Világos,
	hogy $d\left( e_{1},\ldots ,e_{n}\right) =1,$
	hiszen ha $\pi $ nem az identikus permutáció,
	akkor van olyan $i$ melyre $\pi \left( i\right) \neq i,$
	az $e_{i}^{\ast }\left( e_{\pi \left( i\right) }\right) =0$.
	Így a $d\left( e_1,\ldots,e_n \right)$ definíciójában az $n!$ darab permutáció minden elemére $0$-t
	kapunk, kivétel az identikus permutáció,
	amelynek előjele $+1$ és
	$
		\prod_{i=1}^ne_i^\ast e_{\id(i)}
		=
		\prod_{i=1}^n\delta_{i,i}=1.
	$

	A linearitás könnyen következik az $e_{i}^\ast$ duális bázis elemek linearitásából.
	Mondjuk ha a hátsó $n-1$ változót rögzítjük,
	akkor az első változót szabadon hagyva
	\[
		d\left(\,\cdot\,,v_{2},\ldots ,v_{n}\right)
		=
		\sum_{\pi\in S_{n}}\sgn\pi
		\cdot
		\left(\prod_{k=2}^ne_{\pi\left( k\right) }^\ast v_k\right)
		\cdot
		e_{\pi\left( 1\right)}^\ast.
	\]
	Mivel a lineáris funkcionálok vektorteret alkotnak,
	ezért a $d\left( \,\cdot\,,v_2,\ldots,v_n \right)$ függvény is egy lineáris funkcionál.
	Analóg módon a többi változót szabadon hagyva kapjuk, hogy $d$ egy $n$-lineáris függvény.

	Az anti-szimmetria igazolásához szükséges, a $\sgn\pi =-\sgn\pi^\ast$ összefüggés.
	Például az első két változóra:
	\begin{multline*}
		d\left( x,y,v_{3},\ldots ,v_{n}\right)
		=
		\\
		\sum_{\pi \in S_{n}}\sgn\pi \left( \prod_{k=3}^{n}e_{\pi \left( k\right) }^{\ast }v_{k}\right)
		e_{\pi \left( 1\right) }^\ast x
		\cdot
		e_{\pi \left( 2\right)}^\ast y
		=
		\sum_{\pi \in S_{n}}(-\sgn\pi^\ast)
		\left(\prod_{k=3}^{n}e_{\pi ^{\ast }\left( k\right) }^{\ast }v_{k}\right)
		e_{\pi^\ast\left( 1\right) }^{\ast } y
		\cdot
		e_{\pi^\ast\left(2\right) }^\ast x
		=
		\\
		-\sum_{\pi \in S_{n}}\operatorname{sgn}\pi \left( \prod_{k=3}^{n}e_{\pi \left( k\right) }^{\ast }v_{k}\right)
		e_{\pi \left( 1\right) }^{\ast }y
		\cdot
		e_{\pi \left( 2\right) }^{\ast } x
		=
		-d\left(y,x,v_{3},\ldots ,v_{n}\right) .
	\end{multline*}
	Itt $\pi^\ast$ a $\pi$ permutációnak és az $(1,2)$ elemeket felcserélő transzpozíciónak a szorzata.
	Az utolsó egyenlőségben azt használtuk ki,
	hogy ha $\pi$ befutja az $S_n$ csoport összes elemét,
	akkor az evvel párhuzamosan számolt $\pi^\ast$ permutációk is végig mennek a permutáció csoport minden elemén.

	Analóg módon kapjuk, hogy bármely két változót felcserélve $d$ értéke az ellentettjére változik.
\end{proof}

Meggondoltuk tehát,
hogy egy $n$ dimenziós vektortér feletti mértékek vektortere legalább egy dimenziós,
hiszen van e vektortérnek a zérustól különböző eleme.
Azt mutattuk meg, hogy
adott $\left\{ e_1,\ldots,e_n \right\}$ bázis rögzítése mellett,
az ehhez a bázishoz tartozó $d_{\left\{ e_1,\ldots,e_n \right\}}$ függvény egy nem triviális mérték $V$ felett.
Persze sok-sok bázist tudunk rögzíteni, így sok-sok mértéket is tudunk konstruálni.
Leginkább pedig az általunk konstruált mértékeken felül is lehet még mérték a $V$ felett.
Ehhez képest nem sokára látni fogjuk,
hogy a $V$ feletti mértékek vektortere pontosan egydimenziós vektortér.
A legfontosabb gondolat, hogy a fenti $d$-nek csak konstans szorosai lehetnek a $V$ feletti mértékek,
de megszakítjuk itt a tárgyalást, azért hogy a fent definiált mérték fontosságát hangsúlyozzuk.

\subsection{Mátrix determinánsa}
Gondoljunk erre a szakaszra mint egy érdekes kitérőre,
ha már van a kezünkben egy igazi -- értsd nem triviális -- mérték.
\begin{definition}[Mátrix determinánsa]
	\index{mátrix determinánsa}\index{determináns}
	Legyen
	$\left[ A\right]$ egy mátrix.
	E mátrix $\left| \left[ A\right] \right|$ \emph{determinánsa} a következő szám:
	\[
		\left| \left[ A\right] \right|
		=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^n\left[ A\right]_{i,\pi \left( i\right) }.\qedhere
	\]
\end{definition}
Az előbb bevezetett $d$ függvény és a mátrix determinánsának kapcsolata.
\begin{proposition}
	Legyen $V$ egy vektortér,  $\left\{ e_1,\ldots,e_n \right\}$ egy bázis,
	és $a_1,\ldots,a_n\in V$ vektorok.
	Az $A$ mátrix oszlopai legyenek rendre az
	$a_1,\ldots,a_n$ vektorok fenti bázisban felírt koordinátavektorai.
	Ekkor
	\[
		|[A]|
		=
		d_{\left\{e_1,\ldots,e_n \right\}}\left( a_1,\ldots,a_n \right)
		=
		|[A]|^T.\qedhere
	\]
\end{proposition}
\begin{proof}
	Elég azt látni, hogy
	\begin{math}
		[A]_{i,\pi(i)}
	\end{math}
	a mátrix $\pi(i)$-edik oszlopának, tehát $a_{\pi(i)}$-nek $i$-edik koordinátája, azaz
	a
	\begin{math}
		e_i^\ast\left( a_{\pi(i)} \right).
	\end{math}
	Így
	\begin{multline*}
		|[A]|
		=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^n\left[ A\right]_{i,\pi \left( i\right) }
		\\
		=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^ne_i^\ast\left( a_{\pi(i)} \right)
		=
		d\left( a_1,\ldots,a_n \right)
		=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^ne_{\pi(i)}^\ast\left( a_i \right)
		\\
		=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^n\left[ A\right]_{\pi \left( i\right),i }
		=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^n\left[ A\right]^T_{i,\pi \left( i\right)}
		=
		|[A]^T_{\left\{e_1,\ldots,e_n \right\}}|.
	\end{multline*}
	Ezt kellett belátni.
\end{proof}

\subsubsection{Mátrix determinánsának legfontosabb tulajdonságai}

\paragraph{Összefoglaljuk,} amit eddig a pontig megértettünk a mátrix determinánsának fogalmáról.
\footnote{
	Mielőtt tovább megyünk adjunk képletet $1\times 1$-es,
	$2\times 2$-es és $3\times 3$-as mátrixok determinánsának kiszámolására, a definíció alapján.
	Érdemes figyelni arra, mennyire nő a definíció komplexitása a méret emelésével.
}
Legyen tehát $[A]$ egy mátrix. Ekkor
\begin{enumerate}
	\item $|[A]|=|[A]^T|$;
	\item $|[A]|$ az oszlopok (sorok) lineáris függvénye;
	\item $|[A]|$ értéke ellentetjére változik két oszlopának (sorának) felcserélése után;
	\item $|[A]|$ értéke zérus, ha két azonos oszlopa (sora) van;
	\item $|[A]|$ értéke nem változik, ha egyik oszlopához (sorához) egy másik oszlop (sor) skalárszorosát adjuk.
\end{enumerate}

\paragraph{Egy permutáció mátrix determinánsa, a permutáció előjele.}
Ugyanis, ha $\pi$ permutációhoz tartozik a permutáció mátrix, akkor minden $\sigma\neq\pi$ permutáció mellett
a $\sigma$ permutáció egyik bástyája (sőt legalább két bástyája) zérus elemen áll.
Így a determináns definíciója szerint az $n!$ elemszámú összegben a $\pi$ kivételével minden összeadandó zérus.
Speciálisan, az identitás mátrix determinánsa 1.

\paragraph{Egy felső- (alsó-)háromszög alakú mátrix determinánsa a diagonális elemek szorzata.}
Ugyanis minden $\pi\neq\id$ permutáció esetén van egy bástya a diagonális alatt (és egy a diagonális felett).
Ez azt jelenti, hogy a definícióban csak az identitás permutációhoz tartozó tag nem feltétlen zérus.
Az identitáshoz tartozó tag pedig a diagonális elemek szorzata.
Speciálisan, az identitás mátrix determinánsa 1.

\paragraph{Egy mátrix determinánsa Gauss\,--\,Jordan-eliminációval.}
Képzeljük el, hogy adott egy reguláris mátrix, tehát olyan, amelynek oszlopai lineárisan függetlenek.
Az eliminációs algoritmusnak arra az alakjára gondoljunk először, amikor a bázisba bevont oszlopvektornak
is kiírjuk az új bázisban a koordinátáit.
Ilyen módon végig $n\times n$ méretű táblázatokkal dolgozunk.
Tegyük fel, hogy egy táblázat $A_1$ az ebből következő táblázat $A_2$.
$A_2$-t úgy kaptuk $A_1$-ből, hogy a pivot elemmel osztottuk a pivot elem sorát,
majd minden más sorhoz hozzáadtuk a pivot elem sorának skalárszorosát.
Ha $r_1$ jelöli az $A_1$-ben választott pivot-elemet, akkor
\[
	|A_2|=\frac{1}{r_1}|A_1|.
\]
Mivel $A_1$ oszlopai lineárisan függetlenek, ezért minden oszlop a bázisba bevonható,
ami azt jelenti, hogy az utolsó táblázat -- az $A_{n+1}$ -- egy permutáció mátrix.
Tehát az előbbi összefüggést lépésenként alkalmazva
\[
	|A_1|=r_1\cdot |A_2|=r_1\cdot r_2\cdot |A_3|=\dots=r_1\cdot r_2\dots r_{n}\cdot|A_{n+1}|=\left(\prod_{j=1}^nr_j  \right)\cdot\sgn(A_{n+1}).
\]
Ezek szerint az algoritmus a következő:
\emph{
	$A$ determinánsának kiszámításához az összes oszlopot Gauss\,--\,Jordan-eliminációval bevisszük a bázisba.
	A determináns a pivot elemek szorzata szorozva +1, vagy -1-el.
	Az utolsó táblázatban, az új bázis elemeit az eredeti oszlopvektorok alkotják.
	Számoljuk meg, hogy hány sorcserét kell kapnunk ahhoz, hogy az eredeti oszlopvektorok természetes sorrendjébe cseréljük az elimináció
	végén kapott új bázis elemeket.
	Ha ehhez páratlan sok csere kellett, akkor $-1$-el kell még szoroznunk a pivot elemek szorzatát.
	Egyébként a determináns a pivot elemek szorzata.\index{pivot elem}
	\footnote{
		A determináns kiszámolásának ez a preferált és professzionális módja.
		Vessük össze a definíció művelet igényével mondjuk 3,4,5 dimenzió mellett.}
}

\paragraph{Az oszlopok (sorok) lineáris függetlensége} eldönthető a determináns segítségével.
Láttuk ugyanis, hogy a mátrix determinánsa nem változik, ha egyik oszlopához a másik skalárszorosát adjuk.
Ha tehát az egyik oszlop felírható a többi lineáris kombinációjaként,
akkor ehhez az oszlophoz véges sokszor egy másik oszlop skalárszorosát adva olyan mátrixot kapunk,
amelyben ez az oszlop csupa zérus, és a determinánsa azonos a kiinduló mátrix determinánsával.
Látjuk tehát, hogy ha az oszlopok lineárisan összefüggők, akkor a mátrix determinánsa zérus.
Viszont ha az oszlopok lineárisan függetlenek,
akkor Gauss\,--\,Jordan-eliminációval minden oszlop a bázisba vihető, és a determináns előjeltől eltekintve a pivot elemek szorzata.
E szorzat nem lehet zérus, hiszen egyik pivot elem sem lehet zérus.
Meggondoltuk tehát:
\emph{
	Egy mátrix determinánsa pontosan akkor zérus ha az oszlopok (sorok) lineárisan összefüggők.
}

\paragraph{Particionált mátrix determinánsa.}\label{par:particionalt}
Tegyük fel, hogy $A$ egy $(n+m)\times(n+m)$ méretű mátrix,
amely
\[
	A
	=
	\begin{pmatrix}
		A_{1,1} & A_{1,2} \\
		A_{2,1} & A_{2,2}
	\end{pmatrix}
\]
módon van particionálva, ahol $A_{1,1}$ egy $n\times n$, $A_{2,2}$ egy $m\times m$ méretű négyzetes mátrix,
továbbá $A_{2,1}$ egy $m\times n$ méretű és $A_{1,2}$ egy $n\times m$ méretű részmátrix.
Tegyük fel, hogy $A_{2,1}$ minden eleme zérus.
Ekkor
\[
	|A|
	=
	|A_{1,1}|\cdot |A_{2,2}|.
\]
\begin{proof}[Definíció alapján]
	Látható ugyanis, hogy ha vesszük az $A$ egy bástyafelrakását, akkor ha kerül bástya az $A_{1,2}$ részmátrixra,
	akkor kerül bástya az $A_{2,1}$ részmátrixra is.
	Így a determináns definíciójához elég csak olyan $\pi\in S_{n+m}$ permutációkat venni,
	amelyre $\pi(i)\leq n$, ha $i\leq n$ és $\pi(i)>n$, ha $i>n$.
	Egy ilyen $\pi$ permutációt össze lehet állítani $\pi=\left( \sigma,\tau \right)$ alakban,
	ahol $\sigma$ az $\left\{ 1,\ldots,n \right\}$ halmaz permutációja és $\tau$ az $\left\{ n+1,\ldots,n+m \right\}$ halmaz permutációja.
	Ha $\pi$ egy ilyen permutáció,
	akkor olyan inverziója nincs,
	aminek egyik bástyája $A_{1,1}$-ben van és másik bástyája $A_{2,2}$-ben van.
	Ez azt jelenti,
	hogy ha $k$ a $\sigma$ inverzióinak száma és $l$ a $\tau$ inverzióinak száma, akkor
	a $\pi$ inverzióinak száma éppen $k+l$.
	Tehát
	\[
		\sgn\pi=\left( -1^{k+l} \right)=\left( -1 \right)^k\left( -1 \right)^l=\sgn\sigma\cdot\sgn\tau.
	\]
	Jelölje a bizonyítás végéig,
	$S_n$ az $\left\{ 1,\ldots,n \right\}$ halmaz, $S_m$ az $\left\{ n+1,\ldots,n+m \right\}$ halmaz partíció csoportjait, továbbá
	$S_{n+m}'$ az $\left\{ 1,\ldots,n+m \right\}$ halmaz bijekciói közül azon $\pi$ partíciókat,
	amelyek előállnak $\pi=\left( \sigma,\tau \right)\in S_n\times S_m$ alakban.
	Ekkor
	\begin{multline*}
		|A_{1,1}|\cdot |A_{2,2}|=
		\left(
		\sum_{\sigma\in S_n}\sgn\sigma\prod_{i=1}^na_{i,\sigma(i)}
		\right)
		\left(
		\sum_{\tau\in S_m}\sgn\sigma\prod_{i=1}^ma_{n+i,\tau(n+i)}
		\right)
		\\
		=
		\sum_{(\sigma,\tau)\in S_n\times S_m}\sgn\sigma\sgn\tau\prod_{i=1}^na_{i,\sigma(i)}\prod_{i=n+1}^{n+m}a_{i,\tau(i)}
		=
		\sum_{\pi\in S_{n+m}'}\sgn\pi\prod_{i=1}^{n+m}a_{i,\pi(i)}
		\\
		=
		\sum_{\pi\in S_{n+m}}\sgn\pi\prod_{i=1}^{n+m}a_{i,\pi(i)}
		=
		|A|.
	\end{multline*}
	Ezt kellett indokolni.
\end{proof}
\begin{proof}[Gauss\,--\,Jordan-elimináció alapján]
	Először is azt vegyük észre, hogy az $A_{2,1}$ nullmátrix léte miatt,
	ha $A$ oszlopai lineárisan összefüggők,
	akkor az $A_{1,1}$ és $A_{2,2}$ mátrixok egyikének oszlopai is lineárisan összefüggők.
	Az állítás tehát teljesül, ha $|A|=0$.
	Feltesszük, hogy $A$ oszlopai lineárisan függetlenek.
	Végezzünk Gauss\,--\,Jordan-eliminációt az $A$ mátrixon de úgy,
	hogy az első $n$ lépésben $A_{1,1}$ részéből, utána $A_{2,2}$ részéből választjuk a pivot elemeket.
	Ez megtehető, hiszen $A_{1,1}$ oszlopai is lineárisan függetlenek.
	Legyenek a pivot elemek $r_1,\ldots,r_n,r_{n+1},\ldots,r_m$,
	és az utolsó oszlop bevonása után maradt $n+m\times n+m$ méretű permutáció mátrix $P$.
	Mivel csak a balfelső $n\times n$-es és a jobbalsó $m\times m$ részből választottunk pivot elemeket,
	ezért a $P$ mátrix
	\begin{math}
		P=
		\begin{pmatrix}
			P_{1,1} & 0       \\
			0       & P_{2,2}
		\end{pmatrix}
	\end{math}
	alakú, ahol $P_{1,1}$ egy $n\times n$ méretű,
	és $P_{2,2}$ egy $m\times m$ méretű permutáció mátrix.
	Ha $k$ -- illetve $l$ -- sorcsere kell ahhoz,
	hogy $P_{1,1}$-et -- illetve $P_{2,2}$-t -- a diagonálisba transzformáljuk,
	akkor $k+l$ sorcserével $P$ is a diagonális permutációba megy át.
	Mivel egy permutáció mátrix determinánsa a permutáció előjele, ezért
	\(
	|P|=(-1)^{k+l}=\left( -1 \right)^k\left( -1 \right)^l=|P_{1,1}||P_{2,2}|.
	\)
	Világos, hogy $|A_1|=r_1\cdots r_n|P_{1,1}|$.
	No de, $A_{2,1}$ a nullmátrix, ezért az elimináció első $n$ lépésében $A_{2,2}$ változatlan marad,
	így $|A_{2,2}|=r_{n+1}\cdots r_{n+m}|P_{2,2}|$.
	Összefoglalva:
	\[
		|A|
		=r_1\cdots r_n\cdot r_{n+1}\cdots r_{n+m}|P|
		=(r_1\cdots r_n|P_{1,1}|)(r_{n+1}\cdots r_{n+m}|P_{2,2}|)
		=|A_{1,1}|\cdot|A_{2,2}|.\qedhere
	\]
\end{proof}

\section{A mértékek jellemzése}
\index{mérték}

\begin{proposition}
	Legyen $V$ egy $n$-dimenziós vektortér, $f$ egy $V$ feletti mérték.
	Ha a $\left\{ v_{1},\ldots ,v_{n}\right\} \subseteq V$ vektorrendszer lineárisan összefüggő,
	akkor
	\(
	f\left( v_{1},\ldots ,v_{n}\right) =0.
	\)
\end{proposition}
\begin{proof}
	Mivel $f$ értéke nem változik ha egy változójához a másik skalárszorosát hozzá adjuk,
	ezért ha $v_{k}=\sum_{i\neq k}\alpha _{i}v_{i}$,
	akkor
	$f\left(v_{1},\ldots ,v_{n}\right)
		=
		f\left( v_{1},\ldots ,0,\ldots ,v_{n}\right)
		=
		0$.
\end{proof}

\begin{proposition}
	Legyen $V$ egy $n$-dimenziós vektortér, és $f$ egy $V$ feletti mérték.
	Ekkor ha van olyan $e_{1},\ldots ,e_{n}$ bázisa a térnek melyekre
	\(
	f\left( e_{1},\ldots ,e_{n}\right)
	=
	0,
	\)
	akkor $f$ a konstans zéró, azaz a triviális mérték.
\end{proposition}
\begin{proof}
	Mivel $f$ anti-szimmetrikus és minden permutáció előáll transzpozíciók szorzataként,
	ezért tetszőleges $\pi \in S_{n}$ mellett
	$\left| f\left(e_{\pi \left( 1\right) },e_{\pi \left( 2\right) },\ldots ,e_{\pi \left(n\right) }\right) \right|
		=
		\left| f\left( e_{1},\ldots ,e_{n}\right) \right|
		=
		0.
	$
	De az egyes rögzített változókban való linearitás és az alternáló tulajdonság miatt minden
	$v_{1},\ldots ,v_{n}$ vektorrendszerhez léteznek
	$\beta _{\pi }$ $\left( \pi \in S_{n}\right) $ együtthatók,
	melyekre
	$f\left(v_{1},\ldots ,v_{n}\right)
		=
		\sum_{\pi \in S_{n}}\beta _{\pi }f\left( e_{\pi\left( 1\right) },e_{\pi \left( 2\right) },\ldots ,e_{\pi \left( n\right)}\right)
		=
		0.$
\end{proof}
Foglaljuk össze az előző két állítást:
\begin{proposition}
	Legyen $f$ egy nem triviális mérték a $V$ vektortér felett. A
	\begin{math}
		\left\{ v_1,\ldots,v_n \right\}
	\end{math}
	vektorrendszer lineárisan összefüggőségének szükséges és elegendő feltétele,
	hogy
	\begin{math}
		f\left( v_1,\ldots,v_n \right)
		=
		0.
	\end{math}
\end{proposition}

\begin{proposition}
	Tetszőleges (legalább egy dimenziós) vektortér feletti mértékek vektortere egydimenziós vektortér.
\end{proposition}
\begin{proof}
	Rögzítsük a tér egy $\{e_{1},\ldots ,e_{n}\}$ bázisát.
	Láttuk, hogy a
	\[
		d\left(v_{1},\ldots ,v_{n}\right)
		=
		\sum_{\pi \in S_{n}}\sgn\pi\prod_{i=1}^ne_i^\ast v_{\pi\left( i\right)}
	\]
	függvény nem triviális mértéket definiál.
	Elegendő tehát megmutatnunk,
	hogy amennyiben $D\neq 0$ egy másik mérték $V$ felett,
	úgy található
	$\delta\in \mathbb{F}$,
	amelyre
	$D=\delta d.$
	Legyen ezért
	$\delta
		=
		D\left( e_{1},\ldots,e_{n}\right)
	$,
	így
	\[
		\left( D-\delta d \right)\left( e_1,\ldots,e_n \right)
		=
		D\left( e_{1},\ldots ,e_{n}\right) -\delta d\left(e_{1},\ldots ,e_{n}\right)
		=
		\delta-\delta\cdot 1
		=
		0.
	\]
	Ez viszont azt jelenti,
	hogy a $D-\delta d$ mérték az  $\left\{e_{1},\ldots ,e_{n}  \right\}$
	bázison eltűnik,
	ami csak úgy lehetséges ha
	$D-\delta d=0$,
	ergo
	\begin{math}
		D=\delta d.
	\end{math}
\end{proof}

A fejezet legfontosabb gondolatához érkeztünk.
\begin{defprop}[Lineáris transzformáció determinánsa]
	\index{lineáris transzformáció determinánsa}%
	\index{determináns}
	Legyen $A\in L\left( V \right)$ a $V$ véges dimenziós vektortér egy lineáris transzformációja.
	Ehhez létezik egyetlen olyan $\delta\left( A \right)\in\mathbb{F}$ szám,
	amelyre minden $f$ nem triviális $V$ feletti mérték esetén az
	\[
		f\left( Av_1,\ldots,Av_n \right)
		=
		\delta\left( A \right)
		f\left( v_1,\ldots,v_n \right)
	\]
	azonosság minden $v_1,\ldots,v_n\in V$ vektor mellett fenáll.

	Ezt a $\delta\left( A \right)$ számot nevezzük az $A$ lineáris transzformáció \emph{determinánsának.}
\end{defprop}
\begin{proof}
	Vegyünk egy $f$ nem triviális mértéket $V$ felett.
	Láttuk, hogy ilyen valóban létezik, hiszen a mértékek egydimenziós teret alkotnak.
	Világos, hogy a $\left( v_1,\ldots v_{n}\right) \mapsto f\left( Av_{1},\ldots ,Av_{n}\right) $
	függvény is egy mérték.
	No de, a mértékek egy egydimenziós térnek az elemei,
	ezért van egyetlen olyan $\delta _{f}\left( A\right)\in\mathbb{F} $ szám,
	amelyre minden
	$
		v_{1},\ldots ,v_{n}
	$
	mellett
	$
		f\left( Av_{1},\ldots,Av_{n}\right)
		=
		\delta _{f}\left( A\right) f\left( v_{1},\ldots,v_{n}\right)
	$.
	Legyen most $g$ egy másik nem zérus mérték.
	Ekkor
	$g=\gamma f$ valamely $\gamma\neq 0$ számmal.
	Persze $g$-re is igaz, amit eddig igazoltunk,
	így
	\begin{displaymath}
		g\left( Av_{1},\ldots ,Av_{n}\right)
		=
		\delta_{g}\left( A\right) g\left( v_{1},\ldots ,v_{n}\right),
		\text{ valamint }
		f\left( Av_{1},\ldots,Av_{n}\right)
		=
		\delta _{f}\left( A\right) f\left( v_{1},\ldots,v_{n}\right).
	\end{displaymath}
	Ha a baloldali kifejezésben a $g$ mértéket a $\gamma f$ mértékre cseréljük,
	majd osztunk a $\gamma$ számmal,
	akkor
	\[
		f\left(Av_{1},\ldots ,Av_{n}\right)
		=
		\delta _{g}\left( A\right) f\left(v_{1},\ldots ,v_{n}\right)
	\]
	következik.
	Így $\delta _{f}\left( A\right) $ egyértelműsége miatt
	$
		\delta _{f}\left( A\right) =\delta _{g}\left( A\right).
	$
	A $\delta_f\left( A \right)$ értéke tehát minden $f$ nem triviális mérték mellett azonos,
	tehát azt $\delta(A)$-val jelölve, kapjuk az egyetlen olyan számot,
	amelyre a kívánt azonosság fennáll.
\end{proof}

\begin{proposition}[Szorzattétel]\index{szorzattétel}
	\index{determinánsok szorzattétele}
	Legyenek az $A,B\in L\left( V \right)$ lineáris transzformációk a $V$ véges dimenziós vektortéren.
	Ekkor
	\[
		\delta\left( A\circ B\right)
		=
		\delta\left( A\right) \delta\left( B\right).
		\qedhere
	\]
\end{proposition}
\begin{proof}
	A $\delta \left( A\circ B\right) $ definíciója szerint ez az egyetlen olyan
	szám, amelyikre minden $v_{1},\ldots ,v_{n}$ mellett
	\[
		f\left( ABv_{1},\ldots ,ABv_{n}\right) =\delta \left( A\circ B\right)
		f\left( v_{1},\ldots ,v_{n}\right) .
	\]
	Másrészt a $\delta \left( A\right) $ és $\delta \left( B\right) $ értelmezé%
	se szerint
	\[
		f\left( ABv_{1},\ldots ,ABv_{n}\right) =\delta \left( A\right) f\left(
		Bv_{1},\ldots ,Bv_{n}\right) =\delta \left( A\right) \delta \left( B\right)
		f\left( v_{1},\ldots ,v_{n}\right)
	\]
	E két utolsó sort összevetve kapjuk, hogy $\delta \left( A\circ B\right)
		=\delta \left( A\right) \delta \left( B\right) .$
\end{proof}

\begin{proposition}
	Legyen $A\in L\left( V\right)$ egy lineáris transzformáció.
	Ekkor az $A$ bármely bázisban felírt mátrixának a determinánsa azonos
	$A$-nak mint lineáris transzformációnak a determinánsával.
	Formálisan tetszőleges $\left\{ e_1,\ldots,e_n \right\}$ bázis mellett:
	\[
		\delta \left( A\right)
		=
		\left|
		\left[
			A
			\right]_{\left\{ e_{1},\ldots ,e_{n}\right\} }
		\right|.\qedhere
	\]
\end{proposition}
\begin{proof}
	Jelölje $d=d_{\left\{e_1,\ldots,e_n\right\}}$ a bázishoz tartozó nem triviális mértéket.
	Ekkor
	\[
		\left| \left[ A\right] \right| =d\left( Ae_{1},\ldots ,Ae_{n}\right) =\delta
		\left( A\right) d\left( e_{1},\ldots ,e_{n}\right) =\delta \left( A\right) . \qedhere
	\]
\end{proof}

Meggondoltuk tehát, hogy egy transzformáció mátrixának determinánsa nem függ a koordinátázástól,
tehát más-más bázisokban felírt mátrixának determinánsa azonos.
A továbbiakban az $A$ lineáris transzformáció determinánsára az egyszerűség kedvéért az $|A|$ jelölést (is) használjuk.
Ezek szerint $A$-nak $|A|$ determinánsa,
és $A$-nak valamely bázisban felírt mátrixának $|[A]|$ determinánsa közt nincs különbség.

\paragraph{Mátrixok szorzattétele}
Ha $[A],[B]$ két négyzetes mátrix,
akkor $A$-val és $B$-vel jelölve a
mátrixok generálta $\mathbb{F}^n\to\mathbb{F}^n$ lineáris transzformációkat
\[
	|[A][B]|=|[A\circ B]|=\delta\left( A\circ B \right)=\delta\left( A \right)\delta\left( B \right)=
	|[A]|\cdot|[B]|.
\]
Meggondoltuk tehát, hogy mátrixok szorzatának determinánsa a mátrixok determinánsának szorzata.

\paragraph{Az inverz determinánsa}
Legyen most $A\in L\left( V \right)$ egy reguláris transzformáció.
Ekkor $AA^{-1}=I$, igy véve a determinánsokat, $|A|\cdot|A^{-1}|=1$, azaz
$|A^{-1}|=\frac{1}{|A|}$.
Összefoglalva, az inverz determinánsa a determináns reciproka.

Ugyan korábban már két indoklást láttunk a particionált mátrix determinánsának kiszámítására (\ref{par:particionalt}), 
de érdemes még egyszer átgondolnunk, hogyan következik ez a mértékek segítségével.
\paragraph{Particionált mátrix determinánsa.}\label{par:particionalt2}
Tegyük fel, hogy $[A]$ egy $(n+m)\times(n+m)$ méretű mátrix,
amely
\[
    [A]
	=
	\begin{pmatrix}
		A_{1,1} & A_{1,2} \\
		A_{2,1} & A_{2,2}
	\end{pmatrix}
\]
módon van particionálva, ahol $A_{1,1}$ egy $n\times n$, $A_{2,2}$ egy $m\times m$ méretű négyzetes mátrix,
továbbá $A_{2,1}$ egy $m\times n$ méretű és $A_{1,2}$ egy $n\times m$ méretű részmátrix.
Tegyük fel most is, hogy a bal alsó $A_{2,1}$ partíció minden eleme zérus.
Legyen ezért $A\in L\left( \mathbb{F}^{n+m} \right)$ az a transzformáció,
amelynek az $\left\{ e_1,\ldots,e_n,e_{n+1},\ldots,e_m \right\}$ bázisban felírt mátrixa $[A]$.
Világos, hogy az $M=\lin\left\{ e_1,\ldots,e_n \right\}$ jelöléssel az $M$ altér egy $A$-ra nézve invariáns altér, hiszen az $[A_{2,1}]$ partíció zérus.
Definíció szerint a szóban forgó bázisban az $A$ transzformáció $M$ altérre való megszorításának mátrixára $[A|_M]=[A_{1,1}]$.
Jelölje $d$ az $\mathbb{F}^{n+m}$ fent rögzített bázisához tartozó szokásos mértéket, azaz
\[
    d\left( v_1,\ldots,v_n,v_{n+1},\ldots,v_{n+m} \right)
    =
    \sum_{\pi\in S_{n+m}}\sgn \pi\prod_{j=1}^{n+m}e_j^\ast v_{\pi\left( j \right)}.
\]
Tudjuk, hogy $d\left( Ae_1,\ldots,Ae_{n},Ae_{n+1},\ldots,Ae_{n+m} \right)=|[A]|$.
Definiáljuk most az $L\left( M \right)$ feletti mértéket a következőképpen:
\[
    f\left( v_1,\ldots,v_{n} \right)
    =
    d\left( v_1,\ldots,v_n, Ae_{n+1},\ldots,Ae_{n+m} \right)
\]
Világos, hogy $f$ valóban egy mérték $L\left( M \right)$ felett,
és az $A|_M$ megszorított transzformáció determinánsának definíciója szerint
\begin{multline*}
    |[A]|=
    f\left( Ae_1,\ldots,Ae_n \right)
    =
    \det\left( A|_M \right)f\left( e_1,\ldots,e_n \right)
    \\
    =
    \det\left( A|_M \right)d\left( e_1,\ldots,e_n,Ae_{n+1},\ldots,Ae_{n+m} \right)
    =
    |[A_{1,1}]|\cdot
    \left|
    \begin{pmatrix}
        I&A_{1,2}\\0&A_{2,2}
    \end{pmatrix}
    \right|.
\end{multline*}
Meggondoltuk tehát, hogy ha a bal alsó partíció zérus, akkor
\[
    \left|
    \begin{pmatrix}
        A_{1,1}&A_{2,1}\\
        0&A_{2,2}
    \end{pmatrix}
    \right|
    =
    |A_{1,1}|\cdot
    \left|
    \begin{pmatrix}
        I&A_{2,1}\\
        0&A_{2,2}
    \end{pmatrix}
    \right|.\tag{\dag}
\]
Az egész eddigi gondolatot analóg módon ismételhetjük arra az esetre, 
mikor a jobb felső partíció zérus (és persze ekkor a bal alsó $A_{2,1}$ partíció akármilyen lehet).
Kapjuk tehát, hogy
\[
    \left|
    \begin{pmatrix}
        A_{1,1}&0\\
        A_{2,1}&A_{2,2}
    \end{pmatrix}
    \right|
    =
    |A_{2,2}|\cdot
    \left|
    \begin{pmatrix}
        A_{1,1}&0\\
        A_{2,1}&I
    \end{pmatrix}
    \right|.\tag{\ddag}
\]
A fent kiemelt (\dag) és (\ddag) alakokból a (\ref{par:particionalt}) állítás is könnyen adódik.
Csak arra kell még emlékeznünk, hogy a mátrixnak és a transzponáltjának azonos a determinánsa.
Ugyanis:
\begin{multline*}
    \left|
    \begin{pmatrix}
        A_{1,1}&A_{1,2}\\
        0&A_{2,2}
    \end{pmatrix}
    \right|
    =
    |A_{1,1}|\cdot
    \left|
    \begin{pmatrix}
        I&A_{1,2}\\
        0&A_{2,2}
    \end{pmatrix}
    \right|
    =
    |A_{1,1}|\cdot
    \left|
    \begin{pmatrix}
        I&0\\
        A_{1,2}^T&A_{2,2}^T
    \end{pmatrix}
    \right|
    \\
    =
    |A_{1,1}|\cdot |A_{2,2}^T|\cdot
    \left|
    \begin{pmatrix}
        I&0\\
        A_{1,2}^T&I
    \end{pmatrix}
    \right|
    =
    |A_{1,1}|\cdot |A_{2,2}|.
\end{multline*}
Az utolsó lépésben a particionált mátrix egy olyan alsóháromszög mátrix, amelynek a diagonálisa csupa 1 számból áll, ezért a determinánsa is 1.

\section{A determináns kifejtése}
A kifejtési tétel arra való,
hogy a determináns kiszámítását visszavezessük 1-el kisebb méretű mátrixok determinánsának kiszámolására.
Ezt a gondolatot egymásután alkalmazva tetszőleges mátrix determinánsa kiszámítható.

Ebben a fejezetben vannak azok az explicit formulák,
amelyeket a fejezet bevezetőjében említettem.
\begin{definition}[minor, kofaktor, kofaktormátrix]
	\index{minor}\index{kofaktor}\index{kofaktormátrix}
	Legyen $A\in\mathbb{F}^{n\times n}$ egy mátrix.
	E mátrix
	\begin{itemize}
		\item\emph{$\left( i,k \right)$-csonkoltja}
		      a mátrix $i$-edik sorának és $k$-adik oszlopának törlése után megmaradt
		      $n-1\times n-1$ méretű mátrix. Jelölése: $A_{i,k}$.
		\item\emph{$\left( i,k \right)$-minorja}
		      az $(i,k)$-csonkolt $A_{i,k}$ mátrix determinánsa.
		\item\emph{$\left( i,k \right)$-kofaktora}
		      a $\left( -1 \right)^{i+k}|A_{i,k}|$ szám.
		\item\emph{kofaktormátrixa}
		      az az $n\times n$ méretű mátrix,
		      amelynél az $i$-edik sor $k$-adik eleme az $\left( i,k \right)$-kofaktor.\qedhere
	\end{itemize}
\end{definition}
\begin{proposition}
	Legyen $A\in\mathbb{F}^{n\times n}$ egy mátrix.
	Írjuk felül az $A$ mátrix $k$-adik oszlopának elemeit $0$-val,
	majd a $k$-adik oszlop $i$-edik helyére írjunk $1$-et.
	E felülírt mátrix determinánsa az eredeti mátrix $\left( i,k \right)$-kofaktora.
\end{proposition}
\begin{proof}
	Világos, hogy a szóban forgó mátrix $k-1$ oszlopcsere, 
    majd $i-1$ sorcsere után olyan alakra hozható,
	ahol a balfelső elem $1$, alatta az első oszlop minden eleme zérus,
	és a jobb alsó $n-1\times n-1$ méretű részmátrix az eredeti mátrix $\left( i,k \right)$-csonkoltja:
	\[
		\begin{pmatrix}
			1 & \begin{bmatrix}
				a_{i,1}\, & \phantom{A_{i,k}} & \,a_{i,n}
			\end{bmatrix} \\
			\begin{bmatrix}
				0      \\
				\vdots \\
				0
			\end{bmatrix}
			  &
			\begin{bmatrix}
				a_{1,1} & \dots   & a_{1,n} \\
				\vdots  & A_{i,k} & \vdots  \\
				a_{n,1} & \dots   & a_{n,n}
			\end{bmatrix}
		\end{pmatrix}
	\]
	A fenti mátrix determinánsa a particionált mátrix determinánsára kapott formula,
	\apageref{par:particionalt}. oldal,
	szerint $1\cdot|A_{i,k}|$.
	Meggondoltuk tehát, hogy ha az állításban felülírt mátrix determinánsát $\delta$-val jelöljük,
	akkor  $\delta\left( -1 \right)^{k-1+i-1}=1\cdot|A_{i,k}|$-t kapjuk.
	Ebből persze
	\begin{math}
		\delta
		=\left( -1 \right)^{i+k}|A_{i,k}|.
	\end{math}
\end{proof}
\begin{proposition}[oszlopok szerinti kifejtés]
	Legyen $A\in\mathbb{F}^{n\times n}$ mátrix.
	Ekkor
	\begin{enumerate}
		\item minden $1\leq k\leq n$ oszlopra
		      \(
		      |A|=
		      \sum_{i=1}^na_{i,k}\left( -1 \right)^{i+k}|A_{i,k}|,
		      \)
		\item minden $1\leq k,j\leq n$ oszlopra $j\neq k$ mellett
		      \(
		      0=
		      \sum_{i=1}^na_{i,j}\left( -1 \right)^{i+k}|A_{i,k}|.
		      \)
	\end{enumerate}
	A fenti két formula tömörebben: minden $1\leq k,j\leq n$ mellett
	\[
		\sum_{i=1}^na_{i,j}\left( -1 \right)^{i+k}|A_{i,k}|
		=
		\delta_{j,k}|A|.\qedhere
	\]
\end{proposition}
\begin{proof}
	Tekintsük az $\mathbb{F}^n$ vektortér természetes bázisát:
	$\left\{e_1,\ldots,e_n\right\}$. Láttuk, hogy a $d=d_{\left\{ e_1,\ldots,e_n\right\}}$
	mérték mellett
	\[
		|A|=d\left( a_1,\ldots,a_n \right),
	\]
	ahol $a_1,\ldots,a_n$ jelöli rendre az $A$ mátrix oszlopait.
	A $k$-adik oszlopvektorra $a_k=\sum_{i=1}^na_{i,k}e_i$.
	Mivel $d$ az összes nem $k$-adik változója rögzítése mellett a $k$-adik változójában lineáris,
	ezért
	\[
		|A|=d\left( a_1,\ldots,a_k,\ldots,a_n \right)
		=
		d\left( a_1,\ldots,\sum_{i=1}^na_{i,k}e_i,\ldots,a_n \right)
		=
		\sum_{i=1}^na_{i,k}d(a_1,\ldots,e_i,\ldots,a_n).
	\]
	No de, itt $d(a_1,\ldots,e_i,\ldots,a_n)$ az előző állításban felülírt mátrix determinánsa,
	azaz $\left( -1 \right)^{i+k}|A_{i,k}|$.
	Éppen ezt kellett belátni az 1. pont indoklásához.

	Másoljuk a mátrix $j$-edik oszlopát a $k$-adik oszlopra.
	Ennek a mátrixnak két oszlopa azonossá vált,
	ezért determinánsa zérus.
	Felírva erre a mátrixra a $k$-adik oszlop szerinti kifejtést,
	éppen a 2. pont azonosságát kapjuk.
\end{proof}
A mátrix transzponáltjának determinánsa azonos a mátrix determinánsával.
Így ha a transzponált mátrixra alkalmazzuk az oszlopok szerinti kifejtést,
akkor a sorok szerinti kifejtést kapjuk.

A determináns kifejtésével az mátrix inverzére kapunk explicit formulát:
\begin{proposition}
	Legyen $[A]$ egy reguláris mátrix.
	Ekkor az inverz mátrix a kofaktormátrix transzponáltjának a determináns reciprokával vett szorzata.

	Magyarra áttérve:
	\[
		[A^{-1}]_{k,i}=\frac{1}{|A|}\left( -1 \right)^{i+k}|A_{i,k}|.\qedhere
	\]
\end{proposition}
\begin{proof}
	Jelölje a $M$ a kofaktormátrix transzponáltját,
	azaz $m_{k,i}=\left( -1 \right)^{i+k}|A_{i,k}|$.
	Így az $M\cdot A$ szorzat mátrix $k,j$ pozíciója
	\[
		\sum_{i=1}^nm_{k,i}a_{i,j}
		=
		\sum_{i=1}^n
		\left( -1 \right)^{i+k}|A_{i,k}|
		a_{i,j}
		=\delta_{j,k}|A|
	\]
	a kifejtési tétel szerint.
	Ez azt jelenti, hogy $M\cdot A=|A|I$.
	Mivel $A$ reguláris ezért a nem zérus determinánsával osztva kapjuk, hogy
	$\left(\frac{1}{|A|}M\right)A=I$,
	ami azt jelenti, hogy $A^{-1}=\frac{1}{|A|}M$.
\end{proof}
\begin{proposition}[Cramer-szabály]\index{Cramer-szabály}
	Tekintsünk egy $n\times n$ méretű inhomogén lineáris egyenletrendszert.
	Tudjuk, hogy pontosan akkor van egyetlen megoldása minden jobboldal mellett,
	ha az együttható mátrix reguláris.
	Jelölje  ezért $A$ a reguláris együttható mátrixot,
	és $B_k$ azt a mátrixot, amelyet úgy kapunk,
	hogy az egyenlet jobboldalát reprezentáló oszlopvektort az $A$ együttható mátrix $k$-adik
	oszlopára írjuk.
	Ekkor az $x$ megoldásvektor $k$-adik koordinátájára
	\[
		x_k=\frac{|B_k|}{|A|}.\qedhere
	\]
\end{proposition}
\begin{proof}
	Kezdjük a $B_k$ mátrixnak a $k$-adik oszlopa szerinti kifejtésével.
	\(
	|B_k|
	=
	\sum_{i=1}^nb_i\left( -1 \right)^{i+k}A_{i,k}.
	\)
	Na most az $Ax=b$ ekvivalens az $x=A^{-1}b$-vel,
	így a Cramer-szabály szerint
	\[
		x_k=[A^{-1}]_k\cdot b
		=
		\sum_{i=1}^n[A^{-1}]_{k,i}\cdot b_i
		=
		\sum_{i=1}^n\left( \frac{1}{|A|}\left( -1 \right)^{i+k}A_{i,k} \right)b_i
		=
		\frac{1}{|A|}\cdot |B_k|.\qedhere
	\]
\end{proof}

\section{A karakterisztikus polinom}
\begin{definition}
	Legyen $A\in L\left( V \right)$ egy lineáris transzformációja az $\mathbb{F}$ test feletti $V$,
	$n$-dimenziós vektortérnek.
	Definiáljuk a $k:\mathbb{F}\to\mathbb{F}$ függvényt, mint
	$k\left( t \right)=|tI-A|$ minden $t\in\mathbb{F}$ mellett.
	Ekkor $k$ egy pontosan $n$-ed fokú, normált polinom,
	amelyre
	\[
		k\left( t \right)
		=
		t^n-\tr(A)t^{n-1}+\dots+\left( -1 \right)^n|A|.
	\]
	Ezt a $k$ függvényt nevezzük az
	$A$ transzformáció \emph{karakterisztikus}\index{karakterisztikus polinom}
	polinomjának.
\end{definition}
\begin{proof}
	Rögzítsük a tér egy $\left\{ e_1,\ldots,e_n \right\}$ bázisát,
	majd írjuk fel az $tI-A$ transzformáció mátrixát ebben a bázisban.
	E mátrix determinánsa $k\left( t \right)$,
	ami minden konkrét $t\in\mathbb{F}$ mellett az $\mathbb{F}$ testbeli szám.
	Mivel a transzformáció mátrixának determinánsa minden bázis mellett ugyanaz a szám,
	ezért $k\left( t \right)$ konkrét értéke a választott bázistól független.

	A diagonálisban szereplő elemek $\left( t-a_{i,i} \right)$ alakúak,
	és a $t$ változó máshol nem szerepel a mátrixban.
	Ha a determnináns definíciójára gondolunk, akkor nyilvánvaló, hogy $k\left( t \right)$ a $t$
	változó legfeljebb $n$-ed fokú polinomja.

	E polinom konstans tagja $k\left( 0 \right)=|-A|=\left( -1 \right)^n|A|.$

	Ha $\pi\neq\id$ egy permutáció,
	akkor $\pi$ legalább két helyen különbözik az identikus permutációtól,
	ergo a $\pi$-hez tartozó bástyafelrakás legfeljebb $n-2$ diagonális elemet taposhat.
	Látjuk tehát,
	hogy identitástól különböző permutáció mellett nincs $t$-nek $n-2$-nél magasabb fokú hatványa.

	A diagonális bástyafelrakáshoz a
	\[
		\left( t-a_{1,1} \right)\left( t-a_{2,2} \right)\cdots\left( t-a_{n,n} \right)
	\]
	szorzat tartozik, amelynek első két legmagasabb hatványú tagja
	\[
		t^n+t^{n-1}\left( -a_{1,1}\dots-a_{n,n} \right)=t^n-\tr\left( A \right)t^{n-1}.\qedhere
	\]
\end{proof}
A spektrum pontjai nem csak a minimálpolinom zérus helyei,
hanem a karakterisztikus polinom zérushelyei is.
\begin{proposition}[karakterisztikus polinom gyökei]
	Legyen $k\left( t \right)$ az $A\in L\left( V \right)$ lineáris transzformáció karakterisztikus polinomja.
	A $\lambda\in\mathbb{F}$ szám pontosan akkor sajátértéke $A$-nak,
	ha $k\left( \lambda \right)=0$.
\end{proposition}
\begin{proof}
	A $\lambda$ pontosan akkor sajátértéke $A$-nak,
	ha $\lambda I-A$ szinguláris,
	ami avval ekvivalens, hogy $0=|\lambda I-A|=k\left( \lambda \right)$.
\end{proof}
Most tegyük fel egy pillanatra, hogy $\mathbb{C}$ feletti vektortér $A\in L(V)$ lineáris transzformációját
vizsgáljuk. Minimálpolinomja az algebra alaptétele szerint felbomlik
\[
	m\left( t \right)
	=\left( t-\lambda_1 \right)^{m_1}\dots\left( t-\lambda_s \right)^{m_s}
\]
gyöktényezői szorzatára, ahol $\sigma\left( A \right)=\left\{ \lambda_1,\ldots,\lambda_s\right\}$.
Jelölje $k_i$ a $\lambda_i$ sajátérték algebrai multiplicitását,\index{sajátérték algebrai multiplicitása}
azaz $k_i=\nu\left( \left( A-\lambda_i I \right)^{m_i} \right)$.
Tudjuk, hogy $tI-A$ Jordan-normálalakú\index{Jordan-normálalak}
mátrixa egy olyan felsőháromszög alakú mátrix,
amelynek diagonálisában $t-\lambda_i$ kifejezés éppen $k_i$-szer szerepel.
Mivel egy felsőháromszög alakú mátrix determinánsa a diagonális elemek szorzata,
ezért a következő tételt gondoltuk meg.
\begin{proposition}
	Tegyük fel, hogy $V$ egy a $\mathbb{C}$ test feletti vektortér és $A\in L\left( V \right)$
	egy lineáris transzformáció.
	Jelölje $\sigma\left( A \right)=\left\{\lambda_1,\dots\lambda_s  \right\}$ a különböző sajátértékeket,
	és jelölje $k_1,\ldots,k_s$ rendre sajátértékek algebrai multiplicitásait.
	Ekkor az $A$ transzformáció karakterisztikus polinomja
	\[
		k\left( t \right)
		=
		\left( t-\lambda_1 \right)^{k_1}\dots\left( t-\lambda_s \right)^{k_s}.\qedhere
	\]
\end{proposition}
Az $A-\lambda_iI$ a $\ker\left( A-\lambda_iI \right)^{m_i}$ vektortér felett egy $m_i$-ed rendben nilpotens transzformáció,
de tudjuk hogy a nilpotencia rendje legfeljebb a tér dimenziója,
ergo
$m_i\leq\dim\left( \ker\left( A-\lambda_iI \right)^{m_i} \right)
	=
	\nu\left( \left( A-\lambda_iI \right)^{m_i} \right)=k_i$.
Ebből azonnal látjuk, hogy a minimálpolinom osztója a karakterisztikus polinomnak.
Meggondoltuk tehát a Cayley\,--\,Hamilton-tételt abban a speciális esetben,
mikor a test a kompex számtest.

A Cayley\,--\,Hamilton-tétel nem függ az algebra alaptételétől, amint azt rögtön megmutatjuk.
\begin{lemma}
	Legyen $\mathbb{F}$ egy test, amelyben $1+1\neq 0$.
	Tetszőlegesen választott $\alpha_0,\ldots,\alpha_{n-1}\in\mathbb{F}$ számok mellett
	jelölje
	\(
	A=
	\begin{pmatrix}
		-\alpha_{n-1} & 1      & 0      & \dots  & 0 \\
		-\alpha_{n-2} & 0      & 1      & \dots  & 0 \\
		\vdots        & \cdots & \cdots & \ddots & 0 \\
		-\alpha_1     & 0      & 0      & \dots  & 1 \\
		-\alpha_0     & 0      & 0      & \dots  & 0
	\end{pmatrix}
	\)
	az $n\times n$ méretű mátrixot.
	Ekkor ennek karakterisztikus polinomja
	\begin{math}
		k\left( t \right)
		=
		t^n+\alpha_{n-1}t^{n-1}+\dots+\alpha_1t+\alpha_0.
	\end{math}
\end{lemma}
\begin{proof}
	Először is tekintsük a
	\(
	tI-A=
	\begin{pmatrix}
		t+\alpha_{n-1} & -1     & 0      & \dots  & 0  \\
		\alpha_{n-2}   & t      & -1     & \dots  & 0  \\
		\vdots         & \cdots & \cdots & \ddots & 0  \\
		\alpha_1       & 0      & 0      & \dots  & -1 \\
		\alpha_0       & 0      & 0      & \dots  & t
	\end{pmatrix}
	\) mátrixot.
	E mátrix mérete szerinti indukcióval bizonyítunk.

	Ha $n=1$, akkor a polinom $t+\alpha_0$, és a mátrixnak is csak egyetlen $t+\alpha_0$ eleme van.

	Most tegyük fel, hogy igaz az állítás tetszőlegesen választott $n-1$ szám mellett, és lássuk be $n$-re.
	A $tI-A$ mátrix determinánsát megkapjuk az utolsó sor szerint kifejtéssel.
	Tehát
	\[
		k\left( t \right)=
		|tI-A|=
		\alpha_0
		\left( -1 \right)^{1+n}
		\left|
		\begin{pmatrix}
			-1     & 0      & \dots  & 0      \\
			t      & -1     & \dots  & 0      \\
			\vdots & \cdots & \ddots & \vdots \\
			0      & 0      & \dots  & -1     \\
		\end{pmatrix}
		\right|
		+t\left( -1 \right)^{n+n}
		\left|
		\begin{pmatrix}
			t+\alpha_{n-1} & -1     & 0      & \dots  & 0       \\
			\alpha_{n-2}   & t      & -1     & \dots  &   \\
			\vdots         & \cdots & \cdots & \ddots & -1      \\
			\alpha_1       & 0      & 0      & \dots  & t
		\end{pmatrix}
		\right|.
	\]
	Itt az első $\left( n-1 \right)\times \left( n-1 \right)$ méretű determináns $\left( -1 \right)^{n-1}$,
	mert ez egy alsóháromszög alakú mátrix a diagonálisában csupa $-1$ számmal.
	A második determináns értéke az indukciós feltevés az $n-1$ darab
	$ \alpha_1,\ldots, \alpha_{n-1}$ számra.
	A fent kezdett kiemelést folytatva tehát,
	\[
		k\left( t \right)
		=
		\alpha_0
		\left( -1 \right)^{1+n}\left( -1 \right)^{n-1}
		+t\left(
		t^{n-1}+\alpha_{n-1}t^{n-2}+\dots\alpha_2t+\alpha_1
		\right)
		=
		t^n+\alpha_{n-1}t^{n-1}+\dots+\alpha_1t+\alpha_0.\qedhere
	\]
\end{proof}
\begin{proposition}[Cayley\,--\,Hamilton]
	Legyen $V$ egy tetszőleges test feletti vektortér,
	és $A\in L\left( V \right)$ egy lineáris transzformáció.
	Ekkor $A$ gyöke a karakterisztikus polinomjának,
	azaz $k\left( A \right)=0
		(\in L\left( V \right))$.
\end{proposition}
\begin{proof}
	Legyen $m=\dim(V)$, és $v\in V$ egy nem zérus elem.
	Tekintsük a $v$ által generált legszűkebb $A$-invariáns alteret, azaz
	$\lin\left( v;A \right)$-t.
	Ha ez $n$ dimenziós ($1\leq n\leq m$),
	és a $v$-hez tartozó kis minimálpolinom\index{kis minimálpolinom}
	\[
		p_v\left( t \right)
		=
		t^{n}+\alpha_{n-1}t^{n-1}+\dots+\alpha_1t+\alpha_0,
	\]
	akkor a
	$\lin\left( v;A \right)$ invariáns altérben
	$\left\{ A^{n-1}v,\ldots,Av,v \right\}$ egy bázis.
	Ebben a bázisban $A|_{\lin\left( v;A \right)}$ transzformáció mátrixa
	\[
		A_{1,1}=
		\begin{pmatrix}
			-\alpha_{n-1} & 1      & 0      & \dots  & 0 \\
			-\alpha_{n-2} & 0      & 1      & \dots  & 0 \\
			\vdots        & \cdots & \cdots & \ddots & 0 \\
			-\alpha_1     & 0      & 0      & \dots  & 1 \\
			-\alpha_0     & 0      & 0      & \dots  & 0
		\end{pmatrix}
	\]
	alakú.
	Az előző gondolat szerint $A_{1,1}$ karakterisztikus polinomja éppen $p_v\left( t \right)$.

	Egészítsük ki $\lin\left( v;A \right)$ fenti bázisát az egész $V$ vektortér bázisává.
	Ebben a bázisban felírva $A$ mátrixa
	\begin{math}
		[A]=
		\begin{pmatrix}
			A_{1,1} & A_{1,2} \\
			A_{2,1} & A_{2,2}
		\end{pmatrix}
	\end{math}
	alakú, ahol $A_{1,1}$ azonos $A|_{\lin\left( v;A \right)}$ fenti mátrixával, és mivel a
	$\lin\left( v;A \right)$ egy invariáns altér,
	ezért az $A_{1,1}$ alatt elhelyezkedő $\left( m-n \right)\times n$ méretű $A_{2,1}$ mátrix minden eleme zérus.
	Egy ilyen módon partícionált mátrix determinánsa már könnyen számolható:
	\[
		k\left( t \right)=
		|[tI-A]|
		=
		\left|
		\begin{pmatrix}
			tI_{1,1}-A_{1,1} & -A_{1,2}         \\
			-A_{2,1}         & tI_{2,2}-A_{2,2}
		\end{pmatrix}
		\right|
		=
		|tI_{1,1}-A_{1,1}|\cdot|tI_{2,2}-A_{2,2}|
		=
		p_v\left( t \right)k_2\left( t \right),
	\]
	ahol $k_2$ az $A_{2,2}$ karakterisztikus polinomja.
	Azt kaptuk, hogy $p_v$-nek többszöröse $k$,
	ezért $k\left( A \right)v=0$.
\end{proof}
Mivel egy lineáris transzformáció gyöke a saját karakterisztikus polinomjának,
ezért a karakterisztikus polinom egy pontosan $n$-ed fokú polinom,
amely a minimálpolinom többszöröse,
azaz a minimápolinom osztója a karakterisztikus polinomnak.
Persze ebből is következik, hogy a minimálpolinom egy legfeljebb $n$-ed fokú polinom.

\chapter{Skalárisszorzatos terek geometriája}
A továbbiakban $\mathbb{K}$ feletti vektorterekről van szó,
ahol $\mathbb{K}=\mathbb{R}$ vagy $\mathbb{K}=\mathbb{C}$.
\section{Definíciók}

\begin{definition}[skalárisszorzatos-tér]\index{skaláris szorzat}
	Egy $\ip{\cdot}{\cdot}:V\times V\to \mathbb{K}$ függvényt \emph{skaláris szorzatnak} mondunk, ha
	teljesülnek az alábbi axiómák:
	\begin{enumerate}
		\item $\ip{x}{y}=\overline{\ip{y}{x}}$ minden $x,y\in V$ mellett.
		\item
		      $\ip{\alpha x}{y}=
			      \alpha\ip{x}{y}$ minden $x,y\in V$ és minden $\alpha\in V$ mellett.
		\item
		      $\ip{x_1+x_2}{y}=
			      \ip{x_1}{y}+\ip{x_2}{y}$ minden $x_1,x_2,y\in V$ mellett.
		\item
		      $\ip{x}{x}\geq 0$ és $\ip{x}{x}=0$ akkor és csak akkor, ha $x=0$.
	\end{enumerate}
	Ha a $V$ vektortéren adott a fenti skaláris szorzat,
	akkor a $\left( V,\ip{\cdot}{\cdot} \right)$ párt \emph{skalárisszorzatos-térnek} mondjuk.\index{skalárisszorzatos-tér}
\end{definition}
Az első három pont szerint a skaláris szorzat rögzített második változó melett az elsőben lineáris, azaz
$
	\ip{\sum_{j=1}^s\alpha_jx_j}{y}
	=
	\sum_{j=1}^s\alpha_j\ip{x_j}{y}$.
Hasonlóan, a skaláris szorzás operáció rögzített első változó mellett a második változó
\emph{konjugáltan lineáris}\index{konjugáltan lineáris} függvénye,
azaz
\begin{math}
	\ip{x}{\sum_{k=1}^r\beta_ky_j}
	=
	\sum_{k=1}^r\overline{\beta_k}\ip{x}{y_k}.
\end{math}
Persze ezeket nagyon sokszor vegyítjük is,
így a
\[
	\ip{\sum_{j=1}^s\alpha_jx_j}{\sum_{k=1}^r\beta_ky_k}
	=
	\sum_{j=1}^s\alpha_j\ip{x_j}{\sum_{k=1}^r\beta_ky_k}
	=
	\sum_{j=1}^s\alpha_j\sum_{k=1}^r\overline{\beta_k}\ip{x_j}{y_k}
	=
	\sum_{j=1}^s\sum_{k=1}^r\alpha_j\overline{\beta_k}\ip{x_j}{y_k}
\]
formula adódik, amit úgy jegyezhetünk meg, hogy minden tagot minden taggal kell szorozni,
de a másik változóból konjugáltan emeljük ki a test elemeit.

\paragraph{Belső szorzat.} A legfontosabb példa skaláris szorzatra a \emph{belső szorzat}\index{belső szorzat}.
Ha adott a vektortér egy $\left\{ v_1,\ldots,v_n \right\}$ bázisa,
akkor minden vektort egyértelműen meghatároznak a koordinátái,
azaz
\begin{math}
	x=\sum_{j=1}^n\xi_jv_j
\end{math}
és
\begin{math}
	y=\sum_{j=1}^n\eta_jv_j.
\end{math}
Ezek belső szorzatát
\begin{displaymath}
	\left[ x,y \right]=\sum_{j=1}^n\xi_j\bar{\eta_j}
\end{displaymath}
definiálja.
Nagyon fontos észrevenni,
hogy a belső szorzat a definíciója szerint függ a vektorok koordinátáitól,
tehát függ a bázis megválasztásától.% 
\footnote{
	Helyesebb lenne a szörnyen nehézkes
	$\left[ \cdot,\cdot \right]_{\left\{ v_1,\ldots,v_n \right\}}$ jelölés.
}

A másik fontos példa $\mathbb{R}$ feletti vektortén értelmezett.
Jelölje $C\left[ a,b \right]$ valamely $\left[ a,b \right]$ korlátos és zárt intervallum feletti összes folytonos függvények vektorterét.
Definiálja $f,g\in C\left[ a,b \right]$ két folytonos függvény skaláris szorzatát
\begin{math}
	\int_{a}^{b}f\left( x \right)g\left( x \right)\,dx.
\end{math}
Könnyen látható, hogy a fenti függvény kielégíti a skaláris szorzat definíciójában követelteket.
Figyeljünk a folytonosság szerepére!

\begin{definition}[merőlegesség, halmaz ortokomplementere]
	Legyen $\left( V,\ip{\cdot}{\cdot} \right)$ egy skalárisszorzatos-tér.
	\begin{enumerate}
		\item
		      Ha az $x,y\in V$ két pontjára $\ip{x}{y}=0$,
		      akkor azt mondjuk hogy a két vektor \emph{merőleges}\index{merőleges} egymásra.
		      Jelölés: $x\perp y$.
		\item
		      Legyen most, $H\subseteq V$ egy részhalmaz.
		      Definiálja
		      \(
		      H^{\perp}=\left\{ x\in V:x\perp a \text{ minden } a\in H \right\}
		      \)
		      a \emph{$H$ halmaz merőlegesét}, vagy \emph{ortokomplemeterét}\index{ortokomplementer}.\qedhere
	\end{enumerate}
\end{definition}
A zérus vektor minden vektorra merőleges, sőt a zérus vektor az egyedüli ilyenvektor, hiszen ekkor sajátmagára is merőleges,
de saját magára merőleges vektor csak a zérus vektor van.
Pont ezt jelenti az axiómák közt az utolsó.
A $H^\perp$ az a részhalmaza $V$-nek, amely az összes $H$-ra merőleges vektorokból áll.
Tehát $\left\{ 0 \right\}^\perp = V$ és $V^{\perp}=\left\{ 0 \right\}$.

\begin{definition}
	Legyen $V$ egy skalárisszorzatos-tér, $H\subseteq V$.
	Az $\left\{ x_1,\cdots,x_k \right\}$ vektorrendszert
	\begin{enumerate}
		\item \emph{ortogonálisnak} nevezünk, ha minden $i\neq j$ esetén
		      $x_i\perp x_j$.
		\item \emph{ortonormáltnak} nevezünk, ha minden $i,j$ mellett $\ip{x_i}{x_j}=\delta_{i,j}$.\qedhere
	\end{enumerate}
\end{definition}
A felépítés jelen szintjén nem olyan egyszerű egy tetszőleges skaláris szorzatos térben ortonormált rendszert
adni. Persze az ${}$ üres vektorrendszer ortogonális, de ortonormált is.
Az látható, hogy egy ortogonális rendszert tetszőlegesen sok zérus vektorral kiegészítve ortogonális rendszert kapunk.
Persze a zérus vektor egy ortonormált rendszerhez nem tartozhat elemként,
de ha van a vektortérben egy $v\neq 0$ elem,
akkor az egyelemű $\left\{ \frac{1}{\sqrt{\ip{v}{v}}}v \right\}$ vektorrendszer egy ortonormált rendszert alkot.
(Mivel csak egyetlen elem van, ezért bármely két különböző elem merőleges egymásra;
valamint
$\ip{\frac{1}{\sqrt{\ip{v}{v}}}v}{\frac{1}{\sqrt{\ip{v}{v}}}v}
	=
	\frac{1}{\sqrt{\ip{v}{v}}}
	\frac{1}{\sqrt{\ip{v}{v}}}
	\ip{v}{v}=1.
$)

Egynél több elemből álló ortonormált rendszert most nem tudunk mutatni,
de később ennek oka egészen világos lesz.
Látjuk majd, hogy egy $n$-dimenziós skalárisszorzatos térben mindig van $n$ elemű ortonormált rendszert, de annál több elemű soha nincs.
Ebből annyi, hogy $n$-nél több nemzérus vektor nem alkothat ortogonális rendszert, az már látható is:
\begin{proposition}
	Egy zérus elemeket nem tartalmazó ortogonális rendszer lineárisan független rendszer is.
\end{proposition}
\begin{proof}
	Ha a $\left\{ v_1,\ldots,v_n \right\}$ rendszer lineárisan összefüggő,
	és $v_k=\sum_{j=1}^{k-1}\alpha_jv_j$,
	akkor
	\[
		\ip{v_k}{v_k}
		=
		\ip{\sum_{j=1}^{k-1}\alpha_jv_j}{v_k}
		=
		\sum_{j=1}^{k-1}\alpha_j\ip{v_j}{v_k}
		=0.
	\]
	Ez a skaláris szorzás definíciója szerint csak abban az esetben lehetséges,
	ha $v_k=0$, ami ellentmond annak a feltételnek, hogy a rendszer a null vektort nem tartalmazza.
\end{proof}
Speciálisan, egy ortonormált rendszer a zérus vektort nem tartalmazza,
emiatt lineárisan független rendszer.
Így tehát a Steinitz-lemma szerint,
egy $n$-dimenziós vektortérben nem lehet több mint $n$ elemű ortonormált rendszert mutatni.
Később látni fogjuk,
hogy $n$ elemű ortonormált rendszer viszont mindig konstruálható egy $n$-dimenziós vektortérben,
de erre itt még várnunk kell.

Az alábbiakban a merőlegesség legnyilvánvalóbb tualjdonságait foglaljuk össze.
Ezeket a továbbiakban hivatkozás nélkül használjuk.
\begin{proposition}[Merőlegesség tulajdonságai]
	Legyen most is $V$ egy skalárisszorzatos-tér, $H,H_1\subseteq V$.
	Ekkor
	\begin{enumerate}
		\item $H\cap H^{\perp}\subseteq\left\{ 0 \right\},$
		\item $H\subseteq H_1\implies H_1^\perp\subseteq H^\perp,$
		\item $H\subseteq \left( H^\perp \right)^\perp,$
		\item $H^{\perp}=\left( \lin H \right)^\perp,$
		\item $H^\perp$ altér.\qedhere
	\end{enumerate}
\end{proposition}\
\begin{proof}
	Ha $v\in H\cap H^\perp$, akkor $v\perp v$, ergo $v=0$, ami az 1. pontot igazolja.
	Ha $H\subseteq H_1$ és a $v$ vektor a $H_1$ minden pontjára merőleges, akkor persze ez a $v$ a $H$ minden pontjára is merőleges,
	azaz 2. pontot is meggondoltuk.
	Ha $v\in H$ rögzített és $a\in H^\perp$ egy vektor, akkor $v\perp a$, ami azt jelenti, hogy $H\subseteq \left( H^\perp \right)^\perp$.
	A $(\lin H)^\perp\subseteq H^\perp$ tartalmazás nyilvánvaló a már meggondolt 2. tulajdonság szerint.
	Ha viszont $v\in H^\perp$ és $u_1,\ldots,u_n\in H$, akkor ezek tetszőleges lineáris kombinációjára
	\begin{math}
		\ip{v}{\sum_{j=1}^n\alpha_ju_j}
		=
		\sum_{j=1}^n\alpha_j\ip{v}{u_j}
		=
		0,
	\end{math}
	hiszen $v\perp u_j$.
	Meggondoltuk tehát, hogy $v$ merőleges a $H$-beli elemekből képzett tetszőleges lineáris kombinációra,
	ergo $H^\perp\subseteq\left( \lin H \right)^\perp.$
	Vilagos, hogy $0\in H^\perp$, ha $v_1,v_2\in H^\perp$, akkor $\alpha_1v_1+\alpha_2v_2\in H^\perp$, azaz
	$H^\perp$ minden $H$ részhalmaz mellett egy altér.
\end{proof}

\section{Egyenlőtlenségek}
\begin{proposition}[Bessel]\index{Bessel-egyenlőtlenség}\label{pr:Bessel}
	Legyen $\left( V,\ip{\cdot}{\cdot} \right)$ skalárisszorzatos-tér.
	Tegyük fel, hogy $\left\{ x_1,\cdots,x_k \right\}$ egy ortonormált rendszer.
	Ekkor minden $u\in V$ vektorra:
	\begin{enumerate}
		\item
		      fennáll a  \emph{Bessel-egyenlőtlenség:}
		      $\sum_{i=1}^k|\ip{u}{x_i}|^2\leq\ip{u}{u}$;
		\item
		      Az
		      \(
		      \hat{u}=u-\sum_{i=1}^k\ip{u}{x_i}x_i
		      \)
		      vektorra $\hat{u}\perp x_j$
		      ($j=1,\cdots,k$), azaz
		      \(
		      \hat{u}\in
		      \left\{ x_1,\ldots,x_k \right\}^\perp
		      %=
		      %\left(\lin\left\{ x_1,\cdots,x_k \right\} \right)^\perp
		      .
		      \)
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Jelölje a bizonyítás erejéig
	\begin{math}
		\alpha_j=\ip{u}{x_j}.
	\end{math}
	Ekkor az $\hat{u}=u-\sum_{j=1}^k\alpha_jx_j$ vektorra
	\begin{multline*}
		0
		\leq
		\ip{\hat{u}}{\hat{u}}
		=
		\ip{u}{u}+\ip{u}{-\sum_{j=1}^k\alpha_jx_j}+\ip{-\sum_{j=1}^k\alpha_jx_j}{u}+\ip{-\sum_{j=1}^k\alpha_jx_j}{-\sum_{i=1}^k\alpha_ix_i}
		\\
		=
		\ip{u}{u}-\sum_{j=1}^k\overline{\alpha_j}\ip{u}{x_j}-\sum_{j=1}^k\alpha_j\ip{x_j}{u}+\sum_{j=1}^k\sum_{i=1}^k\alpha_j\overline{\alpha_i}\ip{x_j}{x_i}
		\\
		=
		\ip{u}{u}-\sum_{j=1}^k\overline{\alpha_j}\alpha_j-\sum_{j=1}^k\alpha_j\overline{\alpha_j}+\sum_{j=1}^k\alpha_j\overline{\alpha_j}
		=\ip{u}{u}-\sum_{j=1}^k|\alpha_j|^2.
	\end{multline*}
	Ez éppen a Bessel-egyenlőtlenség.
	A merőlegességre vonatkozó második állítás már sokkal egyszerűbb:
	\[
		\ip{\hat{u}}{x_j}
		=
		\ip{u-\sum_{i=1}^k\alpha_ix_i}{x_j}
		=
		\ip{u}{x_j}-\sum_{i=1}^k\alpha_i\ip{x_i}{x_j}
		=
		\alpha_j-\alpha_j
		=
		0.
		\qedhere
	\]
\end{proof}
\begin{proposition}[Schwartz-egyenlőtlenség]
	Legyen $\left( V,\ip{\cdot}{\cdot} \right)$ skalárisszorzatos-tér,
	$u,v\in V$.
	Ekkor
	\[
		|\ip{u}{v}|^2\leq \ip{u}{u}\ip{v}{v}.\qedhere
	\]
\end{proposition}
\begin{proof}
	Ha $v=0$, akkor mindkét oldal zérus.
	Ha $v\neq 0$, akkor a $\left\{ \frac{1}{\sqrt{\ip{v}{v}}}v \right\}$ rendszer egyetlen elemű ortonormált rendszer.
	Alkalmazva a Bessel-egyenlőtlenséget\index{Bessel-egyenlőtlenség} $k=1$ mellett
	\[
		\frac{1}{\ip{v}{v}}
		\left|\ip{u}{v}\right|^2
		=
		\left|\frac{1}{\sqrt{\ip{v}{v}}}
		\ip{u}{v}\right|^2
		=
		\left|\ip{u}{\frac{1}{\sqrt{\ip{v}{v}}}v}\right|^2
		\leq \ip{u}{u},
	\]
	ami a bizonyítandó állítás.
\end{proof}
\begin{definition}[skalárisszorzat indukálta norma]
	Legyen $\left( V,\ip{\cdot}{\cdot} \right)$ skalárisszorzatos-tér,
	jelölje minden $x\in V$ mellett $\|x\|=\sqrt{\ip{x}{x}}$.
\end{definition}
\begin{definition}[norma indukálta metrika]
	Legyen $\left( V,\ip{\cdot}{\cdot} \right)$ skalárisszorzatos-tér,
	tetszőleges $x,y\in V$ mellett jelölje $d\left( x,y \right)=\|x-y\|$.
\end{definition}


\begin{proposition}
	Egy $V$ skalárisszorzatos-térben teljesül a \emph{Schwartz-egyenlőtlenség}, azaz
	\(|\ip{x}{y}|\leq\|x\|\cdot\|y\|,\)
	valamint a skalárisszorzatos-tér kielégíti a \emph{normált tér} axiómákat, azaz
	\begin{enumerate}
		\item $\|x\|=0$ akkor és csak akkor, ha $x=0$,
		\item $\|\alpha x\|=|\alpha|\|x\|$,
		\item $\|x+y\|\leq\|x\|+\|y\|$,
	\end{enumerate}
	továbbá egy normált tér kielégíti a \emph{metrikus tér} axiómákat, azaz
	\begin{enumerate}
		\item $d\left( x,y \right)=0\iff x=y$,
		\item $d\left( x,y \right)=d\left( y,x \right)$,
		\item $d\left( x,z \right)\leq d\left( x,y \right)+d\left( y,z \right)$.
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Egyedül a 3. normált tér axióma nem teljesen evidens.
	Emlékezzünk arra, hogy egy $\Re z\leq |z|$ minden $z\in\mathbb{K}$ szám mellett.
	Így
	\begin{multline*}
		\|x+y\|^2
		=
		\ip{x+y}{x+y}
		=
		\ip{x}{x}+\ip{x}{y}+\ip{y}{x}+\ip{y}{y}
		=\|x\|^2+\|y\|^2+2\Re\ip{x}{y}
		\\
		\leq
		\|x\|^2+\|y\|^2+2|\ip{x}{y}|
		\leq
		\|x\|^2+\|y\|^2+\|x\|\|y\|
		=\left( \|x\|+\|y\| \right)^2.
	\end{multline*}
	Mindkét oldal gyökét véve, kapjuk a háromszög-egyenlőtlenséget.
\end{proof}

\section{Pont és altér távolsága}
Ebben a szakaszban azt mutatjuk meg, hogy ha a $V$ skalárisszorzatos térnek $M$ egy olyan
altere, amelyben van véges ortonormált bázis, akkor a tér tetszőleges $V$ pontjának,
van $M$-hez legközelebbi pontja.
Mi több, ilyen legközelebbi pontból csak egyetlen egy van.

Ehhez először a jól ismert Pythagoras-tételt kell megfogalmaznunk.

\begin{proposition}[Pythagoras]\index{Pythagoras-tétel}
	Ha $u\perp v$, akkor $\|u+v\|^2=\|u\|^2+\|v\|^2$.
	Hasonlóan, ha $\left\{ u_1,\cdots,u_k \right\}$ ortogonális rendszer, akkor
	$\|\sum_{i=1}^ku_i\|^2=\sum_{i=1}^k\|u_i\|^2$.
\end{proposition}
\begin{proof}
	Világos, hogy ha $\ip{u}{v}=0$,
	akkor
	\begin{math}
		\|u+v\|^2
		=
		\ip{u+v}{u+v}
		=\ip{u}{u}+\ip{u}{v}+\ip{v}{u}+\ip{v}{v}
		=
		\|u\|^2+\|v\|^2.
	\end{math}
	Most tegyük fel, hogy $k$-nál kisebb elemszámú ortogonális rendszerre igaz az állítás
	és lássuk be $k$-ra.
	Világos, hogy
	\begin{math}
		\ip{\sum_{j=1}^{k-1}u_j}{u_k}=0.
	\end{math}
	Így a már bizonyított állítás és az indukciós feltevés szerint
	\[
		\|\sum_{j=1}^ku_j\|^2
		=
		\|\left( \sum_{j=1}^{k-1}u_j \right)+u_k\|^2
		=
		\|\sum_{j=1}^{k-1}u_j\|^2+\|u_k\|^2
		=
		\sum_{j=1}^{k-1}\|u_j\|^2+\|u_k\|^2
		=
		\sum_{j=1}^k\|u_j\|^2.
		\qedhere
	\]
\end{proof}

\begin{proposition}[legközelebbi pont]\index{legközelebbi pont}\label{pr:legkozelebb}
	Legyen $\left\{ x_1,\cdots,x_k \right\}$ ortonormált rendszer és
	jelölje $M=\lin\left\{ x_1,\cdots,x_k \right\}$.
	Ekkor minden $u\in V$ vektorhoz létezik egyetlen $u^\ast\in M$
	vektor, amelyre
	\[
		d\left( u,u^\ast\right)
		=
		\inf\left\{ d\left( u,w \right):w\in M \right\}.
	\]
	Erre az $M$ altérbeli $u^\ast$ vektorra teljesül az $u^\ast=\sum_{i=1}^k\ip{u}{x_i}x_i$ egyenlőség.
\end{proposition}
Ez azt jelenti,
hogy minden $u\in V$ mellett a
$\sum_{i=1}^k\ip{u}{x_i}x_i$
vektor ez egyetlen olyan $M$-beli vektor, amelyre
\[
	d\left( u,\sum_{i=1}^k\ip{u}{x_i}x_i \right)
	=
	\min\left\{ d\left( u,w \right):w\in M \right\}.
\]
Ezt az
\begin{math}
	\|u-\sum_{j=1}^k\ip{u}{x_j}x_j\|
\end{math}
számot nevezzük, az \emph{$u$ vektor és az $M$ altér távolságának.}
\begin{proof}
	Minden $w\in M$ mellett az $\hat{u}=u-\sum_{j=1}^k\ip{u}{x_j}x_j$ vektorra
	$\hat{u}\perp \left( \sum_{j=1}^k\ip{u}{x_j}x_j-w \right)$,
	hiszen $\sum_{j=1}^k\ip{u}{x_j}x_j-w\in M$.
	Így a Pythagoras-tétel szerint
	\begin{multline*}
		d\left( u,w \right)^2
		=
		\|u-w\|^2
		\\
		=
		\|u-\sum_{j=1}^k\ip{u}{x_j}x_j+\sum_{j=1}^k\ip{u}{x_j}x_j-w\|^2
		=
		\|\hat{u}+\left( \sum_{j=1}^k\ip{u}{x_j}x_j-w \right)\|^2
		\\
		=
		\|\hat{u}\|^2+\|\sum_{j=1}^k\ip{u}{x_j}x_j-w\|^2.
	\end{multline*}
	A jobboldali első kifejezés $\|\hat{u}\|^2$ független $w\in M$-től,
	ezért
	\begin{math}
		d\left( u,w \right)^2
		\geq
		\|\hat{u}\|^2.
	\end{math}
	Itt szigorú reláció áll fenn, minden $w\in M$-re, amelyre
	$w\neq\sum_{j=1}^k\ip{u}{x_j}x_j $,
	és persze egyenlőség áll fenn, ha
	$w=\sum_{j=1}^k\ip{u}{x_j}x_j $.
	Ez azt jelenti,
	hogy
	\[
		\min\left\{ d\left( u,w \right):w\in M \right\}
		=
		\|\hat{u}\|
		=
		\| u-\sum_{j=1}^k\ip{u}{x_j}x_j\|
		=
		d\left( u,\sum_{j=1}^k\ip{u}{x_j}x_j\right),
	\]
	és az $u^\ast=\sum_{j=1}^k\ip{u}{x_j}x_j $ az egyetlen olyan $M$-beli pont, amelyre
	az
	$\|u-u^\ast\|=\min\left\{ \|u-w\|:w\in M  \right\}$ egyenlőség fennáll.
	Ezt kellett belátni.
\end{proof}

Láttuk tehát, hogy ha az $M$ altérben található véges ortonormált bázis,
akkor tetszőleges ponthoz van egy és csak egy legközelebbi pontja az $M$ altérnek.
Ez a korábbi tapasztalataink szerint egy kicsit sem meglepő.
Az már sokkal érdekesebb, hogy mivel csak egy legközelebbi pont van, ezért
ha az $M$ altérben egy másik ortonormált bázist veszünk kiindulásul, akkor az azzal képzett
$\sum_{j=1}^k\ip{u}{x_j}x_j$ vektor is a ugyanazt a vektort, az $u$-hoz legközelebbi $M$-beli vektort adja.
Ez formálisan nézve már meglepő, hiszen az
\[
	\sum_{j=1}^k\ip{u}{x_j}x_j
\]
vektor felírása szerint függ az $\left\{ x_1,\ldots,x_k \right\}\subseteq M$ ortonormált bázis megválasztásától,
ugyanis a fent kiemelt képletben szerepelnek a bázis elemei.
Viszont tudjuk, hogy a fenti vektor az $u$-hoz legközelebb eső vektora $M$-nek, amiből persze csak egy van,
emiatt a fenti kiemelt $M$-beli vektor nem függ az $M$ altér ortonormált bázisának megválasztásától.
\begin{definition}[merőleges vetület, merőleges vetítés]
	Legyen $M=\lin\left\{ x_1,\cdots,x_k \right\}$,
	ahol $\left\{ x_1,\cdots,x_k \right\}$ ortonormált rendszer.

	Láttuk, hogy a
	$\sum_{i=j}^k\ip{u}{x_j}\in M$ vektor független
	az $\left\{ x_1,\cdots,x_k \right\}$ ortonormált rendszer megválasztásától,
	hiszen ez vektor az $u$ vektorhoz legközelebb eső vektora az $M$ altérnek.
	Definiálja $p_M:V\to V$ az alábbi függvényt
	\[
		p_M\left( u \right)=
		\sum_{i=1}^k\ip{u}{x_i}x_i.
	\]
	A $p_M\left( u \right)\in M$ vektort nevezzük az $u$-nak $M$ altérre eső \emph{merőleges vetületének}.
	A $p_M$ függvény neve: \emph{merőleges vetítés az $M$ altérre}.
\end{definition}
Mivel a skalárisszorzás rögzített második változó mellett,
az első változó lineáris függvénye,
ezért egy rögzített $x\in V$ vektor mellett az $u\mapsto\ip{u}{x}x$ függvény egy $V\to V$ lineáris operáció.
Mivel $p_M$ ilyenek véges összege, azért a fent definiált merőleges vetítés a $V$ vektortér egy lineáris transzformációja.
\section{Ortogonalizáció}
A merőleges vetítés operáció bevezetésének feltétele volt, hogy ha az $M$ altérre vonatkozó
merőleges vetítést akarjuk használni, akkor léteznie kell egy ortonormált bázisnak az $M$ altérben.
Az eddigi tételeinkben ez a tétel feltevése volt.
Most megmutatjuk, hogy egy skalárisszorzatos térnek minden véges dimenziós altere rendelkezik ezzel a
tulajdonsággal, így egy véges dimenziós altérre eső merőleges vetítés mindig definiált.

Először is érdemes újra fogalmazni a Bessel-egyenlőtlenségről szóló \ref{pr:Bessel} gondolatot,
a bevezetett merőleges vetítés és hosszúság fogalom felhasználásával.
Figyeljünk arra, hogy az $M$-beli ortonormált bázis léte, még mindig az állítás feltevése.
\begin{proposition}[Bessel]\index{Bessel-egyenlőtlenség}\label{pr:Bessel2}
	Legyen $\left( V,\ip{\cdot}{\cdot} \right)$ skalárisszorzatos-tér.
	Tegyük fel, hogy $\left\{ x_1,\cdots,x_k \right\}$ egy ortonormált rendszer,
	$M=\lin\left\{ x_1,\cdots,x_k \right\}$
	és $p_M\in L\left( V \right)$ a merőleges vetítés az $M$ altérre.
	Ekkor minden $u\in V$ mellett
	\begin{enumerate}
		\item
		      $\|p_M\left( u \right)\|\leq\|u\|$,
		\item
		      \(
		      u-p_M(u)\in M^\perp.
		      \)
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Világos, hogy
	$
		\|p_M\left( u \right)\|^2
		=
		\|\sum_{j=1}^k\ip{u}{x_j}x_j\|^2
		=
		\sum_{j=1}^k\|\ip{u}{x_j}x_j\|^2
		=
		\sum_{j=1}^k|\ip{u}{x_j}|^2
		\leq
		\ip{u}{u}
		=
		\|u\|^2
	$
	a \ref{pr:Bessel}.~állítás szerint.
	Hasonlóan
	$
		u-p_M\left( u \right)\in \left\{ x_1,\ldots,x_k \right\}^\perp
		=
		\lin\left\{ x_1,\ldots,x_k \right\}^\perp
		=
		M^\perp.
	$
\end{proof}

\begin{proposition}[Gram\,--\,Schmidt-ortogonalizáció]\index{Gram\,--\,Schmidt-ortogonalizáció}\label{pr:GS}
	Legyen a $V$ skalárisszorzatos-térben $\left\{ y_1,\cdots,y_k \right\}$ egy lineárisan független
	vektorrendszer.
	Ehhez létezik olyan $\left\{ x_1,\cdots,x_k \right\}$ ortonormált rendszer,
	amelyre minden $i=1,\cdots,k$ mellett $x_i\in\lin\left\{ y_1,\cdots,y_i \right\}$.
\end{proposition}
\begin{proof}
	Teljes indukció, a lineárisan független rendszer elemszáma szerint.
	Ha $k=1$, akkor $y_1\neq 0$, így az $x_1=\frac{1}{\|y_1\|}y_1$ jelöléssel az
	$\left\{ x_1 \right\}$ egy egyelemű ortonormált rendszer, amelyre $x_1\in\lin\left\{ y_1 \right\}$ is teljesül.

	Tegyük fel, hogy igaz az állítás $k$-nál kisebb elemszámú lineárisan független rendszerre és lássuk be $k\geq 2$-re.
	Jelölje $M=\lin\left\{ y_1,\ldots,y_{k-1} \right\}$.
	Az indukciós feltevés szerint létezik $\left\{ x_1,\ldots,x_{k-1} \right\}$ ortonormált rendszer,
	amelyre még az
	\begin{math}
		\left\{ x_1,\ldots,x_{k-1} \right\}\subseteq M
	\end{math}
	tartalmazás is fennáll.
	No de, $\dim(M)=k-1$, ezért $\left\{ x_1,\ldots,x_{k-1} \right\}$ nem csak lineárisan független,
	de maximális lineárisan független rendszer $M$-ben, ergo generátorrendszer is, ezért
	\[
		\lin\left\{ x_1,\ldots,x_{k-1} \right\}=M
	\]
	is teljesül.
	Találtunk tehát az $M$ altérben egy ortonormált bázist,
	így evvel az ortonormált bázissal felírt merőleges vetítés értelmesen definiált.
	Legyen ez $p_M$, azaz minden $u\in V$ vektorra $p_M\left( u \right)=\sum_{j=1}^{k-1}\ip{u}{x_j}x_j$.
	Tekintsük az
	\[
		\hat{y}_k=y_k-p_M\left( y_k \right)
	\]
	vektort.
	Ha ez a zérus vektor lenne, akkor $y_k\in M$ lenne, ami ellentmondana az $\left\{ y_1,\ldots,y_k \right\}$ rendszer
	lineárisan függetlenségének.
	Ezek szerint az
	\[
		x_k
		=
		\frac{1}{\|\hat{y}_k\|}\hat{y}_k
		%        =
		%        \frac{1}{\|y_k-p_M\left( y_k \right)\|}\left( y_k-p_M\left( y_k \right) \right)
	\]
	vektorra $\|x_k\|=1$, $x_k\in M^\perp$, ezért az $\left\{ x_1,\ldots,x_k \right\}$ vektorrendszer valóban ortonormált.

	Már csak azt kell igazolnunk, hogy minden $j\leq k$ mellett $x_j\in\lin\left\{ y_1,\ldots,y_j \right\}$.
	Ez a $j\leq k-1$ esetben az indukciós feltevés miatt van így.
	A $j=k$ esetre $p_M\left( y_k \right)\in M=\lin\left\{ y_1,\ldots,y_{k-1} \right\}$,
	emiatt $\hat{y}_k\in\lin\left\{ y_1,\ldots,y_k \right\}$,
	amiből persze egy $\mathbb{K}$ testbeli elemmel való szorzás után már $x_k\in\lin\left\{ y_1,\ldots,y_k \right\}$ is következik.
\end{proof}
A fenti bizonyitás kiemelt soraiból leolvasható az ú.n. \emph{Gram\,--\,Schmidt-ortogonalizációs algoritmus}.
Ha adott az $\left\{ y_1,\ldots,y_n \right\}$ lineárisan független vektorok rendszere, akkor abból egy
$\left\{ x_1,\ldots,x_n \right\}$ ortonormált rendszer konstruálható.
Az első lépés:
\[
	x_1=\frac{1}{\|y_1\|}y_1.
\]
Ha az első $k-1$ darab vektort már definiáltuk úgy, hogy $\left\{ x_1,\ldots,x_{k-1} \right\}$ ortonormált rendszer,
akkor a $k$-adik vektort a következőképpen kapjuk.
Legyen $\hat{y}_k=y_k-p_M\left( y_k \right)$,
ahol
$p_M\left( y_k \right)=\sum_{j=1}^{k-1}\ip{y_k}{x_j}x_j,$
majd
\[
	x_k=\frac{1}{\|\hat{y}_k\|}\hat{y}_k.
\]

A Gram\,--\,Schmidt-ortogonalizációról szóló állítás azonnali következménye az ortonormált bázisok egzisztenciáját biztosító állítás:
\begin{proposition}[o.n.b]\index{o.n.b}\label{pr:onb}
	Egy véges dimenziós skalárisszorzatos-térnek mindig van ortonormált bázisa, azaz olyan $\left\{ x_1,\ldots,x_n \right\}$ ortonormált
	rendszere, amely generátorrendszer is.

	Ha $\left\{ x_1,\ldots,x_n \right\}$ egy ortonormált bázis, akkor minden $v\in V$ mellett
	\(
	v=
	\sum_{j=1}^n\ip{v}{x_j}x_j.
	\)
\end{proposition}
\begin{proof}
	Tudjuk, hogy $V$ vektortérnek van $\left\{ y_1,\ldots,y_n \right\}$ bázisa.
	Vegyünk egy ilyen bázist, tehát lineárisan független rendszert, majd alkalmazzuk a \ref{pr:GS}.~állítást.
	Így kapunk egy
	\begin{math}
		\left\{ x_1,\ldots,x_n \right\}\subseteq V
	\end{math}
	ortonormált rendszert.
	No de, $\dim(V)=n$ és az ortonormált rendszer elemei közt a zérus vektor nem szerepel,
	így az ortonormált rendszer lineárisan független rendszer is.
	Mivel annak $n=\dim(V)$ darab eleme van,
	ezért az maximális lineárisan független rendszer $V$-ben, ergo bázis is.
	Konstruáltunk tehát egy ortonormált bázist.

	Jelölje most $P_V$ egy $\left\{ x_1,\ldots,x_n \right\}$
	ortonormált bázis definiálta merőleges vetítést az egész $V$ vektortérre, mint egy altérre.
	Világos, hogy $P_V$ az identitás függvény,
	hiszen minden vektorra igaz, hogy saját maga a hozzá legközelebbi $V$-beli pont.
	Így minden $v\in V$ mellett $v=p_V\left( v \right)=\sum_{j=1}^n\ip{v}{x_j}x_j.$
\end{proof}
Ez utóbbi gondolat szerint,
ha egy $\left\{ x_1,\ldots,x_n \right\}$ ortonormált bázis rögzítünk,
akkor tetszőleges $x\in V$ vektornak ebben a bázisban felírt koordináta-vektora
\[
	[x]_{\left\{x_1,\ldots,x_n\right\}}
	=
	\begin{pmatrix}
		\ip{x}{e_1} \\ \vdots \\\ip{x}{e_n}
	\end{pmatrix}
\]
Ugyanez, kicsit eltérő szintaxissal, hogy az ortonormált bázishoz tartozó
$i$-edik duális lineáris funkcionál\index{duális bázis},
amit korábban $x_i^\ast$-al jelöltünk, az $x\mapsto\ip{x}{x_i}$
függvény.
\section{Projekciós tétel}
Láttuk tehát, hogy egy véges dimenziós altérnek mindig van ortonormált bázisa.
Ezért ha egy véges dimenziós altérre való merőleges vetítést akarunk használni,
nem kell kikötnünk, hogy legyen az altérben egy ortonormált bázis, hiszen ilyen mindig van.
Persze ahogyan bázis is sok van, ugyanúgy ortonormált bázisból is nagyon sok lehet.
Meggondoltuk viszont, hogy függetlenül attól, hogy az $\left\{ x_1,\ldots,x_k \right\}\subseteq M$
vagy az $\left\{ y_1,\ldots,y_k \right\}\subseteq M$ ortonormált bázis segítségével írjuk fel az $M$
altérre vonatkozó merőleges vetítést a vetület mindig azonos, nevezetesen a ponthoz az $M$-beli legközelebbi pont:
\[
	\sum_{j=1}^{k}\ip{u}{x_j}x_j
	=
	P_M\left( u \right)
	=
	\sum_{j=1}^{k}\ip{u}{y_j}y_j.
\]

Ennek fényében először is érdemes újra és újra fogalmazni a Bessel-egyenlőtlenségről szóló \ref{pr:Bessel2}.~ gondolatot.
Figyeljünk arra, hogy az $M$-beli ortonormált bázis léte nem szerepel a feltételek közt,
hanem helyette az altér véges dimenziós volta szerepel.%
\footnote{Majd mikor funkcionálanalízist és mértékelméletet tanulunk látjuk, hogy a feltétel még tovább gyengíthető,
	arra az esetre, mikor $M$ a $V$ Hilbert-tér egy zárt altere.}
\begin{proposition}[Bessel]\index{Bessel-egyenlőtlenség}\label{pr:Bessel3}
	Legyen $\left( V,\ip{\cdot}{\cdot} \right)$ skalárisszorzatos-tér, és $M$ egy
	véges dimenziós altere.
	Ekkor minden $u\in V$ -hez
	\begin{enumerate}
		\item létezik egyetlen $u^\ast\in M$ vektor, amelyre
		      \(
		      d\left( u,u^\ast\right)
		      =
		      \min\left\{ d\left( u,w \right):w\in M \right\}.
		      \)
		      Erre a vektorra teljesül az $u^\ast=p_M\left( u \right)=\sum_{i=1}^k\ip{u}{x_i}x_i$ egyenlőség,
		      az $M$ altér tetszőlegesen választott  $\left\{ x_1,\ldots,x_k \right\}$ ortonormált bázisa mellett;
		\item
		      $\|p_M\left( u \right)\|\leq\|u\|$;
		\item
		      \(
		      u-p_M(u)\in M^\perp.
		      \)
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Mivel $M$ a $V$ skalárisszorzatos tér egy véges dimenziós altere,
	ezért az $M$ altérnek van
	ortonormált bázisa, amint azt \aref{pr:onb}.~állításban Gram\,--\,Schmidt-ortogonalizációnál láttuk.
	Legyen ez $\left\{ x_1,\ldots,x_k \right\}$.
	Írjuk fel evvel az ortonormált rendszerrel \aref{pr:legkozelebb}. legközelebbi pontról szóló,
	és \aref{pr:Bessel2}. Bessel-egyen\-lőt\-lenség\-ről szóló állításokat.
\end{proof}

\begin{proposition}[projekciós-tétel]\index{projekciós-tétel}\label{pr:projekcios}
	Legyen $V$ egy skalárisszorzatos-tér, és $M$ egy véges dimenziós altere.
	Ekkor
	\[
		M\oplus M^\perp=V.
		\qedhere
	\]
\end{proposition}
\begin{proof}
	Az $M$ és az $M^\perp$ egymástól diszjunkt alterek.
	Tetszőleges $u\in V$ vektorra fennáll az
	\[
		u=p_M\left( u \right)+\left( u-p_M\left( u \right) \right)
	\]
	nyilvánvaló egyenlőség.
	Itt az első vektorra $p_M\left( u \right)\in M$, és a második vektorra $u-p_M\left( u \right)\in M^\perp$
	\aref{pr:Bessel3}.~állítás szerint.
	Ez azt jelenti, hogy $M+M^\perp=V$ azonosság is teljesül,
	ahol $M+M^\perp$ az $M$ és az $M^\perp$ ortokomplementerének a Minkowski-összege.
\end{proof}
\begin{proposition}
	Legyen $V$ egy skalárisszorzatos-tér.
	Ekkor
	\begin{enumerate}
		\item Minden $M\subseteq V$ véges dimenziós altér mellett $M=M^{\perp\perp}$,
		\item Ha a $H\subseteq V$ részhalmazra a $\lin H$ egy véges dimenziós altér, 
            akkor $\lin H=H^{\perp\perp}$.
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Láttuk, hogy $M\subseteq M^{\perp\perp}$ minden halmazra is igaz.
	Legyen a fordított tartalmazás igazolásához,
	$u\in M^{\perp\perp}$.
	A projekciós tétel miatt $u=u_1+u_2$,
	ahol $u_1\in M$ és $u_2\in M^{\perp}$.
	Így
	\[
		\ip{u_2}{u_2}
		=
		\ip{u-u_1}{u_2}
		=
		\ip{u}{u_2}-\ip{u_1}{u_2}=0.
	\]
	Itt a jobboldali első szám azért zérus,
	mert $u\in M^{\perp\perp}$ és $u_2\in M^\perp$,
	a második meg azért zérus,
	mert $u_1\in M$ és $u_2\in M^\perp$.
	Azt kaptuk tehát, hogy $u_2=0$, amiből már látszik is, hogy $u=u_1\in M$.

	Tetszőleges $H\subseteq V$ mellett $\lin H$ egy véges dimenziós altér,
	így a már igazolt állítás miatt
	$\lin H=(\lin H)^{\perp\perp}$.
	No de, már korábban is láttuk, hogy
	$H^{\perp}=(\lin H)^\perp$, így
	\[
		\lin H
		=
		(\lin H)^{\perp\perp}
		=
		((\lin H)^\perp)^\perp=(H^{\perp})^\perp=H^{\perp\perp}.
		\qedhere
	\]
\end{proof}
\section{Ortonormált rendszer teljessége}
\begin{definition}[T.O.N.R.]
	Legyen $\left\{ e_1,\cdots,e_k \right\}$ egy ortonormált rendszer a $V$ skalárisszorzatos-térben.
	Azt mondjuk, hogy ez egy \emph{teljes ortonormált rendszer}, ha
	\[
		\ip{u}{e_j}=0\quad\forall j=1,\cdots,k \implies u=0
	\]
	implikáció teljesül.
\end{definition}
Először is vegyük észre, hogy a fenti implikáció ekvivalens az
$\left\{ e_1,\cdots,e_k \right\}^\perp =\left\{ 0 \right\}$
feltétellel.

\begin{proposition}
	Legyen $V$ egy skalárisszorzatos-tér,
	és tekintsünk egy a továbbiakban rögzített
	$\left\{ e_1,\ldots,e_n \right\}$ egy ortonormált rendszert.
	Az alábbi feltevések ekvivalensek.
	\begin{enumerate}
		\item
		      Az $\left\{ e_1,\ldots,e_n \right\}$ vektorrendszer teljes ortonormált rendszer;
		\item
		      Az $\left\{ e_1,\ldots,e_n \right\}$ vektorrendszer ortonormált bázis;
		\item
		      Minden $x\in V$ mellett
		      \(
		      x=
		      \sum_{j=1}^n\ip{x}{e_j}e_j;
		      \)
		\item
		      Minden $x,y\in V$ mellett
		      \(
		      \ip{x}{y}=
		      \sum_{j=1}^n\ip{x}{e_j}\overline{\ip{y}{e_j}};
		      \)
		\item
		      Minden $x\in V$ mellett
		      \(
		      \|x\|^2=
		      \sum_{j=1}^n|\ip{x}{e_j}|^2.
		      \)
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Körbe bizonyítunk:
	\begin{description}
		\item[$1.\Rightarrow 2.$]
		      Mivel egy ortonormált rendszer egy zérus elemet nem tartalmazó ortogonális rendszer,
		      ezért lineárisan független.
		      Jelölje $H=\left\{ e_1,\ldots,e_n \right\}$.
		      Világos, hogy $\lin H$ véges dimeniós, ezért a projekciós tétel előző következménye miatt
		      $\lin H=H^{\perp\perp}$.
		      No de, a teljesség szerint $H^\perp=\left\{ 0 \right\}$,
		      ergo
		      \[
			      \lin H
			      =
			      H^{\perp\perp}
			      =
			      {0}^\perp
			      =
			      V.
		      \]
		      Ez azt jelenti, hogy a $H$ vektorrendszer egy lineárisan független generátorrendszer, ergo bázis.

		\item[$2.\Rightarrow 3.$]
		      Már beláttuk \aref{pr:onb}.~állítás második felében.

		\item[$3.\Rightarrow 4.$]
		      Jelölje $x\in V$ mellett $\xi_j=\ip{x}{e_j}$ és $y\in V$ mellett
		      $\eta_j=\ip{y}{e_j}$.
		      A 3. feltevés szerint
		      \[
			      \ip{x}{y}
			      =
			      \ip{\sum_{j=1}^n\xi_je_j}{\sum_{k=1}^n\eta_ke_k}
			      =
			      \sum_{j=1}^n\sum_{k=1}^n\xi_j\overline{\eta_k}\ip{e_j}{e_k}
			      =
			      \sum_{j=1}^n\sum_{k=1}^n\xi_j\overline{\eta_k}\delta_{j,k}
			      =
			      \sum_{j=1}^n\xi_j\overline{\eta_j}
			      =
			      \sum_{j=1}^n\ip{x}{e_j}\overline{\ip{y}{e_j}}.
		      \]

		\item[$4.\Rightarrow 5.$]
		      Az $x=y$ speciális esetben 4. szerint
		      \[
			      \|x\|^2
			      =
			      \ip{x}{x}
			      =
			      \sum_{j=1}^n\ip{x}{e_j}\overline{\ip{x}{e_j}}
			      =
			      \sum_{j=1}^n|\ip{x}{e_j}|^2.
		      \]

		\item[$5.\Rightarrow 1.$]
		      Ha fennáll 5. és $v\in\left\{ e_1,\ldots,e_n \right\}^\perp$,
		      akkor minden $j$ mellett $\ip{v}{e_j}=0$, ezért
		      \[
			      \|v\|^2
			      =
			      \sum_{j=1}^n\ip{v}{e_j}e_j
			      =
			      0.
		      \]
		      emiatt $v=0$ is fennáll.
		      Ez azt jelenti, hogy $\left\{ e_1,\ldots,e_n \right\}^\perp=\left\{ 0 \right\}$,
		      ergo az $\left\{ e_1,\ldots,e_n \right\}$ ortonormált rendszer teljes.\qedhere
	\end{description}
\end{proof}
A fejezet elején láttunk konkrét példát véges dimenziós skalárisszorzatos-térre:
egy rögzített bázisban felírt belsőszorzatot. Persze ebben a skalárisszorzatban,
a kiinduló bázis ortonormált bázissá is válik.
Most azt látjuk, hogy a fenti példán kívül nem is lehet más skalárisszorzatot definiálni.
Minden skalárisszorzat ugyanis, valamely bázishoz tartozó belső szorzat.
Pontosan ezt állítja a fenti 4-es tulajdonság.

Ezt a tényt a fontossága miatt külön is kiemeljük:
\begin{corollary}
	Minden véges dimenziós skalárisszorzatos-térben a skalárisszorzat nem más,
	mint erre a skalárisszorzatra nézve,
	de tetszőleges ortonormált bázishoz felírt belső szorzat.
\end{corollary}

\chapter{Az adjungált operátor bevezetése}
Az egész fejezetben $V$ egy $\mathbb{K}$ feletti vektortér.
\section{Riesz-reprezentáció véges dimenziós skalárisszorzatos-térben}\index{Riesz-reprezentáció}
\begin{proposition}[Riesz-reprezentació]\index{Riesz-reprezentáció}\label{pr:Riesz}
	Legy $V$ egy véges dimenziós skalárisszorzatos-tér.
	Ekkor minden $\varphi:V\to\mathbb{K}$ lineáris funkcionálhoz létezik egyetlen $z_\varphi\in V$ vektor,
	amelyre minden $x\in V$ mellett
	\[
		\varphi\left( x \right)=\ip{x}{z_\varphi}
		\qedhere
	\]
\end{proposition}
Ezt az egyetlen $z_\varphi$ vektort nevezzük a $\varphi$ lineáris funkcionált \emph{reprezentáló} vektornak.
\begin{proof}
	Először az unicitást lássuk be.
	Tegyük fel, hogy $z_1,z_2\in V$ is reprezentáló vektorok,
	azaz minden $x\in V$ mellett
	\[
		\ip{x}{z_1}
		=
		\varphi\left( x \right)
		=
		\ip{x}{z_2}.
	\]
	Ekkor $(z_1-z_2)\perp x$ minden $x\in V$-re tehát $z_1-z_2=0$,
	ergo $z_1=z_2$.

	Az egzisztencia igazolásához legyen $\left\{ v_1,\ldots,v_n \right\}$ egy ortonormált bázisa a $V$ skalárisszorzatos-térnek.
	Tudjuk -- lásd \aref{pr:onb}.~állítást --, hogy minden $x\in V$ vektor
	\begin{math}
		x=
		\sum_{j=1}^n\ip{x}{v_j}v_j
	\end{math}
	alakú.
	Emiatt
	\[
		\varphi\left( x \right)
		=
		\varphi\left(
		\sum_{j=1}^n\ip{x}{v_j}v_j
		\right)
		=
		\sum_{j=1}^n
		\ip{x}{v_j}
		\varphi\left(
		v_j
		\right)
		=
		\sum_{j=1}^n
		\ip{x}{\overline{\varphi\left(
				v_j\right)}
			v_j}
		=
		\ip{x}{
			\sum_{j=1}^n
			\overline{\varphi\left(
				v_j\right)}
			v_j}.
	\]
	Ha tehát bevezetjük a
	\begin{math}
		z_\varphi
		=
		\sum_{j=1}^n
		\overline{\varphi\left(
			v_j\right)}v_j
	\end{math}
	jelölést, akkor készen is vagyunk.
\end{proof}
A bizonyításból az is látszik, hogy amennyiben
$\left\{ v_1,\ldots,v_n \right\}$ a tér akármelyik ortonormált bázisa,
akkor a reprezentáló vektort a
\[
	z_\varphi=
	\sum_{i=1}^n\overline{\varphi\left( v_i \right)}v_i
\]
formula szolgáltatja.
Mivel csak egyetlen reprezentáló vektor van, ezért ha például
$\left\{ x_1,\ldots,x_n \right\}$ egy másik ortonormált bázisa a térnek,
akkor
\[
	\sum_{i=1}^n\overline{\varphi\left( v_i \right)}v_i
	=
	z_\varphi
	=
	\sum_{i=1}^n\overline{\varphi\left( x_i \right)}x_i.
\]
\section{Az adjungált operáció}
\begin{definition}[adjungált, adjungált azonosság]\index{adjungált}\index{adjungált azonosság}\index{Riesz-reprezentáció}
	Legyen $\left( V,\ip{\cdot}{\cdot}_V \right)$ és
	$\left( W,\ip{\cdot}{\cdot}_W \right)$ véges dimenziós ska\-láris\-szor\-zatos-tér.
	Rögzített $A\in L\left( V,W \right)$ lineáris operátor és rögzített $w\in W$ vektor mellett tekintsük a
	\[
		\varphi_w=
		\ip{\cdot}{w}_W\circ A
	\]
	függvényt.
	Világos, hogy $\varphi_w:V\to\mathbb{K}$ lineáris funkcionál $V$-n,
	így a Riesz-reprezentációs tétel szerint létezik egyetlen $w^\ast\in V$,
	amelyre
	$\varphi_w\left( v \right)=\ip{v}{w^\ast}_V$ minden $v\in V$ mellett teljesül.

	Jelölje most $A^\ast$ a $w\mapsto w^\ast$ függvényt, tehát
	$A^\ast:W\to V$.
	Ekkor minden $v\in V$ és $w\in V$ mellett fennáll az ú.n. \emph{adjungált azonosság}:
	\[
		\ip{Av}{w}_W
		=
		\varphi_w\left( v \right)
		=
		\ip{v}{w^\ast}_V
		=
		\ip{v}{A^\ast w}_V.\tag{\dag}
	\]
	Ezt az $A^\ast:W\to V$ függvényt nevezzük az $A$ lineáris transzformáció \emph{adjungáltjának}.
\end{definition}
\begin{proposition}
	Legyenek $V,W$ skalárisszorzatos-terek, és $A\in L\left( V,W \right)$ lineáris operátor.
	Ekkor az $A^\ast$ adjungált az egyetlen $W\to V$ függvény,
	amelyre a (\dag) adjungált azonosság fennáll.
\end{proposition}
\begin{proof}
	Legyen $B:W\to V$ olyan függvény, amelyre szintén fennáll adjungált azonosság,
	azaz
	\[
		\ip{v}{Bw}
		=
		\ip{Av}{w}
		=
		\ip{v}{A^\ast w}
	\]
	fennáll minden $v\in V$ és $w\in W$ mellett.
	Mivel a skalárisszorzás rögzített első változó mellett a második változóban additív,
	ezért
	$\ip{v}{\left( A^\ast-B \right)w}=0$ fennáll minden $v\in V$ és $w\in W$ mellett.
	Speciálisan ez igaz $v=\left( A^\ast-B \right)w$-re is, ergo minden $w\in W$-re
	$\left( A^\ast-B \right)w=0$.
	Ez éppen azt jelenti, hogy $A^\ast=B$.
\end{proof}
Látni fogjuk, hogy a (\dag) adjungált azonosság szinte fontosabb mint a definíció.
A definíció arra való, hogy megmutassuk: adott $A\in L\left( V,W \right)$ lineáris transzformációhoz van olyan
$A^\ast:W\to V$ függvény, amelyre minden $u\in W$ és minden $w\in W$ mellett fennáll az adjungált azonosság:
\[
	\ip{Av}{w}
	=
	\ip{v}{A^\ast w}.
	\tag{\dag}
\]
Az adjungált unicitása, és összes fontos tulajdonsága az adjungált azonosságon alapul.
\paragraph{Az adjungált függvény linearitása.}
\Aref{pr:Riesz}. Riesz-reprezentációs tétel során meggondoltuk,
hogy ha tetszőlegesen adott egy
$\left\{ v_1,\ldots,v_n \right\}\subseteq V$
ortonormált bázis, akkor
\[
	A^\ast w
	=
	\sum_{j=1}^n\overline{\varphi_w\left( v_j \right)}v_j
	=
	\sum_{j=1}^n\overline{\ip{Av_j}{w}}_Wv_j
	=
	\sum_{j=1}^n\ip{w}{Av_j}_Wv_j.
\]
Ebből már adódik, hogy $A^\ast:W\to V$ lineáris operátor.
Ugyanis a skalárisszorzat rögzített második változó mellett az első változóban lineáris,
emiatt egy rögzített $j$ esetén az $w\mapsto \ip{w}{Av_j}v_j$ a $w$ változó lineáris függvénye. A fenti kiemelt sor szerint $A^\ast$ ilyen függvények összegeként maga is lineáris.

Alternatív indoklást kapunk az adjungált operáció linearitására az (\dag) adjungált azonosság háromszori felírásával.
Itt kihasználjuk a tényt, hogy a skalárisszorzás rögzített első változó mellett a második változó konjugáltan lineáris függvénye.
\begin{multline*}
	\ip{u}{A^\ast\left( \alpha_1w_1+\alpha_2w_2 \right)}
	=
	\ip{Au}{\alpha_1w_1+\alpha_2w_2}
	=\\
	\overline{\alpha}_1\ip{Au}{w_1}+\overline{\alpha}_2\ip{Au}{w_2}
	=
	\overline{\alpha}_1\ip{u}{A^\ast w_1}+\overline{\alpha}_2\ip{u}{A^\ast w_2}
	=
	\ip{u}{\alpha_1 A^\ast w_1}+\ip{u}{\alpha_2 A^\ast w_2}
	=\\
	\ip{u}{\alpha_1 A^\ast w_1+\alpha_2 A^\ast w_2}.
\end{multline*}
Mivel ez minden $u\in V$ és minden $w_1,w_2\in W$ esetén fennáll,
ezért az
\begin{displaymath}
	A^\ast\left( \alpha_1w_1+\alpha_2w_2 \right)
	=
	\alpha_1A^\ast w_1+ \alpha_2A^\ast w_2
\end{displaymath}
azonosság is teljesül.

\begin{proposition}
	Legyen $V$ és $W$ véges dimenziós skalárisszorzatos-tér.
	Rögzítsünk egy $\left\{ v_1,\ldots,v_m \right\}$ ortonormált bázist $V$-ben,
	és rögzítsünk egy $\left\{w_1,\ldots,w_n\right\}$ ortonormált bázist $W$-ben.
	Legyen $A\in L\left( V,W \right)$ egy lineáris operátor és $A^\ast\in L\left( W,V \right)$ az adjungáltja.
	Ekkor $A$-nak a fenti bázisokban rögzített mátrixa azonos
	$A^\ast$ mátrix sorainak és oszlopainak felcserélésével, majd konjugálásával kapott mátrixszal.
	Formálisan:
	\[
		\left[ A \right]_{i,j}=\overline{\left[ A^\ast \right]}_{j,i}
	\]
	minden $j=1,\ldots,m$ és minden $i=1,\ldots,n$ esetén.
\end{proposition}
\begin{proof}
	\begin{math}
		[A]_{i,j}
		=
		\ip{Av_j}{w_i}
		=
		\ip{v_j}{A^\ast w_i}
		=
		\overline{
			\ip{A^\ast w_i}{v_j}
		}
		=
		\overline{
			[A^\ast]
		}_{j,i}
	\end{math},
	\aref{pr:onb}.~állítás, a (\dag) adjungált azonosság, és a skalárisszorzás tulajdonságai szerint.
\end{proof}

Most összefoglaljuk az adjungálás operáció legfontosabb tulajdonságait,
amelyeket későbbi számolásaink során külön hivatkozás nélkül használunk.
\begin{proposition}
	Legyenek $V$ és $W$ véges dimenziós skalárisszorzatos terek, $A,B\in L\left( V,W \right)$ lineáris operátorok.
	Ekkor
	\begin{enumerate}
		\item[1.] $\left( A+B \right)^\ast=A^\ast+B^\ast,$
		\item[2.] $\left( \alpha A \right)^\ast=\overline\alpha A^\ast,$
		\item[3.] $(A^\ast)^\ast=A$
	\end{enumerate}
	Ha $I\in L\left( V \right)$ jelöli az identitás transzformációt, továbbá $A,B\in L\left( V \right)$, akkor
	\begin{enumerate}
		\item[4.] $I^\ast=I$,
		\item[5.] $\left( AB \right)^\ast=B^\ast A^\ast$.
		\item[6.] Az $A$ lineáris operátor pontosan akkor reguláris ha $A^\ast$ adjungált operáció is az.
		      Ebben az estben az adjungált operáció inverze éppen az inverz operáció adjungáltja, magyarul
		      \begin{math}
			      \left( A^\ast \right)^{-1}=\left( A^{-1} \right)^\ast.
		      \end{math}
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Az első öt tulajdonság teljesen nyilvánvaló az adjungált mátrixának felírásából.
	Csak azért, hogy hangsúlyozzuk az adjungált azonosság hasznosságát a mátrixokra való hivatkozások nélküli indoklást is adunk.
	\begin{description}
		\item[$1., 2.$ ]
		      \begin{multline*}
			      \ip{u}{(\alpha A+B)^\ast w}
			      =\\
			      \ip{\left( \alpha A+B \right)u}{w}
			      =
			      \alpha\ip{Au}{w}+\ip{Bu}{w}
			      =
			      \alpha\ip{u}{A^\ast w}+\ip{u}{B^\ast w}
			      =
			      \ip{u}{\overline{\alpha}A^\ast w}+\ip{u}{B^\ast w}
			      =\\
			      \ip{u}{\left( \overline{\alpha}A^\ast +B^\ast \right)w}
		      \end{multline*}
		      Ez minden $\alpha\in\mathbb{K}$, $u\in V$ és minden $w\in W$ mellett, tehát
		      \begin{math}
			      \left( \alpha A+B \right)^\ast=\overline{\alpha}A^\ast+B^\ast.
		      \end{math}
		\item[$3.$]
		      \(
		      \ip{Au}{w}
		      =
		      \ip{u}{A^\ast w}
		      =
		      \overline{
			      \ip{A^\ast w}{u}
		      }
		      =
		      \overline{
			      \ip{w}{A^{\ast\ast} u}
		      }
		      =
		      \ip{A^{\ast\ast} u}{w}.
		      \)
		\item[$4.$]
		      \(
		      \ip{u}{Iv}
		      =
		      \ip{Iu}{v}
		      =
		      \ip{u}{I^\ast v}.
		      \)
		\item[$5.$]
		      \begin{math}
			      \ip{u}{\left( AB \right)^\ast v}
			      =
			      \ip{ABu}{v}
			      =
			      \ip{Bu}{A^\ast v}
			      =
			      \ip{u}{B^\ast A^\ast v}.
		      \end{math}
		\item[$6.$] $A$ pontosan akkor reguláris, ha létezik $B$, amelyre $AB=I$, ami ekvivalens $B^\ast A^\ast=I$-vel,
		      ami ekvivalens avval, hogy $A^\ast$ reguláris.
		      Világos, hogy ebben az esetben $\left( A^\ast \right)^{-1}=B^\ast=\left( A^{-1} \right)^\ast$.
		      \qedhere
	\end{description}
\end{proof}
Felépítésünk sarokköve a skalárisszorzatos terek rangtétele.
Legfontosabb következménye, hogy zérus vektoron kívül nincs olyan vektor,
amely egyszerre $A$ képteréhez is, de $A^\ast$ magteréhez is hozzátartozik.
\begin{proposition}[skalárisszorzatos Rang-tétel]\label{pr:skrang}
	Legyenek $V,W$ skalárisszorzatos-terek, $A\in L\left( V,W \right)$ lineáris operáció.
	Ekkor
	$(\im A)^\perp=\ker A^\ast$ és
	$(\ker A)^\perp=\im A^\ast.$
	Emiatt
	\begin{eqnarray*}
		\im A\oplus\ker A^\ast&=& W;\qquad\qquad\im A\perp\ker A^\ast;\\
		\ker A\oplus\im A^\ast&=& V;\qquad\qquad\ker A\perp\im A^\ast,
	\end{eqnarray*}
	így $\rho\left( A \right)=\rho\left( A^\ast \right)$.
\end{proposition}
\begin{proof}
	Az adjungált azonosság szerint egy $w\in W$ vektorra az alábbi feltevések ekvivalensek
	\begin{multline*}
		w\in\left( \im A \right)^\perp
		\\
		\iff
		\ip{Av}{w}=0 \forall v\in V
		\iff
		\ip{v}{A^\ast w}=0 \forall v\in V
		\iff
		A^\ast w\in V^\perp
		\iff
		A^\ast w=0
		\\
		\iff
		w\in\ker A^\ast,
	\end{multline*}
	ami igazolja az $\left( \im A \right)^\perp=\ker A^\ast$ azonosságot.

	A projekciós tételt alkalmazva:
	\begin{math}
		\im A=(\im A)^{\perp\perp}=(\ker A^\ast)^\perp
	\end{math}.
	Ez persze az $A^\ast$ adjungált operátorra is igaz,
	ergo
	\begin{math}
		\im A^\ast = \left( \ker A \right)^\perp.
	\end{math}

	Újra felírva a projekciós tételt:
	\begin{math}
		W=\im A\oplus\left( \im A \right)^\perp
		=\im A\oplus\ker A^\ast
	\end{math}
	valamint
	\begin{math}
		V=\ker A\oplus\left( \ker A \right)^\perp
		=
		\ker A\oplus\im A^\ast.
	\end{math}

	A direkt összegek dimenziójára vonatkozó állítás és a Rang--defektus\index{rang--defektus--tétel}-tételnek az adjungált operációra vonatkozó alakja szerint
	\begin{math}
		\rho\left( A \right)=\dim(W)-\nu\left( A^\ast \right)=\rho\left( A^\ast \right).
	\end{math}
\end{proof}


\section{Önadjungált transzformációk}
\begin{definition}
	Legyen $V$ egy skalárisszorzatos-tér, és $A\in L\left( V \right)$ egy lineáris transzformáció.
	Azt mondjuk, hogy az $A$ transzformáció
	\emph{önadjungált}\index{ozz@önadjungált transzformáció},
	ha $A^\ast=A$.
\end{definition}

\begin{proposition}
	A skalárisszorzatos-tér egy $A$ lineáris transzformációja pontosan akkor önadjungált,
	ha minden $u,w\in V$ mellett
	\[
		\ip{Au}{v}
		=
		\ip{u}{Av}.
		\qedhere
	\]
\end{proposition}
Ha felírjuk a transzformáció mátrixát egy ortonormált bázisban, akkor messziről látszik,
hogy a transzformáció önadjungált vagy sem.
\begin{proposition}
	A skalárisszorzatos-tér egy $A$ lineáris transzformációja pontosan akkor önadjungált,
	valamely ortonormált bázisban felírt mátrixára
	\[
		[A]_{i,j}=\overline{[A]}_{j,i}.
		\qedhere
	\]
\end{proposition}
Persze a fenti tulajdonság, ha egy ortonormált bázis mellett teljesül,
akkor $A$ önadjungált, ezért minden más ortonormált bázisban is igaz marad.

A skalárisszorzatos-terek rangtétele, -- tehát \aref{pr:skrang}.~állítás -- önadjungált transzformációk mellett azt jelenti,
hogy
a $\ker A$ és az $\im A$ egymásra merőleges alterek,
amelyek direkt összege az egész tér.
Később látjuk majd, hogy ez nem csak önadjungált transzformációkra igaz,
hanem transzformációk egy sokkal szélesebb osztályára\footnote{Lásd a \ref{pr:normalisrang}.~állítást} is teljesül.
\section{Unitér transzformációk}
\begin{definition}
	Legyen $V$ egy skalárisszorzatos-tér, és $B\in L\left( V \right)$ egy lineáris transzformáció.
	Azt mondjuk, hogy $B$
	\emph{unitér}\index{unitér transzformáció},
	ha $B$ reguláris és $B^{-1}=B^\ast$.
\end{definition}
\begin{proposition}
	A $B\in L\left( V \right)$ transzformációra tett alábbi feltevések ekvivalensek.
	\begin{enumerate}
		\item $B$ unitér;
		\item Minden $u,v\in V$ mellett $\ip{Bu}{Bv}=\ip{u}{v}$.
		\item $B$ egy ortonormált bázist ortonormált bázisra képez.
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Körben bizonyítunk:
	\begin{description}
		\item[$1.\Rightarrow 2.$]
		      \begin{math}
			      \ip{Bu}{Bv}
			      =
			      \ip{u}{B^\ast Bv}
			      =
			      \ip{u}{B^{-1}Bv}
			      =
			      \ip{u}{v}.
		      \end{math}
		\item[$2.\Rightarrow 3.$]
		      Legyen $u_1,\ldots,u_n$ egy ortonormált bázis.
		      Ennek $B$ képére
		      \begin{math}
			      \ip{Bu_j}{Bu_k}
			      =
			      \ip{u_j}{u_k}
			      =\delta_{j,k}.
		      \end{math}
		      Ez azt jelenti, hogy a $\left\{ Bu_1,\ldots,Bu_n \right\}$ egy ortonormált rendszer.
		      A tér $n$ dimenziós, hiszen $\left\{ u_1,\ldots,u_n \right\}$ egy bázis.
		      Így
		      $\left\{ Bu_1,\ldots,Bu_n \right\}$ egy maximális lineárisan független rendszerként maga is bázis.
		\item[$3.\Rightarrow 1.$]
		      Vegyünk $V$-ben egy $\left\{ u_1,\ldots,u_n \right\}$ ortonormált bázist.
		      Feltevésünk szerint a $\left\{ Bu_1,\ldots,Bu_n \right\}$ egy ortonormált rendszer.
		      Így minden $j,k$ mellett
		      \begin{displaymath}
			      \ip{u_k}{\left( B^\ast B-I \right)u_j}
			      =
			      \ip{u_k}{B^\ast Bu_j}-\ip{u_k}{u_j}
			      =
			      \ip{Bu_k}{Bu_j}-\ip{u_k}{u_j}
			      =
			      \delta_{k,j}-\delta_{k,j}
			      =
			      0.
		      \end{displaymath}
		      Ez azt jelenti, hogy az $(B^\ast B-I)u_j$ vektor merőleges az $\left\{ u_1,\ldots,u_n \right\}$ bázis minden elemére.
		      Az ortonormált rendszer teljessége szerint ez csak úgy lehetséges, ha minden $j$ indexre $B^\ast Bu_j=Iu_j$.
		      No de, a lineáris transzformációk bázison egyértelműen meghatározottak,
		      ezért $B^\ast B=I$.
		      Ez azt jelenti, hogy $B$ reguláris és $B^{-1}=B^\ast$.
	\end{description}
\end{proof}
Egy ortonormált bázisban felírva a $B$ transzformáció mátrixát,
nagyon könnyen észrevehető, hogy unitér transzformációval állunk szemben.
Ugyanis $B$ mátrixának $j$-edik oszlopa a $j$-edik bázis elem képe.
Így ortonormált bázisból kiindulva a transzformáció pontosan akkor unitér, ha
a felírt $[B]$ mátrix oszlopai egy ortonormált rendszert alkotnak az eredeti bázisban felírt belső szorzattal.
\begin{proposition}
	Legyen $V$ egy skalárisszorzatos-tér és $B\in L\left( V \right)$ egy lineáris transzformáció.
	A $B$ transzformáció pontosan akkor unitér,
	ha tetszőleges ortonormált bázisban felírt mátrixának oszloprendszere egy ortonormált rendszer.
\end{proposition}
Egyszerűbben kifejezve, ha $B$ ortonormált bázisban felírt mátrixának elemei $\beta_{i,j}$,
akkor a $B$ unitér voltának szükséges és elegendő feltétele,
hogy minden $i,j$-re
$\sum_{k=1}^n\beta_{k,i}\overline{\beta_{k,j}}=\delta_{i,j}$.
Ultra fontos következmény az alábbi:
\begin{proposition}
	Tegyük fel, hogy a $V$ skalárisszorzatos-tér, egy $A$ lineáris transzformációjának mátrixát felírtuk
	két ortonormált bázisban.
	Legyen $\left\{ u_1,\ldots,u_n \right\}$ a régi ortonormált bázis és $\left\{ e_1,\ldots,e_n \right\}$ az új ortonormált bázis.
	Jelölje $B$ az áttérés lineáris transzformációt, tehát $Bu_j=e_j$ minden $j=1,\ldots,n$ mellett.
	Ekkor
	\[
		[A]_\uj=\overline{[B]}^T[A]_{\rgi}[B].
		\qedhere
	\]
\end{proposition}

\chapter{Normális transzformációk diagonalizálhatósága}
\begin{definition}[normális lineáris transzformáció]
	A $V$ skalárisszorzatos-tér egy $A\in L\left( V \right)$ lineáris transzformációját
	\emph{normálisnak}\index{normális transzformáció} nevezzük,
	ha $$N^\ast N=NN^\ast$$ tehát, ha a transzformáció kommutál az adjungáltjával.
\end{definition}
Nyilvánvaló példák normális transzformációkra:
\begin{enumerate}
	\item Ha $N$ önadjungált;
	\item Ha $N$ reguláris és $N^{-1}=N^\ast$;\footnote{Az ilyen transzformációt nevezzük
		      \emph{unitér}\index{unitér}
		      transzformációnak.}
	\item Ha létezik olyan $q$ polinom, amelyre $N^\ast=q\left( N \right)$;
	\item
	      Ha $N$ egy ortonormált bázisban diagonalizálható, akkor
	      ebben a bázisban felírt matrixszára, a mátrix kommutál az adjungált mátrixszal,
	      így az eredeti transzformáció is normális.
\end{enumerate}
Ki fog derülni a fejezetben, hogy a fenti példák közül a 3. és a 4. le is fedi valamennyi normális transzformációt.
Az unitér transzformációk olyanok, amelyek $\mathbb{R}$ feletti esetben általában nem diagonalizálhatók.
\begin{proposition}
	Egy normális transzformáció polinomja is normális.
\end{proposition}
\begin{proof}
	Először gondoljuk meg, hogy $(N^j)^\ast$ és $N^k$ kommutálnak minden $j,k≥0$ mellett.
	Ez indukcióval könnyű:
	\begin{multline*}
		(N^j)^\ast N^k
		=
		(N^\ast)^jN^k
		=
		(N^\ast)^{j-1}N^\ast NN^{k-1}
		=
		(N^\ast)^{j-1}NN^\ast N^{k-1}
		\\
		=
		N(N^\ast)^{j-1}N^{k-1}N^\ast
		=
		N(N^{j-1})^\ast N^{k-1}N^\ast
		=
		NN^{k-1}(N^{j-1})^\ast N^\ast
		=
		N^k(N^\ast)^j
		=
		N^k(N^j)^\ast.
	\end{multline*}
	Most legyen $p\in \mathbb{K}[t]$ egy plinom.
	Ha $p\left( t \right)=\sum_{j=0}^m\alpha_jt^j,$
	akkor
	\begin{multline*}
		\left( p\left( N \right) \right)^\ast p\left( N \right)
		=
		\left( \sum_{j=0}^m\alpha_jN^j \right)^\ast
		\left( \sum_{k=0}^m\alpha_kN^k \right)
		=
		\left( \sum_{j=0}^m\bar{\alpha_j}(N^j)^\ast \right)
		\left( \sum_{k=0}^m\alpha_kN^k \right)
		\\
		=
		\sum_{j=0}^m\sum_{k=0}^m\bar{\alpha_j}\alpha_k(N^j)^\ast N^k
		=
		\sum_{k=0}^m\sum_{j=0}^m\alpha_k\bar{\alpha_j}N^k(N^j)^\ast
		\\
		=
		\left( \sum_{k=0}^m\alpha_kN^k \right)
		\left( \sum_{j=0}^m\bar{\alpha_j}(N^j)^\ast \right)
		=
		\left( \sum_{k=0}^m\alpha_kN^k \right)
		\left( \sum_{j=0}^m\alpha_jN^j\right)^\ast
		=
		p\left( N \right)\left( p\left( N \right) \right)^\ast.
	\end{multline*}
	Ezt kellett belátni.
\end{proof}
\begin{proposition}
	Legyen $N$ egy normális transzformáció.
	Ekkor $\ker N^\ast=\ker N$.
\end{proposition}
\begin{proof}
	Legyen $u\in V.$
	Ekkor
	\[
		\|Nu\|^2
		=
		\ip{Nu}{Nu}
		=
		\ip{u}{N^\ast Nu}
		=
		\ip{u}{NN^\ast u}
		=
		\ip{N^\ast u}{N^\ast u}
		=
		\|N^\ast u\|^2.
	\]
	Ez azt jelenti, hogy $Nu=0$ akkor és csak akkor áll fenn,
	ha $N^\ast u$ is fennáll.
\end{proof}
Korábban láttuk,
hogy tetszőleges lineáris transzformáció mellett $(\im A)^\perp=\ker A^\ast.$
Normális transzformáció esetén ez még egyszerűbb:
\begin{proposition}[Normális transzformációk rangtétele]\label{pr:normalisrang}
	Legyen $V$ skalárisszorzatos-tér és $N\in L\left( V \right)$ egy normális lineáris transzformáció.
	Ekkor
	\[
		\im N\oplus\ker N=V\qquad\text{és}\qquad \im N\perp \ker N.
		\qedhere
	\]
\end{proposition}

Tudjuk, hogy $\lambda\in\sigma(N)$ akkor és csak akkor, ha $\ker(N-\lambda I)\neq \left\{ 0 \right\}$.
Mivel normális transzformáció mellett az $N-\lambda I$,
az $N$-nek polinomja lévén,
maga is normális,
ezért az $\ker (N-\lambda I)$ és $\ker (N^\ast-\bar{\lambda}I)$ azonos alterek.
Azt gondoltuk meg tehát, hogy normális $N$ mellett
tetszőleges $u$ vektorra $Nu=\lambda u$ akkor és csak akkor igaz, ha $N^\ast u=\bar{\lambda}u$.
\begin{proposition}\label{pr:normtul}
	Legyen $N$ egy normális transzformáció a $V$ skalárisszorzatos-téren.
	Ekkor
	\begin{enumerate}
		\item
		      A $\lambda\in\sigma\left( N \right)$ akkor és csak akkor,
		      teljesül, ha
		      $\bar{\lambda}\in\sigma\left( N^\ast \right)$;
		\item
		      Minden $\lambda\in\mathbb{K}$ mellett
		      \begin{math}
			      \ker (N-\lambda I)=\ker (N^\ast-\bar{\lambda}I);
		      \end{math}
		\item
		      Különböző sajátértékekhez tartozó sajátalterek merőlegesek egymásra, azaz
		      \begin{math}
			      \ker \left( N-\lambda I \right)\perp\ker\left( N-\mu I \right),
		      \end{math}
		      ahol $\lambda\neq \mu$ és $\lambda,\nu\in\sigma\left( N \right)$.
		\item Ha $N=N^\ast$, azaz $N$ önadjungált transzformáció, akkor
		      \begin{math}
			      \sigma\left( N \right)\subseteq \mathbb{R},
		      \end{math}
		      azaz önadjungált transzformációnak valósak a sajátértékei.
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Az első két állítást már meggondoltuk.

	Legyen
	\begin{math}
		u\in\ker\left( N-\lambda I \right)
	\end{math}
	és
	\begin{math}
        v\in\ker\left( N-\mu I \right)=\ker\left( N^\ast-\bar{\mu}I \right)
	\end{math}.
	Ekkor persze
	\begin{math}
		Nu=\lambda u
	\end{math}
	és
	\begin{math}
		N^\ast v=\bar{\mu}v
	\end{math}.
	Így
	\[
		\left( \lambda-\mu \right)\ip{u}{v}
		=
		\ip{\lambda u}{v}-\ip{u}{\bar{\mu}v}
		=
		\ip{Nu}{v}-\ip{u}{N^\ast v}
		=
		\ip{u}{N^\ast v}-\ip{u}{N^\ast v}
		=
		0.
	\]
	No de, a sajátértékek nem azonosak,
	ezért $u\perp v$.

	Legyen most $N$ önadjungált, és $\lambda\in\sigma\left( N \right)$ egy sajátérték.
	Van tehát olyan $u\neq 0$ vektor, amelyre $Nu=\lambda u$ és $N^\ast u=\bar{\lambda}u$.
	Ekkor
	\[
		\left( \lambda-\bar{\lambda} \right)u
		=Nu-N^\ast u=Nu-Nu=0.
	\]
	No de, az $u\neq 0$ feltétel miatt $\lambda-\bar{\lambda}=0$, tehát $\lambda\in\mathbb{R}.$
\end{proof}
Eddig minden gondolat $\mathbb{K}$ feletti skalárisszorzatos-téren volt.
A komplex és a valós diagonalizálhatóság közös magja a következő állítás.
\begin{proposition}\label{pr:normalis-mult1}
	Legyen $V$ egy $\mathbb{K}$ feletti skalárisszorzatos-tér,
	és $N\in L\left( V \right)$ egy normális transzformáció.
	Ekkor $N$ minimálpolinomja különböző, normált, irreducibilis polinomok elsőfokú hatványainak szorzata.
\end{proposition}
\begin{proof}
	Tegyük fel, hogy $p^k|m$, ahol $m$ a minimálpolinom,
	$p$ egy irreducibilis polinom, és $k\geq 2.$
	Láttuk, hogy bevezetve a $V_1=\ker p^k\left( N \right)$, és
	$A_1=N|_{V_1}$ jelöléseket,
	az $A_1$ minimálpolinomja $p^k$.
	Tehát ha $B=p(N)$, akkor $B|_{V_1}$ transzformáció $k$-ad rendben nilpotens,
	létezik tehát $v\in V_1$,
	amelyre $B^{k-1}v\neq 0$,
	de $B^kv=0$.
	Mivel $k-1\geq 1$, ezért találtunk $x=B^{k-1}v\in\im p\left( N \right)$
	nem zérus vektort,
	amelyre $x\in\ker(p\left( N \right))$.
	Ez ellentmond a normális transzformációk rangtételének.
\end{proof}
\subsection{Normális transzformációk diagonalizálhatósága komplex skalárisszorzatos-tereken}
A fenti gondolat azonnali következménye a normális transzformációk diagonalizálhatósága $\mathbb{C}$ felett.
\begin{proposition}
	Legyen $V$ egy $\mathbb{C}$ feletti skalárisszorzatos-tér.
	Ekkor minden $N$ normális transzformáció ortonormált bázisban diagonalizálható.
	Ez azt jelenti, hogy létezik a térnek $N$ sajátvektoraiból álló ortonormált bázisa.
	Ebben a bázisban felírva $N$ mátrixát egy diagonális mátrixot kapunk,
	amelynek diagonálisában $N$ sajátértékei vannak.
\end{proposition}
\begin{proof}
	Az algebra alaptétele szerint irreducibilis $\mathbb{C}$ feletti polinom
	csak első fokú polinom lehet.
	Így a minimálpolinom, a normalitást kihasználva a fenti (\ref{pr:normalis-mult1}.) gondolat alapján,
	\[
		m\left( t \right)=\left( t-\lambda_1 \right)\dots\left( t-\lambda_s \right).
	\]
	Ezért a transzformációk redukálására vonatkozó \ref{pr:redukcio-primszorzat}.~állítás szerint
	\[
		V=\ker\left( N-\lambda_1I \right)\oplus\dots\oplus\ker\left( N-\lambda_sI \right).
	\]
	Most végezzünk Gram\,--\,Schmidt-ortogonalizációt\index{Gram\,--\,Schmidt-ortogonalizáció}
	minden egyes sajátaltérben külön-külön,
	majd egyesítsük a kapott ortogonális rendszereket az egész $V$ skalárisszorzatos-tér ortogonális bázisává.
	Így $N$ sajátvektoraiból álló ortonormált bázist kapunk.
\end{proof}
Összefoglalva a komplex test feletti esetet:
\emph{
	Egy transzformáció pontosan akkor normális,
	ha ortonormált bázisban diagonalizálható.
}

\subsection{Szimmetrikus transzformációk diagonalizálhatósága valós skalárisszorzatos-tereken}
A valós test feletti eset fontosságát hangsúlyozandó,
az önadjungált transzformációkra külön elnevezést vezetünk be.
\begin{definition}[szimetrikus transzformáció]
	Egy $\mathbb{R}$ feletti skalárisszorzatos-tér egy $A\in L\left( V \right)$ lineáris transzformációját
	\emph{szimmetrikusnak} mondjuk, ha az önadjungált, azaz  $A^\ast=A$.
\end{definition}
Egy kicsit sem nyilvánvaló, hogy miért lenne egy szimmetrikus transzformációnak sajátértéke.
Rögtön kiderül, hogy mindig van sajátértéke.

Ha $\dim(V)=1$, akkor minden $A:V\to V$ transzformációra igaz, hogy ha $v\neq 0$ vektor, 
akkor van olyan $\lambda\in\mathbb{R}$,
hogy $Av=\lambda v$. 
Azt kaptuk tehát, hogy 1 dimenziós téren értelmezett minden lineáris transzformációnak
van sajátértéke, és minden zérustól különböző vektor sajátvektor is.

Most nézzük csak 2 dimenzióban.
Ha $A$ mátrixa egy ortonormált bázisban
\begin{math}
	\begin{pmatrix}
		a & b \\b&c
	\end{pmatrix}
\end{math}
akkor $A$ karakterisztikus polinomja 
$   k\left( t \right)
    =
    \left( t-a \right)\left( t-c \right)-b^2
    =
    t^2-\left( a+c \right)t+ac-b^2.$
E polinom diszkriminánsa 
\(
    \left( a-c \right)^2+4b^2\geq 0.
\)
Ha a diszkrimináns éppen zérus, akkor $b=0$, $a=c$, ergo $A=aI$, így $A$ tetszőleges bázisban diagonális.
Ha a diszkrimináns pozitív, akkor két különböző sajátértéke van $A$-nak, amely egy 2 dimenziós 
vektortéren van értelmezve, ergo diagonalizálható.
Meggondoltuk tehát, hogy egy
\emph{
	2 dimenziós téren értelmezett szimmetrikus transzformációnak van sajátvektorokból bázisa.
}
\begin{proposition}
	Legyen $V$ egy $\mathbb{R}$ feletti skalárisszorzatos-tér.
	Ekkor minden $A$ szimmetrikus transzformáció ortonormált bázisban diagonalizálható.
	Ez azt jelenti, hogy létezik a térnek $A$ sajátvektoraiból álló ortonormált bázisa.
	Ebben a bázisban felírva $A$ mátrixát egy diagonális mátrixot kapunk,
	amelynek diagonálisában $A$ sajátértékei vannak.
\end{proposition}
\begin{proof}
	Az algebra alaptétele szerint irreducibilis $\mathbb{R}$ feletti polinom
	csak első- vagy másodfokú polinom lehet.
	Mivel $A$ szimmetrikus, ezért persze normális is.
	Így a minimálpolinom, a transzformáció normalitását használó \ref{pr:normalis-mult1}.~állítás alapján,
	\[
		m=p_1\dots p_s
	\]
	alakú, ahol $p_1,\ldots,p_s$ egymástól különböző, normált, irreducibilis, legfeljebb másodfokú polinomok.
	Ezért a transzformációk redukálására vonatkozó \ref{pr:minpolelsofokufelbontas}.~állítás szerint
	a tér előáll mint legfeljebb két dimenziós invariáns alterei direkt összege.
	Minden ilyen legfeljebb 2 dimenziós invariáns altérben van $A$ sajátvektoraiból álló bázis,
	amelyeket egyesítve az egész $V$ vektortér egy egy olyan bázisát kapjuk,
	amelynek minden eleme az $A$ sajátvektora.
	Ezért valamely $r\geq s$ mellett
	\[
		V=\ker\left( A-\lambda_1I \right)\oplus\dots\oplus\ker\left( A-\lambda_rI \right).
	\]
	Most végezzünk Gram\,--\,Schmidt-ortogonalizációt\index{Gram\,--\,Schmidt-ortogonalizáció}
	minden egyes fenti $A-\lambda_jI$ sajátaltérben külön-külön,
	majd egyesítsük a kapott ortogonális rendszereket az egész $V$ skalárisszorzatos-tér ortogonális bázisává.
	Így $A$ sajátvektoraiból álló ortonormált bázist kapunk.
\end{proof}
A valós számtest feletti esetet is összefoglaljuk:
\emph{
    Egy transzformáció pontosan akkor szimmetrikus, ha ortonormált bázisban diagonalizálható.
}

\chapter{Ortogonális projekciók}
A fejezetben visszatérünk a skalárisszorzatos terek geometriai tulajdonságainak vizsgálatához.
Emlékezzünk arra, hogy bevezettük a merőleges vetítés transzformációt.
Egy $M\subseteq V$ altér esetén $P_M\in L\left( V \right)$ az a transzformáció, amely egy $v\in V$ vektorhoz hozzárendeli a $v$-hez legközelebb eső $M$-beli vektort.
Megmutattuk, hogy ilyen vektor van és csak egy van az $M$ altérben.
Ez jelöljük $P_Mv$-vel.
Azt is láttuk, hogy $v-P_Mv\in M^\perp$, igy a tér előáll 
$V=M\oplus M^\perp$ ortogonális direktösszeg alakban.

Végül is elmondhatjuk, hogy a tér minden $v$ vektora egyértelműen felbomlik egy $M$-beli és egy $M^\perp$ vektor összegére,
és ebben a felbontásban az $M$-beli komponens a $P_Mv$, a $v$-nek
$M$-re eső merőleges vetülete.
Egy $v$ vektornak pontosan akkor a zérus vektor az $M$-beli komponense, ha $v\in M^\perp$ teljesül, 
ami éppen azt jelenti, hogy $\ker P_M=M^\perp$.
Az is világos, hogy $\im P_M=M$, 
emiatt $\ker P_M\perp\im P_M$, és $P_M^2=P_M$.

Eddig is többször használtuk már, hogy $P|_M=\id$.
Emiatt ha egy $v\in V$ vektorra $v=v_1+v_2$, ahol $v_1\in M$ és $v_2\in M^\perp$, akkor $P_Mv=v_1$ és $P_{M^\perp}v=v_2$, ergo $P_M+P_{M^\perp}=\id$.

Érdemes még meggondolni, hogy milyen szép egy ortogonális projekció mátrixa, ha megfelelő bázist választunk.
Legyen $\left\{ x_1,\ldots,x_k \right\}$ egy $M$-beli teljes ortonormált rendszer, és $\left\{ x_{k+1},\ldots,x_{n} \right\}$ egy $M^\perp$ beli teljes ortonormált rendszer.
Világos, hogy ha ezeket egyesítjük, akkor az egész $V$ vektortérnek ortonormált bázisát kapjuk, ahol $P_Mx_j=x_j$ minden $j=1,\ldots,k$ mellett és $P_{M}x_j=0$ valamennyi $j=k+1,\ldots,n$ esetén.
Ez azt jelenti, hogy $P_M$ mátrixa ebben az ortonormált bázisban olyan diagonáls mátrix, ahol a diagonális első $k$ eleme $1$, minden más elem zérus.
\footnote{A gyakorlatokon azt s meggondoltuk, hogy ha $\left\{ e_1,\ldots,e_n \right\}$ ortonormált bázis $V$-ben, és $\left\{ x_1,\ldots,x_k \right\}$ egy teljes ortonormált rendszer $M$-ben,
akkor a $\sum_{j=1}^k[x_j]\cdot\overline{[x_j]^T}$ diadikus szorzatok összege adja $P_M$ mátrixát az $\left\{ e_1,\ldots,e_n \right\}$ bázisban. 
Itt az $[x_j]$ oszlopvektor az $x_j$ vektor koordinátavektora a fent rögzített bázisban.}
Ez a mátrix a transzponáltjának konjugáltja, ergo $P_M$ egy önadjungált transzformáció.
\footnote{
    A $P_M=P_M^\ast$ azonosság mátrixmentes indoklása: 
    legyen $u=u_1+u_2$, és $v=v_1+v_2$,
    ahol $u_1,v_1\in M$, valamint $u_2,v_2\in M^\perp$.
    Ekkor 
    $
    \ip{P_Mu}{v}=
    \ip{u_1}{v_1+v_2}
    =
    \ip{u_1}{v_1}
    $,
    és hasonlóan
    $
    \ip{u}{P_Mv}
    =
    \ip{u_1+u_2}{v_1}
    =
    \ip{u_1}{v_1}
    $.
    Ergo 
    $
    \ip{P_Mu}{v}
 %   =
 %   \ip{u_1}{v_1}
    =
    \ip{u}{P_Mv}.
    $
}

Most azokat a tulajdonságokat keressük, amelyek karekterizálják a merőleges vetítéseket.
\begin{definition}
	Azt mondjuk, hogy a $P\in L\left( V \right)$ transzformáció egy
	\emph{projekció}\index{projekció},
	ha az idempotens, tehát fennáll a $P^2=P$ azonosság.
\end{definition}
Vegyük észre, hogy $P$ pontosan akkor projekció, ha $P|_{\im P}=\id.$
\begin{proposition}
	Egy $V$ skalárisszorzatos-tér minden $P$ projekciójára
	\[
		\ker P\oplus\im P=V.
		\qedhere
	\]
\end{proposition}
\begin{proof}
    Világos, hogy minden $v\in V$ vektor mellett
    $v=Pv+\left( v-Pv \right)$. 
    Itt $Pv\in\im P$, és $P\left( v-Pv \right)=Pv-P^2v=Pv-Pv=0$,
    ergo $v-Pv\in\ker P$.

    Ha $x\in\ker P\cap\im P$, akkor $x=Pv$ valamely $v$ vektorral, 
    és $0=Px=\id(x)=x$.

    Megmutattuk tehát, hogy $\ker P$ és $\im P$ olyan diszjunkt alterek, amelyek Minkowski-összege $V$.
\end{proof}
\begin{definition}
	A $V$ skalárisszorzatos-tér egy $P\in L\left( V \right)$ lineáris transzformációját
	\emph{ortogonális projekciónak}\index{ortogonális projekció}
	mondunk, ha
	\begin{enumerate}
		\item $P^2=P$ és
		\item $\ker P\perp\im P$.
		      \qedhere
	\end{enumerate}
\end{definition}
Ezek szerint az ortogonális projekció olyan speciális projekciók, amelyekre nem csak a
\[
    \ker P\oplus\im P=V
\]
direktösszeg alakú felbontás áll fenn, hanem azontúl az
\[
    \ker P\oplus\im P=V,\quad \ker P\perp\im P
\]
ortogonális direktösszeg alakú felbontás is igaz. Persze ekkor 
$\left( \im P \right)^\perp=\ker P$ is fennáll.
Gondolhatunk ezért úgy is az ortogonális projekció definíciójára, hogy a két tulajdonság a $P$ transzformációnak az $\im P$ altéren és annak ortokomplementerén való viselkedést írja elő.
Tehát ha $P$ egy ortogonális projekció, akkor
\[
    P|_{\im P}=\id,
    \text{ és }
    P|_{{\left( \im P \right)}^\perp}=0.
\]
Láttuk, hogy tetszőleges $M\subseteq V$ altér mellett a $P_M$ merőleges vetítés egy példa ortogonális projekcióra. 
A következő állítás szerint nincs is más típusú ortogonális projekció,
mint valamely $M$ altérre eső merőleges vetítés.
\begin{proposition}
	Legyen $V$ egy skalárisszorzatos-tér,
	és $P\in L\left( V \right)$ egy ortogonális projekció.
	Jelölje $M=\im P$.
	Ekkor
	\[
		P=P_M.
		\qedhere
	\]
\end{proposition}
\begin{proof}
    Mivel $P$ egy projekció, ezért $P|_M=\id$, és mivel $P$ egy ortogonális projekció, 
    ezért $P|_{M^\perp}=0$.
    Így minden $x\in V$ mellett az $x=P_Mx+P_{M^\perp}x$ azonosságra a $P$ transzformaciót alkalmazva azt kapjuk, hogy
    \[
        Px
        =
        P\underbrace{\left( P_Mx \right)}_{\in M}+P\underbrace{\left( P_{M^\perp}x \right)}_{\in M^\perp}
        =
        P_Mx+0
        =
        P_Mx.
        \qedhere
    \]
\end{proof}
\begin{proposition}
	Legyen $A\in L\left( V \right)$ egy lineáris transzformáció a $V$ skalárisszorzatos-tér felett.
	Az $A$ pontosan akkor ortogonális projekció, ha $A^\ast=A$ és $A^2=A$.
\end{proposition}
\begin{proof}
	Láttuk korábban hogy egy $P_M$ merőleges vetítés önadjugált is, idempotens is.
	No de, minden ortogonális projekció $P_M$ alakú,
	valamely $M$ altér megválasztása mellett, ezért $A$-ra is igaz, hogy önadjungált és idempotens.

	Megfordítva, ha $A$ önadjungált, akkor a skalárisszorzatos rang tétel szerint $(\ker A)^\perp=\im A^\ast=\im A$,
	amiből persze $\ker A\perp\im A$ már könnyen következik.
\end{proof}
Mivel minden $P$ ortogonális projekció $P=P_M$ alakú,
ezért a Bessel-egyenlőség ortogonális projekciókra is igaz:\index{Bessel-egyenlőtlenség}
\begin{proposition}[Bessel]
	Minden  $P\in L\left( V \right)$ ortogonális projekcióra és minden $v\in V$ vektorra
	\(
	\|Pv\|\leq\|v\|
	\),
    és
    \(
    \|Pv\|^2=\ip{v}{Pv}
    \).
\end{proposition}
\begin{proof}
	Jelölje $M=\im P$.
    Láttuk, hogy $P=P_M$ ezért
	\(
	\|Pv\|
	=
	\|P_Mv\|
	\leq
	\|v\|.
	\)
    Mivel $P$ önadjungált és idempotens, 
    ezért
    \(
    \|Pv\|^2=\ip{Pv}{Pv}=\ip{v}{P^\ast Pv}=\ip{v}{P^2v}=\ip{v}{Pv}
    \).
\end{proof}

\begin{proposition}
	Legyen $P,Q\in L\left( V \right)$ ortogonális projekció.
	A $QP=0$ pontosan akkor teljesül, ha $\im P\perp\im Q$.
\end{proposition}
\begin{proof}
	Legyen $u\in \im P$ és $v\in \im Q$.
	Ekkor
	\[
		\ip{u}{v}
		=
		\ip{Pu}{Qv}
		=
		\ip{Q^\ast Pu}{v}
		=
		\ip{QPu}{v}
		=
		\ip{0}{v}
		=
		0.
	\]
	Evvel igazoltuk, hogy $QP=0$ esetén $\im P\perp\im Q$.

	Megfordítva, tetszőleges $u\in V$ mellett $Pu\in\im P$ és $QPu\in\im Q$.
	Így
	\[
		\|QPu\|^2
		=
		\ip{QPu}{QPu}
		=
		\ip{Q^\ast QPu}{Pu}
		=
		\ip{Q^2Pu}{Pu}
		=
		\ip{QPu}{Pu}
		=
		0.
		\qedhere
	\]
\end{proof}
\begin{definition}[ortogonális projekciók merőlegessége]
	Az $Q$ és $P$ ortogonális projekciókat egymásra \emph{merőlegesnek} mondjuk, ha $QP=0$ fennáll.
	Jelölés: $Q\perp P$.
\end{definition}
Világos tehát, hogy $QP=0$, $\im P\perp \im Q$ és $PQ=0$ ekvivalens megfogalmazásai $Q\perp P$-nak,
amennyiben $P,Q\in L\left( V \right)$ ortogonális projelciók.
Az $M_1, M_2\subseteq V$ alterek esetén $P_{M_1}\perp P_{M_2}$ pontosan akkor, ha $M_1\perp M_2$.
\begin{proposition}
	Legyenek $P_1,\ldots,P_r$ ortogonális projekciók.
	Jelölje $P=\sum_{j=1}^rP_j$ ezek összegét.
	A $P$ összeg pontosan akkor ortogonális projekció, ha $P_i\perp P_j$ minden $i\neq j$ mellett.
\end{proposition}
\begin{proof}
    Tegyük fel először, hogy $P_i\perp P_j$ minden $i\neq j$ mellett és legyen $P$ az összeg.
    Ekkor
    \[
        P^\ast
        =
        \sum_{j=1}^rP_j^\ast
        =
        \sum_{j=1}^rP_j
        =
        P,
        \quad\text{és}\quad
        P^2
        =
        \sum_{k=1}^r\sum_{j=1}^rP_iP_j
        =
        \sum_{k=1}^rP_i^2
        =
        \sum_{k=1}^rP_i
        =
        P.
    \]
    Azt kaptuk tehát, hogy a $P$ összeg önadjungált és idempotens,
    ergo egy ortogonális projekció.

    Megfordítva, most azt tegyük fel, hogy $P$ ortogonális projekció.
    Megmutatjuk, hogy tetszőleges $j\neq i$-re
    $P_iP_j=0$.
    Legyen ezért $u\in V$ rögzítve, és jelölje valamely rögzített $j$ index mellett $x_j=P_ju$.
    Ekkor
    \[
        \|x_j\|^2
        \geq
        \|Px_j\|^2
        =
        \ip{x_j}{Px_j}
        =
        \ip{x_j}{\sum_{i=1}^rP_ix_j}
        =
        \sum_{i=1}^r\ip{x_j}{P_ix_j}
        =
        \sum_{i=1}^r\|P_ix_j\|^2
        \geq
        \|P_jx_j\|^2
        =
        \|x_j\|^2.
    \]
    Ezek szerint az utolsó egyenlőtlenség is egyenlőség, 
    ergo minden $i\neq j$ esetén
    \(
    \|P_ix_j\|=0,
    \)
    ergo
    \(P_ix_j=0\),
    ergo
    $P_i\left( P_ju \right)=0.$
\end{proof}

\section{Ortogonális projekciók lineáris kombinációja}
Olyan nem zérus ortogonális projekciók lineáris kombinációit vizsgáljuk,
amelyek összege az identikus transzformáció.
Mivel az identikus transzformáció egyben projekció is,
ezért az előző tételt használva azt kapjuk, hogy az összeadandó projekciók még egymásra merőlegesek is.
\begin{proposition}[komplex együtthatók]\label{pr:normlinkombkomplex}\index{normális transzformáció}
	Legyen $V$ egy $\mathbb{K}$ feletti skalárisszorzatos-tér.
	Tegyük fel, hogy $P_1,\ldots,P_r\in L\left( V \right)$ olyan ortogonális projekciók,
	amelyekre
	\begin{enumerate}
		\item $P_j\neq 0$ minden $j=1,\ldots,r$ mellett.
		\item $I=\sum_{j=1}^rP_j$.
	\end{enumerate}
	Legyenek a $\left\{ \lambda_1,\ldots,\lambda_r \right\}\subseteq \mathbb{K}$
	(nem feltétlenül különböző) számok tetszőlegesen rögzítve, és
	jelölje
	\(
	N
	=
	\sum_{j=1}^r\lambda_jP_j.
	\)
	Ekkor az $N$ egy \emph{normális} transzformáció,
	amelynek spektrumára $\sigma\left( N \right)=\left\{ \lambda_1,\ldots,\lambda_r \right\}$.
\end{proposition}
\begin{proof}
	A feltétel szerint az ortogonális projekciók összege az identikus transzformáció, tehát
	az összeg is egy ortogonális projekció.
	Emiatt a $P_j$ projekciók egymásra páronként merőlegesek,
	így
	\[
		N^\ast N
		=
		\sum_{j=1}^r|\lambda_j|^2P_j
		=
		NN^\ast.
	\]

	Most megmutatjuk, hogy
	$\left\{ \lambda_1,\ldots,\lambda_r \right\}\subseteq \sigma(N)$.
	Legyen $u_j\in\im P_j$ egy nem zérus vektor, azaz $u_j\neq 0$.
	Ilyen vektor létezik, hiszen $P_{j}\neq 0$.
	Világos, hogy minden $k$ mellett
	\begin{math}
		P_k\left( u_j \right)
		=
		P_k\left( P_j\left( u_j \right) \right)
		=
		P_kP_j\left( u_j \right)
		=\delta_{k,j}u_j.
	\end{math}
	Így
	\[
		Nu_j
		=
		\left( \sum_{k=1}^r\lambda_kP_k \right)u_j
		=
		\sum_{k=1}^r\lambda_k\delta_{k,j}u_j
		=
		\lambda_ju_j,
	\]
	ami pont azt jelenti, hogy $\lambda_j$ az $N$ transzformáció egy sajátértéke.

	Most azt mutatjuk meg, hogy
	$\sigma(N)\subseteq \left\{ \lambda_1,\ldots,\lambda_r \right\}$.
	Legyen $\mu\in\sigma(N)$, azaz $Ns=\mu s$ valamely $s\neq 0$ vektorra.
	Jelölje $s_j=P_js\in\im P_j$, minden $j=1,\ldots,r$ mellett.
	Itt
	\[
		s=Is
		=
		\left( \sum_{j=1}^rP_j \right)s
		=
		\left( \sum_{j=1}^rP_js \right)
		=
		\sum_{j=1}^rs_j.
	\]
	Ezért
	\[
		\sum_{j=1}^r\mu s_j
		=
		\mu\sum_{j=1}^rs_j
		=
		\mu s
		=
		Ns
		=
		\left( \sum_{j=1}^r\lambda_jP_j \right)s
		=
		\sum_{j=1}^r\lambda_jP_js
		=
		\sum_{j=1}^r\lambda_js_j.
	\]
	Azt kaptuk tehát, hogy
	\[
		\sum_{\substack{j=1\\s_j\neq 0}}^r\left(\lambda_j-\mu \right)s_j=0.\tag{\dag}
	\]
	Itt a baloldali szummának van legalább egy tagja, hiszen
	\(
	0 \neq s=\sum_{j=1}^rs_j.
	\)
	Az
	$\left\{ s_j:j=1,\ldots,r \right\}$
	egy ortogonális rendszer,
	hiszen a $P_j$ projekciók egymásra merőlegesek, ami ekvivalens avval, hogy
	$\im P_k\perp \im P_j$ minden $k\neq j$ mellett.
	Így az
	$\left\{ s_j:j=1,\ldots,r, s_j\neq 0 \right\}$ vektorrendszer,
	zérus elemet nem tartalmazó ortogonális rendszerként lineárisan független rendszer.
	Ez azt jelenti, hogy a $(\dag)$ lineáris kombináció együtthatói a test nullelemei, ergo
	\begin{math}
		\lambda_j-\mu=0
	\end{math}
	minden szóba jövő $j$-re,
	azaz minden olyan $j$ mellett, amelyre $s_j\neq 0$.
	A már igazoltak szerint
	\begin{math}
		\mu=\lambda_j\in\sigma(N).
	\end{math}
	Ezt kellett belátni.
\end{proof}

\begin{proposition}\label{pr:normpol}
	Tegyük fel, hogy olyan
	$P_1,\ldots,P_r$ ortogonális projekciókra, amelyek összege az identikus transzformáció,
	és a tetszőlegesen megválasztott
	$\lambda_1,\ldots,\lambda_r\in\mathbb{K}$ számokra $N=\sum_{j=1}^r\lambda_jP_j$.
	Ekkor minden $q\in\mathbb{K}[t]$ polinom mellett
	$q\left( N \right)=\sum_{j=1}^rq( \lambda_j )P_j$.
\end{proposition}
\begin{proof}
	Először gondoljuk meg azt, hogy minden $l\geq 0$ mellett
	\begin{math}
		N^l
		=
		\sum_{j=1}^k\lambda_j^lP_j.
	\end{math}
	Ha $l=0$, akkor az ortogonális projekciók összege az identitás feltevést kapjuk.
	Ha az állítás igaz $l$-ig, akkor $l+1$-re:
	\[
		N^{l+1}
		=
		N^lN
		=
		\left( \sum_{j=1}^r\lambda_j^lP_j \right)\left( \sum_{h=1}^r\lambda_hP_h \right)
		=
		\sum_{j=1}^r\sum_{h=1}^r\lambda_j^l\lambda_hP_jP_h
		=
		\sum_{j=1}^r\lambda_j^{l+1}P_j.
	\]
	Itt azt használtuk ki, hogy amennyiben az ortogonális projekciók összege is egy ortogonális projekció,
	akkor $P_jP_h=\delta_{j,h}P_j^2=\delta_{j,h}P_j$.
	No de, ha a $q$ polinom $q\left( t \right)=\sum_{l=0}^m\alpha_lt^l$ alakú,
	akkor
	\begin{multline*}
		q\left( N \right)
		=
		\sum_{l=0}^m\alpha_lN^l
		=
		\sum_{l=0}^m\alpha_l
		\left(
		\sum_{j=1}^r\lambda_j^lP_j
		\right)
		=
		\sum_{l=0}^m\sum_{j=1}^r\alpha_l\lambda_j^lP_j
        =
		\sum_{j=1}^r\sum_{l=0}^m\alpha_l\lambda_j^lP_j
		=
		\sum_{j=1}^r
		\left(
		\sum_{l=0}^m\alpha_l\lambda_j^l
		\right)P_j
		=
		\sum_{j=1}^r
		q\left(\lambda_j \right)P_j.
	\end{multline*}
	Ezt kellett belátni.
\end{proof}
\subsection{Valós együtthatós lineáris kombináció}
Most nézzük a valós együtthatós lineáris kombinációk esetét.
\begin{proposition}\label{pr:normlinkombvalos}\index{ozz@önadjungált transzformáció}
	Legyen $V$ egy $\mathbb{K}$ feletti skalárisszorzatos-tér.
	Tegyük fel, hogy $P_1,\ldots,P_r\in L\left( V \right)$ olyan ortogonális projekciók,
	amelyekre
	\begin{enumerate}
		\item $P_j\neq 0$ minden $j=1,\ldots,r$ mellett.
		\item $I=\sum_{j=1}^rP_j$.
	\end{enumerate}
	Legyenek a $\left\{ \lambda_1,\ldots,\lambda_r \right\}\subseteq \mathbb{R}$
	(nem feltétlenül különböző) valós számok tetszőlegesen rögzítve,
	és jelölje
	\(
	N
	=
	\sum_{j=1}^r\lambda_jP_j.
	\)
	Ekkor az $N$ egy \emph{önadjungált} transzformáció,
	amelynek spektrumára $\sigma\left( N \right)=\left\{ \lambda_1,\ldots,\lambda_r \right\}$.
\end{proposition}
\begin{proof}
	Mivel az itt szereplő $\lambda_j$ számok valósak,
	ezért azonosak a konjugáltjukkal.
	Így
	\begin{displaymath}
		N^\ast=
		\sum_{j=1}^r\overline{\lambda_j}P_j^\ast
		=
		\sum_{j=1}^n\lambda_jP_j
		=N.
		\qedhere
	\end{displaymath}
\end{proof}

\subsection{Egy abszolútértékű együtthatókkal képzett lineáris kombináció}
Most nézzük az olyan lineáris kombinációk esetét, amikor az együtthatók 1 abszolútértékű számok.
\begin{proposition}\label{pr:normlinkombegys}\index{unitér transzformáció}
	Legyen $V$ egy $\mathbb{K}$ feletti skalárisszorzatos-tér.
	Tegyük fel, hogy $P_1,\ldots,P_r\in L\left( V \right)$ olyan ortogonális projekciók,
	amelyekre
	\begin{enumerate}
		\item $P_j\neq 0$ minden $j=1,\ldots,r$ mellett.
		\item $I=\sum_{j=1}^rP_j$.
	\end{enumerate}
	Legyenek a $\left\{ \lambda_1,\ldots,\lambda_r \right\}\subseteq \mathbb{K}$
	(nem feltétlenül különböző),
	1 abszolútértékű számok tetszőlegesen rögzítve,
	és jelölje
	\(
	N
	=
	\sum_{j=1}^r\lambda_jP_j.
	\)
	Ekkor az $N$ egy \emph{unitér transzformáció} transzformáció,
	amelynek spektrumára $\sigma\left( N \right)=\left\{ \lambda_1,\ldots,\lambda_r \right\}$.
\end{proposition}
\begin{proof}
	$N$ spektruma a zérust nem tartalmazza, tehát $N$ reguláris.
	Persze $$NN^\ast=\sum_{j=1}^n|\lambda_j|^2P_j=\sum_{j=1}^nP_j=I$$ ami azt jelenti,
	hogy $N^{-1}=N^\ast$.
\end{proof}

\chapter{Spektrális felbontások}
\scwords A közös mag az identikus operáció alábbi felbontása.
\begin{proposition}
	Tegyük fel, hogy $V$ egy $\mathbb{K}$ feletti skalárisszorzatos-tér,
	és tegyük fel, hogy az $M_1,\ldots,M_r$ alterekre
	\[
		V=M_1\oplus\dots\oplus M_r,
		\qquad\text{ahol }M_j\perp M_k\text{ minden }j\neq k.
	\]
	Jelölje $P_j$ az $M_j$ altérre való merőleges vetítést.
	Ekkor
	\[
		I=\sum_{j=1}^rP_j
	\]
	és persze $P_j\perp P_k$ minden $j\neq k$ mellett.
\end{proposition}
\begin{proof}
	Minden $s\in V$ vektor egyértelműen előáll
	$s=\sum_{j=1}^rs_j$
	alakban, ahol $s_j\in M_j$.
	Felhasználva, hogy $P_j|M_j=\id$ és $\sum_{\substack{k=1\\k\neq j}}^rM_k\subseteq M_j^\perp=\ker P_j$ minden $k\neq j$ mellett
	\[
		\left(
		\sum_{j=1}^rP_j
		\right)s
		=
		\sum_{j=1}^rP_js
		=
		\sum_{j=1}^rP_j\left( s_j+\sum_{\substack{k=1\\k\neq j}}^rs_k \right)
		=
		\sum_{j=1}^r\left( P_js_j+P_j\left( \sum_{\substack{k=1\\k\neq j}}^rs_k \right) \right)
		=
		\sum_{j=1}^rs_j
		=
		Is.
		\qedhere
	\]
\end{proof}
Emlékezzünk arra, hogy ha $\left\{ \lambda_1,\ldots,\lambda_r \right\}$ az $N$ normális transzformáció
sajátértékei,
akkor bevezetve az $M_i=\ker\left( N-\lambda_i I \right)$ jelölést
\[
	M_1\oplus\dots\oplus M_r\subseteq V
	\qquad\text{ahol }M_j\perp M_k\text{ minden }j\neq k.
\]
Még azt is láttuk, hogy $\mathbb{K}=\mathbb{C}$ esetben itt egyenlőség is van,
de azt is tudjuk, hogy az egyenlőség még a $\mathbb{K}=\mathbb{R}$ és $N^\ast=N$ esetben is fennáll.
\section{Komplex eset}

\subsection{Normális transzformáció}
\begin{proposition}[spektrális felbontás]
	Legyen $V$ egy $\mathbb{C}$ feletti skalárisszorzatos-tér, és $N\in L\left( V \right)$
	egy normális lineáris transzformáció.
	Jelölje $\sigma\left( N \right)=\left\{\lambda_1,\ldots,\lambda_k  \right\}$ az $N$ különböző sajátértékeit.
	Láttuk, hogy $N$ diagonalizálható, ezért $k\geq 1$.
	Jelölje $P_j$ a $\ker\left\{ N-\lambda_jI \right\}$ altérre való merőleges vetítést.
	Ekkor
	\begin{enumerate}
		\item Minden $j=1,\ldots,r$ mellett $P_j\neq 0$;
		\item $I=\sum_{j=1}^rP_j$;
		\item $N=\sum_{j=1}^r\lambda_jP_j$;
		\item $N^\ast=\sum_{j=1}^r\overline{\lambda_j}P_j$.
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Láttuk, hogy $N$ diagonalizálható, azaz létezik a térnek $N$ sajátvektoraiból álló
	bázisa,
	ezért
	\[
		M_1\oplus\dots\oplus M_r=V
		\qquad\text{itt }M_j\perp M_k\text{ minden }j\neq k,\tag{\dag}
	\]
	ahol $M_j=\ker\left( N-\lambda_jI \right)$.

	Világos,
	hogy minden $j$ mellett létezik $u_j\neq 0$, hogy $Nu_j=\lambda_ju_j$,
	így $M_j\neq \left\{ 0 \right\}$,
	ergo $P_j\neq 0.$

	Láttuk korábban, hogy (\dag)-ből $I=\sum_{j=1}^{r}P_j$ is következik.

	A 3. azonossághoz azt vegyük észre,
	hogy tetszőleges $v\in V$ mellett $P_jv\in M_j$ a $\lambda_j$-hez tartozó sajátaltér egy eleme,
	így $NP_jv=\lambda_jP_jv$. Tehát
	\[
		Nv
		=
		N\left( Iv \right)
		=
		N\left( \sum_{j=1}^rP_jv \right)
		=
		\sum_{j=1}^rNP_jv
		=
		\sum_{j=1}^r\lambda_jP_jv
		=
		\left( \sum_{j=1}^r\lambda_jP_j \right)v.
	\]
	A 4. azonosság a 3. tulajdonságból az adjungálás tulajdonságai és $P^\ast=P$ figyelembe vételével adódik.
\end{proof}
Az eddigiek összefoglalásaként (\ref{pr:normlinkombkomplex} és \ref{pr:normpol}) kapjuk az alábbi szükséges és elegendő feltételét annak,
hogy egy komplex mátrix ortonormált bázisban diagonalizálható.
\begin{proposition}[Normalitás karakterizációja a spektrális felbontással]
	Legyen $V$ egy $\mathbb{C}$ feletti skalárisszorzatos-tér és $N\in L\left( V \right)$ egy lineáris transzformáció.
	Az $N$ transzformáció pontosan akkor normális,
	ha létezik $r\geq 1$ egész, léteznek $P_1,\ldots,P_r$ ortogonális projekciók és
	léteznek $\lambda_1,\ldots,\lambda_r\in\mathbb{C}$ komplex számok,
	amelyekre
	\begin{enumerate}
		\item $P_j\neq 0$, minden $j=1,\ldots,r$;
		\item $I=\sum_{j=1}^rP_j$;
		\item $N=\sum_{j=1}^r\lambda_jP_j$.
	\end{enumerate}
	Ha $N$ normális,
	akkor $\sigma\left( N \right)=\left\{ \lambda_1,\ldots,\lambda_r \right\}$,
	$N^\ast=\sum_{j=1}^r\overline{\lambda_j}P_j$,
	és tetszőleges $q\in\mathbb{C}[t]$ komplex együtthatós polinom mellett
	$q\left( N \right)$ is normális,
	továbbá
	$q\left( N \right)=\sum_{j=1}^rq\left( \lambda_j \right)P_j$.
\end{proposition}
\subsubsection{Önadjungált transzformáció}
Először azt kell látnunk,
hogy komplex vektortér esetében a normális transzformációk közül éppen azok az önadjungáltak,
amelyeknek a -- nem üres -- spektruma valós.
\begin{proposition}
	Legyen $V$ egy a komplex számtest feletti skalárisszorzatos-tér.
	$A\in L\left( V \right)$ egy lineáris transzformáció.
	Az $A$ pontosan akkor önadjungált, ha $A$ normális és $\sigma\left( A \right)\subseteq\mathbb{R}$
\end{proposition}
\begin{proof}
	Az, hogy önadjungált transzformáció normális és a (lehetséges, hogy üreshalmaz) spektruma valós,
	még $\mathbb{K}$ feletti skalárisszorzatos-tér mellett is igaz. (\ref{pr:normtul}).
	Megfordítva,
	ha egy komplex vektortéren $A$ normális, akkor van spektrális felbontása:
	\[
		A=\sum_{j=1}^r\lambda_jP_j,
	\]
	ahol $\left\{ \lambda_1,\ldots,\lambda_k \right\}$ a spektrum elemei.
	Mivel ezek valós számok,
	ezért
	\[
		A^\ast=\sum_{j=1}^r\overline{\lambda_j}P_j=
		\sum_{j=1}^r\lambda_jP_j=A.
		\qedhere
	\]
\end{proof}
Az önadjungált transzformációk spektrális felbontása
a normális transzformációk spektrális felbontásának speciális esete.
\begin{proposition}[spektrális felbontás]
	Legyen $V$ egy $\mathbb{C}$ feletti skalárisszorzatos-tér, és $A\in L\left( V \right)$
	egy önadjungált lineáris transzformáció.
	Jelölje $\sigma\left( N \right)=\left\{\lambda_1,\ldots,\lambda_k  \right\}$ az $A$ különböző sajátértékeit.
	Láttuk, hogy $k\geq 1$, és $\sigma\left( A \right)\subseteq \mathbb{R}$.
	Jelölje $P_j$ a $\ker\left\{ N-\lambda_jI \right\}$ altérre való merőleges vetítést.
	Ekkor
	\begin{enumerate}
		\item Minden $j=1,\ldots,r$ mellett $P_j\neq 0$;
		\item $I=\sum_{j=1}^rP_j$;
		\item $A=\sum_{j=1}^r\lambda_jP_j$.
		      \qedhere
	\end{enumerate}
\end{proposition}
Összefoglalásként (\ref{pr:normlinkombvalos} és \ref{pr:normpol}) kapjuk az alábbi szükséges és elegendő feltételét annak,
hogy egy komplex mátrix ortonormált bázisban diagonalizálható, és a diagonális elemei valós számok.
\begin{proposition}[Önadjungáltság karakterizációja a spektrális felbontással]
	Legyen $V$ egy $\mathbb{C}$ feletti ska\-lá\-ris\-szor\-za\-tos-tér és $A\in L\left( V \right)$ egy lineáris transzformáció.
	Az $A$ transzformáció pontosan akkor önadjungált,
	ha létezik $r\geq 1$ egész, léteznek $P_1,\ldots,P_r$ ortogonális projekciók és
	léteznek $\lambda_1,\ldots,\lambda_r\in\mathbb{R}$ valós számok,
	amelyekre
	\begin{enumerate}
		\item $P_j\neq 0$, minden $j=1,\ldots,r$;
		\item $I=\sum_{j=1}^rP_j$;
		\item $A=\sum_{j=1}^r\lambda_jP_j$.
	\end{enumerate}
	Ha $A$ önadjungált,
	akkor $\sigma\left( A \right)=\left\{ \lambda_1,\ldots,\lambda_r \right\}$,
	és tetszőleges $q\in\mathbb{R}[t]$ valós együtthatós polinom mellett
	$q\left( N \right)$ is önadjungált,
	továbbá
	$q\left( N \right)=\sum_{j=1}^rq\left( \lambda_j \right)P_j$.
\end{proposition}

\subsubsection{Unitér transzformáció}
Először azt kell látnunk,
hogy komplex vektortér esetében a normális transzformációk közül éppen azok az unitér transzformációk,
amelyekre a -- nem üres -- spektruma minden pontja egységnyi abszolútértékű.
\begin{proposition}
	Legyen $V$ egy a komplex számtest feletti skalárisszorzatos-tér.
	$A\in L\left( V \right)$ egy lineáris transzformáció.
	Az $A$ pontosan akkor unitér, ha $A$ normális és minden $\lambda\in\sigma\left( A \right)$ sajátértékre
	$|\lambda|=1$.
\end{proposition}
\begin{proof}
	Még $\mathbb{K}$ feletti skalárisszorzatos-tér mellett is igaz,
	hogy egy unitér transzformáció normális,
	és a (lehetséges, hogy üreshalmaz) spektruma minden pontja egységnyi abszolútértékű:
	Ugyanis a $\lambda$ spektrum pontra $\lambda\neq 0$,
	és valamely $u\neq 0$ vektor mellett
	$Au=\lambda u$, $A^\ast u=\overline{\lambda}u$. Így
	\[
		\left( \frac{1}{\lambda}-\overline{\lambda} \right)u
		=
		\frac{1}{\lambda}u-\overline{\lambda}u
		=
		A^{-1}u-A^\ast u
		=0.
	\]
	Ebből már
	$u\neq 0$ miatt $|\lambda|=1$ következik.

	Megfordítva,
	mivel komplex tér felett vagyunk,
	ezért $A$-nak normális volta miatt van
	$A=\sum_{j=1}^r\lambda_jPj$ spektrális felbontása,
	ahol $\lambda_1,\ldots,\lambda_r$ az $A$ sajátértékei.
	Tehát $A$ ortogonális projekciók egységnyi abszolútértékű együtthatókkal képzett lineáris kombinációja,
	amiről korábban már láttuk, hogy mindig unitér transzformációt eredményez.
\end{proof}
Az unitér transzformációk spektrális felbontása is
a normális transzformációk spektrális felbontásának speciális esete.
\begin{proposition}[spektrális felbontás]
	Legyen $V$ egy $\mathbb{C}$ feletti skalárisszorzatos-tér, és $A\in L\left( V \right)$
	egy unitér lineáris transzformáció.
	Jelölje $\sigma\left( N \right)=\left\{\lambda_1,\ldots,\lambda_k  \right\}$ az $A$ különböző sajátértékeit.
	Láttuk, hogy $k\geq 1$, $|\lambda_j|=1$ minden $j=1,\ldots,r$ mellett
	Jelölje $P_j$ a $\ker\left\{ N-\lambda_jI \right\}$ altérre való merőleges vetítést.
	Ekkor
	\begin{enumerate}
		\item Minden $j=1,\ldots,r$ mellett $P_j\neq 0$;
		\item $I=\sum_{j=1}^rP_j$;
		\item $A=\sum_{j=1}^r\lambda_jP_j$;
		\item $A^\ast=\sum_{j=1}^r\frac{1}{\lambda_j}P_j$.
		      \qedhere
	\end{enumerate}
\end{proposition}
Összefoglalásként (\ref{pr:normlinkombegys} és \ref{pr:normpol}) kapjuk az alábbi szükséges és elegendő feltételét annak,
hogy egy komplex mátrix ortonormált bázisban diagonalizálható, és a diagonális elemei egy abszolútértékű komplex számok.
\begin{proposition}[Unitér karakterizáció a spektrális felbontással]
	Legyen $V$ egy $\mathbb{C}$ feletti ska\-lá\-ris\-szor\-za\-tos-tér és $A\in L\left( V \right)$ egy lineáris transzformáció.
	Az $A$ transzformáció pontosan akkor unitér,
	ha létezik $r\geq 1$ egész, léteznek $P_1,\ldots,P_r$ ortogonális projekciók és
	léteznek $\lambda_1,\ldots,\lambda_r\in\mathbb{C}$ egységnyi abszolútértékű komplex számok,
	amelyekre
	\begin{enumerate}
		\item $P_j\neq 0$, minden $j=1,\ldots,r$;
		\item $I=\sum_{j=1}^rP_j$;
		\item $A=\sum_{j=1}^r\lambda_jP_j$.
	\end{enumerate}
	Ha $A$ unitér,
	akkor $\sigma\left( A \right)=\left\{ \lambda_1,\ldots,\lambda_r \right\}$,
	és $A^{-1}=\sum_{j=1}^r\frac{1}{\lambda_j}P_j=A^\ast$ is fennáll.
\end{proposition}




\section{Valós eset}
Mivel valós test feletti skalárisszorzatos-tér felett még az is előfordulhat,
hogy egy normális transzformációnak még sajátértéke sincs,
ezért nem várható általános eredmény normális transzformációk diagonalizálhatóságára.
\subsection{Szimmetrikus transzformáció}
Láttuk, hogy szimmetrikus transzformációk diagonalizálhatók,
ezért a komplex önadjungált esettel analóg állítás igaz valós vektortér esetére.
\begin{proposition}[spektrális felbontás]
	Legyen $V$ egy $\mathbb{R}$ feletti skalárisszorzatos-tér, és $A\in L\left( V \right)$
	egy szimmetrikus lineáris transzformáció.
	Jelölje $\sigma\left( N \right)=\left\{\lambda_1,\ldots,\lambda_k  \right\}\subseteq\mathbb{R}$ az $A$ különböző sajátértékeit.
	Láttuk, hogy $k\geq 1$.
	Jelölje $P_j$ a $\ker\left\{ N-\lambda_jI \right\}$ altérre való merőleges vetítést.
	Ekkor
	\begin{enumerate}
		\item Minden $j=1,\ldots,r$ mellett $P_j\neq 0$;
		\item $I=\sum_{j=1}^rP_j$;
		\item $A=\sum_{j=1}^r\lambda_jP_j$.
		      \qedhere
	\end{enumerate}
\end{proposition}
\begin{proof}
	Láttuk, hogy $A$ diagonalizálható
	\footnote{Innen a bizonyítás szó szerint azonos a komplex tér feletti normális transzformáció spektrálfebontásának igazolásával.}%
	, azaz létezik a térnek $A$ sajátvektoraiból álló
	bázisa,
	ezért
	\[
		M_1\oplus\dots\oplus M_r=V
		\qquad\text{itt }M_j\perp M_k\text{ minden }j\neq k,\tag{\dag}
	\]
	ahol $M_j=\ker\left( N-\lambda_jI \right)$.

	Világos,
	hogy minden $j$ mellett létezik $u_j\neq 0$, hogy $Au_j=\lambda_ju_j$,
	így $M_j\neq \left\{ 0 \right\}$,
	ergo $P_j\neq 0.$

	Láttuk korábban, hogy (\dag)-ből $I=\sum_{j=1}^{r}P_j$ is következik.

	A 3. azonossághoz azt vegyük észre,
	hogy tetszőleges $v\in V$ mellett $P_jv\in M_j$ a $\lambda_j$-hez tartozó sajátaltér egy eleme,
	így $AP_jv=\lambda_jP_jv$. Tehát
	\[
		Av
		=
		A\left( Iv \right)
		=
		A\left( \sum_{j=1}^rP_jv \right)
		=
		\sum_{j=1}^rAP_jv
		=
		\sum_{j=1}^r\lambda_jP_jv
		=
		\left( \sum_{j=1}^r\lambda_jP_j \right)v.
		\qedhere
	\]
\end{proof}
Összefoglalásként (\ref{pr:normlinkombkomplex} és \ref{pr:normpol}) kapjuk az alábbi szükséges és elegendő feltételét annak,
hogy egy valós szimmetrikus mátrix ortonormált bázisban diagonalizálható.
\begin{proposition}[Szimmetrikus karakterizáció spektrális felbontással]
	Legyen $V$ egy valós ska\-lá\-ris\-szor\-za\-tos-tér
	és $A\in L\left( V \right)$ egy lineáris transzformáció.
	Az $A$ transzformáció pontosan akkor szimmetrikus,
	ha létezik $r\geq 1$ egész, léteznek $P_1,\ldots,P_r$ ortogonális projekciók és
	léteznek $\lambda_1,\ldots,\lambda_r\in\mathbb{R}$ valós számok,
	amelyekre
	\begin{enumerate}
		\item $P_j\neq 0$, minden $j=1,\ldots,r$;
		\item $I=\sum_{j=1}^rP_j$;
		\item $A=\sum_{j=1}^r\lambda_jP_j$.
	\end{enumerate}
	Ha $A$ szimmetrikus,
	akkor $\sigma\left( A \right)=\left\{ \lambda_1,\ldots,\lambda_r \right\}$,
	és tetszőleges $q\in\mathbb{R}[t]$ valós együtthatós polinom mellett
	$q\left( N \right)$ is szimmetrikus,
	továbbá
	$q\left( N \right)=\sum_{j=1}^rq\left( \lambda_j \right)P_j$.
\end{proposition}

%\subsection{Ortogonális transzformáció}


%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix             %
%%%%%%%%%%%%%%%%%%%%%%%%

\section{Operátornorma}
A spektrális felbontási tétel egy fontos következménye,
hogy egy normális transzformációt egyértelműen meghatározza a spektruma, és a sajátaltereire való
ortogonális projekciók.
Ez lehetőséget ad egy tetszőleges $A$ lineáris operáció normájának meghatározására az $A^\ast A$ spektrumának ismeretében.

Ebben a szakaszban feltesszük,
hogy az operátornorma definíciója és alaptulajdonságai ismertek.
Összefoglalom az általunk használt legfontosabb tényeket.
Ha $A\in L\left( V,W \right)$ véges dimenziós normált téren értelmezett és a $W$ normált térbe képező lineáris operáció,
akkor
\[
	\max\left\{ \|Ax\|:\|x\|\leq 1 \right\}
	=
	\min\left\{ K\geq 0:\|Ax\|\leq K\|x\|\quad\forall x\in V \right\}
\]
számok azonosak és végesek. A közös érték az $\|A\|$ \emph{operátornorma}.\index{operátornorma}
Ha az $L\left( V,W \right)$ lineáris operációk vektorterét ellátjuk az operátornormával,
akkor egy normált teret kapunk.
A $W=V$ speciális esetben $A,B\in L\left( V \right)$-re $\|AB\|\leq \|A\|\|B\|$ egyenlőtlenség is fennáll.

A továbbiakban $V$ egy véges dimenziós $\mathbb{K}$ feletti skalárisszorzatos-tér.
\begin{definition}
	Legyen $A\in L\left( V \right)$ egy lineáris transzformáció,
	amelynek spektruma nem üres.
	Ekkor az $r\left( A \right)=\max\left\{ |\lambda|:\lambda\in\sigma\left( A \right) \right\}$
	számot az $A$ transzformáció
	\emph{spektrálsugarának}\index{spektrálsugár}
	mondjuk.
\end{definition}
Világos, hogy $r\left( A \right)$ a legkisebb olyan sugár,
amellyel a komplex síkra rajzolt origó középpontú kör tartalmazza az $A$ valamennyi sajátértékét.
Ezzel ekvivalens azt mondani,
hogy a spektrálsugár a sajátértékek origótol vett távolságainak maximuma.

Ha például $\sigma\left( A \right)\subseteq\mathbb{R}$, azaz valós sajátértékek vannak
\footnote{Például ha $A$ önadjungált, akkor így van.}, akkor a
$\lambda_{max}=\max\sigma\left( A \right)$ és
$\lambda_{min}=\min\sigma\left( A \right)$ jelölések értelmesek,
és $r\left( A \right)=\max\left\{ |\lambda_{min}|,|\lambda_{max}| \right\}$.
Ha még azt is feltesszük, hogy minden sajátérték nem negatív,
akkor
$r\left( A \right)=\lambda_{max}.$

\begin{proposition}
	Lgyen $V,W$ ugyanazon $\mathbb{K}$ test feletti skalárisszorzatos-terek.
	Legyen $A\in L\left( V,W \right)$ egy lineáris operáció.
	Ekkor az $A^\ast A$ egy önadjungált transzformáció,
	így $\sigma\left( A^\ast A \right)\neq \emptyset$,
	$A^\ast A$ valamennyi sajátértéke nem negatív valós szám,
	ezért
	$r\left( A^\ast A \right)=\max\sigma\left( A^\ast A \right)$.
\end{proposition}
\begin{proof}
	Világos, hogy $(A^\ast A)^\ast=A^\ast (A^\ast)^\ast=A^\ast A$,
	ergo $A^\ast A$ valóban egy önadjungált lineáris transzformációja a $V$ vektortérnek.
	Láttuk, hogy $A$ még diagonalizálható is, tehát spektruma nem üres.
	Azt is láttuk, hogy önadjungált transzformáció minden sajátértéke valós.
	No de, ha $\lambda\in\sigma\left( A^\ast A \right)$ és $x\in V$ egy nem zérus vektor,
	amelyre $A^\ast Ax=\lambda x$, akkor
	\[
		\lambda\ip{x}{x}
		=
		\ip{\lambda x}{x}
		=
		\ip{A^\ast Ax}{x}
		=
		\ip{Ax}{Ax}
		=\|Ax\|^2.
		\
	\]
	Ebből persze $\lambda\geq 0$ azonnal következik.
\end{proof}
\begin{proposition}
	Legyen mint az előző állitásban $A\in L\left( V,W \right)$.
	Definálja $Q\left( v \right)=\ip{A^\ast Av}{v}$ a $Q:V\to\left[ 0,\infty \right)$ nem negatív, valós függvényt.
	Legyen $\lambda_{max}=\max\sigma\left( A^\ast A \right)$,
	továbbá legyen
	$v_{max}\in V$ az $A^\ast A$ transzformációnak a $\lambda_{max}$ sajátértékéhez tartozó olyan sajátvektora,
	amelyre $\|v_{max}\|=1$.
	Ekkor
	\[
		\max\left\{ Q\left( v \right):v\in V,\|v\|\leq 1 \right\}
		=
		\lambda_{max}
		=
		Q\left( v_{max} \right).
		\qedhere
	\]
\end{proposition}
\begin{proof}
	Tudjuk, hogy $A^\ast A$ önadjungált, ezért mind $\mathbb{R}$, mind $\mathbb{C}$ felett diagonalizálható.
	Létezik tehát $u_1,\ldots,u_n$ ortonormált bázis $V$-ben,
	amelynek minden vektora sajátvektora $A^\ast A$-nak.
	Ha $v=\sum_{j=1}^n\alpha_ju_j$ olyan vektor,
	amelyre $\|v\|^2=\sum_{j=1}^n|\alpha_j|^2=1$,
	akkor
	\begin{multline*}
		Q\left( v \right)
		=
		\ip{A^\ast A\left( \sum_{j=1}^n\alpha_ju_j \right)}{\left( \sum_{k=1}^n\alpha_ku_k \right)}
		=
		\sum_{j=1}^n\sum_{k=1}^n\alpha_j\bar{\alpha}_k\ip{A^\ast Au_j}{u_k}
		\\
		=
		\sum_{j=1}^n\sum_{k=1}^n\alpha_j\bar{\alpha}_k\lambda_j\ip{u_j}{u_k}
		=
		\sum_{j=1}^n\alpha_j\bar{\alpha}_j\lambda_j
		=
		\sum_{j=1}^n|\alpha_j|^2\lambda_j
		\leq
		\lambda_{max}\sum_{j=1}^n|\alpha_j|^2
		=
		\lambda_{max}.
	\end{multline*}
	Itt az $u_i$ sajátvektorok tartoznak a $\lambda_i$ sajátértékekhez.
	Így
	\begin{math}
		\sup\left\{ Q\left( v \right):v\in V,\|v\|\leq 1 \right\}
		\leq
		\lambda_{max}.
	\end{math}

	No de,
	\begin{math}
		Q\left( v_{max} \right)
		=
		\ip{A^\ast Av_{max}}{v_{max}}
		=
		\lambda_{max}\ip{v_{max}}{v_{max}}
		=\lambda_{max},
	\end{math}
	amiből már az is látszik, hogy
	\begin{math}
		\lambda_{max}
		=
		Q\left( v_{max} \right)
		\leq
		\sup\left\{ Q\left( v \right):v\in V,\|v\|\leq 1 \right\}.
	\end{math}
\end{proof}
Összefoglalásként kapjuk az operátornorma és a spektrálsugár közti kapcsolatot:
\begin{proposition}
	Legyen $A\in L\left( V,W \right)$ egy lineáris operáció.
	Ekkor
	\[
		\|A\|^2=r\left( A^\ast A \right)=\max\sigma(A^\ast A).
		\qedhere
	\]
\end{proposition}
\begin{proof}
	Az $x \mapsto x^2$ függvény a számegyenes nem negatív része felett szigorúan monoton növő,
	ezért
	\[
		\|A\|^2
		=
		\left(
		\sup\left\{ \|Av\|:\|v\|\leq 1 \right\}
		\right)^2
		=
		\sup\left\{ \|Av\|^2:\|v\|\leq 1 \right\}
		=
		\sup\left\{ Q(v):\|v\|\leq 1 \right\}
		=
		\max\sigma\left( A^\ast A \right).
		\qedhere
	\]
\end{proof}
Ha ezt egy önadjungált transzformációra írjuk fel,
akkor még egyszerűbb állítást kapunk.
\begin{proposition}
	Legyen $A\in L\left( V \right)$ egy önadjungált lineáris transzformáció a
	$V$ véges dimenziós, $\mathbb{K}$ feletti ska\-lá\-ris\-szor\-za\-tos-té\-ren.
	Ekkor
	\[
		\|A\|
		=
		r(A)
		=
		\max\left\{ |\min\sigma(A)|,|\max\sigma(A)| \right\}.
		\qedhere
	\]
\end{proposition}
\begin{proof}
	Mivel $A$ önadjungált,
	ezért $A$-nak van spektrálfelbontása, és ha ez $A=\sum_{j=1}^r\lambda_jPj$,
	akkor az $A^2$ spektrálfelbontása $A^2=\sum_{j=1}^r\lambda_j^2P_j$,
	ahol $\lambda_1,\ldots,\lambda_r$ valós számok.
	Ez azt jelenti, hogy $A^2$ minden sajátértéke négyzete $A$ valamelyik sajátértékének.
	Így
	\[
		r\left( A^2 \right)
		=
		\max\left\{ \lambda_j^2:j=1,\ldots,r \right\}
		=
		\left(
		\max\left\{ |\lambda_{min}|,|\lambda_{max}| \right\}
		\right)^2
		=
		r\left( A \right)^2.
	\]
	No de, újra használva $A^\ast=A$ feltevést
	\[
		\|A\|^2
		=
		r\left( A^\ast A \right)
		=
		r\left( A^2 \right)
		=
		r\left( A \right)^2.
		\qedhere
	\]
\end{proof}
\begin{proposition}
	Tetszőleges $A\in L\left( V,W \right)$ lineáris operáció mellett
	\begin{enumerate}
		\item Teljesül az úgynevezett
		      \emph{\CStar-azonosság}\index{Ccsillag@\CStar-azonosság}:
		      \[
			      \|A\|^2=\|A^\ast A\|.
		      \]
		\item A transzformációnak és az adjungáltjának azonos a normája, azaz
		      \[
			      \|A\|
			      =
			      \|A^\ast\|.
		      \]
		\item Az $A^\ast A\in L\left( V \right)$ és az $AA^\ast\in L\left( W \right)$
		      transzformációk spektrálsugara azonos, azaz
		      \[
			      r\left( A^\ast A \right)=r\left( AA^\ast \right).
			      \qedhere
		      \]
	\end{enumerate}
\end{proposition}
\begin{proof}
	Mivel $A^\ast A$ önadjungált, ezért
	\[
		\|A^\ast A\|=r\left(A^\ast A  \right)=\|A\|^2,
	\]
	ami éppen a \CStar-azonosság.

	Felhasználva, hogy transzformációk szorzatának normája legfeljebb a normák szorzata,
	a \CStar-azonosság szerint
	\begin{math}
		\|A\|^2
		\leq
		\|A^\ast\|\|A\|,
	\end{math}
	amiből $\|A\|\leq\|A^\ast\|$ következik.
	No de, ez minden lineáris operácációra igaz,
	speciálisan az $A^\ast$ adjungáltra is.
	Emiatt az
	\begin{math}
		\|A^\ast\|
		\leq
		\|(A^\ast)^\ast\|
		=
		\|A\|
	\end{math}
	egyenlőtlenség is teljesül igazolva,
	hogy egy lineáris operácációnak és adjungáltjának azonos a normája.

	Alkalmazva az eddig igazoltatakat
	\[
		r\left( A^\ast A \right)
		=
		\|A\|^2
		=
		\|A^\ast\|^2
		=
		r\left( AA^\ast \right).
		\qedhere
	\]
\end{proof}
Fontos megérteni az utolsó azonosság jelentőségét.
Az $A^\ast A$ és az $AA^\ast$ más-más vektortereken vannak értelmezve.
Képzeljük el, hogy $V$ egy nyolc dimenziós tér és $W$ egy két dimenziós tér, $A\in L\left( V,W \right)$.
Ekkor $A$ mátrixa egy $2\times 8$ méretű mátrix, ezért $A^\ast A$ mátrixa egy $8\times 8$ méretű mátrix.
Hasonlóan $AA^\ast$ egy $2\times 2$ méretű mátrix.
Ha az $A$ operátor normáját kell meghatároznunk,
akkor nagyon nem mindegy,
hogy egy $8\times 8$ méretű-, vagy egy $2\times 2$ méretű önadjungált mátrix legnagyobb sajátértékét kell megtalálnunk.

Legvégül a \CStar-azonosság és következményei egy sorban
\[
	\|A\|^2=\|A^\ast A\|=r\left( A^\ast A \right)
	=
	r\left( AA^\ast \right)=\|A A^\ast\|=\|A^\ast\|^2.
\]

%%% Appendix %%%
\appendix
\renewcommand{\chaptername}{függelék}
\renewcommand{\appendixpagename}{Függelékek}\renewcommand{\appendixtocname}{\appendixpagename}
\appendixpage

\chapter{A komplex számokról}
Az algebra alaptétele, és a komplex számtest egyértelműsége. Elsősorban \parencite{MR1415833} és \parencite{10.2307/3647746} alapján
\section{Lineáris algebrai megközelítés}
Ha $\left\{ e_1,\ldots,e_n \right\}$ bázisa egy $\mathbb{C}$ feletti $V$ vektortérnek
és $m\left( t \right)\in\mathbb{C}\left[ t \right]$ egy pontosan $n$-ed fokú,
normált polinom,
például
$m\left( t \right)=\alpha_0+\alpha_1 t+\dots+\alpha_{n-1}t^{n-1}+t^{n}$,
akkor definiálja az $A\in L\left( V \right)$ lineáris transzformációt
\[
	A\left( e_k \right)=
	\begin{cases}
		e_{k+1}                           & , \text{ ha } 1\leq k \leq n-1 \\
		-\sum_{j=0}^{n-1}\alpha_j e_{j+1} & , \text{ ha } k=n.
	\end{cases}
\]
Világos, hogy $A^je_1=e_{j+1}$ tetszőleges $0\leq j\leq n-1$ mellett.
Ebből azonnal következik, hogy
\begin{enumerate}
	\item
	      $\left\{ e_1, Ae_1,A^2e_1,\ldots,A^{n-1}e_1\right\}$ rendszer lineárisan független,
	\item
	      $\left\{ e_1, Ae_1,A^2e_1,\ldots,A^{n-1}e_1,A^ne_1\right\}$ rendszer már lineárisan összefüggő,
	      hiszen
	      $A^ne_1=AA^{n-1}e_1=Ae_n=-\sum_{j=0}^{n-1}\alpha_j A^je_1$.
\end{enumerate}
Látjuk tehát, hogy $m$ a legalacsonyabb fokú normált polinom, melyre
$m\left( A \right)e_1=0$,
így az $e_1$ vektorhoz tartozó kis minimálpolinom éppen $m$.
\index{kis minimálpolinom}
Mivel a tér $n$-dimenziós, ezért $m$ egyben minimálpolinomja is $A$-nak.

Persze egy lineáris transzformációnak egy szám pontosan akkor sajátértéke,
ha az a minimálpolinomjának gyöke,
emiatt
az algebra alaptétele a következőképpen is fogalmazható:
\begin{center}
	\emph{Minden komplex vektortér feletti lineáris transzformációnak van sajátvektora.}
\end{center}

Jelölje $\rho\left( A \right)$ az $A$ transzformáció rangját,
azaz a képtér dimenzióját,
és $\nu\left( A \right)$ a defektust, azaz a magtér dimenzióját.
Jól ismert, hogy ha $A\in L\left( V \right)$ egy lineáris transzformáció
és $p\left( t \right)\in\mathbb{F}\left[ t \right]$ egy polinom, akkor
$\ker p\left( A \right)$ és
$\im p\left( A \right)$ is $A$-ra invariáns alterek.
Két lineáris trafóról azt mondjuk, hogy \emph{kommutálnak},\index{kommutál}
ha $A_1A_2=A_2A_1.$
Könnyen látható, hogy ha $A_1$ és $A_2$ kommutálnak, akkor tetszőleges két $p,q$
polinom mellett $p\left( A_1 \right)$ és $q\left( A_2 \right)$ is kommutálnak.\index{kommutál}
\begin{lemma}
	Legyenek az $A_1,A_2\in L\left( V \right)$ kommutáló lineáris transzformációk,
	valamint $p,q\in\mathbb{F}\left[ t \right]$ polinomok.
	Ekkor $\ker p\left( A_1 \right)$ és
	$\im p\left( A_1 \right)$ is invariáns alterek $q\left( A_2 \right)$-re.
\end{lemma}
\begin{proof}
	Elég megmutatni, hogy $\ker\left( A_1 \right)$-re és
	$\im\left( A_1 \right)$-re invariáns $A_2$, hiszen $p\left( A_1 \right)$
	és $q\left( A_2 \right)$ is kommutálnak.
	No de, az
	\(
	A_1 A_2 x=A_2A_1x=A_20=0
	\)
	szerint a magra,
	és $u=A_1v$ jelöléssel az
	\(
	A_2u=A_2 A_1v =A_1A_2v
	\)
	azonosságból a képre vonatkozó állítás következik.
\end{proof}
\begin{lemma}
	\label{le:indukcio}
	Legyen a $d>1$ pozitív egész rögzítve.
	Tegyük fel, hogy az $\mathbb{F}$ test rendelkezik avval a tulajdonsággal,
	hogy minden az $\mathbb{F}$ feletti $d$-vel nem osztható dimenziós vektortér tetszőleges
	lineáris trafójának van sajátvektora.
	Ekkor minden olyan $\mathbb{F}$ feletti vektortérre,
	amelynek dimenziója $d$-vel nem osztható igaz,
	hogy bármely két kommutáló lineáris transzformációjának van közös sajátvektora is.\index{kommutál}
\end{lemma}
\begin{proof}
	A tér dimenziója szerinti indukció.
	Egy egydimenziós tér minden nem nulla vektora sajátvektora tetszőleges lineáris transzformációjának,
	így persze bármely két egydimenziós téren értelmezett lineáris transzformációnak is van közös
	sajátvektora.
	Tegyük fel,
	hogy az állítás igaz minden legfeljebb $n$-dimenziós vektortérre
	és tekintsünk egy olyan $\mathbb{F}$ feletti vektorteret,
	amely éppen $n$-dimenziós és $d$ nem osztója $n$-nek.
	Jelölje $A_1$ és $A_2$ a szóban forgó két lineáris transzformációt.
	A feltétel szerint mondjuk $A_1$-nek van sajátvektora,
	így valamely $\mu\in\mathbb{F}$ mellett
	$\nu\left( A_1-\mu I \right)>0.$
	Ha az
	$\nu\left( A_1-\mu I \right)=n$, akkor a tér minden vektora sajátvektora $A_1$-nek,
	így mivel $A_2$-nek is van sajátvektora, ezért ez közös sajátvektoruk is.
	Ha
	$\nu\left( A_1-\mu I \right)<n$, akkor
	$\nu\left( A_1-\mu I \right)+\rho\left( A_1-\mu I \right)=n$ miatt az
	$K=\ker\left( A_1-\mu I \right)$ és a
	$L=\im\left( A_1-\mu I \right)$
	valódi alterek dimenziójának egyike nem osztható $d$-vel.
	No de $A_2$ és $A_1$ invariáns $K$-ra is és $L$-re is
	az előző lemma miatt,
	így alkalmazhatjuk az indukciós feltevést $K$ és $L$ közül
	a $d$-vel nem osztható dimenziós altérre,
	amely garantálja az $A_1$ és $A_2$ közös sajátvektorát.
\end{proof}

Mivel a karakterisztikus polinom gyökei a sajátértékek, és mivel egy $n$-dimenziós
téren értelmezett lineáris transzformációnak pontosan $n$-edfokú a karakterisztikus polinomja,
ezért a Bolzano-tétel szerint egy $\mathbb{R}$ feletti páratlan dimenziós vektortér lineáris transzformációjának van sajátvektora.
A fenti lemma tehát kommutáló transzformációk esetében közös sajátvektort garantál
páratlan dimenziójú valós vektortér felett.

\begin{proposition}
	Egy páratlan dimenziós komplex vektortér
	minden lineáris transzformációjának van sajátvektora.
\end{proposition}

\begin{proof}
	Legyen $V$ a $\mathbb{C}$ feletti vektortér, $n$ páratlan szám a dimenziója,
	$A\in L\left( V \right)$ a transzformáció.
	Jelölje $\mathcal{H}=\left\{ A\in L\left( V \right):A=A^* \right\}$
	az önadjungált transzformációkat.
	Világos, hogy $\mathcal{H}$ egy valós, $n^2$ dimenziós vektortér.
	Minden $C\in L\left( V \right)$ lineáris transzformáció előáll
	\[
		C=
		\frac{C+C^*}{2}+
		i\frac{C-C^*}{2i}
	\]
	alakban, ahol persze
	$\frac{1}{2}\left( C+C^* \right)$ és
	$\frac{1}{2i}\left( C-C^* \right)$ is önadjungált transzformációk.
	A továbbiakban rögzített $A\in L\left( V \right)$ mellett jelölje
	$L_1,L_2:\mathcal{H}\to\mathcal{H}$ függvényeket.
	\[
		L_1\left( B \right)=
		\frac{AB+BA^*}{2}
		\text{ és }
		L_2\left( B \right)=
		\frac{AB-BA^*}{2i}
	\]
	Világos, hogy $L_1$ és $L_2$ lineáris transzformációk a $\mathcal{H}$ valós
	vektortéren,
	amelyekre minden $B\in \mathcal{H}$ mellett
	\[
		AB=L_1\left( B \right)+iL_2\left( B \right).
	\]
	E két operátor felcserélhető, hiszen tetszőleges $B\in\mathcal{H}$ mellett, ugyanis
	\begin{eqnarray*}
		L_1\circ L_2\left( B \right)=
		\frac{1}{4i}\left( A\left( AB-BA^* \right)+\left( AB-BA^* \right)A^* \right)=
		\frac{1}{4i}\left( A^2B+BA^{*2} \right)&&\\
		L_2\circ L_1\left( B \right)=
		\frac{1}{4i}\left( A\left( AB+BA^* \right)-\left( AB+BA^* \right)A^* \right)=
		\frac{1}{4i}\left( A^2B+BA^{*2} \right)&.&
	\end{eqnarray*}
	Alkalmazhatjuk az $n^2$ páratlan dimenziós valós vektortérre az előző lemmát.
	Létezik $B\in\mathcal{H}$ nem a konstans zéró transzformáció és
	létezik $\alpha_1,\alpha_2$ valós szám,
	melyekre
	\(
	L_1\left( B \right)=\alpha_1B
	\text{ és }
	L_2\left( B \right)=\alpha_2B.
	\)
	Tehát ha valamely $v\in V$ vektorra $Bv\neq 0$,
	akkor
	\[
		A\left( Bv \right)=
		\alpha_1Bv+
		i\alpha_2Bv=
		\left(\alpha_1+i\alpha_2\right)Bv,
	\]
	ergo $\alpha_1+i\alpha_2$ sajátértéke, és $Bv$ sajátvektora $A$-nak.
\end{proof}

Minden komplex számnak van gyöke, ezért minden legfeljebb másodfokú komplex együtthatós
polinomnak van zérushelye.
Az algebra alaptételével ekvivalens állítás tehát, hogy egy komplex vektortér felett
minden lineáris transzformáció minimálpolinomjának van legfeljebb másodfokú faktora.%
\footnote{Azaz, van legfeljebb két dimenziós nem triviális invariáns altere.}
Az is nyilvánvaló, hogy minden egész szám egyértelműen áll $2^kn$ alakban, ahol $n$ páratlan.

A következő állítás tehát az algebra alaptételének egy ekvivalens megfogalmazása.
\begin{proposition}
	Tekintsünk egy $V$ komplex vektorteret, amelynek dimenziója $2^kn$ alakú,
	ahol n páratlan egész.
	Ekkor $V$ minden lineáris transzformációja minimálpolinomjának
	van legfeljebb másodfokú faktora.
\end{proposition}
\begin{proof}
	A $k$ szerinti indukció.
	A $k=0$ esetben az előző állítás szerint van sajátvektor is,
	tehát első fokú faktora is van a minimálpolinomnak.
	Tegyük fel, hogy igaz az állítás minden $k$-nál kisebb szám mellett.
	E feltétel azt jelenti, hogy minden olyan vektortérre igaz az állítás,
	-- így az algebra alaptétele --
	melynek dimenziója $2^l$ páratlan szorosa $l<k$ mellett,
	azaz amelynek dimenzióját a $d=2^k$ szám nem osztja.
	Alkalmazva a lemmát azt kapjuk, hogy ilyen dimenziójú vektortér kommutáló
	lineáris transzformációinak van közös sajátvektora is.
	Legyen tehát $V$ dimenziója $2^kn$ alakban felírva, ahol $n$ páratlan.

	Rögzítsünk a térnek egy bázisát, és jelölje
	\(
	\mathcal{S}\subseteq L(V)
	\)
	azon lineáris transzformációk összességét,
	amelyeknek mátrixa az itt rögzített bázisban szimmetrikus.
	Ez egy komplex vektortér,
	amelyre
	\[
        \dim\left( \mathcal{S} \right)=
		\frac{2^kn\left( 2^kn+1 \right)}{2}=
		2^{k-1}n\left(2^kn+1  \right)=
		2^{k-1}n',
	\]
	ahol $n'$ páratlan.
	Alkalmazhatjuk tehát a komplex $\mathcal{S}$ vektortérre az indukciós feltevést.
	Ehhez, rögzített $A\in L\left( V \right)$ lineáris transzformáció mellett,
	vezessük be az $L_1,L_2:\mathcal{S}\to\mathcal{S}$ függvényeket.
	\[
		L_1\left( B \right)=AB+BA^T \text{ és }
		L_2\left( B \right)=ABA^T.
	\]
	Könnyű számolgatás mutatja, hogy az $L_1$ és $L_2$ lineáris transzformációk kommutálnak:\index{kommutál}
	\begin{multline*}
		\left( L_1\circ L_2\right)B=
		A\left( ABA^T \right)+\left( ABA^T \right)A^T=
		A\left( ABA^T+BA^TA^T \right)
		=\\
		A\left( AB+BA^T \right)A^T =
		\left( L_2\circ L_1 \right)B.
	\end{multline*}
	Létezik tehát közös sajátvektora az $L_1$ és $L_2$ transzformációknak, ergo
	létezik $\lambda,\mu\in\mathbb{C}$ komplex szám,
	és létezik nem az azonosan zérus $B\in\mathcal{S}$ lineáris transzformáció,
	amelyekre
	$
		L_1\left( B \right)=\lambda B
	$
	és
	$
		L_2\left( B \right)=\mu B,
	$
	azaz
	$
		AB+BA^T=\lambda B
	$
	és
	$
		ABA^T=\mu B.
	$
	Ebből
	\[
		A\lambda B=
		A\left( AB+BA^T \right)=
		A^2B + ABA^T=
		A^2B + \mu B.
	\]
	Ha tehát $u=Bv\neq 0$, akkor
	\[
		A^2u-\lambda Au+\mu u=0.
	\]
	Ha $u$ nem sajátvektora $A$-nak, akkor a
	$p\left( t \right)=t^2-\lambda t+\mu$
	egy nem zérus vektor kis minimálpolinomja,
	ezért a $p$ polinom osztója a minimálpolinomnak.
\end{proof}

\section{Analízis megközelítés}
Az algebra alaptételének (\ref{Th:FundOfAlg}. tétel) bizonyításához felhasználunk néhány az elemi analízisből
jól ismert állítást.
Ezek közül a lényegesebbek az alábbiak:
\begin{enumerate}
	\item  Az origó középpontú zárt kör a sík
	      kompakt részhalmaza.

	\item  Kompakt halmazon folytonos függvény felveszi minimumát.

	\item  Minden komplex számnak van legalább egy $k$-adik gyöke ($k>0$).

	\item  Komplex síkon differenciálható függvények
	      folytonosak is.
\end{enumerate}

A főtételt könnyen megérthetjük, ha áttekintjük
a felé vezető utat.
Két lényeges pontot kell látnunk.
Az első (\ref{Th:Cauchy1}. állítás) kompaktsági meggondolás, polinomok növekedési ütemére (\ref{Th:Grow}. lemma) támaszkodva.
Ez utóbbi lemma talán önmagában is érdekes, hiszen azt állítja, hogy egy polinom legalább a fokszáma nagyságrendjében növekszik.
A másik döntő lépés (\ref{Th:Cauchy2}. állítás) az Argand-féle becslésen (\ref{Th:Argand}. lemma) nyugszik.
Ez a holomorf függvényekre vonatkozó nyílt leképezés tételnek az itt éppen elegendő speciális esete.

\begin{lemma}
	\label{Th:Grow}Legyen $f:\mathbb{C}\rightarrow \mathbb{C}$ nem a konstans nulla,
	komplex $n$-edfokú polinom.
	Ekkor létezik olyan $r>0$ valós szám, hogy minden $z\in \mathbb{C}$ $,\left| z\right| >r$ esetén $\left| f\left( z\right) \right| >\frac{1}{2}\left| a_{n}\right| \left|
		z^{n}\right| $.
\end{lemma}

\begin{proof}
	Nyilván $f\left( z\right) =a_{n}z^{n}+a_{n-1}z^{n-1}+\ldots +a_{1}z+a_{0}
	$ alakú, ahol $a_{n}\neq 0$.
	Világos, hogy $z\neq 0$ esetén
	\[
		f\left( z\right) =z^{n}\left( a_{n}+a_{n-1}\frac{1}{z}+\ldots +a_{1}\frac{1}{z^{n-1}}+a_{0}\frac{1}{z^{n}}\right) \text{.}
	\]
	Legyen $h:\mathbb{C}\rightarrow \mathbb{C}$ komplex polinom a következőképpen definiálva:
	\[
		h\left( w\right) :=a_{n-1}w+a_{n-2}w^{2}+\ldots +a_{1}w^{n-1}+a_{0}w^{n}.
	\]
	Ekkor minden $z\in \mathbb{C}$, $z\neq 0$ mellett
	\begin{equation}
		f\left( z\right) =z^{n}\left( a_{n}+h\left( 1/z\right) \right) .
		\label{Eq:C}
	\end{equation}
	A $h$ folytonos $0$-ban, és $h\left( 0\right) =0$, így létezik
	olyan $\delta >0$ valós szám, melyre $w\in \mathbb{C}$, $\left|
		w\right| <\delta $ esetén $\left| h\left( w\right) \right| <\frac{1}{2}\left| a_{n}\right| $.
	\'{I}gy ha $\left| z\right| >1/\delta $, akkor $\left| 1/z\right| <\delta $, amiből következik, hogy
	\[
		\left| h\left( 1/z\right) \right| <\frac{1}{2}\left| a_{n}\right| .
	\]
	A háromszög-egyenlőtlenség és (\ref{Eq:C}) miatt
	az $r:=1/\delta $ választással minden
	$z\in \mathbb{C},\left| z\right| >r$ mellett
	%\begin{multline*}
	\[
		\left| f\left( z\right) \right|
		=
		\left| z^{n}\right| \left| a_{n}+h\left(1/z\right) \right| \geq \left| z^{n}\right| \left( \left| a_{n}\right|
		-\left| h\left( 1/z\right) \right| \right)
		>
		\left| z^{n}\right| \left( \left| a_{n}\right| -\frac{1}{2}\left|
		a_{n}\right| \right) =\frac{1}{2}\left| a_{n}\right| \left| z^{n}\right| .\qedhere
		%\end{multline*}
	\]
\end{proof}

\begin{proposition}
	\label{Th:Cauchy1}Legyen $f:\mathbb{C}\rightarrow \mathbb{C}$ komplex polinom.
	Ekkor létezik $c\in \mathbb{C}$ komplex szám, melyre
	\[
		\left| f\left( c\right) \right| =\inf \left\{ \left| f\left( z\right)
		\right| :z\in \mathbb{C}\right\} .\qedhere
	\]
\end{proposition}
\begin{proof}
	Most úgy válasszuk meg az $r$ pozitív valós számot,
	hogy egyrészt az előző lemma, másrészt az
	$\frac{1}{2}\left|a_{n}\right| r^{n}>\left| a_{0}\right|$
	feltétel is teljesüljön.
	Ekkor
	persze minden $z\in \mathbb{C}$, $\left| z\right| >r$ esetén
	\[
		\left| f\left( z\right) \right| >\frac{1}{2}\left| a_{n}\right| \left|
		z^{n}\right| >\left| a_{0}\right| =\left| f\left( 0\right) \right|
	\]
	is teljesül.
	Ez azt jelenti, hogy ha bevezetjük az $\alpha :=\inf \left\{ \left|
		f\left( z\right) \right| :z\in \mathbb{C}\text{, }\left| z\right| \leq
		r\right\} $ jelölést, akkor minden $z\in \mathbb{C},\left| z\right| >r$
	esetén $\left| f\left( z\right) \right| \geq \left| f\left( 0\right)
		\right| \geq \alpha .$ Persze ha $\left| z\right| \leq r$ ez utóbbi
	akkor is teljesül, ezért
	\[
		\alpha \leq \inf \left\{ \left| f\left( z\right) \right| :z\in \mathbb{C}\right\} .
	\]
	Mivel a fordított irányú egyenlőtlenség triviális,
	azt kapjuk, hogy
	\[
		\inf \left\{ \left| f\left( z\right) \right| :z\in \mathbb{C}\text{, }\left|
		z\right| \leq r\right\} =\inf \left\{ \left| f\left( z\right) \right| :z\in
		\mathbb{C}\right\} .
	\]
	De láttuk, hogy $\left\{ z\in \mathbb{C}:\left| z\right| \leq r\right\}
		\subseteq \mathbb{C}$ a komplex számsík kompakt halmaza, így az $f$
	polinom és az abszolútérték-függvény folytonossága miatt létezik $c\in \mathbb{C}$, $\left| c\right| \leq r$, amelyre
	\[
		\left| f\left( c\right) \right| =\alpha =\inf \left\{ \left| f\left(
		z\right) \right| :z\in \mathbb{C}\text{, }\left| z\right| \leq r\right\} =\inf
		\left\{ \left| f\left( z\right) \right| :z\in \mathbb{C}\right\}. \qedhere
	\]
\end{proof}

\begin{lemma}[Argand]
	\label{Th:Argand}Legyen $k\in \mathbb{N}$, $k>0$ egész és $b\in \mathbb{C}$, $b\neq 0$ komplex szám, valamint $g:\mathbb{C}\rightarrow \mathbb{C}$, $g\left( 0\right) =0$ olyan függvény, amely a $0\in \mathbb{C}$ pontban
	folytonos.
	Tekintsük a következőképpen definiált $h:\mathbb{C}\rightarrow \mathbb{C}$ leképezést:
	\[
		h\left( z\right) :=1+bz^{k}+z^{k}g\left( z\right) .
	\]
	Ekkor létezik $z\in \mathbb{C}$ komplex szám, melyre $\left| h\left(
		z\right) \right| <1.$
\end{lemma}

\begin{proof}
	Azt fogjuk megmutatni, hogy található $d\in \mathbb{C}$ és $t\in
		\mathbb{R}$, $t\in \left( 0,1\right) $ melyekre $\left| h\left( dt\right)
		\right| <1.$\[
		h\left( dt\right) =1+bd^{k}t^{k}+d^{k}t^{k}g\left( dt\right) .
	\]
	Válasszuk $d$ komplex számot úgy, hogy $bd^{k}=-1$ teljesüljön, azaz $d$ legyen a $-1/b$ komplex szám egyik $k$ -adik gyöke.
	Ekkor
	\[
		h\left( dt\right) =1-t^{k}+d^{k}t^{k}g\left( dt\right) .
	\]
	Amiből
	\[
		\left| h\left( dt\right) \right| \leq 1-t^{k}+t^{k}\left| d^{k}g\left(
		dt\right) \right| =1-t^{k}\left( 1-\left| d^{k}g\left( dt\right) \right|
		\right) .
	\]
	Ebből látszik, hogy elegendő megválasztani $t\in \left(
		0,1\right) $ -et olyan módon, hogy $\left| d^{k}g\left( dt\right)
		\right| <1.$ Ez pedig nyilván megtehető $g\left( 0\right) =0$ és
	$g$ -nek a $0$ pontban feltett folytonossága miatt.
\end{proof}

\begin{proposition}
	\label{Th:Cauchy2}Legyen $f:\mathbb{C}\rightarrow \mathbb{C}$ legalább elsőfokú polinom.
	Ekkor minden $c\in \mathbb{C}$, $f\left( c\right) \neq 0$
	komplex számhoz létezik olyan $\hat{c}\in \mathbb{C}$ komplex szám,
	melyre
	\[
		\left| f\left( \hat{c}\right) \right| <\left| f\left( c\right) \right| .
	\]
\end{proposition}

\begin{proof}
	Tekintsük a
	\[
		h\left( z\right) :=\frac{f\left( z+c\right) }{f\left( c\right) }
	\]
	legalább elsőfokú polinomot.
	Vegyük észre, hogy $h\left(
		0\right) =1$, így e polinom
	\[
		h\left( z\right) =1+a_{k}z^{k}+\ldots +a_{n}z^{n}
	\]
	alakú, ahol $a_{k}\neq 0$ valamely $k\geq 1$ -re, hiszen az $f$ és
	ebből következően a $h$ polinom nem konstans.
	Tovább alakítva:
	\[
		h\left( z\right) =1+a_{k}z^{k}+z^{k}\left( a_{k+1}z+\ldots
		+a_{n}z^{n-k}\right) .
	\]
	Világos, hogy a fenti $h$ polinomra alkalmazható az előző
	lemma, így létezik $u\in \mathbb{C}$, melyre $\left| h\left( u\right)
		\right| <1.$ Ez viszont azt jelenti, hogy
	\[
		\left| \frac{f\left( u+c\right) }{f\left( c\right) }\right| <1
	\]
	amiből $\hat{c}:=u+c$ választással kapjuk, hogy
	\[
		\left| f\left( \hat{c}\right) \right| =\left| f\left( u+c\right) \right|
		<\left| f\left( c\right) \right| .\qedhere
	\]
\end{proof}

\begin{FA}\label{Th:FundOfAlg}
	Legyen $f\left( t \right)\in\mathbb{C}[t]$ nem konstans polinom.
	Ekkor $f$-nek van gyöke a komplex számok körében.
\end{FA}

\begin{proof}
	Láttuk (\ref{Th:Cauchy1}. állítás), hogy van olyan $c\in \mathbb{C}$ melyre
	\[
		\left| f\left( c\right) \right| =\inf \left\{ \left| f\left( z\right)
		\right| :z\in \mathbb{C}\right\} \text{.}
	\]
	Ha $f(c)\neq 0$ lenne, akkor lenne (\ref{Th:Cauchy2}. állítás) $\hat{c}\in \mathbb{C}$ melyre
	\[
		\left| f\left( \hat{c}\right) \right| <\left| f\left( c\right) \right| =\inf
		\left\{ \left| f\left( z\right) \right| :z\in \mathbb{C}\right\}
	\]
	is teljesülne, ami nem lehetséges.
\end{proof}

\section{A komplex számok egyértelműsége}

Az alábbiakban azt fogjuk megvizsgálni, hogy lehet-e a sík pontjain
a komplex számok bevezetésénél megadott szorzástól eltérő
módon bevezetni szorzás műveletet úgy, hogy ez a valós számokon
már megszokott szorzás kiterjesztése legyen, és a sík ellátva
a szokásos összeadással valamint evvel a szorzásnak nevezett művelettel test legyen.
Meg fogjuk mutatni, hogy ez nem lehetséges.
Sőt
azt is látni fogjuk, hogy az $\mathbb{R}$ -től illetve a $\mathbb{C}$ -től
eltekintve nincs véges dimenziós $\mathbb{R}$ feletti vektortér, amely
test lesz olyan összeadásnak illetve szorzásnak nevezett művelettel,
amely az $\mathbb{R}$ -ben szokásos összeadás és szorzás kiterjesztése.

\begin{proposition}
	\label{Th:CUnique1}Legyen $\mathbb{G}$ az $\mathbb{R}$ valós számtestet
	tartalmazó olyan test, amelyben az $\left( +,\cdot \right) $ műveletek az $\mathbb{R}$ -ben szokásos $\left( +,\cdot \right) $ műveletek kiterjesztései, valamint amely kétdimenziós vektortér $\mathbb{R}$ felett.
	Ekkor $\mathbb{G}$ test izomorf $\mathbb{C}$-vel.
\end{proposition}

\begin{proof}
	Meg fogjuk mutatni, hogy létezik $w\in \mathbb{G}$ melyre $w^{2}=-1$.
	Ekkor
	készen is leszünk, mert nyilván $w\notin \mathbb{R}$ így $\lin\{1,w\}=\mathbb{G}$ amiből könnyen látható, hogy
	az $\alpha +w\beta \longmapsto \alpha +i\beta $ megfeleltetés
	izomorfizmus $\mathbb{G}$ és $\mathbb{C}$ között.
	\newline
	Legyen $v\in \mathbb{G}\smallsetminus \mathbb{R}$.
	Ilyen $v$ létezik, mivel $\mathbb{G}
	$ kétdimenziós.
	Világos, hogy az $\left\{
		1,v,v^{2}\right\} $ vektorrendszer lineárisan összefüggő,
	hiszen három vektor egy kétdimenziós vektortérben.
	\'{I}gy léteznek $\alpha ,\beta \in \mathbb{R}$ valós számok melyekre $v^{2}=\alpha +\beta v$.
	Most legyen $\gamma :=\frac{-\beta }{2}
	$.
	Ekkor nyilván $v^{2}=\alpha -2\gamma v$.
	Most tekintsük a $\left(
		v+\gamma \right) ^{2}$ kifejezést.

	\[
		\left( v+\gamma \right) ^{2}=v^{2}+2\gamma v+\gamma ^{2}=\alpha +\gamma ^{2}
	\]
	Azt kaptuk tehát, hogy létezik $r\in \mathbb{R}$ valós szám
	melyre $\left( v+\gamma \right) ^{2}=r$.
	De vegyük észre, hogy ha $r\geq 0$ lenne,
	akkor $t=\sqrt{r}\in\mathbb{R}$
	jelöléssel $\left( v+\gamma \right)^2=t^2$ következne, amiből pedig $v\in \mathbb{R}$ következtetésre juthatnánk ellentétben a $v$ -re kiindulásul tett feltétellel.
	Ebből már
	világos, hogy ha $w$ -t
	\[
		w:=\frac{v+\gamma }{\sqrt{\left| r\right| }}\in \mathbb{G}
	\]
	módon definiáljuk akkor $w^{2}=\frac{r}{\left| r\right| }=-1$,
	hiszen $r<0$.
\end{proof}

\begin{definition}
	Legyen $\mathbb{G}$ az $\mathbb{R}$ valós számtestet tartalmazó olyan
	test amelyben az $\left( +,\cdot \right) $ műveletek az $\mathbb{R}$ -ben
	szokásos $\left( +,\cdot \right) $ műveletek kiterjesztései.
	Az $x\in \mathbb{G}$ elemet $\mathbb{R}$ felett algebrainak nevezzük, ha létezik nem konstans zéró valós együtthatós polinom,
	melynek $x$ gyöke.
\end{definition}

\begin{proposition}
	Legyen $\mathbb{G}$ az $\mathbb{R}$ valós számtestet tartalmazó olyan
	test amelyben az $\left( +,\cdot \right) $ műveletek az $\mathbb{R}$ -ben
	szokásos $\left( +,\cdot \right) $ műveletek kiterjesztései,
	valamint amely véges dimenziós vektortér $\mathbb{R}$ felett.
	Ekkor $\mathbb{G}$ minden eleme algebrai $\mathbb{R}$ felett.
\end{proposition}

\begin{proof}
	Legyen $\mathbb{G}$ dimenziója $n$ és $v\in \mathbb{G}$ tetszőleges
	vektor.
	Tekintsük az
	\[
		\left\{ 1,v,v^{2},\ldots ,v^{n}\right\}
	\]
	vektorrendszert.
	Ez nyilván lineárisan összefüggő,
	hiszen $n+1$ vektor egy $n$ dimenziós vektortérben.
	\'{I}gy van nem
	triviális $0$ -t adó lineáris kombinációja.
	Azaz léteznek $\alpha _{0},\alpha _{1},\ldots \alpha _{n}$ nem csupa nulla valós számok melyekre
	\[
		\sum_{i=0}^{n}\alpha _{i}v^{i}=0
	\]
	De ez pont azt jelenti, hogy ha a $p$ polinomot
	\[
		p\left( x\right) :=\sum_{i=0}^{n}\alpha _{i}x^{i}
	\]
	módon definiáljuk, akkor $p\left( v\right) =0$.
\end{proof}

\begin{proposition}[Weierstrass]
	Legyen $\mathbb{G}$ az $\mathbb{R}$ valós számtestet tartalmazó olyan
	test, amelyben az $\left( +,\cdot \right) $ műveletek az $\mathbb{R}$-ben
	szokásos $\left( +,\cdot \right) $ művelet kiterjesztései,
	valamint amelynek minden eleme algebrai $\mathbb{R}$ felett.
	Ekkor két eset
	lehetséges: Vagy $\mathbb{G}$ test-izomorf $\mathbb{R}$-rel, vagy $\mathbb{G}$
	test-izomorf $\mathbb{C}$-vel.
\end{proposition}

\begin{proof}
	Tegyük fel, hogy $\mathbb{G}$ tartalmaz $\mathbb{R}$ -től különböző $v$ vektort.
	Ekkor azt kell megmutatni, hogy $\mathbb{G}$ izomorf $\mathbb{C}$ -vel.\newline
	Először azt mutatjuk meg, hogy
	\begin{equation}
		v^{2}\in \lin\left\{ 1,v\right\}
	\end{equation}
	\label{Eq:Alg}Mivel $v$ algebrai $\mathbb{R}$ felett, ezért létezik $p$
	valós együtthatós polinom melyre $p\left( v\right) =0$.
	De láttuk, hogy valós együtthatós polinom első- és másodfokú tényezők szorzatára bomlik (\ref{pr:RealPolFact}.),
	ami azt jelenti, hogy van olyan $q$ első, vagy másodfokú valós együtthatós polinom melyekre $q\left( v\right) =0$.
	Ha $q$ első
	fokú lenne az azt jelentené, hogy található $\alpha ,\beta
		\in \mathbb{R}$ valós számok melyekre $\alpha +\beta v=0$, azaz $v\in \mathbb{R}$ lenne.
	Tehát $q$ pontosan másodfokú.
	Ez viszont azt
	jelenti, hogy léteznek $\left\{ \alpha ,\beta ,\gamma \right\} \subset
		\mathbb{R}$ valós számok $\gamma \neq 0$, melyekre $\alpha +\beta
		v+\gamma v^{2}=0$.
	Ebből persze
	\[
		v^{2}=\frac{-\alpha }{\gamma }+\frac{-\beta }{\gamma }v
	\]
	már könnyen következik, bizonyítva (\ref{Eq:Alg})-et.\newline
	Most megmutatjuk, hogy $\mathbb{G}$ tartalmaz $\mathbb{C}$ -vel izomorf testet.
	Tekintsük a
	\[
		K:=\lin\left\{ 1,v\right\}
	\]
	kétdimenziós alterét $\mathbb{G}$-nek.
	Világos, hogy ennek test
	voltához elegendő megmutatni, hogy a $\mathbb{G}$-beli műveletekre zárt.
	Az összeadásra való zártság triviális a $K$
	altér mivoltából, a szorzásra való zártság pedig
	(\ref{Eq:Alg}) következménye.
	\'{I}gy tehát $K$ két dimenziós test kiterjesztése $\mathbb{R}$ -nek, ami azt jelenti (\ref{Th:CUnique1}.),
	hogy $K$ test izomorf $\mathbb{C}$ -vel.\newline
	Utoljára megmutatjuk, hogy $\mathbb{G}$ minden $w$ eleme $\mathbb{C}$-beli
	is.
	Mivel $w$ algebrai $\mathbb{R}$ felett, ezért létezik $p$ valós együtthatós polinom, melyre $p\left( w\right) =0$.
	De tekinthetjük $p$ -t a komplex számtest feletti polinomnak is, így $p$
	felbomlik elsőfokú komplex polinomok szorzatára (\ref{pr:PolFact}).
	Ez viszont azt jelenti, hogy létezik olyan $c$ komplex szám,
	melyre $w-c=0$, tehát $w\in \mathbb{C}$ valóban teljesül.
\end{proof}

\chapter{A Frobenius-normálalak}

A kis minimálpolinom fogalmát általánosítjuk.
\begin{definition}[irányító polinom]\index{irányító polinom}
    Legyen $A\in L\left( V \right)$ egy lineáris transzformáció,
    $M\subseteq V$ egy az $A$-ra nézve invariáns altér, 
    és $v\in V$ egy rögzített vektor.
    Tekintsük az $\mathbb{F}\left[ t \right]$ polinomgyűrű következő részhalmazát.
    \[
        S\left( v;M \right)
        =
        \left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)v\in M \right\}.
    \]
    Látható, hogy $S\left( v;M \right)$ egy ideálja $\mathbb{F}\left[ t \right]$-nek.
    A fenti ideált generáló normált polinomot nevezzük az $A$ transzformáció $v$-t $M$-be vivő \emph{irányító polinomjának}
    és $p_{v;M}$ módon jelöljük.
\end{definition}
Világos, hogy ha $M_1\subseteq M_2$ invariáns alterek, akkor $S\left( v;M_1 \right)\subseteq S\left( v;M_2 \right)$, 
ezért $p_{v;M_2}|p_{v;M_1}$.
Speciálisan, ha $M=\left\{ 0 \right\}$ triviális altér, akkor $p_{v;\left\{ 0 \right\}}$ egybeesik a $p_{v}$ kis minimálpolinommal.
Meggondoltuk tehát, hogy minden $M$ invariáns altér mellett a $p_{v;M}$ irányító polinom osztója a $p_v$ kis minimálpolinomnak $p_{v;M}|p_v$, ergo az irányitó polinom is legfeljebb $\dim\left( V \right)$-ed fokú.
Ebből az is nyilvánvaló, hogy ha egy irányító polinomra $p_{v;M}\left( A \right)v=0$ teljesül,
akkor $p_{v}|p_{v;M}$ is fennáll, 
tehát a $v$-hez tartozó irányító polinom egybeesik a $v$-hez tartozó kisminimálpolinommal.

Látható az is, hogy $p_{v,M}\left( t \right)=1$ akkor és csak akkor teljesül, ha $v\in M$.

Most elegendő feltételt adunk arra, 
hogy két különböző vektorhoz tartozó irányitó polinom azonos legyen.
\begin{proposition}
    Legyen $M\subseteq V$ egy $A$-invariáns altér, 
    és $v,w\in M$ rögzített vektorok.
    Ha $v-w\in M$ fennáll, akkor $p_{v;M}=p_{w;M}$ is teljesül.
\end{proposition}
\begin{proof}
    Mivel $M$ egy $A$-invariáns altér, ezért $p\left( A \right)\left( v-w \right)\in M$ minden $p$ polinom mellett.
    No de
    \[
        p\left( A \right)\left( v-w \right)=p\left( A \right)v-p\left( A \right)w,
    \]
    így $p\left( A \right)v\in M$ pontosan akkor teljesül, ha $p\left( A \right)w\in M$ is fennáll.
    Ez azt jelenti, hogy az $S\left( v,M \right)$ és az $S\left( w,M \right)$ főideálok azonosak,
    ergo a generáló polinom is ugyanaz.
\end{proof}

\begin{definition}[megengedhető altér]\index{megengedhető altér}
    Legyen $M\subseteq V$ egy $A$-invariáns altér.
    Azt mondjuk, hogy $M$ \emph{megengedhető altér},
    ha minden $p\in\mathbb{F}\left[ t \right]$ polinomra és minden $v\in V$ vektorra,
    a $p\left( A \right)v\in M$ tartalmazásból következik, 
    hogy létezik olyan $w\in M$ vektor is, amelyre $p\left( A \right)v=p\left( A \right)w$.
\end{definition}
\begin{proposition}
    Ha egy $M$ invariáns altérnek van invariáns direktkiegészítője, akkor $M$ megengedhető altér.
\end{proposition}
\begin{proof}
    Legyen tehát $M\oplus N=V$ valamely $N$ invariáns altér mellett,
    és tegyük fel, hogy valamely $u\in V$ esetén $p\left( A \right)u\in M$.
    Az $u$ vektor előáll $u=v+w$ alakban, 
    ahol $v\in M$ és $w\in N$.
    Persze
    \[
        p\left( A \right)u-p\left( A \right)v=p\left( A \right)w.
    \]
    A bal oldal mindkét vektora $M$-beli, 
    a jobb oldali vektor $N$-beli az $N$ direktkiegészítő invarianciája miatt.
    Így $p\left( A \right)w=0$.
    Találtunk tehát $v\in M$ vektort, amelyre $p\left( A \right)u=p\left( A \right)v$.
\end{proof}
Példaképpen nézzünk egy $m$-ed rendben nilpotens 
$B\in L\left( V \right)$ lineáris transzformációt.
Válasszunk egy $v\in V$ vektort, amelyre $B^{m-1}v\neq 0$.
Ekkor $\lin\left( v;B \right)$ egy megengedhető altér.
Legyen ugyanis valamely $x\in V$ vektorra $Bx\in\lin\left( v;B \right)$.
Ekkor $Bx$ előáll
\[
    Bx
    =
    \alpha_0v+\alpha_1Bv+\ldots+\alpha_{m-1}B^{m-1}v
\]
alakban. 
Na most $B^{m-1}v\neq 0$, és $m$ a nilpotencia rendje, ergo $v\notin \im B$. 
No de, a fent kiemelt azonosságban a bal oldal,
de az első tag kivételével a jobb oldal valamennyi tagja is az $\im B$ eltér egy-egy eleme,
ezért $\alpha_0 v\in\im B$. 
Ebből persze $\alpha_0=0$ következik, 
ergo $Bx$ előáll mint egy $\lin\left( v;B \right)$-beli vektor képe:
\[
    Bx
    =B
    \left( 
    \alpha_1v+\ldots+\alpha_{m-1}B^{m-2}v
    \right).
\]
Ez azt jelenti, hogy $\lin\left(v;B  \right)$ valóban egy megengedhető altér.

A nilpotens transzformációk felbontásáról szóló \ref{pr:nilpfelb}.~állítás általánosításaként meg fogjuk mutatni,
hogy tetszőleges lineáris transzformáció mellett is igaz, 
hogy egy megengedhető altérnek mindig van invariáns direktkiegészítője.

A triviális -- de nagyon fontos -- példa megengedhető altérre az $M=\left\{ 0 \right\}$ altér, 
persze tetszőleges lineáris transzformáció mellett.

A megengedhető altér definíciója kicsit élesíthető az irányító polinom használatával.
Ha minden irányító polinomra teljesül a megengedehető altér definíciójában előírt tulajdonság, 
akkor már minden más polinomra is fennáll.
\begin{proposition}\label{pr:megengedheto_v}
    Legyen $M$ egy $A$-invariáns altér a $V$ vektortérben.
    Az $M$ pontosan akkor megengedhető, 
    ha minden $w\in V$ vektorhoz létezik $v\in M$,
    amelyre $p_{w,M}\left( A \right)w=p_{w,M}\left( A \right)v$.
\end{proposition}
\begin{proof}
    Legyen $p$ tetszőleges olyan polinom, amelyre $p\left( A \right)w\in M$, valamely $w\in M$ mellett.
    Persze $p\in S\left( w;M \right)$.
    Ennek az ideálnak a generáló eleme $p_{w;M}$, 
    ergo létezik $h$ polinom, amelyre 
    $p\left( t \right)=h\left( t \right)p_{w;M}\left( t \right)$.
    Így a feltevés szerint valamely $v\in M$ mellett
    \begin{displaymath}
        p\left( A \right)w
        =
        h\left( A \right)\left( p_{w;M}\left( A \right)w \right)
        =
        h\left( A \right)\left( p_{w;M}\left( A \right)v \right)
        =
        p\left( A \right)v.\qedhere
    \end{displaymath}
\end{proof}
\begin{lemma}
    Legyen $M$ egy megengedhető altér az $A\in L\left( V \right)$ lineáris transzformációra nézve és $v\notin M$ egy vektor.
    Ekkor létezik olyan $w\in V$ vektor, amelyre 
    \begin{enumerate}
        \item
            \(
            p_w
            =
            p_{w;M}
            =
            p_{v;M};
            \)
        \item 
            $\lin\left( w;A \right)\cap M=\left\{ 0 \right\}$;
        \item
            \(
            M+\lin\left( w;A \right)
            =
            M+\lin\left( v;A \right).
            \)
            \qedhere
    \end{enumerate}
    \label{le:iranyito}
\end{lemma}
\begin{proof}
    Jelölje $p_{v;M}$ a $v$ vektort $M$-be vivő irányító polinomot.
    Mivel $M$ megengedhető, ezért létezik $u\in M$, amelyre $p_{ v;M}\left( A \right)v=p_{v;M}\left( A \right)u$. 
    Legyen $w=v-u$.
    \begin{enumerate}
        \item 
        Ekkor $w-v\in M$, ezért $p_{w;M}=p_{v;M}$. 
        Persze $p_{v;M}\left( A \right)w=0$,
        ezért $p_{w,M}\left( A \right)w=0$ is fennáll,
        amiből már következik, hogy $p_{w;M}=p_w$.

        \item
        Most legyen $x\in M$, amelyre $x\in\lin\left( w;A \right)$. 
        Ekkor létezik $f\in\mathbb{F}\left[ t \right]$ polinom, amelyre
        \(
            x=f\left( A \right)w.
        \)
        Az irányító polinom definíciója szerint $p_{w,M}|f$.
        Már láttuk, hogy $p_{w;M}=p_w$, így $f$ voltaképpen a $p_w$ kis minimálpolinom többszöröse, 
        ezért $f\left( A \right)w=0$, 
        ergo $x=0$.

        \item
        A harmadik egyenlőséghez:
        \begin{multline*}
            M+\lin\left( w;A \right)
            =
            \left\{ m+f\left( A \right)w:m\in M, f\in\mathbb{F}\left[ t \right] \right\}
            =
            \left\{ \left( m-f\left( A \right)u \right)+f\left( A \right)v:m\in M, f\in\mathbb{F}\left[ t \right] \right\}\\
            \subseteq
            \left\{ m+f\left( A \right)v:m\in M, f\in\mathbb{F}\left[ t \right] \right\}
            =
            M+\lin\left( v;A \right).
        \end{multline*}
        Itt a tartalmazás azért áll fenn, mert $M$ egy invariáns altér, így $f\left( A \right)u\in M$.
        Az ellenkező irányú tartalmazás a fentivel analóg:
        \begin{multline*}
            M+\lin\left( v;A \right)
            =
            \left\{ m+f\left( A \right)v:m\in M, f\in\mathbb{F}\left[ t \right] \right\}
            =
            \left\{ \left( m+f\left( A \right)u \right)+f\left( A \right)w:m\in M, f\in\mathbb{F}\left[ t \right] \right\}\\
            \subseteq
            \left\{ m+f\left( A \right)w:m\in M, f\in\mathbb{F}\left[ t \right] \right\}
            =
            M+\lin\left( w;A \right).
        \end{multline*}\qedhere
    \end{enumerate}
\end{proof}
\begin{lemma}
    Legyen $M$ a $V$ vektortér egy valódi altere, 
    amely az $A$ lineáris transzformációra nézve megengedhető.
    Válasszuk meg a $v\in V$ vektort olyan módon, hogy $\deg p_{v;M}$ a lehető legnagyobb legyen,
    azaz minden $x\in V$ mellett
    \[
        \deg p_{v;M}
        \geq
        \deg p_{x;M}.
    \]
    Jelölje 
    \[
        \bar{M}
        =
        M+\lin\left( v;A \right)
    \]
    invariáns alteret.
    Tegyük fel, hogy egy $w\in V$ mellett 
    \[
        p_{w;\bar{M}}\left( A \right)w
        =
        m+g\left( A \right)v
        \tag{\dag}
    \]
    valamely $m\in M$ vektor és valamely $g\in\mathbb{F}\left[ t \right]$ polinom mellett.
    Ekkor alkalmas $m'\in M$ mellett
    \[
        p_{w;\bar{M}}|g 
        \quad\text{ és }\quad
        p_{w;\bar{M}}\left( A \right)m'=m.
        \qedhere
    \]
    \label{le:megengedhetolepes}
\end{lemma}
\begin{proof}
    Először gondoljuk meg, hogy valóban megválasztható egy $v$ vektor úgy, hogy $\deg p_{v;M}$ maximális legyen.
    Mivel $M\neq V$, ezért van $x\in V\smallsetminus M$.
    Minden ilyen $x$ vektorra 
    \begin{math}
        0<\deg p_{x;M}\leq \dim \left( V \right).
    \end{math}
    Ezért bármely olyan $x$ meg is felel $v$-nek, amelynél $\deg p_{x;M}$ a lehető legnagyobb.

    Mivel invariáns alterek Minkowski-összege invariáns, ezért $\bar{M}$ valóban egy az $M$ alteret szigorúan tartalmazó invariáns altér.

    Az egyszerűbb jelölés kedvéért legyen $f=p_{w,\bar{M}}$ a $w$-t $\bar{M}$-be vivő irányító polinom.
    A maradékos osztás tétele szerint léteznek olyan $h,r$ polinomok, amelyekre
    \[
        g=h\cdot f  + r, 
        \quad\text{ ahol }\quad
        \deg r<\deg f.
    \]
    Definiálja $u=w-h\left( A \right)v.$
    Világos, hogy $u-w\in \bar{M}$ hiszen $v\in\bar{M}$, így 
    \[
        p_{u;\bar{M}}=p_{w;\bar{M}}=f.
    \]
    Az $f\left( A \right)$ transzformációt az $u$ definíciójára alkalmazva, 
    majd felhasználva a lemma (\dag) feltételét azt kapjuk, hogy
    \begin{multline*}
        f\left( A \right)u
        =
        f\left( A \right)w
        -f\left( A \right)h\left( A \right)v
        =
        m+g\left( A \right)v
        -f\left( A \right)h\left( A \right)v
        \\
        =
        m+
        \left( g\left( A \right)-f\left( A \right)h\left( A \right) \right)v
        =
        m+
        r\left( A \right)v.
        \tag{\ddag}
    \end{multline*}
        Innen azonnal látszik, hogy összesen azt kell megmutatnunk, hogy az $r$ a konstans zérus polinom.
    Ekkor nyilván $f|g$, és $f\left( A \right)u=m$. 
    Mivel $m\in M$ és $M$ egy megengedhető altér, ezért persze létezik $m'\in M$, 
    amelyre $f\left( A \right)m'=m.$

    Az $r\left( t \right)=0$ megmutatásához jelölje $p=p_{u;M}$ az $u$-t $M$-be vivő irányító polinomot.
    Mivel $M\subset\bar{M}$, ezért $p_{u;\bar{M}}|p_{u;M}$, tehát a bevezetett jelöléseinkkel $f|p$.
    Létezik tehát valamely $q$ nem zérus polinom, amelyre
    \begin{math}
        p=fq.
    \end{math}
    Alkalmazzuk a $q\left( A \right)$ transzformációt (\ddag) azonosságra.
    \[
        p\left( A \right)u
        =
        q\left( A \right)f\left( A \right)u
        =
        q\left( A \right)m+
        q\left( A \right)r\left( A \right)v.
    \]
    Itt $p\left( A \right)u\in M$ és $q\left( A \right)m\in M$, ergo $(q\cdot r)(A)v\in M$ is fennáll.
    Na most, ha $r$ nem a konstans zérus, 
    akkor $q\cdot r$ többszöröse a $p_{v;M}$ irányító polinomnak, ami pedig $v$ konstrukciója szerint 
    nem kisebb fokú, mint $p_{u;M}$.
    Így
    \[
        \deg\left( q\cdot r \right)
        \geq
        \deg p_{v;M}
        \geq
        \deg p_{u;M}.
    \]
    Innen persze 
    $
    \deg q+\deg r
    \geq \deg p
    \geq 
    \deg f+\deg q
    $ következik, ami képtelenség az $r$ konstrukciója szerint.
    Ezt kellett belátni. 
\end{proof}
Első alkalmazásként lássuk, 
hogy egy megengedhető altérhez mindig található olyaninvariáns altér,
amelyet hozzáadva újra egy megengedhető alteret kapunk.
Ez szolgál a ciklikus felbontási tétel rekurziójának alapjaként.
\begin{proposition}
    Legyen $M$ egy megengedhető altér az $A\in L\left( V \right)$ lineáris transzformációra nézve.
    Válasszunk egy maximális fokszámú $M$-be vivő vezető polinommal rendelkező $v\in V$ vektort.
    Ekkor az $\bar{M}=M+\lin\left( v;A \right)$ altér is megengedhető.
\end{proposition}
\begin{proof}
    Legyen $w\in V$, és a \ref{pr:megengedheto_v}.~állításnak megfelelően 
    tekintsük a $p_{w;\bar{M}}\left( A \right)w\in M+\lin\left( v;A \right)$ vektort.
    Világos, hogy valamely $m\in M$ és valamely $g$ polinom mellett
    \[
        p_{w;\bar{M}}\left( A \right)w
        =
        m+g\left( A \right)v.
    \]
    \Aref{le:megengedhetolepes}.~lemma szerint valamely $m'\in M$ mellett $p_{w;\bar{M}}\left( A \right)m'=m$ és
    van olyan $h$ polinom, amelyre $g=p_{w;\bar{M}}\cdot h$.
    Ezeket helyettesítve kapjuk, hogy 
    \[
        p_{w;\bar{M}}\left( A \right)w
        =
        p_{w;\bar{M}}\left( A \right)m'+p_{w;\bar{M}}\left( A \right)\left( h\left( A \right)v\right)
        =
        p_{w;\bar{M}}\left( A \right)\left(m'+h\left( A \right)v \right).
    \]
    Itt $m'\in M\subseteq\bar{M}$ és $v\in \bar{M}$ miatt az argumentumban szereplő
    \begin{math}
         m'+h\left( A \right)v
    \end{math}
    vektor $\bar{M}$-ban van. 
    \Aref{pr:megengedheto_v}.~állítás szerint éppen ezt kellett belátni.
\end{proof}
\begin{proposition}[Ciklikus felbontás]
    Legyen $V$ egy véges dimenziós vektortér az $\mathbb{F}$ test felett, $A\in L\left( V \right)$ egy lineáris transzformáció.
    Tegyük fel, hogy adott egy $M_0\subset V$ megengedhető altér, amelyre $M_0\neq V$.
    Ekkor létezik $r$ pozitív egész, és léteznek $w_1,\dots,w_r\in V$ vektorok, amelyekre
    \begin{enumerate}
        \item A $w_1,\dots,w_r$ egyike sem zérus;
        \item $V=M_0\oplus\lin\left( w_1;A \right)\oplus\ldots\oplus\lin\left( w_r;A \right)$;
        \item A $w_1,\dots,w_r$ vektorok kis minimálpolinomjai olyanok, hogy mind osztója az előzőnek, 
            azaz ha $p_k=p_{w_k}$ a $w_k$ vektorhoz tartozó kis minimálpolinom, 
            akkor minden $k=2,\dots,r$ mellett $p_k|p_{k-1}$.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Válasszunk egy $v_1\notin M_0$ vektort, amelyre minden $x\in V$ mellett
    \[
        \deg p_{v_1;M_0}\geq \deg p_{x;M_0}.
    \]
    Alkalmazzuk \aref{le:iranyito}.~lemmát az $M_0$ megengedhető altérre és a $v_1$ vektorra.
    Azt kapjuk, hogy létezik $w_1\notin M_0$ vektor, amelyre
    \begin{enumerate}
        \item
            \(
            p_{w_1}
            =
            p_{w_1;M_0}
            =
            p_{v_1;M_0};
            \)
        \item 
            $\lin\left( w_1;A \right)\cap M_0=\left\{ 0 \right\}$;
        \item
            \(
            M_0+\lin\left( w_1;A \right)
            =
            M_0+\lin\left( v_1;A \right).
            \)
    \end{enumerate}
    Azt is láttuk, hogy $v_1$ maximalitási konstrukciója szerint $M_0+\lin\left( v_1;A \right)$ is megengedhető altér.
    Legyen $$M_1=M_0+\lin\left( v_1;A \right)=M_0\oplus\lin\left( w_1;A \right).$$
    Ha $M_1=V$, akkor $r=1$ választással készen is vagyunk.

    Ha $M_1\neq V$, akkor megismételjük a fenti eljárást az $M_1$ megengedhető altérrel:
    Válasszunk tehát egy $v_2\notin M_1$ vektort, amelyre minden $x\in V$ mellett
    \[
        \deg p_{v_2;M_1}\geq \deg p_{x;M_1}.
    \]
    Alkalmazzuk \aref{le:iranyito}.~lemmát az $M_1$ megengedhető altérre és a $v_2$ vektorra.
    Azt kapjuk, hogy létezik $w_2\notin M_1$ vektor, amelyre
    \begin{enumerate}
        \item
            \(
            p_{w_2}
            =
            p_{w_2;M_1}
            =
            p_{v_2;M_1};
            \)
        \item 
            $\lin\left( w_2;A \right)\cap M_1=\left\{ 0 \right\}$;
        \item
            \(
            M_1+\lin\left( w_2;A \right)
            =
            M_1+\lin\left( v_2;A \right).
            \)
    \end{enumerate}
    Tudjuk azt is, hogy $v_2$ maximalitási konstrukciója szerint $M_1+\lin\left( v_2;A \right)$ is egy megengedhető altere $V$-nek.
    Legyen $$M_2=M_1+\lin\left( v_2;A \right)=M_1\oplus\lin\left( w_2;A \right).$$
    Most meg kell mutatnunk, hogy $p_{w_2}|p_{w_1}$. 
    Ehhez először azt vegyük észre, hogy $v_1$ helyett $w_1$ vektorra is teljesül a
    \[
        \deg p_{w_1,M_1}
        \geq
        \deg p_{x,M_1}
    \]
    maximalitási feltétel minden $x\in V$ mellett.
    Másodszor vegyük észre, hogy triviálisan teljesül az
    \[
        p_{w_2;M_1}\left( A \right)w_2=0+p_{w_1;M_0}\left( A \right)w_1
    \]
    egyenlőség, hiszen mindkét oldal zérus.
    \Aref{le:megengedhetolepes}.~lemmát alkalmazzuk és kapjuk, hogy
    $p_{v_2;M_1}|p_{v_1;M_0}$, ami a mi jelöléseinkkel azt jelenti, hogy $p_2|p_1$.
    Ha $M_2=V$, akkor $r=2$ választással készen is vagyunk.

    Ha $M_2\neq V$, akkor megismételjük a fenti eljárást $M_1$ helyett az $M_2$ megengedhető altérrel, stb.

    Mivel minden egyes lépésben az $M_k$ altér dimenziója nő, ezért van olyan $r$ egész, amelyre $M_r=V$.
    Így az $M_0;\lin\left( w_1;A \right),\dots,\lin\left( w_r;A \right)$ alterek olyanok, 
    hogy mind diszjunkt az előzőek összegétől, ezért a direktösszeg értelmes és
    \[
        M_0\oplus\lin\left( w_1;A \right)\oplus,\ldots,\oplus\lin\left( w_r;A \right)=M_r=V
    \]
    és minden $k\geq 2$ mellett $p_{k}|p_{k-1}$.
\end{proof}

Korábban már láttuk, hogy ha egy invariáns altérnek van invariáns direktkiegészítője, akkor az megengedhető is.
A ciklikus felbontás szerint ha $M_0$ megengedhető, akkor annak van $\lin\left( w_1;A \right)\oplus\ldots\oplus\lin\left( w_r;A \right)$ invariáns direktkiegészítője.
Igazoltuk tehát a következő állítást:
\begin{proposition}
    Legyen $A\in L\left( V \right)$ egy lineáris transzformáció,
    és $M_0\subseteq V$ egy invariáns altér az $A$ transzformációra nézve.
    Az $M$-nek pontosan akkor van invariáns direktkiegészítóje, ha $M$ egyben megengedhető is.
\end{proposition}

A ciklikus felbontási tételben az első $p_1$ polinom különösen nevezetes. 
Amennyiben $M_0=\left\{ 0 \right\}$, akkor $p_1$ az $A$ minimálpolinomja,
amint azt rögtön megértjük.

Általános esetben tekintsük az összes olyan polinomot, amelyre $p\left( A \right)v\in M_0$ minden $v\in V$ mellett:
\[
S\left( M_0 \right)=\left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)v\in M_0,\text{ minden } v\in V \right\}.
\]
Világos, hogy az $M_0$ invarianciája szerint $S\left( M_0 \right)$ egy nemzérus ideál.
Jelölje $s_{M_0}$ az $S\left( M_0 \right)$ normált generátor elemét.

Világos, hogy ha visszatérünk az $M_0=\left\{ 0 \right\}$ esethez,
akkor $s_{\left\{ 0 \right\}}$ éppen az $A$ lineáris transzformáció minimálpolinomja.

\begin{proposition}
    Legyen $V$ egy véges dimenziós vektortér az $\mathbb{F}$ test felett, $A\in L\left( V \right)$ egy lineáris transzformáció.
    Legyen $M_0\subset V$ egy az $A$-ra nézve invariáns altér, 
    és legyenek $w_1,\dots,w_r\in V$ vektorok, amelyekre és ezek $p_k$ kis minimálpolinomjaira
    fenállnak a következők.
    \begin{enumerate}
        \item A $w_1,\dots,w_r$ egyike sem zérus;
        \item $V=M_0\oplus\lin\left( w_1;A \right)\oplus\ldots\oplus\lin\left( w_r;A \right)$;
        \item minden $k=2,\dots,r$ mellett $p_k|p_{k-1}$.
    \end{enumerate}
    Ekkor $p_1=s_{M_0}$, azaz $p_1$ a fenti $S\left( M_0 \right)$ ideál normált generáló eleme.
    Speciálisan, ha $M_0=\left\{ 0 \right\}$, akkor $p_1$ az $A$ minimálpolinomja.
\end{proposition}
\begin{proof}
    Először megmutatjuk, hogy minden $v\in V$ mellett $p_1\left( A \right)v\in M$.
    Legyen tehát $v\in V$ rögzítve. 
    Ekkor a 2. feltétel szerint
    \[
        v=m_0+\sum_{k=1}^rf_k\left( A \right)w_k
    \]
    valamely $f_k$ polinomokra, és $m_0\in M_0$ vektorra.
    Alkalmazzuk mindkét oldalra a $p_1\left( A \right)$ transzformációt.
    Így
    \[
        p_1\left( A \right)v=p_1\left( A \right)m_0+\sum_{k=1}^rp_1\left( A \right)f_k\left( A \right)w_k.
    \]
    No de minden $k$-ra $p_k|p_1$, és $p_k\left( A \right)w_k=0$, azért a szumma minden tagja zérus,
    így $M_0$ invarianciáját használva kapjuk, hogy $p_1\left( A \right)v\in M=p_1\left( A \right)m_0\in M.$
    Megmutattuk tehát, hogy $s_{M_0}|p_1$.

    Világos, hogy $s_{M_0}\left( A \right)w_1\in M_0\cap\lin\left( w_1;A \right)=\left\{ 0 \right\}$,
    hiszen a $\lin\left( w_1;A \right)$ egy az $A$ transzformációra nézve invariáns altér.
    Így $s_{M_0}\left( A \right)w_1=0$, amiből már $p_1|s_{M_0}$ is következik.
\end{proof}
Ezek szerint megmutattuk, hogy a minimálpolinom mindig valamely vektorhoz tartozó kis minimálpolinom.
Mitöbb ez a vektor a maximális fokú kis minimálpolinomhoz tartozó vektor.
Megmutattuk tehát a következőt.
\begin{proposition}
    Legyen $A\in L\left( V \right)$ egy lineáris transzformáció. 
    Legyen $v\in V$ olyan vektor, amelyhez tartozó kis minimálpolinom lehető legnagyobb fokszámú,
    azaz
    \[
        \deg p_v\geq \deg p_x
    \]
    minden $x\in V$ mellett.
    Ekkor a $p_v$ kis minimálpolinom egyben a transzformáció minimálpolinomja is.
\end{proposition}
Persze ebből az állításból is látszik, hogy a transzformáció minimálpolinomjának fokszáma legfeljebb  a tér dimenziója.

Az eddigiektől független következő lemma invariáns altereknek egy polinom által képzett direktképéről szól.
\begin{lemma}
    Legyen az $A\in L\left( V \right)$ lineáris transzformáció és az $f\in\mathbb{F}\left[t  \right]$
    polinom rögzítve.
    Ekkor
    \begin{enumerate}
        \item Minden $v\in V$ mellett 
            \begin{math}
                f\left( A \right)\lin\left( v;A \right)
                =
                \lin\left( f\left( A \right)v;A \right);
            \end{math}
        \item Ha $V$ előáll mint invariáns alterek $V=V_1\oplus\ldots\oplus V_k$ direktösszegének alakjában, 
            akkor ezek $f$ képére az
            \begin{math}
                f\left( A \right)V
                =
                f\left( A \right)V_1
                \oplus\ldots\oplus
                f\left( A \right)V_k
            \end{math}
            direktösszeg alakú előállítás is teljesül;
        \item
            Ha az $u$ és $v$ vekotorok kis minimálpolinomja azonos,
            akkor az $f\left( A \right)u$ és $f\left( A \right)v$ vektoroknak is ugyanaz a kis minimálpolinomja.
            \qedhere
    \end{enumerate}
    \label{le:cikl-egyert}
\end{lemma}
\begin{proof}
    Az első állítás egyszerűen azért igaz,
    mert a transzformáció polinomjai kommutálnak egymással.

    A második állításhoz csak azt kell észrevenni, 
    hogy ha invariáns alterek direktösszege értelmes,
    akkor ezek $f\left( A \right)$ transzformációval képzett direktösszege is értelmes.

    A harmadik állításhoz, mint \ref{def:kisminimal}.~definícióban jelölje
    \begin{math}
        J_{A,v}
    \end{math}
    azon $p$ polinomok halmazát, amelyekre $p\left( A \right)v=0$ teljesül. 
    A feltétel szerint 
    \begin{math}
        J_{A,u}=J_{A,v}.
    \end{math}
    Na most $p\in J_{A,f\left( A \right)v}$ pontosan akkor,
    ha $p\cdot f\in J_{A,v}$,
    ami persze ugyan az, mint 
    $p\cdot f\in J_{A,u}$, ami már a $p\in J_{A,f\left( A \right)u}$ feltétellel ekvivalens.
    Meggondoltuk tehát, hogy 
    \begin{math}
        J_{A,f\left( A \right)v}
        =
        J_{A,f\left( A \right)u}
    \end{math}
    is fennáll, ami éppen azt jelenti, hogy a fenti ideálok generáló elemei is azonosak.
\end{proof}
\begin{proposition}[Ciklikus felbontás egyértelműsége]
    Legyen $M_0\subset V$ egy $A$-invariáns altér a $V$ vektortérben.
    Tegyük fel, hogy adott az $r,s$ pozitív egész, adottak az 
    $v_1,\dots,v_r\in V$, és a $w_1,\dots,w_s\in V$ vektorok
    a hozzájuk tartozó $p_1,\dots,p_r$ és a $q_1,\dots,q_s$ kis minimálpolinomokkal.
    Tegyük fel, hogy 
    \begin{enumerate}
        \item $v_1,\dots,v_r$ egyike sem zérus;
        \item $V=M_0\oplus\lin\left( v_1;A \right)\oplus\ldots\oplus\lin\left( v_r;A \right)$;
        \item $p_{k}|p_{k-1}$ minden $k=2,\ldots,r$ mellett.
    \end{enumerate}
    Hasonlóan a $w_1,\dots,w_s$ vektortokra is tegyük fel, hogy 
    \begin{enumerate}
        \item $w_1,\dots,w_s$ egyike sem zérus;
        \item $V=M_0\oplus\lin\left( w_1;A \right)\oplus\ldots\oplus\lin\left( w_s;A \right)$;
        \item $g_{k}|g_{k-1}$ minden $k=2,\ldots,s$ mellett.
    \end{enumerate}
    Ekkor $r=s$, $p_k=g_k$ minden $k=1,\ldots,r$ mellett és minden $k$ indexre
    $\lin\left( v_k;A \right)$ és $\lin\left( w_k;A \right)$ izomorf alterek.
\end{proposition}
\begin{proof}
    Tudjuk, hogy $p_1=q_1$, hiszen a feltételekből következik, hogy $p_1$ és $q_1$ is az a legalacsonyabb fokszámú normált polinom, amely a tér minden elemét az $M_0$ altérbe viszi.
    Ebből kiindulva azt mutatjuk meg, 
    hogy minden $k=1,\ldots,r$ mellett $k\leq s$ és $q_k=p_k$.
    Nézzük ezt $k$ szerinti indukcióval.

    Ha $k=1$, akkor persze $k\leq s$ automatikusan teljesül és éppen az imént láttuk, hogy $p_1=q_1$ is fennáll.

    Most tegyük fel, hogy valamely $k<r$ mellett $k\leq s$ és $p_j=q_j$ minden $j=1,\ldots,k$.
    Meg kell mutatnunk, hogy ebből $k+1\leq s$ és $p_{k+1}=q_{k+1}$ is következik.
    Mivel $k<r$, és a $\lin\left( w_j;A \right)$ alterek izomorfak a $\lin\left( v_j;A \right)$ alterekkel,
    ezért
    \begin{displaymath}
        \dim(M_0\oplus\lin\left( w_1;A \right)
        \oplus\ldots\oplus
        \lin\left( w_k,A \right))
        =
        \dim(M_0\oplus\lin\left( v_1;A \right)
        \oplus\ldots\oplus
        \lin\left( v_k,A \right))
        <
        \dim\left( V \right),
    \end{displaymath}
    ezért $k<s$, ergo $k+1\leq s$ valóban fennáll.
    Legyen most az egyszerűbb jelölés kedvéért $f=p_{k+1}$.
    Ekkor alkalmazva az előző \ref{le:cikl-egyert}.~lemma első két pontját 
    $f\left( A \right)\left( \lin\left( v_j;A \right) \right)
    =
    \lin\left( f\left( A \right)v_j;A \right)
    $ és ez a 
    $\left\{ 0 \right\}$ 
    altér, ha $j\geq k+1$, hiszen az ilyen $j$ indexek mellett $p_j|f$.
    Így az is világos, hogy
    \begin{eqnarray*}
        f\left( A \right)(V)
        &=& 
        f\left( M_0 \right)
        \oplus
        \lin\left( f\left( A \right)v_1;A \right)
        \oplus\ldots\oplus
        \lin\left( f\left( A \right)v_k;A \right)
        \text{ és }
        \\
        f\left( A \right)(V)
        &=& 
        f\left( M_0 \right)
        \oplus
        \lin\left( f\left( A \right)w_1;A \right)
        \oplus\ldots\oplus
        \lin\left( f\left( A \right)w_k;A \right)
        \oplus
        \\
        &&
        \lin\left( f\left( A \right)w_{k+1};A \right)
        \oplus\ldots\oplus
        \lin\left( f\left( A \right)w_s;A \right).
    \end{eqnarray*}
    A lemma harmadik pontja és az indukciós feltevés szerint a $f\left( A \right)v_j$ és az
    $f\left( A \right)w_j$ vektoroknak azonos a kis minimálpolinomjuk a $j=1,\ldots,k$ mellett,
    így az 
    $\lin\left( f\left( A \right)v_j;A \right)$ és a 
    $\lin\left( f\left( A \right)w_j;A \right)$ alterek izomorfak.
    Ez csak úgy lehetséges, ha
    \[
        \lin\left( f\left( A \right)w_{k+1};A \right)
        \oplus\ldots\oplus
        \lin\left( f\left( A \right)w_s;A \right)
        =
        \left\{ 0 \right\}
    \]
    teljesül,
    speciálisan $f\left( A \right)w_{k+1}=0$, ergo $q_{k+1}|f$, ergo $q_{k+1}|p_{k+1}$.
    Jelölje most $f=q_{k+1}$ és ismételjük meg a fenti gondolatot a $v$-vel és $w$-vel jelölt vektorok felcserélésével.
    Így
    \begin{eqnarray*}
        f\left( A \right)(V)
        &=& 
        f\left( M_0 \right)
        \oplus
        \lin\left( f\left( A \right)w_1;A \right)
        \oplus\ldots\oplus
        \lin\left( f\left( A \right)w_k;A \right)
        \text{ és }
        \\
        f\left( A \right)(V)
        &=& 
        f\left( M_0 \right)
        \oplus
        \lin\left( f\left( A \right)v_1;A \right)
        \oplus\ldots\oplus
        \lin\left( f\left( A \right)v_k;A \right)
        \oplus
        \\
        &&
        \lin\left( f\left( A \right)v_{k+1};A \right)
        \oplus\ldots\oplus
        \lin\left( f\left( A \right)v_r;A \right),
    \end{eqnarray*}
    ami fenti érveléssel párhuzamosan csak akkor lehetséges, 
    ha
    \[
        \lin\left( f\left( A \right)v_{k+1};A \right)
        \oplus\ldots\oplus
        \lin\left( f\left( A \right)v_r;A \right)
        =
        \left\{ 0 \right\}
    \]
    is teljesül.
    Speciálisan $f\left( A \right)v_{k+1}=0$, ergo $p_{k+1}|f$, ergo $p_{k+1}|q_{k+1}$ is fennáll.

    Az indukció szerint tehát $k=r$ mellett is igaz az állításunk,
    tehát azt mutattuk meg, hogy $r\leq s$ és minden $k=1,\ldots,r$ mellett $p_k=q_k$.
    Az állítás feltételeinek szimmetriáját használva az egész eddigi érvelés megismételhetnénk, 
    a $v$-vel és $w$-vel jelölt vektor rendszer felcserélésével.
    Ekkor azt kapjuk, hogy $s\leq r$ is fennáll, tehát $r=s$ valóban teljesül, 
    és persze minden $j=1,\ldots,r$ mellett $p_j=q_j$,
    amiből 
    $
    \dim\left( \lin\left( v_j;A \right) \right)
    =
    \deg p_j
    =
    \deg q_j
    =
    \dim\left( \lin\left( w_j;A \right) \right)
    $
    is következik.
    Ezt kellett belátni. 
\end{proof}
\begin{proposition}[Cayley\,--\,Hamilton II.]\index{Cayley\,--\,Hamilton}
    Legyen $A\in L\left( V \right)$ az $\mathbb{F}$ test feletti $V$ véges dimenziós vektortér
    lineáris transzformációja.
    Jelölje $m\left( t \right), k\left( t \right)\in\mathbb{F}\left[ t \right]$
    a transzformáció minimálpolinomját és a karakterisztikus polinomját.
    Ekkor
    \begin{enumerate}
        \item a minimálpolinom osztója a karakterisztikus polinomnak;
        \item a minimálpolinom és a karakterisztikus polinom irreducibilis osztói azonosak;
        \item ha a minimálpolinomnak és a karakterisztikus polinomnak a primtényezős előállítása
            \[
                m\left( t \right)
                =
                f_1^{\alpha_1}\left( t \right)
                \cdot\ldots\cdot
                f_r^{\alpha_r}\left( t \right)
                \quad\text{ és }\quad
                k\left( t \right)
                =
                f_1^{\beta_1}\left( t \right)
                \cdot\ldots\cdot
                f_r^{\beta_1}\left( t \right),
            \]
            akkor minden $j=1,\ldots,r$ mellett a $\nu\left( f_j^{\alpha_j} \left( A \right)\right)$ defektus
            és az $\deg f_j$ fok közötti összefüggés:
            \[
                \beta_j
                =
                \frac{\nu\left( f_j^{\alpha_j}\left( A \right) \right)}{\deg f_j}.
                \qedhere
            \]
    \end{enumerate}
\end{proposition}
\begin{proof}
    A ciklikus felbontási tétel értelmében a tér előáll
    \[
        V=
        \lin\left( v_1;A \right)
        \oplus\ldots\oplus
        \lin\left( v_s;A \right)
    \]
    alakban ahol $p_j$ a $v_j$ vektorhoz tartozó kis minimálpolinom.
    Tudjuk, hogy $p_1=m$ a minimálpolinom,
    és minden $j=2,\ldots,s$ mellett $p_j|p_{j-1}$.
    \begin{enumerate}
        \item 
    Az is világos, hogy ha megszorítjuk a transzformációt a 
    $\lin\left( v_j;A \right)$ invariáns altérre, akkor ennek a megszorított transzformációnak
    a minimálpolinomja egybeesik a karakterisztikus polinomjával, és pedig ez éppen a $p_j$
    kis minimálpolinom.
    A determináns definíciója miatt a karakterisztikus polinomja $A$-nak az egyes invariáns altereken vett karakterisztikus polinomok szorzata, azaz
    \[
        k\left( t \right)
        =
        m\left( t \right)\cdot
        p_2\left( t \right)\cdot\ldots\cdot p_s\left( t \right).
        \tag{\dag}
    \]
    Így persze a minimálpolinom valóban osztója a karakterisztikus polinomnak.
    
        \item
    Ebből azonnal nyilvánvaló, 
    hogy a minimálpolinom minden osztója a karakterisztikus polinomnak is osztója.
    Most tegyük fel, hogy $f\left( t \right)$ irreducibilis polinom osztója a karakterisztikus
    polinomnak.
    Mivel irreducibilis polinom prim tulajdonságú is, 
    ezért (\dag) felírást alkalmazva $f\left( t \right)|p_j\left( t \right)$ valamely
    $j$-re, 
    no de a ciklikus felbontási tétel szerint bármelyik $j$ mellett is
    $p_j\left( t \right)|m\left( t \right)$.

    Meggondoltuk tehát, hogy $m\left( t \right)$ és $k\left( t \right)$
    primtényezős felbontásában azonosak az irreducibilis polinomok,
    így ezek multipilicásában lehet csak különbség, és persze $\alpha_j\leq\beta_j$.

        \item
    A minimálpolinom faktorizációja szerint is szétesik a tér invariáns alterek direktösszegére.
    Jelölje 
    $
    V_j=\ker f_j^{\alpha_j}\left( A \right)
    $ minden $j=1,\ldots,r$ mellett,
    így 
    \begin{math}
        V
        =
        V_1\oplus\ldots\oplus V_r,
    \end{math}
    és ha az $A$ transzformációt visszaszorítjuk a $V_j$ invariáns altérre,
    akkor ennek a megszorított transzformációnak $f_j^{\alpha_j}\left( t \right)$ 
    lesz a minimálpolinomja.
    A 2. állítást már igazoltuk, így $A|V_j$ transzformáció minimálpolinomja
    és karakterisztikus polinomja is csak azonos irreducibilis polinomokat tartalmazhat, 
    ezért a megszorított transzformáció karakterisztikus polinomja
    \[
        f_j^{\gamma_j}\left( t \right),
        \quad\text{ ahol }\quad
        \gamma_j\deg f_j
        =
        \dim\left( V_j \right)
        =
        \nu\left( f_j^{\alpha_j}\left( A \right) \right),
    \]
    hiszen egy karakterisztikus polinom foka mindig a tér dimenziója.
    Újra alkalmazva, hogy a karakterisztikus polinom az egyes invaráns alterekre szorított transzformációk karakterisztikus polinomjainak szorzata azt kapjuk, hogy
    \[
        k\left( t \right)
        =
        f_1^{\gamma_1}\left( t \right)
        \cdot\ldots\cdot
        f_r^{\gamma_r}\left( t \right).
    \]
    Persze a primtényezős előállítás egyértelmű, ezért minden szóba jövő $j$ indexre
    $\beta_j=\gamma_j$.
    \qedhere
    \end{enumerate}
\end{proof}
%% a vége következik

\chapter{Kvadratikus alakok definitségének egy naiv targyalása}
A kvadratikus alaknak mondunk egy speciális többváltozós függvényt.
\begin{definition}
    Legyen $A\in\mathbb{R}^{n\times n}$ egy szimmetrikus mátrix.
    Amint az szokásos jelölje $a_{k,j}$ a mátrix $i,j$ pozicíojának elemét.
    Definiálja a $Q_A:\mathbb{R}^n\to\mathbb{R}$ függvényt
    \[
    Q\left( x_{1},x_2,\cdots,x_n \right)=
    \sum_{k=1}^n\sum_{j=1}^na_{k,j}x_kx_j.
    \]
    Ekkor $Q_A$ az $A$ szimmetrikus mátrixhoz tartozó kvadratikus alak.
\end{definition}
Például $n=1$ és az $A$ mátrix egyetlem eleme az $a\in\mathbb{R}$ szám, akkor $Q(x_1)=ax_1^2$.
Ha $n=2$ mellett $A\in\mathbb{R}^{2\times 2}$, akkor
$$
Q\left( x_1,x_2 \right)=a_{1,1}x_1^2+a_{1,2}x_1x_2+a_{2,1}x_2x_1+a_{2,2}x_2^2.
$$
Gyakorlásképpen írjuk fel milyen három változós kvadratikus alakok lehetségesek.
\begin{proposition}
    Legyen $A\in\mathbb{R}^{n\times n}$ egy szimmetrikus mátrix.
    Ekkor az $A$ által meghatározott kvadratikus alakra
    \[
        Q(x)=\ip{x}{Ax}
    \]
    tetszőleges $x\in\mathbb{R}^n.$
\end{proposition}
Ebből az alakból már világos, hogy az $A+B$ összeghez tartozó kvadratikus alak azonos az $A$-hoz és a $B$-hez
tartozó kvadratikus alakok összegével, 
és hasonlóan az $\alpha A$ mátrixhoz tartozó kvadratikus alakot úgy is megkaphatjuk, hogy az $A$-hoz tartozó kvadratikus alakot mint $n$-változós függvényt szorozzuk az $\alpha$ számmal.
Formálisabban:
\begin{proposition}
    Legyenek $A,B\in\mathbb{R}^{n\times n}$ szimmetrikus mátrixok és $\alpha,\beta\in\mathbb{R}$.
    Ekkor
    \[
    Q_{\alpha A+\beta B}=\alpha Q_A+\beta Q_B.
    \qedhere
    \]
\end{proposition}
A nemzérus kvadratikus alakokat az értékkészletük alapján osztályozzuk.
\begin{definition}
    Legyen $Q:\mathbb{R}^n\to\mathbb{R}$ egy nemzérus kvadratikus alak.
    Azt mondjuk, hogy ez
    \begin{description}
        \item[{\normalfont \emph{pozitív definit,}}] ha $Q\left( x \right)>0$ minden $x\in\mathbb{R}^n,x\neq 0$;
        \item[{\normalfont\emph{pozitív szemidefinit,}}] ha $ Q\left( x \right)\geq 0$ minden $x\in\mathbb{R}^n$,
            de létezik egy $z\in\mathbb{R}^n, z\neq 0$ vektor, amelyre $Q\left( z \right)=0$;
        \item[{\normalfont\emph{negatív definit,}}] ha $ Q\left( x \right)<0$ minden $x\in\mathbb{R}^n,x\neq 0;$
        \item[{\normalfont\emph{negatív szemidefinit,}}] ha $ Q\left( x \right)\leq 0$ minden $x\in\mathbb{R}^n$,
            de létezik egy $z\in\mathbb{R}^n, z\neq 0$ vektor, amelyre $Q\left( z \right)=0$;
        \item[{\normalfont\emph{indefinit,}}] ha vannak olyan $x,y\in\mathbb{R}^n$ vektorok, amnelyekre
            $Q\left( x \right)>0$  és $Q\left( y \right)<0$.
    \end{description}
\end{definition}
\subsection{A szimmetrikus diádok szerepe}
Emlékezzünk, hogy $n\times n$ méretű szimmetrikus diádnak nevezünk egy $\mathbb{R}^n$ beli vektornak
oszlopalakú és soralakú mátrixszorzatát. 
Kicsit formálisabban az $A=a\cdot a^T$ tetszőleges $a\in\mathbb{R}^{n\times 1}$ mellett.
A diádok tehát speciális szimmetrikus mátrixok.
Látható, hogy egy szimmetrikus mátrix pontosan akkor diád, 
ha az 1 rangú szimmetrikus mátrix.

A szimmetrikus diádok azért érdekesek számunkra, 
mert éppen az ezekhez tartozó kvadratikus alakok a teljes négyzetek.
\begin{proposition}
   Egy kvadratikus alak pontosan akkor teljes négyzet, ha az őt reprezentáló szimetrikus mátrix egy diád.
   Pontosabban:
   Ha $A=a\cdot a^T$, ahol $a=\left( a_1,a_2,\ldots,a_n \right)$, akkor
      \(
         Q_A\left( x_1,x_2,\ldots,x_n \right)=
         \left( a_1x_1+a_2x_2+\ldots+a_nx_n \right)^2.
      \)
\end{proposition}

\section{Szimmetrikus mátrix diád felbontása}
Be fogjuk bizonyítani, hogy 
\begin{proposition}
    Minden nem zérus szimmetrikus mátrix előáll mint szimmetrikus diádok lineáris kombinációja.
    Sőt, ha egy szimmetrikus mátrix rangja $r$, 
    akkor az előáll mint $r$ db.~szimmetrikus diád lineáris kombinációja, 
    ahol a diádokat alkotó vektorok rendszere lineárisan független rendszert alkot.
\end{proposition}
A bizonyítás úgy folyik, hogy konkrét algoritmust adunk a felbontásra.

Az algoritmus első mozzanata annak észrevétele, 
hogy egy szimmetrikus mátrixból leválasztható egy szimmetrikus diád olyan módon,
hogy a maradék szimmetrikus mátrixhoz tartozó kvadratikus alak már egy változóval kevesebbett tartalmazzon:
\begin{proposition}
    Tekintsünk egy $A\in\mathbb{R}^{n\times n}$ szimmetrikus mátrixot.
    Tegyük fel, hogy a balfelső elemre $a_{1,1}\neq 0$.
    Jelölje $d=\frac{1}{a_{1,1}}a_1$, ahol $a_1$ a mátrix első oszlopa $A$.
    Ekkor
    \[
        R=A-a_{1,1}d\cdot d^T
   \]
   egy olyan szimmetrikus mátrix, 
   ahol az első oszlop és első sor kizárólag zérus elemeket tartalmaz.
\end{proposition}
\begin{proof}
    Az persze nyilvánvaló, hogy szimmetrikus mátrixok egy lináris kombinációja is szimmetrikus marad.
    Jelölje $\delta_k=\frac{a_{k,1}}{a_{1,1}}=\frac{a_{1,k}}{a_{1,1}}$ a $d$ vektor $k$-adik koordinátáját.
    Ekkor 
    \[
        r_{k,j}=
        a_{k,j}-a_{1,1}\delta_k\delta_j=
        a_{k,j}-a_{1,1}\frac{a_{k,1}}{a_{1,1}}\delta_j=
        a_{k,j}-a_{k,1}\delta_j.
        \tag{\dag}
    \]
    Ha $j=1$, akkor itt $r_{k,1}=a_{k,1}-a_{k,1}\delta_1=0$.
    Látjuk, hogy az $R$ mátrix első oszlopa csupa zérust tartalmaz. 
    No de, az $R$ egy szimmetrikus mátrix, ergo az első sorban is csak zérus elemek vannak.
    \end{proof}
    Illusztrációként nézzük a következő kvadratikus alakot:
    \[
        Q\left( z_1,z_2,z_3 \right)
        =
        2z_1^2+\frac{3}{2}z_3^2+2z_1z_2-4z_1z_3+2z_2z_3.
    \]
    Ezt a kvadratikus alakot az 
    \(A=
    \begin{pmatrix}
        2&1&-2\\
        1&0&1\\
        -2&1&\frac{3}{2}
    \end{pmatrix}
    \)
    szimmetrikus mátrix reprezentálja, 
    legyen tehát 
    \(d=
    \begin{bmatrix}
        1\\ \frac{1}{2}\\ -1
    \end{bmatrix}.
    \)
    Így 
    \[
        R=A-2d\cdot d^T
        =
        \begin{pmatrix}
        2&1&-2\\
        1&0&1\\
        -2&1&\frac{3}{2}
        \end{pmatrix}
        -2
        \begin{bmatrix}
        1\\ \frac{1}{2}\\ -1
        \end{bmatrix}
        \cdot
        \begin{bmatrix}
        1& \frac{1}{2}& -1
        \end{bmatrix}
        =
        \begin{pmatrix}
            0&0&0\\
            0&-\frac{1}{2}&2\\
            0&2&-\frac{1}{2}
        \end{pmatrix}
    \]
    No persze $A=2d\cdot d^T+R$.
    Áttérve a kvadratikus alakokra
    \(
        Q\left( z_1,z_2,z_3 \right)=2\left( z_1+\frac{1}{2}z_2-z_3 \right)^2+Q_R\left( z_1,z_2,z_3 \right),
    \)
    ahol $Q_R$ már nem függ a $z_1$ változótól, 
    hiszen $Q_R\left( z_1,z_2,z_3 \right)=-\frac{1}{2}z_2^2-\frac{1}{2}z_3^2+4z_2z_3$.
    \\
    Sikeresen megszabadultunk az első változótól, elegendő tehát a 
    \(
    \begin{pmatrix}
        -\frac{1}{2}&2\\
        2&-\frac{1}{2}
    \end{pmatrix}
    \)
    mátrix reprezentálta kétváltozós kvadratikus alakkal foglalkozni, amelynek változóit most $z_2$ és $z_3$ jelöli.
    Válasszunk le egy újabb szimmetrikus diádot:
    \[
        \begin{pmatrix}
            -\frac{1}{2}&2\\
            2&-\frac{1}{2}
        \end{pmatrix}
        -\left( -\frac{1}{2} \right)
        \begin{bmatrix}
            1\\-4
        \end{bmatrix}
        \cdot
        \begin{bmatrix}
            1&-4
        \end{bmatrix}
        =
        \begin{pmatrix}
            0&0\\
            0&\frac{15}{2}
        \end{pmatrix}.
    \]
    Lefordítva ezt a kvadratikus alakok nyelvére azt kaptuk,
    hogy
    \(
        Q_R\left( z_1,z_2,z_3 \right)
        =
        -\frac{1}{2}\left( z_2-4z_3 \right)^2+\frac{15}{2}z_3^2.
    \)
    Összefoglalva tehát
    \[
        Q\left( z_1,z_2,z_3 \right)
        =
        2\left( z_1+\frac{1}{2}z_2-z_3 \right)^2
        -\frac{1}{2}\left( z_2-4z_3 \right)^2
        +\frac{15}{2}z_3^2.
    \]
    Az $A$ mátrix diádfelbontását is leolvashatjuk a fenti számolásból:
    \[
        A
        =
        2
        \begin{bmatrix}
        1\\ \frac{1}{2}\\ -1
        \end{bmatrix}
        \cdot
        \begin{bmatrix}
        1& \frac{1}{2}& -1
        \end{bmatrix}
        -\frac{1}{2}
        \begin{bmatrix}
            0\\1\\-4
        \end{bmatrix}
        \cdot
        \begin{bmatrix}
            0&1&-4
        \end{bmatrix}
        +
        \frac{15}{2}
        \begin{bmatrix}
            0\\0\\1
        \end{bmatrix}
        \cdot
        \begin{bmatrix}
            0&0&1
        \end{bmatrix}.
    \]

Voltaképpen készen is lennénk, mert
ez a módszer minden olyan esetben megadja a szimetrikus diádokra való felbontást,
mikor az algoritmus kezdő és minden további lépésében a maradék kvadratikus alak mátrixának balfelső eleme nem zérus.%
\footnote{
Evvel a problémával most ne foglalkozzunk később ki fog derülni, 
hogy könnyen megszabadulhatunk e kellemetlennek tűnő feltéteteltől.}

A maradék kvadratikus alak mátrixa, az $R=A-a_{1,1}d\cdot d^T$, a fentinél sokkal
hatékonyabban számolható.
Nézzük rá az állítás igazolásában kiemelt $(\dag)$ azonosságra de úgy,
hogy gondoljunk közben a Gauss\,--\,Jordan-eliminációra:
    \[
        r_{k,j}=
        a_{k,j}-a_{k,1}\delta_j.
        \tag{\dag}
    \]
Tudjuk, hogy az $R$ maradék első oszlopa és első sora zérus.
Ezért a kérdés csak az, 
hogy e mátrix jobb alsó $n-1\times n-1$ méretű részének mik az elemei.
A fenti formulát kell ehhez alkalmazni, mikor $k>1$ és $j>1$.
Ismerjük fel, hogy (\dag)  éppen az eliminációs algoritmus számolási szabálya abban az esetben, 
amikor a balfelső elem a pivot elem és a pivot elemet nem tartalmazó sorok elemeit számoljuk.
Ez azt jelenti, hogy az $A-a_{1,1}d\cdot d^T$ művelet diád szorzás és mátrix kivonás helyett Gauss\,--\,Jordan-eliminációval is számolható.
Bebizonyítottuk hát a következő állítást.
\begin{proposition}
   Legyen $A\in\mathbb{R}^{n\times n}$ egy olyan szimmetrikus mátrix,
   amelyre $a_{1,1}\neq 0$.
   Legyen $R$ az első diád leválasztása után maradó kvadratikus alak mátrixa,
   azaz 
   $$R=A-a_{1,1}d\cdot d^T,$$
   ahol $d$ az $A$ mátrix első oszlopának az $a_{1,1}$ reciprokával vett szorzata.
   Tudjuk, hogy $R$ első oszlopa és sora csak zérus elemeket tartalmaz, viszont e
   mátrix a jobb alsó $\left(n-1\times n-1  \right)$ méretű része megkapható
   az $A$ mátrixon végzett Gauss\,--\,Jordan-eliminációval, 
   olyan módon hogy a balfelső $a_{1,1}$ elemet választjuk pivot elemnek, 
   és az elimináció első sorát eldobjuk.
\end{proposition}
Ennek fényében az előző példa számolása radikálisan egyszerűsíthető. Nézzük újra.
\[
\begin{array}{c|ccc}
     & z_1       & z_2 & z_3 \\
     \hline
     & \boxed{2} & 1   & -2  \\
     & 1         & 0   & 1   \\
     & -2        & 1   & 3/2 \\
    \hline
    & \delta    & \frac{1}{2}   & -1
\end{array}
\implies
\begin{array}{c|ccc}
     & z_1       & z_2  & z_3 \\
     \hline
     & &                &       \\
     & & \boxed{-1/2}   & 2     \\
     & &            2   & -1/2  \\
     \hline
     & & \delta         & -4
\end{array}
\implies
\begin{array}{c|ccc}
     & z_1       & z_2 & z_3 \\
     \hline
     & &   &     \\
     & &   &     \\
     & &   & \boxed{15/2}    \\
    \hline
    \phantom{\delta}
\end{array}
\]
Ezután csak értelmeznünk kell a fenti eliminációt.
A második táblázat az elő diád leválasztása után maradt kvadratikus alak mátrixa.
A leválasztott diád az első táblázat segédsorából olvasható le, 
és a diád szorzója az első táblázatban választott pivot elem.
Tehát
\[
    \begin{pmatrix}
        2&1&-2\\
        1&0&1\\
        -2&1&\frac{3}{2}
    \end{pmatrix}
    =
    2
    \begin{bmatrix}
        1\\1/2\\-1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        1&1/2&-1
    \end{bmatrix}
    +
    \begin{pmatrix}
        0&0&0\\
        0&-1/2&2\\
        0&2&-1/2
    \end{pmatrix}.
\]
A harmadik táblázat az előző maradékból leválasztott kvadratikus alak mátrixa.
A leválasztott diád a második táblázat segédsorából olvasható le, 
és a szorzó a második táblázat pivot eleme.
Tehát
\[
    \begin{pmatrix}
        0&0&0\\
        0&-1/2&2\\
        0&2&-1/2
    \end{pmatrix}
    =
    -\frac{1}{2}
    \begin{bmatrix}
        0\\1\\-4
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        0&1&-4
    \end{bmatrix}
    +
    \begin{pmatrix}
        0&0&0\\
        0&0&0\\
        0&0&15/2
    \end{pmatrix}
\]
Világos, hogy az utolsó maradék $15/2$ szerese a 
\(
\begin{pmatrix}
    0\\0\\1
\end{pmatrix}
\)
vektor által meghatározott diádnak, tehát a diád felbontás a fenti eliminációs táblázatból azonnal leolvasható:
\[
    A
    =
    2
    \begin{bmatrix}
    1\\ \frac{1}{2}\\ -1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
    1& \frac{1}{2}& -1
    \end{bmatrix}
    -\frac{1}{2}
    \begin{bmatrix}
        0\\1\\-4
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        0&1&-4
    \end{bmatrix}
    +
    \frac{15}{2}
    \begin{bmatrix}
        0\\0\\1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        0&0&1
    \end{bmatrix}
\]
A diád felbontás a kvadratikus alakra nézve azt jelenti, 
hogy előállítottuk azt olyan teljes négyzetek lineáris kombinációjaként,
ahol a teljes négyzeteknek megfelelő diádokat generáló vektorok rendszere linárisan független rendszert ad.
Konkrétan itt
\[
    2z_1^2+\frac{3}{2}z_3^2+2z_1z_2-4z_1z_3+2z_2z_3
    =
    2\left( z_1+\frac{1}{2}z_2-z_3 \right)^2
    -\frac{1}{2}\left( z_2-4z_3 \right)^2
    +\frac{15}{2}z_3^2.
\]

\paragraph{Gyakorlat.}
Írjuk fel a 
\(
Q\left( x_1,x_2,x_3 \right)=
x_1^2+3x_2^2+6x_3^2+
2x_1x_2+2x_1x_3+
6x_2x_3
\)
kvadratikus alakot teljesnégyzetek lineáris kombinációjaként.
Az elimináció lépései:
\[
\begin{array}{c|ccc}
     & x_1       & x_2 & x_3 \\
     \hline
     & \boxed{1} & 1   & 1   \\
     & 1         & 3   & 3   \\
     & 1         & 3   & 6   \\
    \hline
    & \delta     & 1   & 1
\end{array}
\implies
\begin{array}{c|ccc}
     & x_1& x_2  & x_3 \\
     \hline
     & &                &     \\
     & & \boxed{2}      & 2   \\
     & & 2              & 5   \\
     \hline
     & & \delta         & 1
\end{array}
\implies
\begin{array}{c|ccc}
     & x_1 & x_2 & x_3      \\
     \hline
     &     &     &          \\
     &     &     &          \\
     &     &     & \boxed{3}\\
    \hline
    \phantom{\delta}
\end{array}
\]
Ez azt jelenti, hogy 
\(
Q\left( x_1,x_2,x_3 \right)
=
\left( x_1+x_2+x_3 \right)^2
+2\left( x_2+x_3 \right)^2
+3x_3^2.
\)
Az is világos, hogy a három pivot elem pozitivitása szerint a kvadratikus alak pozitív definit,
hiszen az
\begin{eqnarray*}
    x_1+x_2+x_3&=& 0\\
    x_2+x_3&=& 0\\
    x_3&=& 0
\end{eqnarray*}
homogén lineáris egyenletrendszernek, 
az együttható mátrix oszlopainak lináris függetlensége miatt,
csak triviális megoldása van!

\subsubsection{Zérus elem a bal felső sarokban}
Hangsúlyoznunk kell, hogy az algoritmusban a pivot elem megválasztása nem opcionális. 
A soron következő diád leválasztásához mindig a balfelső elemet kell választanunk.
Persze kérdés mi történik, ha a balfelső elem zérus?
Ennek illusztrációjaként nézzük az alábbi kvadratikus alakot.
\[
    Q\left( x_1,x_2,x_3 \right)=
    3x_2^2+
    2x_1x_2+2x_1x_3+
    6x_2x_3
\]
Persze el sem tudjuk kezdeni az algoritmust, hiszen a balfelső elem zérus.
Azt vegyük észre, hogy át tudjuk címkézni a változókat úgy, 
hogy a kapott kvadratikus alakban az első változónak legyen négyzete.
Ehhez csak az $x_1$ és az $x_2$ változókat kell felcserélnünk.
Vezessük be tehát a $z_1=x_2$, $z_2=x_1$ és $z_3=x_3$ jelöléseket.
Ezek szerint 
\[
    3x_2^2+
    2x_1x_2+2x_1x_3+
    6x_2x_3
    =
    3z_1^2+
    2z_2z_1+2z_2z_3+
    6z_1z_3
\]
Így viszont már indulhat is az elimináció:
\[
\begin{array}{c|ccc}
     & z_1       & z_2 & z_3 \\
     \hline
     & \boxed{3} & 1   & 3   \\
     & 1         & 0   & 1   \\
     & 3         & 1   & 0   \\
    \hline
    & \delta     & 1/3   & 1 
\end{array}
\implies
\begin{array}{c|ccc}
     & z_1& z_2  & z_3 \\
     \hline
     & &                &     \\
     & & \boxed{-1/3}      & 0   \\
     & & 0              & -3   \\
     \hline
     & & \delta         & 0
\end{array}
\implies
\begin{array}{c|ccc}
     & z_1 & z_2 & z_3      \\
     \hline
     &     &     &          \\
     &     &     &          \\
     &     &     & \boxed{-3}\\
    \hline
    \phantom{\delta}
\end{array}
\]
A teljes négyzetek lineáris kombinációját leolvasva:
\[
    3z_1^2+
    2z_2z_1+2z_2z_3+
    6z_1z_3
    =
    3\left( z_1+\frac{1}{3}z_2+z_3 \right)^2-\frac{1}{3}z_2^2-3z_3^2
    =
    3\left( \frac{1}{3}x_1+x_2+x_3 \right)^2-\frac{1}{3}x_1^2-3x_3^2,
\]
ami a diádfelbontásra nézve azt jelenti, hogy
\[
    \begin{pmatrix}
        0&1&1\\
        1&3&3\\
        1&3&0
    \end{pmatrix}
    =
    3
    \begin{bmatrix}
        1/3\\1\\1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        1/3&1&1
    \end{bmatrix}
    -\frac{1}{3}
    \begin{bmatrix}
        1\\0\\0
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        1&0&0
    \end{bmatrix}
    -3
    \begin{bmatrix}
        0\\0\\1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        0&0&1
    \end{bmatrix}.
\]
Ha ezt az átcímkézési trükköt megértettük, akkor még egyszerűbb jelöléseket is alkalmazhatunk.
Végül is csak az volt a lényeg, hogy az első két változót felcseréltük. 
Jelezhetjük ezt a táblázatban a változók indexével is. 
Ugyanez mégegyszer:
\[
\begin{array}{c|ccc}
     & x_1       & x_2 & x_3 \\
     \hline
     \phantom{\boxed{3}} & 0 & 1   & 1   \\
     & 1 & 3   & 3   \\
     & 0 & 3   & 0   \\
    \hline
    & \phantom{\delta}     &\phantom{1/3}    &\phantom{1}
\end{array}
\implies
\begin{array}{c|ccc}
     & x_2       & x_1 & x_3 \\
     \hline
     & \boxed{3} & 1   & 3   \\
     & 1         & 0   & 1   \\
     & 3         & 1   & 0   \\
    \hline
    & \delta     & 1/3   & 1 
\end{array}
\implies
\begin{array}{c|ccc}
     & x_2& x_1  & x_3 \\
     \hline
     & &                &     \\
     & & \boxed{-1/3}      & 0   \\
     & & 0              & -3   \\
     \hline
     & & \delta         & 0
\end{array}
\implies
\begin{array}{c|ccc}
     & x_2 & x_1 & x_3      \\
     \hline
     &     &     &          \\
     &     &     &          \\
     &     &     & \boxed{-3}\\
    \hline
    \phantom{\delta}
\end{array}
\]
A változók indexére ügyelve kell leolvasnunk a teljesnégyzeteket, és a legegszerűbb, ha a teljesnégyzetekből olvassuk le a diád felbontást.

Már csak azt kell meggondolnunk, hogyan kaptuk az első táblázatból a másodikat?
Azt kell látnunk, hogy az $i$-edik és a $j$-edik változó felcserélése avval jár, 
hogy a mátrixban felcseréljük az $i$-edik sort a $j$-edik sorral és az $i$-edik oszlopot a $j$-edik oszloppal.
Persze, hiszen a mátrixban minden az $i,k$ és a $j,k$ pozícióban álló szám helyet cserél, 
és hasonlóan minden a $k,i$ és a $k,j$ pozícióban álló szám is helyet cserél minden $k$ mellett.
A többi elem viszont nem változik.
Az első rész az $i$ és $j$ indexű sorok felcserélését jelenti, míg a második rész az $i$ és $j$ indexű oszlopok kicseréléséből áll.
A többi sort és oszlopot nem éri változás.

Látjuk tehát, hogy ha a balfelső elem zérus, de a diagonálisnak van nem zérus eleme, 
akkor a most tanult felcseréléssel a nem zérus elem a balfelső pozícióba hozható, így az algoritmusunk továbbra is működik.

\subsubsection{A diagonálisban minden elem zérus}
Most nézzük azt az esetet, amikor a diagonális minden eleme zérus, de van nem zéró elem.

Először is azt vegyük észre, hogy ha a feladat a kvadratikus alak definitségének meghatározása, 
akkor készen is vagyunk mert indefinit kvadratikus alakkal állunk szemben.
Ha ugyanis a $k,l$ pozíció tartalmaz nem zérus elemet, 
akkor a kvadratikus alak összes többi változóját állítsuk zérusra és a $k$-adik és az $l$-edik változót vizsgáljuk.
Így a kvadratikus alakunk az $2a_{k,l}x_kx_l$ kifejezésre egyszerűsödik, 
amely persze indefinit, hiszen az értékkészlete negatív és pozitív elemet egyaránt tartalmaz.

A teljesség kedvéért megmutatjuk, 
hogy az algoritmusunk ebben az esetben is használható.
Ha az $a_{k,l}\neq 0$, akkor vezessük be a 
\[
    z_l=x_l+x_k\qquad z_j=x_j, \text{ ha }j\neq l
\]
változókat.
Ekkor $k\neq l$-et is figyelembe véve
\begin{multline*}
z_kz_l=x_k\left( x_l+x_k \right)=x_kx_l+x_k^2=z_k^2+x_kx_l,
\text{ majd $j\neq k$, $j\neq l$ mellett: }\\
z_jz_l=x_j\left( x_l+x_k \right)=x_jx_l+x_jx_k=x_jx_l+z_jz_k.
\end{multline*}
Ez azt jelenti, hogy a $x_kx_l$ kifejezést a $(z_kz_l-z_k^2)$-re kell cserélnünk,
és minden további $x_jx_l$ alakú kifejezést a $(z_jz_l-z_jz_k)$ kifejezéssel kell kicserélnünk.
A többi változó indexe nem változik.
Így pontsan egy négyzetet kapunk, 
tehát a mátrix diagonálisában a $k$-adik sorban jelenik meg egy nem zérus elem.
Ahogyan azt az előzőekben már megértettük ezt az elemet kell a balfelső pozícióba csempészni, 
és az algoritmusunk már működik is.

\section{Definitség osztályozása a teljes négyzetek segítségével}
Miután elkészült a kvadratikus alakot reprezentáló mátrix diádfelbontása,
a kvadratikus alak definitsége az alábbiak szerint osztályozható.
Legyen tehát $Q$ egy $n$ változós nemzérus kvadratikus alak.\\
Ha a diád felbontás
\begin{itemize}
    \item tartalmaz pozitív és negatív pivot elemet, 
        akkor $Q$ \emph{indefinit}.
\end{itemize}
Most tegyük fel, hogy $Q$ felbontása pontosan $n$ teljes négyzetet tartalmaz.
Ha
\begin{itemize}
    \item az összes pivot elem pozitív,
        akkor $Q$ \emph{pozitív definit};
    \item az összes pivot elem negatív,
        akkor $Q$ \emph{negatív definit};
\end{itemize}
Ha $Q$ felbontása $n$-nél kevesebb teljes négyzetből áll,
akkor nagyon hasonló karakterizációt kapunk, de a mátrix szingulátis, ezért csak szemidefinitség jöhet szóba.
Tehát ha
\begin{itemize}
\item az összes pivot elem pozitív,
    akkor $Q$ \emph{pozitív szemidefinit};
\item az összes pivot elem negatív,
    akkor $Q$ \emph{negatív szemidefinit};
\end{itemize}

\backmatter
\pagestyle{empty}
%\bibliography{\jobname}
\printbibliography
\printindex
\end{document}
% arara: latexmk: { 
% arara: --> engine: lualatex,
% arara: --> options: [ '-pvc' ]
% arara: --> }

Created: Sat 20 Jul 2019 05:02:12 AM CEST
Last Modified: Sat 26 Feb 2022 10:42:08 AM CET Europe Standard Time 
