\documentclass[9pt, a4paper, showtrims]{memoir}
\let\Aref\relax
\usepackage[x11names]{xcolor}
%%%%%%% pdflatex %%%%%%%%%%
%\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
%\usepackage[hungarian]{babel}[2015/11/24]
%%%%%%% pdflatex %%%%%%%%%%

%%%%%% lualatex %%%%%%%%%%%
%\usepackage{polyglossia}\setdefaultlanguage{magyar}
\usepackage[hungarian]{babel}[2015/11/24]
\usepackage{fontspec}
\defaultfontfeatures{Ligatures=TeX}
\setmainfont{TeX Gyre Pagella}
\setsansfont{Kurier}[Scale=MatchLowercase]
\setmonofont{inconsolata}[Scale=MatchLowercase]
%%%%%% lualatex %%%%%%%%%%%

\frenchspacing
\usepackage{csquotes,amsmath,amsthm,amsfonts,amssymb,systeme,fixme}
\usepackage[backend=biber,
            bibencoding=utf8,
            style=authoryear,
            autocite=inline,
            backref=true]{biblatex}
\addbibresource{\jobname.bib}
\BiblatexHungarianWarningOff
\fxsetup{status=draft, theme=color, layout={inline}}
\renewcommand{\fixmelogo}{\textcolor{black}{\colorbox{Firebrick1}{\textsf{\textbf{FIX}}}}}

\usepackage[unicode]{hyperref}\hypersetup{final}\usepackage{memhfixc}

\usepackage[inline]{enumitem}
\usepackage[a4paper]{geometry}
\usepackage[missing={GitHub: \today},dirty={Continuous integration},mark]{gitinfo2}
%\usepackage[mark,dirty={(Dirty)}]{gitinfo2}
%\edef\gitBranch{\gitBranch}
\renewcommand{\gitMark}{Branch: \gitBranch\,@\,\gitFirstTagDescribe{}
    \textbullet{}
    Date: \gitAuthorIsoDate
}


%\setsecheadstyle{\Large\sffamily\bfseries\raggedright}
%\setsubsecheadstyle{\large\sffamily\bfseries\raggedright}
%\setsubsubsecheadstyle{\sffamily\bfseries\raggedright}


\nouppercaseheads
%\makeoddhead{myheadings}{\footnotesize\selectfont\sffamily\leftmark}{}{\footnotesize\selectfont\sffamily\thepage}
%\makeevenhead{myheadings}{\footnotesize\selectfont\sffamily\thepage}{}{\footnotesize\selectfont\sffamily\rightmark}
\makeoddhead{myheadings}{\footnotesize\leftmark}{}{\footnotesize\thepage}
\makeevenhead{myheadings}{\footnotesize\thepage}{\footnotesize\myBotmark}{\footnotesize\rightmark}
\makepsmarks{myheadings}{%
    \renewcommand\chaptermark[1]{%
    \markboth{%
%      \ifnum \value{secnumdepth} > 1
%      \if@mainmatter %
            \thechapter.~\chaptername:~%
%      \fi
%      \fi
        ##1}{\thepart.~\partname}}%
    }
\makeatletter
\let\ps@plain\ps@empty
\newcommand\arraybslash{\let\\\@arraycr}
\patchcmd{\@makechapterhead}
    {\printchaptername \chapternamenum \printchapternum}
    {\printchapternum.\@\chapternamenum \printchaptername}
    {}{}
\renewenvironment{proof}[1][\proofname]
    {\par\pushQED{\qed}%
    \normalfont \topsep6\p@\@plus6\p@\relax
    \trivlist
    \item[\hskip\labelsep
        \itshape
    #1\@addpunct{:}]\ignorespaces}
    {\popQED\endtrivlist\@endpefalse}
\makeatother

\newcommand{\addQEDstyle}[2]{\AtBeginEnvironment{#1}{\pushQED{\qed}\renewcommand{\qedsymbol}{#2}}\AtEndEnvironment{#1}{\popQED}}
%% qed trükkök:
%% https://tex.stackexchange.com/questions/16453/denoting-the-end-of-example-remark
%\swapnumbers %% a magyar.ldf megfordítja. A polyglossia nem. De a magyar ldf pontot is tesz a számcimke után

\renewcommand{\qedsymbol}{$\centerdot$}
\newcommand{\myqedsymbol}{$\lrcorner$}
\theoremstyle{plain}

\newtheorem{proposition}{állítás}[chapter]
\newtheorem{lemma}[proposition]{lemma}
\newtheorem*{SL}{Steinitz-lemma}
\newtheorem*{FA}{Az algebra alaptétele}
%
\theoremstyle{remark}
\newtheorem{note}[proposition]{megjegyzés}

\theoremstyle{definition}
\newtheorem{definition}[proposition]{definíció}
\newtheorem{corollary}[proposition]{következmény}
\newtheorem{defprop}[proposition]{definíció-állítás}
\addQEDstyle{definition}{\myqedsymbol}\addQEDstyle{proposition}{\myqedsymbol}\addQEDstyle{lemma}{\myqedsymbol}\addQEDstyle{note}{\myqedsymbol}\addQEDstyle{corollary}{\myqedsymbol}\addQEDstyle{SL}{\myqedsymbol}\addQEDstyle{defprop}{\myqedsymbol}


%%% https://tex.stackexchange.com/questions/319474/put-current-theorem-like-items-name-number-in-header
% \myBotmark feltöltése a lapon lévő utolsó tétel környezettel
\makeatletter
    \@ifdefinable\@my@claim@mark{\newmarks\@my@claim@mark}
    \newcommand*\myMark[1]{\marks\@my@claim@mark{#1}}
    \newcommand*\myBotmark{\botmarks\@my@claim@mark}
    \patchcmd{\@begintheorem}{% search for:
        \thm@swap\swappedhead\thmhead % more specific than before
    }{% replace with:
        \myMark{#2.\@ifnotempty{#1}{\ #1}\@ifnotempty{#3}{\ (#3)}}%
        \thm@swap\swappedhead\thmhead
    }{
        \typeout{>>> Made patch specific for amsthm.}
    }{
        \typeout{>>> Patch specific for amsthm FAILED!}
    }

 %part
\long\def\@part[#1]#2{%
  \M@gettitle{#1}%
  \def\f@rtoc{#1}%
  \@nameuse{part@f@rtoc@before@write@hook}%
  \phantomsection
  \mempreaddparttotochook
  \ifnum \c@secnumdepth >-2\relax
    \refstepcounter{part}%
    \addcontentsline{toc}{part}%
      {\protect\partnumberline{\thepart}\f@rtoc}%
    \mempartinfo{\thepart}{\f@rtoc}{#2}%
  \else
    \addcontentsline{toc}{part}{\f@rtoc}%
    \mempartinfo{}{\f@rtoc}{#2}%
  \fi
  \mempostaddparttotochook
  \partmark{#1}%
  {\centering
   \interlinepenalty \@M
   \parskip\z@
   \normalfont
   \ifnum \c@secnumdepth >-2\relax
     \printpartnum.\ \printpartname \partnamenum%%MGy
     \midpartskip
   \fi
   \printparttitle{#2}\par}%
  \@endpart}
\makeatother

%\renewcommand{\mathbf}{\mathbb}
\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator{\gen}{gen}
\DeclareMathOperator{\crank}{crank}
\DeclareMathOperator{\rrank}{rrank}
\DeclareMathOperator{\srank}{srank}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\im}{Im}
%\DeclareMathOperator{\arg}{arg}

\def\scwords #1 #2 #3 {\textsc{#1} \textsc{#2} \textsc{#3} }
\newcommand{\uj}{\text{új}}
\newcommand{\rgi}{\text{régi}}
%\citeindextrue
\makeindex
\synctex=1
\begin{document}
\frontmatter*
\section*{Verzió információk}
\begin{center}
\begin{tabular}{l|r}
    \hline
References & \gitReferences\\
Branch & \gitBranch\\
Dirty & \gitDirty\\
Hash&\gitHash\\
Author Iso Date & \gitAuthorIsoDate\\
\hline
First Tag Describe & \gitFirstTagDescribe \\
Reln& \gitReln \\
Roff & \gitRoff \\
Tags & \gitTags \\
Describe & \gitDescribe \\
\hline
\end{tabular}
\end{center}
\chapter*{Előszó}
\scwords%
A legfontosabb forrás \parencite{DancsPuskas2001}.

\dots

Igyekszem strukturáltan írni.
Kicsi, atomszerű építőkövek egymás utáni megértése visz az anyagban előre, 
ezek az egymástól feltűnő módon szeparált ,,állítások'' és azok érvekkel való alátámasztása,
amit ,,bizonyításnak'' is szokás mondani.
Az írásmód oka, 
hogy evvel is hangsúlyozzam, hogy az olvasónak igyekeznie kell strukturáltan gondolkodni.
A hátulütője, 
hogy hibásan azt a helytelen képzetet keltheti, 
mintha az egyes állítások mintegy puzzle-ként állnának össze.
Nem, nem erről van szó. 
A puzzle-ban minden elem egyenrangú, 
az egyik elem hiánya éppen annyira fájdalmas mint a másiké.
Ez egyetlen matematikai diszciplína esetében sem igaz!
Az olvasónak igyekeznie kell, 
hogy meglássa mi a legfontosabb gondolat a sok-sok állításnak, 
mint építménynek egy-egy ,,nyilvánvaló következményében''.

Hogy e kis lépések egymástól még határozottabban váljanak el azt az írás
tipográfiája is erősíti azzal, 
az állítás-szerű környezeteket a \,\myqedsymbol~, 
és a bizonyítás környezetet a \,\qedsymbol~ karakterekkel zárom le.

Stb.

\bigskip\noindent 
Magyarkuti Gyula
\hfill{Budapest, \ontoday}


\clearpage
\tableofcontents*
\pagestyle{myheadings}
\mainmatter*
%\part{F-dúr hegedűverseny No. 3, Op. 8, RV 293, ,,L'autunno''}
\part{Ősz}
\chapter{Előzmények}
\scwords A lineáris algebra tárgyalásához elengedhetetlenül szükséges általános algebrai ismereteket foglaljuk össze.
\section{Algebrai struktúrák}
\begin{definition}[$n$-változós művelet]\index{művelet}\index{algebrai struktúra}
    Legyen $H$ egy halmaz. Egy 
    \[
        \varphi\colon H^n\to H
    \]
    függvényt $n$-változós \emph{műveletnek} nevezünk.
    Egy halmazt és rajta véges sok műveletet együtt \emph{algebrai struktúrának} mondunk.
    Jelölés: 
    $$\left(H,\varphi_1,\dots,\varphi_n  \right),$$ 
    ahol $H$ a halmaz és
    $\varphi_1,\dots,\varphi_n$ a $H$ halmazon értelmezett műveletek.
\end{definition}
\begin{definition}[félcsoport]\index{félcsoport}
    Egy $\left( S,\star \right)$ algebrai struktúrát \emph{félcsoportnak} mondjuk,
    ha $\star$ egy kétváltozós \emph{asszociatív}\index{asszociatív}
    művelete az $S$ halmaznak,
    azaz minden $a,b,c\in S$ mellett
    \[
        a\star\left( b\star c \right)=\left( a\star b\right)\star c.\qedhere
    \]
\end{definition}
Lefordítva ez azt jelenti, hogy 
\begin{enumerate}
    \item minden $a,b\in S$ mellett $a\star b\in S$, és
    \item minden $a,b,c\in S$ esetén $a\star\left( b\star c \right)=a\star\left( b\star c \right)$
\end{enumerate}
\begin{definition}[neutrális elem]\index{neutrális elem}\index{neutrális elemes félcsoport}
    Az $\left( S,\star \right)$ félcsoportban az $s\in S$ elem \emph{balról (jobbról) neutrális},
    ha $s\star t=t$ ($t\star s=t$) minden $t\in S$ mellett.
    Ha $s\in S$ balról is és jobbról is neutrális, akkor $s$-et egy \emph{neutrális elemnek}
    mondjuk.
    A félcsoportot \emph{neutrális elemes félcsoportnak} nevezzük, ha van benne neutrális elem.
\end{definition}
\begin{proposition}
    Ha egy félcsoportban, van egy balról neutrális elem és egy jobbról neutrális elem, 
    akkor ezek megegyeznek. 
    Emiatt egy neutrális elemes félcsoportban neutrális elem csak egy van.
\end{proposition}
\begin{proof}
    Legyen $s_1$ balról-- és $s_2$ jobbról neutrális elem.
    Ekkor
    \(
    s_1=s_1\star s_2=s_2.
    \)
\end{proof}
A félcsoport additív írásmódja esetén természetes a neutrális elemet \emph{zérusnak},
míg multiplikatív írásmód esetén \emph{egységnek} nevezni.
\begin{definition}[csoport]\index{csoport}
    Egy $\left( G,\star \right)$ algebrai struktúrát \emph{csoportnak} nevezünk,
    ha neutrális elemes félcsoport, amelyben minden $g\in G$-hez létezik $g'\in G$, hogy
    \[
        g\star g'=e=g'\star g.\tag{\dag}
    \]
    Itt $e\in G$ jelöli a $G$ csoport neutrális elemét.
\end{definition}
\begin{defprop}[inverz elem]\index{inverz}
    Legyen $\left( G,\star \right)$ egy csoport.
    Ekkor minden $g\in G$-hez, csak egyetlen $g'\in G$ létezik, 
    amelyre a fenti ($\dag$) azonosság fennáll.
    Adott $g$-hez ezt ez egyetlen $g'\in G$ elemet, 
    amelyre ($\dag$) teljesül a $g$ elem \emph{inverzének} mondjuk.
\end{defprop}
\begin{proof}
    Tegyük fel, hogy $g',g''$ inverz elemei $g$-nek.
    Azt mutatjuk meg, hogy ha $g'$ baloldali-- és $g''$ jobboldali inverze $g$-nek,
    akkor a két elem megegyezik:
    \[
        g'=g'\star e=
        g'\star\left( g\star g'' \right)=
        \left(g'\star g\right)\star g'' =
        e\star g''=g''.\qedhere
    \]
\end{proof}
Példaként gondoljuk meg, hogy a $H\to H$ függvény halmaza a kompozíció művelettel
neutrális elemes félcsoport, és a $H\to H$ kölcsönösen egyértelmű függvények halmaza a kompozíció művelettel csoportot alkotnak. 
Ez utóbbi csoportot mondjuk \emph{permutáció csoportnak}\index{permutációk}.
\begin{proposition}[egyszerűsítési szabály]\index{egyszerűsítési szabály}
    Csoportban igaz az egyszerűsítési szabály, azaz
    \[
        a\star c=b\star c\implies a=b.\qedhere
    \]
\end{proposition}
\begin{proof}
    \begin{math}
        a=a\star e
        =
        a\star \left( c\star c'\right)=
        \left( a\star c \right)\star c'=
        \left( b\star c \right)\star c'=
        b\star\left( c\star c' \right)=
        b\star e=
        b.
    \end{math}
\end{proof}
\begin{definition}[Abel--csoport]\index{Abel--csoport}
    Egy $\left( G,\star \right)$ csoportot \emph{Abel--csoportnak} nevezünk,
    ha a művelete \emph{kommutatív}\index{kommutatív} is, 
    azaz minden $s,t\in G$ mellett $s\star t=t\star s$.
\end{definition}
\begin{definition}[gyűrű]\index{gyűrű}
    A kétműveletes $\left( R,+,\cdot \right)$ algebrai struktúrát \emph{gyűrűnek} nevezzük,
    ha
    \begin{enumerate}
        \item $\left( R,+ \right)$ Abel--csoport;
        \item $\left( R,\cdot \right)$ félcsoport;
        \item és a két műveletet összeköti a következő két disztributivitás:\index{disztributív}
            \[
                a\cdot\left( b+c \right)=a\cdot b + a\cdot c\qquad
                \left( a + b \right)\cdot c=a\cdot c+a\cdot b.
            \]
    \end{enumerate}
    Ha $\left( R,\cdot \right)$ neutrális elemes félcsoport, akkor azt mondjuk, hogy $R$ egy
    \emph{egységelemes gyűrű}, és ha $\left( R,\cdot \right)$ kommutatív félcsoport, akkor
    azt mondjuk, hogy $R$ egy \emph{kommutatív gyűrű}.
\end{definition}
\begin{definition}[test]
    Egy $\left( \mathbb{F},+,\cdot \right)$ kétműveletes algebrai struktúrát \emph{testnek}
    nevezünk, 
    ha olyan kommutatív egységelemes gyűrű, 
    amelyben minden nemzérus\footnote{értsd: minden elemnek, amely a $+$ műveletre nézve neutrális elemtől különbözik}
    elemnek van inverze\footnote{értsd: a $\cdot$ szorzás neutrális elemére mint egységelemre nézve}, 
    és $0\neq 1$\footnote{érts: az összeadásra nézve és a szorzásra nézve képzett neutrális elemek nem azonosak.}.
\end{definition}
A test az algebrai struktúra, ahol a az összeadás és szorzás műveletekkel úgy számolhatunk, mint amit a valós számok során megszoktuk.
Példaként néhány tulajdonság.
\begin{proposition}
    Az $\left( R,+,\cdot \right)$ gyűrűben minden $a\in R$ mellett 
    \begin{equation*}
        a\cdot 0=0\text{ és }
        \left( -1 \right)a=-a.\qedhere
    \end{equation*}
\end{proposition}
\begin{proof}
    \begin{math}
        0+a\cdot 0=
        a\cdot 0=
        a\left( 0+0 \right)=
        a\cdot 0+a\cdot 0.
    \end{math}
    A jobboldali $a\cdot 0$-val való egyszerűsítés után kapjuk, 
    hogy $0=a\cdot 0$.
    A második azonosságot az első felhasználásával kapjuk:
    \begin{math}
        0
        =
        0a
        =
        \left( 1+\left( -1 \right) \right)\cdot a
        =
        1\cdot a + \left( -1 \right)\cdot a
        =
        a +\left( -1 \right)\cdot a.
    \end{math}
    Az additív inverz definíciója szerint ez éppen azt jelenti, hogy $-a=\left( -1 \right)\cdot a$.
\end{proof}
Ami nagyon fontos, hogy egy gyűrűben nem feltétlen teljesül, 
hogy elemek szorzata csak úgy lehet zérus, ha legalább az egyik elem zérus.
Számunkra a legfontosabb példa  a mátrixok gyűrűje\footnote{lásd kicsit később},
ahol pont ennek a hiánya jelenti nehézséget.

Egy testben ilyen nem fordulhat elő.
\begin{definition}[nullosztómentes gyűrű]
    Egy gyűrűt \emph{nullosztómentesnek} nevezzük, 
    ha két elem szorzata csak úgy lehet nulla, 
    ha legalább az egyik elem nulla.
\end{definition}
\begin{proposition}
    Egy test egyben nullosztómentes gyűrű, azaz
    ha $\mathbb{F}$ egy test, és $a,b\in\mathbb{F}$.
    Akkor 
    \[
        ab=0\implies a=0\text{ vagy }b=0.\qedhere
    \]
\end{proposition}
\begin{proof}
    Tegyük fel, hogy $ab=0$.
    Ha $b\neq 0$, akkor létezik $b'\in\mathbb{F}$, hogy $bb'=1$.
    Így 
    \[
        0= 0b'=\left( ab \right)b'=a\left( bb' \right)=a1=a.\qedhere
    \]
\end{proof}
\begin{proposition}
    Nullosztómentes gyűrűben nem zérus elemmel való szorzatot egyszerűsíteni lehet
    azaz, ha $a,b,c\in R,b\neq 0$ esetén
    \[
        ab=cb\implies a=c.\qedhere
    \]
\end{proposition}
\begin{proof}
    \begin{math}
        \left( a-c \right)b=ab-cb=0\implies a-c=0.
    \end{math}
\end{proof}
\begin{definition}[ideál]\index{ideál}\index{főideál}\index{főideál-gyűrű}
    Egy $\left( R,+,\cdot \right)$ kommutatív gyűrű egy $J\subseteq R$ nem üres részhalmazát
    \emph{ideálnak} nevezzük,
    ha 
    \begin{enumerate}
        \item 
            minden $a,b\in J$ mellett $a+b\in J$;
        \item
            minden $c\in R$ és minden $a\in J$ mellett $ca\in J$.
    \end{enumerate}
    Ha egy $d\in R$ adott, akkor a 
    \[
        \left\{ da:a\in R \right\}
    \]
    halmaz egy ideálja $R$-nek. 
    Ez a $d$ elem többszöröseiből álló ideál, amelyet \emph{főideálnak}\index{főideál} is nevezünk.
    Ha egy gyűrűben minden ideál egy főideál, akkor a gyűrűt \emph{főideál-gyűrűnek}\index{főideál-gyűrű} mondjuk.
\end{definition}
A generált ideál fogalma nagyon fontos.
\begin{defprop}[generált ideál]\index{generált ideál}
    Legyen adott a  kommutatív, egységelemes $\left( R,+,\cdot \right)$ gyűrűben véges sok $a_1,\dots,a_r$ elem.
    Az e véges sok elemet tartalmazó ideálok közös része maga is ideál, 
    és e metszet az eredeti véges halmazt
    tartalmazó \emph{legszűkebb ideál}.
    Jelöljük ezt $J\left( a_1,a_2,\dots,a_r \right)$ módon.

    Tekintsük a 
    $
    \left\{ \sum_{j=1}^ra_jb_j:b_1,\dots,b_r\in R \right\}
    $ 
    halmazt.
    Világos, 
    hogy ez egy ideál az $R$ gyűrűben. 
    A gyűrű egységelemes, 
    ezért ennek $J\left( a_1,\dots,a_r \right)$ egy részhalmaza.
    Másrészt minden az $\left\{ a_1,\dots,a_r \right\}$ elemeket tartalmazó ideál, 
    egyben tartalmazza a
    $
    \left\{ \sum_{j=1}^ra_jb_j:b_1,\dots,b_r\in R \right\}
    $ 
    halmazt is,
    ami azt jelenti, hogy 
    \[
        J\left( a_1,\dots,a_r \right)=
        \left\{ \sum_{j=1}^ra_jb_j:b_1,\dots,b_r\in R \right\}
    \]
    az $a_1,\dots,a_r$ elemeket tartalmazó legszűkebb ideál.
    Nevezzük ezt az ideált az $a_1,\dots,a_r$ elemek \emph{generálta ideálnak} is.
\end{defprop}



Világos, hogy $\left\{ 0 \right\}$ és maga az egész $R$ ideálok.
A legfontosabb struktúrák számunkra a következők:
\begin{itemize}
    \item 
        Egységelemes gyűrű, amelyben a nullosztómentesség nem teljesül: mátrixok.
    \item
        Kommutatív egységelemes gyűrű, amely nullosztómentes de mégsem test: polinomok.
    \item
        Test:
        a valós vagy a komplex számok.
\end{itemize}
\section{Polinomgyűrűk}
\begin{definition}[polinom]\index{polinom}
    Legyen $\mathbb{F}$ egy test.
    E test feletti polinomokon az összes 
    \[
        p\left( t \right)=
        \alpha_0+\alpha_1t+\alpha_2t^2+\dots+\alpha_nt^n
    \]
    alakú formális algebrai kifejezést értjük.
    Itt $n$ tetszőleges nem negatív egész 
    és $\alpha_0,\dots,\alpha_n$ tetszőleges, az $\mathbb{F}$ testbeli elemek.
    Az $\mathbb{F}$ test feletti összes polinomok halmazát $\mathbb{F}\left[ t \right]$ módon jelöljük.
\end{definition}
A fenti definícióban az \emph{algebrai kifejezés} szó arra utal,  hogy az
\begin{math}
    \alpha_0+\alpha_1t+\alpha_2t^2+\dots+\alpha_nt^n
\end{math}
műveletek minden $t\in\mathbb{F}$ mellett értelmesek, 
és eredményük egy újabb $\mathbb{F}$ testbeli elem.
Ha $t\in\mathbb{F}$ konkrétan meg van adva, 
akkor a behelyettesítés után kapott elemet mondjuk a $p$ polinom helyettesítési értékének.

A \emph{formális algebrai kifejezés}\index{formális algebrai kifejezés} arra utal, 
hogy egy polinomot az együtthatói határozzák meg, 
azaz két polinom akkor és csak akkor azonos,
ha az összes együtthatói azonosak.
Ez szemben áll avval, hogy ha a polinomokra mint függvényekre tekintenénk, 
akkor a helyettesítési értékek egyenlősége jelentené a két polinom azonos voltát.
A formális szó tehát azt jelenti, hogy nem mint függvényre gondolunk, 
hanem egyszerűen az adott $\alpha_0,\dots,\alpha_n$ rögzített elemek -- ezeket mondjuk együtthatóknak --,
által előírt műveletekre. 
Az az előírás ugyanis, hogy tetszőleges $t\in\mathbb{F}$ mellett hajtsuk végre az
\[
    \alpha_0+\alpha_1t+\alpha_2t^2+\dots+\alpha_nt^n
\]
műveletsort. 
A műveletsorról és nem annak eredményéről van szó. 


Két műveletsor akkor azonos, ha ugyanazok a műveletsort meghatározó 
$\left( \alpha_0,\alpha_1,\dots,\alpha_n \right)$%
\footnote{Az előbbi zárójellel azt hangsúlyozzuk, hogy az együtthatók sorrendje is számít.}
együtthatók.%
\footnote{Persze felmerül a kérdés, 
    hogy ha két polinom minden helyettesítési értéke azonos,
    akkor igaz-e, 
    hogy mint formális polinomok is azonosak,
    tehát a két polinom együtthatói is rendre azonosak-e?
    A pozitív választ később látjuk nem véges számtest, például a valós vagy a komplex test, feletti polinomok esetén.
    Lásd \aref{pr:polinomfv}. állítás utáni megjegyzést \apageref{pr:polinomfv}. oldalon.%
}
A jelölések megértése is fontos.
$p\left( t \right)\in\mathbb{F}\left[ t \right]$ semmi mást nem jelent, 
minthogy $p\left( t \right)$ egy polinom.
Persze a polinom nem keverendő össze a helyettesítési értékével, 
hiszen az egyik egy algebrai kifejezés-együttes, a másik egy az adott testbeli elem.
Szokásos viszont, hogy ha nincs konkrét $t$ a szövegkörnyezetben, akkor is $p\left( t \right)$ jelöli a polinomot. 
Néha egyszerűbben csak $p$-vel jelöljük, főleg akkor ha nincs szó behelyettesítésről,
emiatt érdektelen a változó jele.
Ritkábban, de előfordul, hogy egy konkrét értékre, mondjuk $s\in\mathbb{F}$-re kell kiértékelnünk a polinomot ilyenkor $p\left( s \right)$ jelöli azt a testbeli elemet,
amelyet $t$ helyett $s$-et téve az előírt műveletek kiértékelése után kapunk.
A szövegkörnyezetben mindig világosnak kell lennie, hogy $p\left( t \right)$ a polinomot jelenti,
vagy egy konkrét $t$-re kiértékelt testbeli elemet.

\begin{definition}[polinom foka]\index{polinom foka}
    Legyen $p\left( t \right)\in\mathbb{F}\left[ t \right]$ egy polinom.
    Azt mondjuk, hogy a $n$ nem negatív egész szám e \emph{polinom fokszáma},
    ha $n$ a legnagyobb nemzérus együttható indexe.
    A legnagyobb nemzérus együtthatót \emph{főegyütthatónak}\index{főegyüttható} nevezzük.
    Azt mondjuk, hogy egy nemzérus polinom \emph{normált}\index{normált polinom}, ha $1$ a főegyütthatója.

    A $p\left( t \right)=0$ konstans zérus polinom foka megállapodás szerint legyen $-\infty$.
    A $p$ polinom fokszámát $\deg p$ módon jelöljük.
\end{definition}
Látni fogjuk, hogy a konstans zérus polinomra $\deg p=-\infty$ csak egy kényelmes jelölés.
Időnként a polinom fokszámával műveleteket is végzünk.
Megegyezés szerint ilyenkor $-\infty+a=-\infty$ minden $a$ nem negatív egész számra, 
és $\left( -\infty \right)+\left( -\infty \right)=-\infty.$
A $-\infty$ szimbólumot minden egész számnál határozottan kisebbnek gondoljuk.

Két polinom összegét és szorzatát a szokásos módon definiáljuk:
\begin{definition}\label{def:polmuveletek}
    Legyen $p,q\in\mathbb{F}[t]$, két polinom.
    \[
        p\left( t \right)
        =
        \sum_{j=0}^n\alpha_jt^j
        \text{ és }
        q\left( t \right)
        =
        \sum_{j=0}^m\beta_jt^j,
        \qquad
        \alpha_j,\beta_j\in\mathbb{F}, 
        0\leq n,m\in\mathbb{Z}.
    \]
    Ekkor a $p$ és $q$ összegének definíciója:
    \[
        \left( p+q \right)\left( t \right)
        =
        \sum_{j=0}^{\max{\left\{ m,n \right\}}}\left( \alpha_j+\beta_j \right)t^j;
    \]
    míg a két polinom szorzatának definíciója:
    \[
        \left( pq \right)\left( t \right)
        =
        \sum_{j=0}^{n+m}c_jt^j
        \text{ ahol }
        c_j
        =
        \sum_{k=0}^j\alpha_k\beta_{j-k}.\qedhere
    \]
\end{definition}
\begin{proposition}
    Legyenek $p,q\in\mathbb{F}[t]$ polinomok az $\mathbb{F}$ test felett.
    Ekkor
    \begin{enumerate}
        \item $\deg \left( pq \right)=\deg p+\deg q$;
        \item $\deg \left( p+q \right)\leq\max\left\{ \deg p,\deg q \right\}$.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Figyeljünk arra, hogy a konstans zérus polinom esetében is működik a tétel,
    és vegyük észre, hogy az szorzat polinomra vonatkozó állítás azért igaz, 
    mert a test nullosztómentes.
\end{proof}
\begin{proposition}
    Egy $\mathbb{F}$ test feletti $\mathbb{F}\left[ t \right]$ formális polinomok 
    a fent bevezetett összeadás és szorzás műveletekkel,
    nullosztómentes,
    kommutatív, egységelemes gyűrűt alkotnak.
\end{proposition}

\section{Polinomok oszthatósága és a maradékos osztás}
\begin{definition}[oszthatóság]\index{polinom osztója}
    Azt mondjuk, hogy a $p\in\mathbb{F}\left[ t \right]$ \emph{osztója} az $f\in\mathbb{F}\left[ t \right]$ nem zérus polinomnak,
    ha létezik $h\in\mathbb{F}\left[ t \right]$, hogy $f\left( t \right)=p\left( t \right)h\left( t \right)$.
    Ilyenkor $f$-et a $p$ egy \emph{többszörösének}\index{polinom többszöröse} is mondjuk.
    Jelölés: $p|f$.
\end{definition}
Világos, hogy egy $p$ polinom összes többszörösei -- tehát azok, amelyeknek $p$ osztója --
ideált alkotnak. 
Ez a $p$ generálta legszűkebb ideál, azaz  a $J(p)=\left\{ fp:f\in\mathbb{F}\left[ t \right] \right\}$ főideál.
Ha $q\in J\left( p \right)$, akkor $J\left( q \right)\subseteq J\left( p \right)$, azaz ha $q$ egy többszöröse $p$-nek,
akkor $q$ minden többszöröse $p$-nek is többszöröse.
Ha $p,q$ polinomok, 
amelyekre $p|q$ és $q|p$ akkor a két polinom csak konstans szorzóban különbözik egymástól.
Ha például a két polinom még normált is, akkor $p|q$ és $q|p$ csak $p=q$ esetben lehetséges.
A polinomgyűrű ideáljaira fokuszálva, azt gondoltuk éppen meg, 
hogy \emph{$J\left( p \right)=J\left( q \right)$ normált $p,q$ polinomokra csak úgy teljesülhet, ha $p=q$},
azaz a polinomok gyűrűjében minden főideálnak csak egy generáló eleme van a normált polinomok körében.

A következő állítás szerint a polinomok közt is működik a maradékos osztás,
ahogyan azt az egész számok közt megszoktuk.
\begin{proposition}[maradékos osztás]
    Legyenek $p,q\in\mathbb{F}\left[ t \right]$ polinomok, $q\neq 0$.
    Ekkor létezik egyetlen $h,r\in\mathbb{F}\left[ t \right]$ polinom, amelyre
    \[
        p
        =
        hq+r;
        \quad
        \deg r < \deg q.\qedhere
    \]
\end{proposition}
\begin{proof}
    Először is azt vegyük észre, hogy $\deg p<\deg q$ esetben $r=p$, 
    $h=0$ szereposztással készen is vagyunk.

    Tegyük fel tehát, hogy $n=\deg p\geq \deg q=m$, és lássuk be az állítást $n$ szerinti indukcióval.
    Ha $n=0$, akkor $p\left( t \right)=\alpha_0$ és $q\left( t \right)=\beta_0\neq 0$.
    Ekkor persze
    \[
        \alpha_0=\frac{\alpha_0}{\beta_0}\beta_0+0,
    \]
    ami azt jelenti, hogy $h\left( t \right)=\frac{\alpha_0}{\beta_0}$ és $r\left( t \right)=0$ szereposztás
    megfelelő.

    Most tegyük fel, 
    hogy igaz az állítás $n+1$-nél kisebb fokú $p$ polinomokra ($n\geq 0$),
    és lássuk be egy pontosan $n+1$-ed fokú polinomra.
    Legyen tehát
    \[
        p\left( t \right)=\alpha_{n+1}t^{n+1}+\dots+\alpha_0
        \quad\text{ és }\quad
        q\left( t \right)=\beta_{m}t^m+\dots+\beta_0,
    \]
    ahol $m\leq n+1$.
    Tekintsük a következő polinomot:
    \[
        \frac{\alpha_{n+1}}{\beta_m}t^{n+1-m}q\left( t \right).
    \]
    Világos, hogy ennek főegyütthatója éppen $\alpha_{n+1}$ és foka éppen $n+1=\deg p$.
    Így a 
    \[
        p_1\left( t \right)
        =
        p\left( t \right)-
        \frac{\alpha_{n+1}}{\beta_m}t^{n+1-m}q\left( t \right).
    \]
    polinomra $\deg p_1<\deg p$.
    Na most, ha $\deg p_1<\deg q$, akkor a bizonyítás első mondatában említett helyzetben vagyunk,
    tehát nyilvánvaló szereposztással az állítás igaz $p_1$-re és $q$-ra.
    Ha viszont $\deg p_1\geq \deg q$ még mindig igaz, akkor az indukciós feltétel szerint található
    $h,r\in\mathbb{F}\left[ t \right]$ polinom, amelyre igaz az állítás.
    Mindkét esetben találtunk tehát $h,r$ polinomokat, amelyre
    \[
        p\left( t \right)-
        \frac{\alpha_{n+1}}{\beta_m}t^{n+1-m}q\left( t \right)
        =
        p_1\left( t \right)
        =
        h\left( t \right)q\left( t \right)+r\left( t \right);
        \quad
        \deg r < \deg q
    \]
    teljesül.
    Ekkor persze
    \[
        p\left( t \right)
        =
        \left( h\left( t \right)
        +
        \frac{\alpha_{n+1}}{\beta_m}t^{n+1-m}
        \right)
        q\left( t \right)
        +
        r\left( t \right);
        \quad
        \deg r < \deg q
    \]
    is fennáll. Ezt kellett belátni az állítás egzisztencia részéhez.

    Az unicitás részhez tegyük fel, hogy valamely $h,h_1,r,r_1$ polinomokra
    \[
        h\left( t \right)q\left( t \right)+r\left( t \right)
        =
        p\left( t \right)
        =
        h_1\left( t \right)q\left( t \right)+r_1\left( t \right)
    \]
    teljesül, ahol $\deg r<\deg q$ és $\deg r_1<\deg q$.
    Persze átrendezve ekkor
    \[
        \left( h\left( t \right)-h_1\left( t \right) \right)q\left( t \right)
        =
        r_1\left( t \right)-r\left( t \right)
    \]
    is fennáll.
    Ekkor a fokszámokra figyelve
    \[
        \deg\left( h-h_1 \right)+\deg q 
        = 
        \deg\left( r_1-r \right)
        \leq
        \max\left\{ \deg r_1,\deg (-r) \right\}
        <
        \deg q.
    \]
    Ez csak akkor lehetséges, ha $\deg\left( h-h_1 \right)=-\infty$,
    ami azt jelenti, hogy $h=h_1$, 
    amiből persze $r_1=r$ már látszik is.
\end{proof}
\begin{proposition}[a polinomgyűrű egy főideál-gyűrű]\label{pr:pgyurufoidealgyuru}
    A polinomok $\mathbb{F}\left[ t \right]$ kommutatív, egységelemes, nullosztómentes gyűrűjében
    minden a $\left\{ 0 \right\}$-tól különböző ideált generál az ideálban lévő egyetlen normált minimális fokszámú polinom.
    Így $\mathbb{F}\left[ t \right]$ egy főideál-gyűrű.
\end{proposition}
\begin{proof}
    Legyen a $J$ egy ideálja $\mathbb{F}\left[ t \right]$-nek, 
    amely nem csak a zérus elemből áll.
    Vegyünk egy minimális fokszámú de nem zérus polinomot $J$-ben,
    tehát olyat, 
    amely maga sem zérus és nála kisebb fokszámú polinom már nincs $J$-ben a $0$ elemen kívül.
    Legyen ez $d$. 
    Most megmutatjuk, hogy minden $p\in J$-re $d|p$.
    A maradékos osztás szerint
    valamely $h,r$ polinomokra
    \[
        p\left( t \right)=
        h\left( t \right)d\left( t \right)+r\left( t \right);
        \text{ ahol }
        \deg r<\deg d.
    \]
    Mivel $p,d\in J$, és $J$ egy ideál, ezért $r\in J$.
    No de $d$ konstrukciója szerint ilyen csak a zérus polinom van,
    ezért valóban $d|p$.
    Ez éppen azt jelenti, hogy
    \(
    J=\left\{ dh:h\in\mathbb{F}\left[ t \right] \right\}
    \)
    azaz $d$ generálja a $J$ ideált.
    Azt viszont már korábban is meggondoltuk, 
    hogy egy főideált csak egyetlen normált polinom generál.

    Megmutattuk tehát, 
    hogy egyetlen normált, minimális fokszámú polinom van $J$-ben,
    és minden $J$-beli polinom ennek többszöröse.
\end{proof}
Érdemes eltenni magunknak, hogy az ideál generáló eleme, tehát az ideálbeli elemek közös osztója
éppen az ideál minimális fokszámú nem zérus polinomja.
Ilyenből a normált polinomok közül csak egy van.

\begin{definition}
    Legyenek most $p_1,\dots,p_k$ polinomok. 
    \begin{enumerate}
        \item A $d$ polinom a \emph{legnagyobb közös osztója}\index{legnagyobb közös osztó} az adott polinomoknak, 
            ha
            \begin{enumerate}
                \item $d|p_j$ minden $j=1,\dots,k$-ra,
                \item ha $d_1|p_j$ minden $j=1,\dots,k$ mellett akkor $d_1|d$ is fennáll,
                \item $d$ normált.
            \end{enumerate}
            A $p_1,\dots,p_k$ polinomokat \emph{relatív prímeknek}\index{relatív prím polinomok} nevezzük, 
            ha közös osztójuk csak a konstans polinomok, 
            azaz a $d\left( t \right)=1$ a legnagyobb közös osztó.
        \item A $d$ polinom a \emph{legkisebb közös többszöröse}\index{legkisebb közös többszörös} az adott polinomoknak, 
            ha
            \begin{enumerate}
                \item $p_j|d$ minden $j=1,\dots,k$-ra,
                \item ha $p_j|d_1$ minden $j=1,\dots,k$ mellett akkor $d|d_1$ is fennáll.
                \item $d$ normált.\qedhere
            \end{enumerate}
    \end{enumerate}
\end{definition}
Persze az első kérdés, hogy van-e a polinomoknak legnagyobb közös osztója vagy legkisebb közös többszöröse, és hány ilyen van?
\begin{proposition}\label{pr:lkkt}
    Bármely $p_1,\dots,p_r\in\mathbb{F}\left[ t \right]$ nem zérus polinomoknak
    létezik egyetlen legkisebb közös többszörösük.
\end{proposition}
\begin{proof}
    Világos, hogy ideálok metszete is ideál, emiatt 
    \(
    \cap_{j=1}^rJ\left( p_j \right)
    \)
    is ideál $\mathbb{F}\left[ t \right]$ gyűrűben.
    De itt minden ideál főideál, létezik tehát $d\in\mathbb{F}\left[ t \right]$ normált polinom, amelyre
    \[
        J\left( d \right)
        =
        \cap_{j=1}^rJ\left( p_j \right)
    \]
    Világos, hogy $d\in J\left( p_j \right)$ minden $j$-re, 
    ergo $d$ többszöröse minden $p_j$-nek.
    Ha $p_j|d_1$ fennáll, minden $j$-re
    az azt jelenti, hogy $d_1\in J\left( p_j \right)$, minden $j$-re, azaz 
    \(
    d_1
    \in
    \cap_{j=1}^rJ\left( p_j \right)
    =
    J\left( d \right),
    \)
    tehát $d|d_1$ valóban fennáll.

    Ha $d$ mellett $g$ is legkisebb közös többszörös, akkor $d|g$ és $g|d$ szerint $g$ és $d$ foka azonos,
    így csak egymás konstans szorosai lehetnek, de mivel mindketten normáltak, ezért e konstans csak 1 lehet.
\end{proof}
\begin{proposition}
    Bármely $p_1,\dots,p_r\in\mathbb{F}\left[ t \right]$ nem zérus polinomoknak
    létezik egyetlen legnagyobb közös osztójuk.
    A $d$ legnagyobb közös osztó kifejezhető
    \[
        d\left( t \right)=f_1\left( t \right)p_1\left( t \right)+
        \dots+
        f_r\left( t \right)p_r\left( t \right)
    \]
    alakban valamely $f_1,\dots,f_r\in\mathbb{F}\left[ t \right]$ polinomok segítségével.
\end{proposition}
\begin{proof}
    Láttuk, hogy létezik egyetlen normált $d$ polinom, amelyre $J\left( d \right)=J\left( p_1,\dots,p_r \right)$.
    Világos, hogy $d|p_j$ minden $j=1,\dots,r$ és $d\in J\left( p_1,\dots,p_r \right)$,
    azaz
    \[
        d=f_1p_1+\dots+f_rp_r
    \]
    valamely $f_1,\dots,f_r$ polinomokra.
    Ha valamely $d_1$ polinomra $d_1|p_j$ minden $j=1,\dots,r$ mellett,
    akkor a fenti azonosság szerint $d_1|d$ is fennáll.

    Az egyértelműség mint a legkisebb közös többszörösnél.
\end{proof}
A következő állítás azonosságát Bezout--azonosságnak mondjuk.
\begin{proposition}[Bezout--azonosság]\index{Bezout--azonosság}
    Legyenek a $p_1,\dots,p_r\in\mathbb{F}\left[ t \right]$ tetszőleges $\mathbb{F}$ test feletti polinomok.
    Ezek pontosan akkor relatív prímek, 
    ha léteznek $f_1,\dots,f_r\in\mathbb{F}\left[ t \right]$ polinomok, hogy
    \[
        f_1\left( t \right)p_1\left( t \right)+f_2\left( t \right)p_2\left( t \right)+\dots+f_r\left( t \right)p_r\left( t \right)=1\qedhere
    \]
\end{proposition}

A szakaszt a maradékos osztás módszerének másik fontos következményeivel zárjuk.
Azt gondoljuk meg, 
hogy a gyöktényező a polinomból mindig kiemelhető, 
emiatt egy akármilyen test feletti $n$-ed fokú polinom gyökeinek száma $n$-nél nagyobb nem lehet.
\begin{proposition}
    Legyen $p\in\mathbb{F}\left[ t \right]$ egy nem zérus polinom,
    és $t_0$ egy gyöke, azaz $p\left( t_0 \right)=0$.
    Ekkor létezik $h\in\mathbb{F}\left[ t \right]$ nem zérus polinom,
    amelyre
    \[
        p\left( t \right)=\left( t-t_0 \right)h\left( t \right).\qedhere
    \]
\end{proposition}
\begin{proof}
    Maradékos osztással $p$-re és az elsőfokú $t-t_0$ polinomra
    \[
        p\left( t \right)=h\left( t \right)\left( t-t_0 \right)+r\left( t \right),
        \text{ ahol }
        \deg r<1.
    \]
    No de, $t_0$ egy gyök, tehát $0=p\left( t_0 \right)=r\left( t_0 \right)$. 
    Ez azt jelenti, hogy $\deg r=-\infty$, ami éppen az állítás.
\end{proof}
\begin{definition}[gyök multiplicitása]\index{gyökök multiplicitása}
    Legyen $t_0$ gyöke a $p\left( t \right)$ polinomnak.
    Azt mondjuk, hogy a $k$ pozitív egész e $t_0$ gyök \emph{multiplicitása},
    ha van olyan $h\left( t \right)$ polinom, hogy 
    \begin{math}
        p\left( t \right)=\left( t-t_0 \right)^kh\left( t \right),
    \end{math}
    de $h\left( t_0 \right)\neq 0$.
    Néha azt is mondjuk, hogy $t_0$ egy $k$-szoros gyöke $p$-nek.
\end{definition}
Teljesen világos, hogy a gyöktényező kiemelhetősége miatt minden gyök legalább egyszeres multiplicitású.
A következő állítás szerint 
a gyökök száma még a multiplicitásukkal együtt számolva sem lehet több mint a polinom foka.
\begin{proposition}
    Legyen(ek) a $p\in\mathbb{F}\left[ t \right]$ nem zérus polinom különböző gyökei $t_1,\dots,t_k$,
    és ezen gyökök multiplicitásai rendre $m_1,\dots,m_k$.
    Ekkor $m_1+\dots+m_k\leq\deg p$.
\end{proposition}
\begin{proof}
    A test nullosztó mentessége és a gyöktényező kiemelhetősége miatt 
    \[
        p\left( t \right)=
        \left( t-t_1 \right)^{m_1}\cdot\left( t-t_2 \right)^{m_2}\dots\left( t-t_k \right)^{m_k}\cdot 
        h\left( t \right),
    \]
    ahol $h$ olyan nem zérus polinom, amelynek már nincsen gyöke.
    A fokszámok összehasonlításából kapjuk, hogy
    $m_1+\dots+m_k\leq m_1+\dots+m_k+\deg h=\deg p$.
\end{proof}
A fenti gondolat szerint, ha egy legfeljebb $n$-ed fokú polinomnak $n+1$ különböző gyöke van, 
akkor csak úgy lehetséges, ha a polinom minden együtthatója nulla.
Ezt úgy is szoktuk fogalmazni, 
hogy egy legfeljebb $n$-ed fokú polinomot $n+1$ helyettesítési értéke már egyértelműen meghatározza:
\begin{proposition}\label{pr:polinomfv}
    Tegyük fel, hogy a $p,q\in\mathbb{F}\left[ t \right]$ polinomok legfeljebb $n$-ed fokúak, ahol $n$ egy nemnegatív egész,
    és tegyük fel, 
    hogy létezik $n+1$ különböző $t_0,t_1,\dots,t_n$ pont a testben,
    amelyekre $p\left( t_j \right)=q\left( t_j \right)$ minden $j=0,\dots,n$.
    Ekkor $p\left( t \right)=q\left( t \right)$, 
    azaz $p$ és $q$ együtthatói azonosak.
\end{proposition}
\begin{proof}
    Legyen $h=p-q$. 
    Világos, hogy $\deg h\leq n$ és $h$-nak van $n+1$ különböző gyöke.
    Az előző állítás szerint ez csak a $h=0$ polinomra igaz, ami azt jelenti, 
    hogy $p$ és $q$ együtthatói azonosak.
\end{proof}
A maradékos osztás tételének szép következménye tehát,
hogy ha $\mathbb{F}$ egy nem véges test, és a $p,q\in\mathbb{F}\left[ t \right]$ polinomok,
akkor $p$-nek és $q$-nak pontosan akkor azonosak az együtthatói, ha 
\(
p\left( t \right)=q\left( t \right)
\)
fennáll minden $t\in\mathbb{F}$ mellett.

Itt fontos, hogy $\mathbb{F}$ nem véges test,
hiszen például ha $\mathbb{F}=\left\{ 0,1 \right\}$ a két elemű test,
akkor a $p\left( t \right)=t^2+t$ polinomra minden $t\in\left\{ 0,1 \right\}$ mellett $p\left( t \right)=0$,
de a polinom együtthatói rendre a $\left\{ 0,1,1 \right\}$ számok a testből, 
tehát ez nem a összeadásra nézve neutrális eleme az $\mathbb{F}\left[ t \right]$ polinomgyűrűnek.

Konklúzióképpen: megnyugodhatunk, hogy az iménti szörnyűség nem véges testek esetén nem fordulhat elő,
tehát mondjuk a valós vagy a komplex számtest felett mindegy, 
hogy a polinomokat függvényeknek, vagy formális algebrai kifejezéseknek gondoljuk. 
A lényeg hogy egy $n$-ed fokú polinomot az $n+1$ együtthatója, definíció szerint, 
de az $n+1$ különböző helyen felvett helyettesítési értéke is egyértelműen meghatározza.

\section{Az Euklideszi--algoritmus}
Algoritmust keresünk polinomok legnagyobb közös osztójának és legkisebb közös többszörösének meghatározására.
Ha egy pillanatra $\left( p_1,\dots,p_n \right)$ jelöli az adott $p_1,\dots,p_n$ polinomok legnagyobb közös osztóját,
akkor nem nehéz meggondolni, 
hogy 
\[
    \left( \left( p_1,\dots,p_{n-1} \right),p_n \right)=\left( p_1,\dots,p_{n}\right).
\]
Ezt $n=3,4,\dots$ számokra alkalmazva azt kapjuk, 
hogy ha módszerünk van két polinom legnagyobb közös osztójának meghatározására, 
akkor evvel már akárhány -- persze véges sok -- polinom legnagyobb közös osztója is meghatározható.
Analóg módon ugyanez igaz a legkisebb közös többszörösre is.
Azt gondoltuk meg tehát, hogy ha meg tudnánk határozni két polinom legnagyobb közös többszörösét és legkisebb közös osztóját,
akkor ugyan ezt mát meg tudnánk tenni véges sok polinom esetén is.

Az Euklideszi-algoritmus két polinom legnagyobb közös osztójának meghatározására szolgál.
Az eddigi ismereteink szerint a $p,q$ legnagyobb közös osztója a $J\left( p,q \right)$ ideál legalacsonyabb 
fokú, normált tagja. 
Véges sok lépésben végrehajtható, ezért a fentinél sokkal egyszerűbben működő algoritmust ad.
Emlékezzünk arra, hogy ha $p,q\in\mathbb{F}\left[ t \right]$ valamely polinomok, akkor
\(
J\left( p,q \right)=\left\{ fp+gq:f,g\in\mathbb{F}\left[ t \right] \right\}
\)
jelöli a $p$ és a $q$ polinomokat tartalmazó legszűkebb ideált.
Világos, hogy ha $r_1,r_2\in J\left( p,q \right)$, akkor 
$J\left( r_1,r_2 \right)\subseteq J\left( p,q \right)$.
\begin{proposition}[Euklidesz]\index{Euklideszi algoritmus}
    Legyen $p,q\in\mathbb{F}\left[ t \right]$ nem zérus polinomok.
    Definiálja $p_{-1}=p,p_{0}=q$. 
    Folytatva, ha valamely $i\geq 0$ számra $p_{i-1}$ és $p_i$ már definiált és $p_i\neq 0$, 
    akkor a maradékos osztás szerint létezik egyetlen $h_{i+1},p_{i+1}\in\mathbb{F}\left[ t \right]$ polinom,
    amelyre 
    \[
        p_{i-1}=h_{i+1}p_i+p_{i+1};
        \text{ ahol }
        \deg p_{i+1}<\deg p_i.\tag{\dag}
    \]
    Mivel minden egyes lépésben csökken a fokszám,
    ezért van olyan $s>0$, hogy $p_s\neq 0$, de $p_{s+1}=0$.
    \\
    Erre a $p_s$ polinomra $J\left( p_s \right)=J\left( p,q \right)$, ezért
    $p_s$ normáltja a $p\text{ és a }q$ polinomok legnagyobb közös osztója.
\end{proposition}
\begin{proof}
    A fenti algoritmussal olyan
    $p_{-1}, p_0,p_1,\dots,p_s,p_{s+1}$ polinomokat kapunk, 
    amelyekre minden $i=0,\dots,s$ mellett a (\dag) azonosság fennáll, 
    és $\deg p_{s+1}=-\infty$, 
    azaz $p_{s+1}=0$.

    A (\dag) azonosság szerint, 
    minden $i=0,1,\dots,s$ mellett
    \begin{math}
        p_{i-1}\in J\left( p_i,p_{i+1} \right)
    \end{math},
    amiből a 
    \begin{math}
        J\left( p_{i-1},p_i \right)\subseteq J\left( p_i,p_{i+1} \right) 
    \end{math}
    tartalmazás adódik.
    Másrészt a (\dag) azonosságot értelmezhetjük úgyis, hogy
    \(
    p_{i+1}\in J\left( p_{i-1},p_i \right)
    \),
    amiből persze 
    \begin{math}
        J\left( p_i,p_{i+1} \right)\subseteq J\left( p_{i-1},p_i \right)
    \end{math}
    következik.
    Így minden $i=0,\dots,s$-re végül is
    \(
    J\left( p_{i-1},p_i \right)
    =
    J\left( p_i,p_{i+1} \right).
    \)
    Alkalmazva ezt minden $i=0,\dots,s$ mellett
    \[
        J\left( p,q \right)
        =
        J\left( p_{-1},p_0 \right)
        =
        J\left( p_0,p_1 \right)
        =
        J\left( p_1,p_2 \right)
        =\dots=
        J\left( p_s,p_{s+1} \right)
        =J\left( p_s \right).\qedhere
    \]
\end{proof}
Most a legkisebb közös többszörös algoritmikus meghatározására törekszünk.
\begin{proposition}\label{pr:rprim}
    Legyenek a $p,q\in\mathbb{F}\left[ t \right]$ polinomok relatív prímek,
    és tegyük fel, hogy 
    \begin{math}
        p|qr.
    \end{math}
    Ekkor $p|r$.
\end{proposition}
\begin{proof}
    Mivel a $p$ és a $q$ polinomok relatív prímek,
    ezért a Bezout--azonosság szerint van $f$ és $g$ polinom, amelyekre
    \(
    fpr+gqr=r.
    \)
    A feltétel szerint $qr$ a $p$ többszöröse, 
    így az iménti azonosság baloldala a $p$ többszöröse,
    ami éppen azt jelenti, hogy $p|r$.
\end{proof}
\begin{proposition}
    Tekintsük a $p,q\in\mathbb{F}\left[ t \right]$ normált polinomokat.
    Jelölje $d$ a legnagyobb közös osztót, 
    és
    $m$ a legkisebb közös többszöröst.
    Ekkor
    \begin{displaymath}
        d\left( t \right)m\left( t \right)=p\left( t \right)q\left( t \right).\qedhere
    \end{displaymath}
\end{proposition}
\begin{proof}
    Legyen $p=dr_1$ és $q=dr_2$.
    Először megmutatjuk, hogy $r_1$ és $r_2$ relatív prímek.
    Ha $s$ polinomom közös osztójuk, akkor $ds$ is közös osztója $p$-nek és $q$-nak,
    amiből $ds|s$ következik.
    A fokszámokat összehasonlítva ez csak akkor lehetséges, ha $s$ konstans polinomom, 
    azaz $s|1$ valóban fennáll.

    Most megmutatjuk, hogy az $m$ legkisebb közös többszörösre
    \[
        m=dr_1r_2.\tag{\dag}
    \]
    Világos, hogy $dr_1r_2$ egy közös többszöröse a $p,q$ polinomoknak.
    Tegyük fel, hogy $s$ egy másik közös többszörös, azaz 
    $s=ps_1$ és $s=qs_2$.
    Ekkor 
    \begin{math}
        dr_1s_1=ps_1=s=qs_2=dr_2s_2,
    \end{math}
    amiből a nullosztómentesség szerint 
    \[
        r_1s_1=r_2s_2.
    \]
    Na most,
    a fent kiemelt azonosság szerint szerint $r_1|r_2s_2$, ahol $r_1$ és $r_2$ relatív prímek.
    Ebből azonnal kapjuk, hogy $r_1|s_2$.
    No de $s=qs_2=dr_1s_2$, amiből már látszik, 
    hogy $s$ egy többszöröse a $dr_1r_2$ polinomnak.
    Ez éppen (\dag) azonosságot jelenti.
    Innen
    $d\cdot m=d(dr_1r_2)=(dr_1)(dr_2)=p\cdot q$ már nyilvánvaló.
\end{proof}
A fenti állítás csak két polinomra igaz, többre nem,
de nekünk csak két polinomra kell.
Úgy interpretáljuk, 
hogy ha a szorzatot osztom maradékosan a legnagyobb közös osztóval, akkor a maradék mindig 
zérus, 
és a hányados éppen a legkisebb közös többszörös.
Azt gondoltuk meg tehát, 
hogy az Euklideszi-algoritmus módszert ad két polinom legkisebb közös többszörösének algoritmikus meghatározására is.

Visszatérve a szakasz elején felvetett gondolatra, 
ilyen módon véges sok lépésben végrehajtható algoritmust kapunk véges sok polinom legkisebb közös többszörösének meghatározására.
Például öt polinom legkisebb közös többszöröséhez, 
egy Euklideszi-algoritmussal meghatározzuk az első kettő polinom legnagyobb közös osztóját, 
majd egy újabb maradékos osztással az első kettő legkisebb közös többszörösét.
Ugyanezt teszem az így kapott és a harmadik polinommal, 
az eredmény az első három polinom legkisebb közös többszöröse.
Az így kapott polinommal és a negyedik polinommal egy újabb Euklideszi-algoritmus és egy újabb maradékos osztás után kapjuk az első négy polinom legkisebb közös többszörösét, 
majd ennek eredményével és az ötödik polinommal mint két polinomnak a legkisebb közös többszörösével kapjuk az eredeti öt polinom legkisebb közös többszörösét.
\section{Polinom faktorizáció}
Kicsi korunk óta sulykolják belénk, hogy minden egész szám előáll, méghozzá lényegében csak egyféleképpen
prímek szorzataként.
Ha ismerjük két szám prímtényezős előállítását, akkor nagyon könnyű megmondani a két szám legkisebb közös többszörösét,
vagy a legnagyobb közös osztóját.
Evvel a probléma csak annyi, 
hogy nagyon nehéz megmondani két, esetleg jó nagy, szám prímtényezős előállítását,
így még az egész számok gyűrűjében is az Euklideszi-algoritmus a megfelelő, véges sok lépésben,
végrehajtható módszer a legnagyobb közös osztó és a legkisebb közös többszörös konkrét felírására.

Ebben a szakaszban azt mutatjuk meg, hogy prímtényezős előállításról szóló tétel a polinomok gyűrűjében is igaz marad.
Természetesen konkrét algoritmust nem adunk, hiszen ilyen még számokra sem igen van.%
\footnote{Ha ilyen lenne senki nem tudna interneten két számla közt pénzt mozgatni. Lásd például: \url{https://en.wikipedia.org/wiki/RSA_(cryptosystem)}}
\begin{definition}[reducibilis polinom]\index{reducibilis polinom}\index{irreducibilis polinom}
    Egy polinomot \emph{reducibilisnek} mondunk, 
    ha előáll mint két legalább elsőfokú polinom szorzata.
    Egy nem reducibilis polinomot \emph{irreducibilisnek} nevezünk.
\end{definition}
Világos, hogy minden legfeljebb elsőfokú polinom tetszőleges test felett irreducibilis.
A magasabb fokú polinomok esetében a probléma nagyban függ a testtől is, 
ahonnan a polinom együtthatói származnak.
A következő állítás viszont minden test mellett igaz.
\begin{proposition}[polinom faktorizáció]\label{pr:polfact}
    Tetszőleges test feletti polinomgyűrűben, 
    minden (normált) polinom előáll mint (normált) irreducibilis polinomok szorzata.
\end{proposition}
\begin{proof}
    Az előállítandó polinom foka szerinti teljes indukció.
    Elsőfokú polinom maga irreducibilis.

    Most tegyük fel, hogy az állítás igaz $n$-nél alacsonyabb fokú polinomokra
    és lássuk be $\deg p=n$ mellett, ahol $n>1$.
    Ha $p$ irreducibilis, akkor megint készen vagyunk.
    Ha $p=f g$ valamely $\deg f\geq 1$ és $\deg g\geq 1$ mellett,
    akkor $\deg f<n$ és $\deg g<n$.
    Az indukciós feltevés szerint $f$ és $g$, emiatt $p=fg$ is előáll irreducibilis polinomok szorzataként.
\end{proof}

Érdemes látni, hogy ha $p$ irreducibilis, 
és $f$ egy tetszőleges polinom, akkor vagy $p|f$ vagy $p$ és $f$ relatív prímek.
Ugyanis ha $g$ egy közös osztójuk, akkor $p$ iredducibilitása miatt,
$\deg g=0$, vagy $\deg g=\deg p$. 
Ez utóbbi esetben $g$ a $p$ konstans szorosa, ergo $p|f$,
az előbbi eset pedig éppen azt jelenti, hogy $p$ és $f$ relatív prímek.
\begin{proposition}[prím tulajdonság]
    Legyen $p\in\mathbb{F}\left[ t \right]$ irreducibilis polinom, amelyre
    $p|(f_1\cdots f_n)$, valamely $f_j\in\mathbb{F}\left[ t \right]$ polinomokra, ahol $j=1\dots,n\geq 1$.
    Ekkor létezik $1\leq j\leq n$, amelyre $p|f_j$.\qedhere
\end{proposition}
\begin{proof}
    A polinomok $n$ száma szerinti indukció.
    Az $n=1$ eset semmitmondó módon teljesül.
    Tegyük fel, hogy igaz az állítás $n$-nél kevesebb polinomra,
    és lássuk be $n$-re. Itt $n\geq 2$.
    Induljunk ki tehát abból, hogy 
    \[
        p|\left( f_1\cdots f_{n-1} \right)\cdot f_n
    \]
    Ha $p$ osztója lenne az $f_1\cdots f_{n-1}$ szorzatnak, 
    akkor az indukciós feltevés szerint készen is lennénk.
    Ha $p$ nem osztója a szorzatnak, 
    akkor $p$ irreducibilis volta miatt relatív prímek.
    Ekkor \aref{pr:rprim}. állítás szerint $p|f_n$.
\end{proof}
Az állítás fordítva is igaz, de azt a gyakorlatokra hagyjuk.
Az irreducibilis polinomok prím tulajdonsága segítségével a polinomok faktorizáció egyértelműségét is igazolhatjuk.
\begin{proposition}
    Legyen $p \in\mathbb{F}\left[ t \right]$ egy legalább elsőfokú normált polinom.
    Ekkor ezek sorrendjétől eltekintve egyértelműen léteznek legalább elsőfokú, normált, irreduciblis $q_1,\dots,q_s\in\mathbb{F}\left[ t \right]$ polinomok,
    hogy $p=q_1\cdots q_s$.
\end{proposition}
A ,,sorrendtől eltekintés'' alatt azt értjük, hogy ha $p$ előáll 
\[
    p_1\cdots p_s=p=q_1\cdots q_r
\]
legalább elsőfokú, normált, irreducibilis polinomok szorzataként,
akkor $s=r$ és a $p_1,\dots,p_s$ polinomok alkalmas átindexelése után $p_j=q_j$,
minden $j=1,\dots,s$ mellett.
\begin{proof}
    \Aref{pr:polfact}. állításban már tisztáztuk az egzisztenciális részt, 
    így már csak az unicitás maradt,
    amit a felbontandó polinom fokszáma szerinti indukcióval végzünk most is.
    Ha a polinom elsőfokú, akkor az unicitás is nyilvánvaló.

    Tegyük fel, hogy igaz az egyértelműség $n$-nél alacsonyabb fokszámú polinomokra,
    és tegyük fel, hogy $\deg p=n>1$.
    Nézzünk két lehetséges előállítást
    \[
        p_1\cdots p_s=p=q_1\cdots q_r,
    \]
    ahol $p_1,\dots,p_s,q_1,\dots,q_r$ legalább elsőfokú, normált, irreducibilis polinomok.
    A $q_1$ polinom osztója a jobboldalnak, ezért a baloldalnak is.
    A prím tulajdonság miatt, \ref{pr:rprim}. állítás, $q_1$ osztója az egyik baloldali polinomnak.
    Alkalmas átindexelés után feltehető, hogy $q_1|p_1$.
    No de, $p_1$ is irreducibilis, és $\deg q_1\geq 1$ miatt csak $\deg q_1=\deg p_1$ lehetséges,
    tehát a normáltság szerint $q_1=p_1$.
    A polinomgyűrű nullosztó mentessége szerint az első polinomokkal egyszerűsíthetünk, ergo
    \[
        p_2\cdots p_s=p=q_2\cdots q_r,
    \]
    is fennáll. 
    A fenti polinom már $n$-nél alacsonyabb fokú, 
    így az indukciós feltevés szerint $s-1=r-1$,
    és alkalmas átindexelés után minden $j=2,\dots,s$ esetén is teljesül a $p_j=q_j$ egyenlőség.
\end{proof}
Az egész szakaszt összefoglalhatjuk így is:
\begin{proposition}
    Minden legalább elsőfokú normált polinomhoz léteznek,
    méghozzá sorrendjüktől eltekintve egyértelműen léteznek 
    $q_1,\dots,q_s$ normált, irreducibilis polinomok,
    és $n_1,\dots,n_s$ pozitív egészek, 
    amelyekre
    \[
        p\left( t \right)
        =
        q_1^{n_1}\left( t \right)\cdots q_s^{n_s}\left( t \right).\qedhere
    \]
\end{proposition}

\section{Mátrixok}
\begin{definition}[mátrix]
    Egy tetszőleges test feletti mátrixnak nevezzünk,
    a test elemeiből képzett táblázatot.
    Ha $m,n\in\mathbb{N}$ előre rögzített pozitív egészek és 
    az $A$ táblázatnak $m$ sora és $n$ oszlopa van, 
    akkor azt mondjuk, hogy $A$ egy $m\times n$ méretű mátrix.
    Az $\mathbb{F}$ test feletti $m\times n$-es mátrixok halmazát $\mathbb{F}^{m\times n}$
    módon jelöljük.

    Ha $A\in\mathbb{F}^{m\times n}$ egy mátrix, 
    akkor $A_i$ jelöli az $i$-edik sort, ami persze egy $1\times n$-es mátrix;
    $A^j$ jelöli a $j$-edik oszlopot, ami persze egy $m\times 1$-s mátrix;
    $A_i^j$ jelöli az $i$-edik sor $j$-edik elemét.
    Sokszor használjuk az $A_{i,j}=A_i^j$ jelölést is.

    Időnként, azt hangsúlyozandó hogy mátrixokról van szó a mátrixot jelölő betűt kapcsos zárójelbe teszem.
    Pl. $\left[ A \right]\in\mathbb{F}^{m\times n}$.

    A mátrixot a mérete és az elemei határozzák meg.
    Emiatt két mátrix akkor azonos, ha azonos méretűek, és a megfelelő elemeik is azonosak.

    \emph{Diádnak}\index{diad@diád} nevezzük egy oszlop és egy sor szorzatát.
    Ha az oszlopnak és a sornak rendre azonosak az elemei, akkor \emph{szimmetrikus diádról} beszélünk.\index{szimmetrikus diád}
\end{definition}
Az azonos típusú mátrixok közt műveleteket definiálunk:
\begin{definition}[mátrixok összege]\index{matrix@mátrixok összege}
    Rögzített $m,n\in\mathbb{N}$ mellett, ha $A,B\in\mathbb{F}^{m\times n}$,
    akkor ezek összege az a $C\in\mathbb{F}^{m\times n}$ mátrix, amelyre
    \[
        C_{i,j}=A_{i,j}+B_{i,j}
    \]
    minden $i=1,\dots,m$ és $j=1,\dots, n$.
    Jelölés: $C=A+B$.
\end{definition}
\begin{proposition}\label{pr:matrixokVS1}
    Az $m\times n$ méretű mátrixok az fent definiált összeadás művelettel
    Abel-csoportot alkotnak.
    A $[0]$-val jelölt neutrális elem az az $m\times n$-s mátrix, 
    amelynek minden eleme a test zérus eleme:
    \[
        [0]_{i,j}=0;
    \]
    az $[A]$ mátrix additív inverze az az $[-A]$-val jelölt $m\times n$ méretű mátrix,
    amelyre
    \[
        [-A]_{i,j}=-([A]_{i,j}).\qedhere
    \]
\end{proposition}

Most definiáljuk egy számnak és egy mátrixnak a szorzatát.
\begin{definition}[szám és mátrix szorzata]\index{szám és mátrix szorzata}
    Ha $\alpha\in\mathbb{F}$ egy szám és $A\in\mathbb{F}^{m\times n}$ egy mátrix, akkor ezek szorzata
    az $\alpha A\in\mathbb{F}^{m\times n}$ módon jelölt mátrix, melynek elemeire
    \[
        [\alpha A]_{i,j}=\alpha[A]_{i,j}.\qedhere
    \]
\end{definition}
Könnyen ellenőrizhetőek a következő számolási szabályok:
\begin{proposition}\label{pr:matrixokVS2}
    Legyenek $A,B\in\mathbb{F}^{m\times n}$ mátrixok, és $\alpha,\beta\in\mathbb{F}$ tetszőleges számok.
    Ekkor
    \begin{enumerate}
        \item $\alpha\left( A+B \right)=\alpha A+\alpha B$;
        \item $\left( \alpha+\beta \right)A=\alpha A+\beta A$;
        \item $\left( \alpha\beta \right)A=\alpha\left( \beta A \right)$;
        \item $1 A=A$.\qedhere
    \end{enumerate}
\end{proposition}
Az utolsó két állítást, \ref{pr:matrixokVS1}. és \ref{pr:matrixokVS2}.,
együtt később úgy fogjuk fogalmazni, 
hogy adott test feletti tetszőleges méretű mátrixok \emph{vektorteret}\index{vektortér} alkotnak.

Most mátrixok szorzatát definiáljuk:
\begin{definition}[mátrixok szorzata]\index{matrix@mátrixok szorzata}
    Legyen $A\in\mathbb{F}^{m\times k}$ és $B\in\mathbb{F}^{k\times n}$ mátrix.
    Fontos, hogy $A$ oszlopainak száma azonos $B$ sorainak számával.
    Ezek $C=AB$ szorzata egy $C\in\mathbb{F}^{m\times n}$ mátrix,
    melynek elemeit az alábbi egyenlőség definiálja
    \[
        [C]_{i,j}=\sum_{s=1}^k[A]_{i,s}[B]_{s,j}.
    \]
    Itt persze $i=1,\dots,m$ és $j=1,\dots,n.$
\end{definition}
Az összeadás és szorzás műveleteket a disztributivitás kapcsolja össze:
\begin{proposition}
    Legyen $A\in\mathbb{F}^{m\times k}$, valamint a $B,C\in\mathbb{F}^{k\times n}$ mátrixok.
    Ekkor
    \begin{displaymath}
        A\left( B+C \right)=AB+AC.
    \end{displaymath}
    Hasonlóan,
    ha $A,B\in\mathbb{F}^{m\times k}$, valamint a $C\in\mathbb{F}^{k\times n}$ mátrixok,
    akkor
    \begin{displaymath}
        \left( A+B \right)C=AC+BC.\qedhere
    \end{displaymath}
\end{proposition}
\begin{proposition}
    Legyen $A\in\mathbb{F}^{m\times k}$ és $B\in\mathbb{F}^{k\times n}$ mátrix.
    Jelölje $C=AB$ ezek szorzatát ebben a sorrendben.
    Ekkor
    \begin{enumerate}
        \item A szorzat mátrix $i$-edik sorának $j$-edik eleme Az $A$ mátrix $i$-edik sorának és a $B$ mátrix $j$-edik
            oszlopának szorzata. Magyarul: minden $1\leq i\leq m$ és $1\leq j \leq n$ mellett
            \[
                [C]_{i,j}=[A]_i\cdot [B]^j.
            \]
        \item
            A szorzat mátrix minden oszlopa az $A$ mátrix oszlopainak a $B$ mátrix megfelelő oszlopából vett elemekkel képzett
            lineáris kombinációja.
            Magyarul: minden $1\leq j\leq n$ mellett
            \[
                [C]^j=\sum_{s=1}^k[B]_s^j[A]^s.
            \]
        \item
            A szorzat mátrix minden sora a $B$ mátrix sorainak az $A$ mátrix megfelelő sorából vett elemekkel képzett
            lineáris kombinációja.
            Magyarul: minden $1\leq i\leq n$ mellett
            \[
                [C]_i=\sum_{s=1}^k[A]_i^s[B]_s.
            \]
        \item
            A szorzat mátrix az $A$ oszlopaiból, és a $B$ soraiból alkotott diádok összege.\index{diad@diád}
            Magyarul:
            \[
                [C]=\sum_{s=1}^k[A]^s[B]_s.\qedhere
            \]
    \end{enumerate}
\end{proposition}
\begin{proof}[1. bizonyítása]
    $[A]_i\cdot [B]^j=\sum_{s=1}^k[A]_{i,s}[B]_{s,j}=[C]_{i,j}.$
\end{proof}
\begin{proof}[2. bizonyítása]
    \(
    \left[ \sum_{s=1}^k[B]_s^j[A]^s \right]_i=
    \sum_{s=1}^k[B]_s^j[A]_i^s =
    \sum_{s=1}^k[A]_i^s[B]_s^j =
    [C]_{i,j}=
    [C]_i^j
    \)
    minden $i$-re.
\end{proof}
\begin{proof}[3. bizonyítása]
    \(
    \left[ \sum_{s=1}^k[A]_i^s[B]_s \right]^j=
    \sum_{s=1}^k[A]_i^s[B]_s^j=
    [C]_{i,j}=
    [C]_i^j
    \)
    minden $j$-re.
\end{proof}
\begin{proof}[4. bizonyítása]
    \(
    \left[ \sum_{s=1}^k[A]^s[B]_s \right]_{i,j}=
    \sum_{s=1}^k\left[ [A]^s[B]_s \right]_{i,j}=
    \sum_{s=1}^k [A]_i^s[B]_s^j=
    C_{i,j}
    \)
    minden $i$-re $j$-re.
\end{proof}
\begin{definition}[Kronecker-delta, identitás mátrix]\index{Kronceker-delta}\index{identitás mátrix}
    \emph{Kroencker-deltának} nevezzük az alábbi egyszerű szimbólumot:
    \[
        \delta_{i,j}=
        \begin{cases}
            1,\text{ ha }i=j;\\
            0,\text{ egyébként.}
        \end{cases}
    \]
    Adott $n\geq 1$ természetes számra az $n\times n$ méretű identitás mátrix azaz $I\in\mathbb{F}^{n\times n}$ mátrix,
    amelyre
    \[
        [I]_{i,j}=\delta_{i,j}.\qedhere
    \]
\end{definition}
Nyilvánvaló, hogy ha $A\in\mathbb{F}^{m\times n}$ mátrix és $I\in\mathbb{F}^{n\times n}$ méretű identitás mátrix,
akkor $A\cdot I=A$. Hasonlóan, ha most $I$ az $m\times m$ identitás mátrixot jelöli, akkor pedig $I\cdot A=A$ azonosság teljesül.
Persze, ha $A\in\mathbb{F}^{n\times n}$ négyzetes mátrix és $I\in\mathbb{F}^{n\times n}$ az ugyanilyen méretű identitás mátrix,
akkor
\[
    IA=AI=A
\]
is fennáll.

A mátrixok szorzásának legérdekesebb tulajdonsága a szorzás asszociativitása.
\begin{proposition}
    Legyen az $A,B,C$ mátrixok úgy megadva, hogy $AB$ is értelmes és $BC$ is értelmes legyen, 
    azaz $A\in\mathbb{F}^{m\times k},B\in\mathbb{F}^{k\times l}, C\in\mathbb{F}^{l\times n}$.
    Ekkor
    \[
        A\left( BC \right)=\left( AB \right)C.\qedhere
    \]
\end{proposition}
\begin{proof}
    Először is azt vegyük észre, 
    hogy ha az $A$ és a $C$ mátrixok egyike egy szám, 
    és a másik két mátrix összeszorozható, 
    akkor a mátrix szorzás definíciója szerint az állítás nyilvánvaló.

    Másodszor azt vegyük észre, hogy mindkét oldalon azonos méretű 
    konkrétan $m\times n$ méretű mátrixok szerepelnek.

    Azt kell tehát még meggondolnunk,
    hogy az $i$-edik sor $j$-edik eleme mindkét oldalon ugyanaz.
    A jobboldalon ez
    \begin{multline*}
        [AB]_i\cdot [C]^j
        =
        \left( \sum_{s=1}^k[A]_i^s[B]_s \right)[C]^j
        =
        \sum_{s=1}^k\left([A]_i^s[B]_s\right)[C]^j
        =
        \sum_{s=1}^k[A]_i^s\left([B]_s[C]^j\right)
        \\
        =
        \sum_{s=1}^k[A]_i^s\left(\sum_{r=1}^l[B]_s^r[C]_r^j\right)
        =
        \sum_{s=1}^k\sum_{r=1}^l[A]_i^s([B]_s^r[C]_r^j).
    \end{multline*}
    A baloldalon
    az $i$-edik sor $j$-edik eleme hasonló számolgatással:
    \begin{multline*}
        [A]_i[BC]^j=
        [A]_i\left( \sum_{r=1}^l[B]^r[C]_r^j \right)
        =
        \sum_{r=1}^l[A]_i\left([B]^r[C]_r^j\right)
        =
        \sum_{r=1}^l\left([A]_i[B]^r\right)[C]_r^j
        \\
        \sum_{r=1}^l\left(\sum_{s=1}^k[A]_i^s[B]_s^r\right)[C]_r^j
        =
        \sum_{r=1}^l\sum_{s=1}^k\left([A]_i^s[B]_s^r\right)[C]_r^j.
    \end{multline*}
    A testben fennálló asszociativitás és kommutativitás miatt a bal- és a jobboldali kifejezés azonos.
\end{proof}
\begin{proposition}
    Legyen $n\in\mathbb{N}$ természetes szám, és tekintsük az $n\times n$ méretű mátrixok
    halmazát, ellátva ezt a halmazt a mátrix összeadással és a mátrixszorzással.
    Az $\left( \mathbb{F}^{n\times n},+,\cdot \right)$ algebrai struktúra egy egységelemes gyűrű.
\end{proposition}
Ez a gyűrű, az $n>1$ esetben biztosan nem kommutatív. 
Például $n=2$ mellett
\[
    \begin{pmatrix}
        0&1\\
        0&0
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        0&0\\
        1&0
    \end{pmatrix}
    =
    \begin{pmatrix}
        1&0\\
        0&0
    \end{pmatrix},
    \text{ amíg }
    \begin{pmatrix}
        0&0\\
        1&0
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        0&1\\
        0&0
    \end{pmatrix}
    =
    \begin{pmatrix}
        0&0\\
        0&1
    \end{pmatrix}.
\]
Az sem igaz, hogy ez a gyűrű nullosztómentes lenne, hiszen például $n=2$ mellett az
\[
    A
    =
    \begin{pmatrix}
        0&1\\
        0&0
    \end{pmatrix}
\]
mátrix nyilván nem a zérus mátrix (az összeadásra nézve neutrális elem), 
de $A\cdot A=0$. 
Ebből persze már az is következik, hogy a fenti $A$ mátrixnak nincs a szorzásra nézve inverze,
de ez e nélkül is nagyon egyszerűen látszik.%
\footnote{
    Ha 
    \(
    \begin{pmatrix}
        a&b\\
        c&d
    \end{pmatrix}
    \)
    inverze lenne akkor a jobb alsó sarokra figyelve $0\cdot b +0\cdot d=1$ lenne, de egy testben $1\neq 0$.
}
Gyakorlatként próbáljunk magasabb $n$ számok mellett is a kommutativitás és a nullosztómentesség hiányára 
példát találni.

Nagyon fontos látni, hogy a kommutativitás hiánya, az eddigiektől eltérő számolási gyakorlatot eredményez.
A számolás közben a mátrixok sorrendjén nem változtathatunk. 
Persze előfordul, hogy két mátrix szorzata nem függ a sorrendtől. 
Ilyenkor a két mátrixot egymással \emph{felcserélhetőnek}, vagy \emph{kommutálónak} mondjuk.\index{kommutáló mátrixok}
Például, az identitás mátrixszal minden más mátrix kommutál.
Egy mátrixot \emph{diagonális alakúnak}\index{diagonális mátrix} mondunk, 
ha minden nem zérus eleme a fő diagonálisában van.
Az is világos, hogy a diagonális mátrixok egymással kommutálnak.
Fontos része az első féléves anyagnak, hogy ha két négyzetes mátrix szorzata az identitás mátrix,
akkor e két mátrix egymással kommutál.
Ez az eredmény távolról sem nyilvánvaló, és most nem is tudjuk belátni, ehhez már szükség van a lineáris függetlenség fogalmára vagy a Gauss-Jordan\index{Gauss--Jordan-eliminácio} eliminációs algoritmusra, 
amiket majd később vezetünk be.\label{pg:kommutal}\index{kommutáló mátrixok}

Nagyon is triviális mégis érdemes észrevenni, hogy a fentiek $n=1$ esetben nem jelentenek problémát.
Ilyenkor az $1\times 1$-es mátrixok tere voltaképpen azonos az $\mathbb{F}$-testtel, hiszen csak az a különbség,
hogy egy testbeli $a$ elemet $[a]$ módon írjuk. A szorzás és az összeadás definíciója ugyanazt adja,
ha mint a testbeli elemre, vagy az ebből képzett $1\times 1$-es mátrixra gondolunk.

Az $n\times n$-es négyzetes mátrixok másik érdemleges részstruktúrája az
\[
    \mathcal{F}=
    \left\{ c\cdot I:c\in\mathbb{F} \right\}.
\]
Itt $I$ az $n\times n$ méretű identitás mátrix, tehát $\mathcal{F}$ elemei azon diagonális alakú mátrixok, 
ahol minden elem a diagonálisban azonos.
Világos, hogy két ilyen mátrix összege és szorzata is ilyen marad:
\[
    aI+bI=\left( a+b \right)I\text{ és } aI\cdot bI=\left( ab \right)I.
\]
Ez azt jelenti, hogy az $\left( \mathcal{F},+,\cdot \right)$ struktúra egy egységelemes gyűrű.
Világos, hogy itt bármely két elem kommutál, ergo egy kommutatív egységelemes gyűrűvel állunk szemben és
az is teljesen nyilván való, hogy minden nem zérus elemnek van a szorzásra nézve inverze.
Azt kaptuk tehát, hogy a fenti $\mathcal{F}$ minden $n$ mellett egy test.

\section{A komplex számok mint mátrixok}
\begin{definition}[Izomorf testek]\index{izomorfizmus}
    Legyenek $\mathbb{F}$ és $\mathbb{G}$ testek.
    Azt mondjuk, hogy a két test \emph{izomorf} egymással, 
    ha létezik köztük \emph{művelettartó bijekció}, azaz létezik
    \[
        \varphi:\mathbb{F}\to\mathbb{G}
    \]
    bijekció, amely tartja a műveleteket is, azaz
    \[
        \varphi\left( a+b \right)=\varphi\left( a \right)+\varphi\left( b \right)
        \text{ és }
        \varphi\left( a\cdot b \right)=\varphi\left( a \right)\cdot \varphi\left( b \right).
    \]
    A művelettartó bijekciót \emph{izomorfizmusnak} nevezzük.
\end{definition}
Az izomorf testek közt nem teszünk különbséget. Úgy tekintjük őket, hogy csak jelölésükben különböznek.
Például, ha az $\mathbb{R}$ valós számokra gondolunk, 
akkor a $2\times 2$-es diagonális alakú valós mátrixok közül azok, ahol a diagonális mindkét eleme azonos,
a valós testtel izomorf testet alkot.
\[
    \mathcal{R}=\left\{ 
        \begin{pmatrix}
            a&0\\
            0&a
        \end{pmatrix}
        :a\in\mathbb{R}
    \right\}
    \text{ és }
    \varphi:\mathbb{R}\to\mathcal{R},
    \text{ ahol } 
    \varphi\left( a \right)
    =
    \begin{pmatrix}
        a&0\\
        0&a
    \end{pmatrix}.
\]

A $2\times 2$-es valós mátrixok egységelemes gyűrűjében tehát $\mathcal{R}$ egy olyan részgyűrű, ami még test is,
és izomorf az $\mathbb{R}$ valós számtesttel.
Voltaképpen azt csináltuk, hogy a valós számtestet beágyaztuk a $2\times 2$-es mátrixok közé,
azaz egy $a$ valós számot az
\begin{math}
    \begin{pmatrix}
        a&0\\
        0&a
    \end{pmatrix}
\end{math}
mátrixszal reprezentálunk (írunk le).

A $2\times 2$ méretű valós mátrixok még sok-sok más testet is tartalmaznak.%
\footnote{Karakterizációjukat lásd: \parencite{MR1415833}-ben.}
Ezek közül számunkra a legfontosabb a következő részhalmaz.
\begin{defprop}[komplex számtest]
    Jelölje két tetszőleges $a,b\in\mathbb{R}$ valós szám mellett $M_{a,b}$ az 
    az $\left( a,b \right)$ valós számpárhoz tartozó 
    \(
    M_{a,b}
    =
    \begin{pmatrix}
        a&-b\\
        b&a
    \end{pmatrix}
    \)
    mátrixot.
    Tekintsük az ilyen típusú mátrixok $\mathcal{C}$-vel jelölt halmazát:
    \[
        \mathcal{C}
        =
        \left\{ 
            M_{a,b}
            :a,b\in\mathbb{R}
        \right\}
    \]
    E részhalmaz 
    \begin{enumerate}
        \item 
            zárt a mátrix összeadásra és a mátrix szorzásra, 
            így a $2\times 2$-es valós mátrixok egy speciális egységelemes részgyűrűje.
        \item
            E részgyűrűben a mátrix szorzás kommutatív művelet, 
            és 
        \item
            és e részgyűrűben minden nem zérus mátrixnak van inverze is a mátrixszorzás műveletre nézve.
    \end{enumerate}
    Eszerint a $\left( \mathcal{C},+,\cdot \right)$ algebrai struktúra egy test.
    Ezt a testet nevezzük a \emph{komplex számtestnek}\index{komplex számok},
    vagy a \emph{komplex számtest mátrix reprezentációjának}.\index{komplex számok mátrix reprezentációja}
\end{defprop}
\begin{proof}
    Az $a,b,c,d\in\mathbb{R}$ valós számok mellett
    \[
        M_{a,b}+M_{c,d}=
        \begin{pmatrix}
            a&-b\\
            b&a
        \end{pmatrix}
        +
        \begin{pmatrix}
            c&-d\\
            d&c
        \end{pmatrix}
        =
        \begin{pmatrix}
            a+c&-b-d\\
            b+d&a+c
        \end{pmatrix}
        =
        M_{a+c,b+d}
    \]
    és hasonlóan a mátrix szorzás definíciója szerint
    \[
        M_{a,b}\cdot M_{c,d}=
        \begin{pmatrix}
            a&-b\\
            b&a
        \end{pmatrix}
        \cdot
        \begin{pmatrix}
            c&-d\\
            d&c
        \end{pmatrix}
        =
        \begin{pmatrix}
            ac-bd&-ad-bc\\
            bc+ad&-bd+ac
        \end{pmatrix}
        =
        M_{ac-bd,ad+bc}.
    \]
    Mivel $M_{1,0}$ a $2\times 2$-es identitás mátrix, ezért $\mathcal{C}$ a mátrix összeadásra és a mátrixszorzásra nézve
    egységelemes gyűrűt alkot.

    A szorzás kommutativitása is látszik a fenti számolásból, hiszen
    \[
        M_{c,d}\cdot M_{a,b}=M_{ca-db,cb+da}=M_{ac-bd,ad+bc}=M_{a,b}\cdot M_{c,d}
    \]
    a valós számok összeadásának és szorzásának 
    kommutativitása miatt.

    Legyen most $M_{a,b}$ egy nem zérus mátrix, így $a^2+b^2\neq 0$.
    Világos, hogy 
    \[
        M_{a,b}\cdot M_{a,-b}=M_{a^2+b^2,0}=\left( a^2+b^2 \right)M_{1,0}=\left( a^2+b^2 \right)I.
    \]
    Ebből már látszik is, hogy $M_{a,b}\cdot M_{\frac{a}{a^2+b^2},\frac{-b}{a^2+b^2}}=I$.
    Ez a már igazolt kommutativitással éppen azt jelenti, hogy minden nem zérus elemnek van multiplikatív inverze,
    ergo $\mathcal{C}$ valóban test.
\end{proof}
Ebben a $\mathbb{C}$ testben az 
\(
M_{0,-1}=
\begin{pmatrix}
    0&-1\\
    1&0
\end{pmatrix}
\)
elem olyan, hogy a négyzete a szorzásra nézve reprodukáló elemnek az összeadásra nézve képzett inverze:
\[
    M_{0,-1}\cdot M_{0,-1}
    =
    \begin{pmatrix}
        0&-1\\
        1&0
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        0&-1\\
        1&0
    \end{pmatrix}
    =
    \begin{pmatrix}
        -1&0\\
        0 &-1
    \end{pmatrix}
    =
    M_{-1,0}
    =
    -I.
\]
Ez a tulajdonság azért figyelemre méltó, 
mert ha a szokásoknak megfelelően a test multiplikatív neutrális elemét az $1$ szimbólummal jelöljük, 
akkor olyan elemet találtunk a komplex számtestben, 
amelynek négyzete éppen $-1$.
Tudjuk, hogy a valós számtest esetében ez nem lenne lehetséges.

Tekintsük most valamely $a,b\in\mathbb{R}$ mellett az 
\[
    M_{a,b}=
    \begin{pmatrix}
        a&-b\\
        b&a
    \end{pmatrix}
    =
    \begin{pmatrix}
        a&0\\
        0&a
    \end{pmatrix}
    +
    \begin{pmatrix}
        0&-1\\
        1&0
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        b&0\\
        0&b
    \end{pmatrix}
\]
felbontást.
Ha bevezetjük az 
\(
i=M_{0,1}=
\begin{pmatrix}
    0&-1\\
    1&0
\end{pmatrix}
\)
jelölést\index{i komplex szám@$i$ komplex szám}, akkor minden komplex szám
\[
    M_{a,b}=
    \begin{pmatrix}
        a&0\\
        0&a
    \end{pmatrix}
    +
    i
    \cdot
    \begin{pmatrix}
        b&0\\
        0&b
    \end{pmatrix}
\]
alakban írható.
Emlékezzünk arra, 
hogy a valós számtest is részhalmaza a komplex számoktestnek abban az értelemben, 
ha minden $a$ valós számot az
\begin{math}
    \begin{pmatrix}
        a&0\\
        0&a
    \end{pmatrix}
\end{math}
mátrixszal reprezentálunk. ($\mathcal{R}\subseteq\mathcal{C}$).
Ha tehát megegyezünk abban, hogy az $a$ valós számra nézve mi az
\(
\begin{pmatrix}
    a&0\\
    0&a
\end{pmatrix}
\)
mátrixra gondolunk,%
\footnote{Kicsit pontosabban: a valós számok $2\times 2$-es mátrix reprezentációját használjuk.}
akkor azt kapjuk, hogy minden komplex szám
\[
    M_{a,b}=a+ib
\]
alakú, ahol $i$ egy olyan komplex szám, amelyre $i^2=-1$, $a,b\in\mathbb{R}$.
Ezt nevezzük a komplex szám \emph{normálalakjának}.\index{komplex szám normálalakja}

Ne felejtsük a műveleteket:
Láttuk, hogy $M_{a,b}+M_{ c,d}=M_{a+c,b+d}$. 
Ez a normálalak reprezentáció mellett azt jelenti, hogy az összeadás definíciója csak
\[
    (a+ib)+(c+id)=(a+c)+i(b+d)\tag{\dag}
\]
lehet.
Hasonlóan emlékszünk, hogy
\begin{math}
    M_{a,b}\cdot M_{c,d}=M_{ac-bd,ad+bc}
\end{math},
ami normálalak reprezentáció mellett az
\[
    (a+ib)\cdot (c+id)=(ac-bd)+i(ad+bc)\tag{\ddag}
\]
definíciót eredményezi.

A következőket gondoltuk meg:
\begin{defprop}[komplex számtest a normálakkal]
    Definiálja
    \[
        \mathbb{C}
        =
        \left\{ 
            a+ib:a,b\in\mathbb{R}
        \right\}
    \]
    a \emph{komplex számok normálalakját}.
    Az összeadás műveletet definiálja (\dag), és a szorzás műveletet definiálja (\ddag).
    Az így kapott algebrai struktúra test, amely izomorf a komplex számtest mátrix reprezentációjával.
    Az izomorfizmust a 
    \[
        \varphi:\mathcal{C}\to\mathbb{C},\quad 
        \varphi
        \left( 
        \begin{matrix}
            a&-b\\
            b&a
        \end{matrix}
        \right)
        =
        a+ib
    \]
    művelettartó bijekció hozza létre.
\end{defprop}

Ha már megértettük, hogy a komplex számok normálalak reprezentációja testet alkot,
akkor a (\dag) és (\ddag) definíciók megjegyzése nagyon könnyű.
Más nem is lehet:
Az összeadáshoz (\dag) csak el kell végezni a műveletet majd kiemelni $i$-t,
a szorzás definíciójához (\ddag) 
az $i$ kiemelése után jutunk.

\section{A komplex számok abszolút értéke}
A valós számokra jól ismert abszolút érték függvényt terjesztjük ki komplex számtest elemeire.
\begin{definition}[valós rész, képzetes rész]\index{komplex szám valós része}\index{komplex szám képzetes része}
    Legyen $z\in\mathbb{C}$, $z=a+ib$.
    Ekkor $a\in\mathbb{R}$ a $z$ komplex szám \emph{valós része}, 
    és $b\in\mathbb{R}$ a $z$ komplex szám \emph{képzetes része}.
    $\Re z$ jelöli a valós részt, és $\Im z$ a képzetes részt.
\end{definition}
\begin{definition}[konjugált]\index{komplex szám konjugáltja}
    Legyen $z\in\mathbb{C}$, $z=a+ib$.
    Ekkor $z$ \emph{konjugáltja} $\bar{z}=a-ib$.
\end{definition}
\begin{proposition}
    Minden $z\in\mathbb{C}$ komplex szám mellett
    \[
        \bar{\bar{z}}=z,\quad
        z+\bar{z}=2\Re z,\quad
        z-\bar{z}=2i\Im z,\quad
        z\in\mathbb{R}\iff z=\bar{z},\quad
        z\bar{z}=(\Re z)^2+(\Im z)^2\geq 0,
    \]
    és bármely két $z,w\in\mathbb{C}$ komplex szám esetén
    \[
        \overline{z+w}=\bar{z}+\bar{w},\quad
        \overline{zw}=\bar{z}\overline{w},\quad
        \overline{z-w}=\bar{z}-\bar{w},\quad
        \overline{\left( \frac{z}{w} \right)}=\frac{\bar{z}}{\bar{w}}\quad\text{ feltéve, hogy }w\neq 0.\qedhere
    \]
\end{proposition}
Most használjuk először a valós számok rendezését.
Az $\mathbb{R}$ testen a $\geq $ relációt az algebrai műveletekkel a következő két axióma kapcsolja össze:
Minden $a,b,c\in\mathbb{R}$ mellett
\[
    a\geq b\text{ esetén }a+c\geq b+c,\qquad a,b\geq 0\text{ esetén }ab\geq 0.
\]
Az $a^2-b^2=\left( a+b \right)\left( a-b \right)$ azonosság szerint,
ha $a\geq b\geq 0$ akkor $a^2\geq b^2$ is fennáll.

\begin{definition}[komplex szám abszolút értéke]\index{abszolút érték}
    Legyen $z\in\mathbb{C}$ egy komplex szám.
    Láttuk, hogy 
    $z\bar{z}\in\mathbb{R}$ és  
    $z\bar{z}\geq 0$.
    E szám négyzetgyökét nevezzük a $z$ komplex szám \emph{abszolút értékének}.
    Jelölés: $|z|=\sqrt{z\bar{z}}$.
\end{definition}
Fontos látni, hogy ha speciálisan $\Im z=0$, 
tehát ha $z$ egy valós szám,
akkor e definíció szerint
\[
    |z|=
    \sqrt{z^2}=
    \begin{cases}
        z&\text{, ha }z\geq 0\\
        -z&\text{, ha }z<0\text{,}
    \end{cases}
\]
ami egybeesik a valós számok abszolút értékének definíciójával.
Világos, hogy $|\Re z|^2\leq(\Re z)^2+(\Im z)^2=z\bar{z}$, emiatt
\[
    \Re z\leq |z|.
\]
Az abszolút érték legfontosabb tulajdonságai:
\begin{proposition}
    Legyen $z,w\in\mathbb{C}$ komplex szám.
    Ekkor
    \begin{enumerate}
        \item $|z|=0$ akkor és csak akkor, ha $z=0$,
        \item $|zw|=|z||w|$,
        \item $|z+w|\leq |z|+|w|$.\qedhere
    \end{enumerate}
\end{proposition}
Az utolsó egyenlőtlenséget \emph{háromszög egyenlőtlenségnek}\index{háromszög egyenlőtlenség} nevezik.
Egy kevésbé népszerű, de ekvivalens alakja
\[
    \left|\left( |z|-|w| \right)\right|\leq|z-w|.
\]
Vegyük észre, hogy a valós abszolút érték függvény tulajdonságait sem használtuk,
és a fenti tulajdonságokat valós esetre is újra igazoltuk.
\section{A komplex számok trigonometrikus alakja}
Láttuk, hogy $z,w\in\mathbb{C}$ komplex szám mellett
\[
    \Re\left( z+w \right)=\Re z+\Re w
    \qquad
    \text{ és }
    \qquad
    \Im\left( z+w \right)=\Im z+\Im w.
\]
Ez azt jelenti, hogy ha az $a+ib$ komplex számot azonosítjuk az $\mathbb{R}^2$ sík
$\left( a,b \right)$ pontjával, 
akkor egyszerűen koordinátánként kell összeadni a komplex számokat, mintha a $z$ komplex szám
az origóból az $\left( a,b \right)$ pontra mutató vektor lenne.

A kérdés, hogy ha így képzeljük a komplex számokat, 
akkor a komplex számok szorzása mit jelent a komplex számoknak megfeleltetett vektorok körében?

\begin{defprop}[komplex szám trigonometrikus alakja]\index{komplex szám trigonometrikus alakja}
    Legyen $z\in \mathbb{C}$ egy nem zérus komplex szám.
    Ekkor létezik $\varphi\in\mathbb{R}$ valós szám, hogy
    \[
        z=
        |z|\left( \cos\varphi+i\sin\varphi \right)\tag{\dag}
    \]
    Ezt a $\varphi$ számot nevezzük a $z$ komplex szám \emph{argumentumának}, és $\arg z$ módon jelöljük.
    \index{komplex szám argumentuma}
    A (\dag) alak a komplex szám \emph{trigonometrikus alakja}.

    Két nem zérus komplex szám egyenlősége azt jelenti, hogy az abszolút értékük azonos,
    és az argumentumaik különbsége $2\pi$ többszöröse.
\end{defprop}
\begin{proof}
    Világos, hogy ha $z=a+ib\neq 0$, akkor $a^2+b^2>0$, így
    \[
        z=a+ib
        =
        \sqrt{a^2+b^2}\left( \frac{a}{\sqrt{a^2+b^2}}+i\frac{b}{\sqrt{a^2+b^2}} \right).
    \]
    Ha $x$ jelöli a fenti zárójelben a valós részt és $y$ a képzetes részt,
    akkor $x^2+y^2=1$.
    Így az $\left( x,y \right)$ pár a sík egységkörének egy pontja.
    A $\cos$ és $\sin$ függvény definíciója szerint,
    ha $\varphi$ jelöli az origót az $\left( x,y \right)$ 
    ponttal a körcikk peremén mért ív hosszát,
    akkor $x=\cos\varphi$ és $y=\sin\varphi$.
\end{proof}
\begin{proposition}
    Legyenek a $z,w\in \mathbb{C}$ nem nulla komplex számok a trigonometrikus alakjukban felírva, 
    azaz
    \[
        z=|z|\left( \cos\varphi+i\sin\varphi \right),\qquad 
        w=|w|\left( \cos\psi+i\sin\psi \right).
    \]
    Ekkor a két szám szorzatának abszolútértéke az abszolútértékek szorzata
    és a szorzat argumentuma az argumentumok összege, azaz
    \[
        zw=
        |z||w|\left( \cos\left( \varphi+\psi \right)+i\sin\left( \varphi+\psi \right) \right).
    \]
    Speciálisan minden $n\in\mathbb{Z}$ egész számra\index{Moivre-formula}
    \[
        z^n=|z|^n\left( \cos n\varphi+i\sin n\varphi \right).\qedhere
    \]
\end{proposition}
A szakasz bevetőjében feltett kérdésre tehát a válasz,
hogy a $z$ komplex számmal való szorzást a sík olyan geometriai transzformációjának képzelhetjük,
amely egy $\arg z$ szöggel való forgatásból és egy $|z|$-szeres origó középpontú nyújtásból áll.
\begin{definition}[egységgyök]\index{komplex $n$-edik egységgyökök}
    Legyen az $n\in\mathbb{N}$ természetes szám rögzítve.
    Tetszőleges $k=0,1,\dots,n-1$ mellett jelölje
    \[
        \epsilon_k^{(n)}=\cos k\frac{2\pi}{n}+i\sin k\frac{2\pi}{n}.
    \]
    az úgynevezett \emph{$n$-edik komplex egységgyököket}.
\end{definition}
Az $n$-edik komplex egységgyökök a komplex számsík pontosan $n$ különböző pontját alkotják,
és az $n$-edik hatványuk $1$, azaz 
és $(\epsilon_k^{(n)})^n=1$.
Emiatt minden $w\neq 0$ komplex szám mellett pontosan $n$ komplex gyöke van a $t^n-w$ polinomnak.
Ha ugyanis $w$ trigonometrikus alakja $w=|w|\left( \cos\psi+i\sin\psi \right)$,
akkor legyen például $z_0=\sqrt[n]{|w|}\left( \cos(\psi/n)+i\sin(\psi/n) \right)$,
és így $(z_0\epsilon_k^{(n)})^n=w$ minden $k=0,\dots,n-1$ szám mellett.
\section{Polinom faktorizáció a komplex- és a valós számtest felett}
A komplex számtest felett minden nem konstans polinomnak van gyöke.
Formálisabban:
\begin{FA}
    Ha $p\left( t \right)\in\mathbb{C}[t]$ nem konstans polinom,
    akkor létezik $z\in\mathbb{C}$ komplex szám, 
    amelyre $p\left( z \right)=0$.
\end{FA}
Az állítást itt nem tudjuk igazolni és egyelőre érdemes bizonyítás nélkül elfogadni.
Illusztrációként megértettük, hogy miért igaz $p\left( t \right)=\alpha_0+t^n$ alakú polinomra.
A felépítés jelen pontján tekintsük a komplex számtest egy még nem igazolt tulajdonságának.

Most összefoglaljuk, hogy az algebra alaptétele mit jelent a komplex test feletti, majd a valós test feletti polinomok
faktorizációjára nézve.
Láttuk, hogy minden normált polinom előáll mint normált irreducibilis polinomok szorzata,
emiatt azt kell meggondolnunk, hogy mik az irreducibilis polinomok.
\begin{proposition}
    A $\mathbb{C}\left[ t \right]$ polinomgyűrűben minden legalább másodfokú polinom reducibilis.
\end{proposition}
\begin{proof}
    Legyen $p$ egy legalább másodfokú komplex polinom.
    Az algebra alaptétele miatt van gyöke, és
    tudjuk hogy a gyöktényező mindig kiemelhető.
    Így azt kapjuk, hogy $p(t)=\left( t-z \right)h\left( t \right)$ alakú,
    emiatt $2\leq\deg p=1+\deg h$, ergo
    $p$ valóban előállt mint két legalább elsőfokú polinom szorzata.
\end{proof}
A polinomok szorzattá bontásáról szóló tételnek a komplex számtest feletti speciális esete tehát:
\begin{proposition}\label{pr:PolFact}
    A $\mathbb{C}[t]$ polinomgyűrű minden legalább elsőfokú normált polinomjához léteznek
    a sorrendjüktől eltekintve egyetlen $z_1,\dots,z_s\in\mathbb{C}$ egymástól különböző komplex számok,
    és léteznek $n_1,\dots,n_s$ pozitív egészek, 
    amelyekre
    \[
        p\left( t \right)=
        \left( t-z_1 \right)^{n_1}
        \cdots
        \left( t-z_s\right)^{n_s}.\qedhere
    \]
\end{proposition}
Persze ez azt jelenti, hogy egy $n$-ed fokú komplex polinomnak mindig pontosan $n$ komplex gyöke van, 
ha a gyökök számát multiplicitással számoljuk.
Az állítást egy kicsit egyszerűbben úgy is fogalmazhatjuk, 
hogy 
\emph{
    minden nem konstans komplex polinom előáll mint első fokú komplex polinomok szorzata.
}
Az iménti mondat nyilván nem igaz a ,,komplex'' szót a ,,valós'' szóra cserélve, 
ugyanis minden negatív diszkriminánsú másodfokú valós polinom jó is ellenpéldának.
Ez a jelenség az oka annak, 
hogy a komplex számtest felett sokkal kényelmesebb dolgoznunk mint a valós számok felett.

Most nézzük, hogy mit jelent az algebra alaptétele a valós test feletti polinom gyűrűre nézve.
\begin{proposition}
    Legyen $p\left( t \right)\in\mathbb{C}[t]$ a komplex polinomgyűrű egy olyan eleme, 
    amelynek minden együtthatója valós.
    Ekkor a $z\in \mathbb{C}$ komplex szám pontosan akkor gyöke $p$-nek, 
    ha $\bar{z}$ is gyöke $p$-nek.
    Sőt, ha $z$ egy $k$-szoros multiplicitású gyök, 
    akkor $\bar{z}$ is pontosan $k$-szoros multiplicitású gyök.
\end{proposition}
\begin{proof}
    A $p$ polinomra tehát 
    \begin{math}
        p\left( t \right)
        =
        \sum_{j=0}^n\alpha_jt^j,
    \end{math}
    ahol $\alpha_0,\alpha_1,\dots,\alpha_n\in\mathbb{R}$.
    A konjugálás tulajdonságai szerint egy $z$ komplex számra
    \[
        p\left( z \right)=
        \overline{
            \sum_{j=0}^n\alpha_jz^j
        }
        =
        \sum_{j=0}^n\overline{\alpha_jz^j}
        =
        \sum_{j=0}^n\overline{\alpha_j}\overline{(z^j)}
        =
        \sum_{j=0}^n\bar{\alpha_j}\bar{z}^j
        =
        \sum_{j=0}^n\alpha_j\bar{z}^j
        =
        p\left(\bar{z} \right).
    \]
    Emiatt valós együtthatós $p$ polinomra és $z$ komplex számra 
    $p\left( z \right)=0$ akkor és csak akkor, ha $p\left( \bar{z} \right)=0.$

    Ha a gyök speciálisan valós szám, akkor az állítás semmit mondó.
    Ha most $z$ egy nem valós komplex gyök,
    akkor $p(t)=\left( t-z \right)\left( t-\bar{z} \right)h\left( t \right)$, ahol 
    $h\left( z \right)=0$ pontosan akkor, ha $h\left( \bar{z} \right)=0$.
    Így, ha $z$ egy $k$ szoros gyök,
    akkor
    \[
        p\left( t \right)=\left( t-z \right)^{k}\left( t-\bar{z} \right)^{k}\cdot h\left( t \right)
    \]
    alakú, ahol $h$-nak már sem $z$ sem $\bar{z}$ nem gyöke.
\end{proof}
Minden valós együtthatós polinom előáll mint első vagy másodfokú polinomok szorzata:
\begin{proposition}\label{pr:RealPolFact}
    Az $\mathbb{R}[t]$ polinomgyűrű minden legalább elsőfokú normált polinomjához
    léteznek $x_1,\dots,x_r\in\mathbb{R}$ valós számok,
léteznek $\alpha_1,\beta_1,\dots,\alpha_s,\beta_s$ valós együttható párok, 
    és léteznek
    $n_1,\dots,n_r, m_1,\dots,m_s$ pozitív egészek úgy, hogy
    \[
        p\left( t \right)
        =
        \left( t-x_1 \right)^{n_1}
        \cdots
        \left( t-x_r \right)^{n_r}
        \cdot
        \left(\alpha_1 + \beta_1t +t^2\right)^{m_1}
        \cdots
        \left(\alpha_s + \beta_st +t^2\right)^{m_s}.
    \]
    Itt $r,s\geq 0$, de $r+s>0$, és $n_1+\dots+n_r+2\left( m_1+\dots+m_s \right)=\deg p$, 
    továbbá a jobboldalon szereplő másodfokú polinomok irreducibilisek.
\end{proposition}
\begin{proof}
Tekintsük a $p$ polinomot mint a komplex számtest feletti polinomgyűrű egy elemét,
    és alkalmazzuk a komplex polinom faktorizációról szóló tételt.
    A $p$ így előáll mint első fokú, esetleg komplex polinomok szorzata.
    A gyököket osszuk két része.
    Legyenek $x_1,\dots,x_r$ a különböző valós gyökök, amelyek multiplicitása rendre
    $n_1,\dots,n_r$.
    Legyenek $z_1,\overline{z_1},\dots,z_s,\overline{z_s}$ a különböző nem valós de komplex gyökök,
    ahol $m_1,\dots,m_s$ rendre a konjugált gyök párok multiplicitása.
    Így
    \[
        p\left( t \right)=
        \left( t-x_1 \right)^{n_1}
        \cdots
        \left( t-x_r \right)^{n_r}
        \cdot
        \left( \left( t-z_1 \right)\left( t-\bar{z_1} \right) \right)^{m_1}
        \cdots
        \left( \left( t-z_s \right)\left( t-\bar{z_s} \right) \right)^{m_s}.
    \]
    Világos, hogy $r,s\geq 0$, de $r+s>0$, és $n_1+\dots+n_r+2\left( m_1+\dots+m_s \right)=\deg p$.
    A komplex faktorokra végezzük el a szorzást, 
    így
    $
    \left( t-z_k \right)\left( t-\bar{z_k} \right)
    =
    t^2-2\Re z_k+|z_k|^2
    $.
    Így $\alpha_k=|z_k|^2$ és $\beta_k=2\Re z_k$, választással készen is vagyunk.
\end{proof}
A fenti állításból két dolog azonnal látszik.
Az első, 
hogy \emph{valós számtest felett minden legalább harmadfokú polinom reducibilis},
a második pedig, 
hogy \emph{minden páratlan fokú valós együtthatós polinomnak van valós gyöke.}

%\chapter{Gauss-Jordan elimináció}
%\scwords Lineáris egyenletrendszerek megoldását automatizáljuk.\index{lineáris egyenletrendszer}


\chapter{A vektortér fogalma}
\scwords A lineáris algebra kezdő fejezetéhez érkeztünk, 
miután áttekintettük azokat az általános algebrai ismereteket,
amelyek nélkül nem tárgyalhatók a lineáris algebrához szükséges gondolatok.
\begin{definition}[Vektortér]\index{vektortér}
    Legyen adva egy $\mathbb{F}$ test, és egy $V$ halmaz.
    Tegyük fel, hogy adott egy $+:V\times V\to V$ két változós művelet (ezt összeadásnak nevezzük)
    és adott egy $\cdot:\mathbb{F}\times V\to V$ szintén kétváltozós művelet (ezt skalárral való szorzásnak, vagy számmal való mondjuk).
    A $V$-t az $\mathbb{F}$ test feletti \emph{vektortérnek} nevezzük,
    ha $\left( V,+ \right)$ egy Ábel-csoport, 
    és a számmal valós szorzás műveletre teljesülnek az alábbi axiómák:
    \begin{enumerate}
        \item Minden $\alpha\in\mathbb{F}$ és minden $u,v\in V$ mellett 
            \begin{math}
                \alpha\cdot\left( u+v \right)=\alpha\cdot u+\beta\cdot v,
            \end{math}
        \item Minden $\alpha,\beta\in\mathbb{F}$ és minden $u\in V$ mellett 
            \begin{math}
                \left( \alpha+\beta \right)\cdot u=\alpha\cdot u+\beta\cdot v,
            \end{math}
        \item Minden $\alpha,\beta\in\mathbb{F}$ és minden $u\in V$ mellett 
            \begin{math}
                \alpha\cdot\left( \beta\cdot u \right)=\left( \alpha\beta \right)\cdot u,
            \end{math}
        \item Minden $u\in V$ esetén $1\cdot u=u$.\qedhere
    \end{enumerate}
\end{definition}
Nagyon hasonlóan ahhoz, ahogyan testben is meggondoltuk igazak a következő számolási szabályok.
Minden $\alpha\in\mathbb{F}$ mellett
\begin{enumerate}[label=\roman*.)]
    \item $\alpha\cdot 0=0$,
    \item $0\cdot v=0$,
    \item $\left( -1 \right)\cdot v=-v$,
    \item $\alpha\cdot v=0$ esetén $\alpha=0$ vagy $v=0$.
\end{enumerate}
A számmal való szorzás $\cdot:\mathbb{F}\times V\to V$ művelet eredményének szokásos rövidítése, 
hogy a kissé körülményes $\alpha\cdot v$ helyett csak $\alpha v$-t írunk.
Az is előfordul, 
különösen amikor egy konkrét vektortér konkrét műveletéről van szó, 
hogy az $\alpha v$ és a $v\alpha$ jelölést is
ugyanarra az $\alpha\cdot v\in \mathbb{F}$ elemre használjuk.

Alapvető példa vektortérre a mátrixok tere.
Az $m\times n$ méretű mátrixok $\mathbb{F}^{m\times n}$ halmaza a mátrix összeadással és a számmal való szorzással
vektorteret alkot az $\mathbb{F}$ test felett.
Ha $n=1$, akkor kapjuk a az oszlopvektorok $\mathbb{F}^m$ terét, ami így szintén egy $\mathbb{F}$ feletti vektortér.
Speciális esetként $\mathbb{R}^m$ egy $\mathbb{R}$ feletti vektortér,
$\mathbb{C}^m$ egy $\mathbb{C}$ feletti vektortér.

Fontos példa még, egy adott $X$ halmazból az $\mathbb{F}$ testbe képező összes függvények halmaza 
a függvények közt szokásos összeadás művelettel, és számmal való szorzással. 
Ezt a teret $\mathbb{F}^X$ módon szokás jelölni, és minden nehézség nélkül ellenőrizhető, 
hogy $\mathbb{F}^X$ egy $\mathbb{F}$ feletti vektortér.
Hasonlóan látszik, hogy például az összes valós--valós folytonos függvények is egy $\mathbb{R}$
feletti vektorteret alkotnak, vagy ha ezek közül csak a differenciálható függvényekre szorítkozunk,
akkor ezen függvénytér is vektortér az $\mathbb{R}$ test felett.

Rögzítenünk kell magunkban, hogy a vektortér definíciójában a test is fontos szerepet játszik.
Más test felett ugyan az Ábel-csoport már egy másik vektorteret alkot.
Világos például, hogy $\mathbb{R}$ az $\mathbb{R}$ test felett vektortér a szokásos műveletekkel,
de látjuk majd, hogy egészen más tulajdonságai vannak annak a vektortérnek, 
amit akkor kapunk, 
ha az $\mathbb{R}$ valós számok Ábel-csoportját, 
mint a $\mathbb{Q}$ racionális test feletti vektortérnek tekintjük.

Játékos példaként gondoljuk meg, 
hogy a pozitív valós számok egy az $\mathbb{R}$ feletti vektorteret alkot a következő fura műveletekkel:
Tetszőleges $a,b$ pozitív valós szám mellett
\(
a\#b=a\cdot b, 
\)
majd tetszőleges $\alpha$ valós szám és $a$ pozitív valós számra legyen
\(
\alpha\star a=a^\alpha.
\)
\footnote{Itt $a\star\alpha$ mit jelent?}

A vektorteret sokszor csak az additív művelet alaphalmazával jelöljük.
Kicsit pontosabb ha az alaphalmaz mellett a testet is konkrétan specifikáljuk, 
de sokszor feltesszük, hogy a szövegkörnyezetből nyilvánvaló, hogy mely testre gondolunk.
Hasonlóan pontosabb lenne a két műveletet is mindig kijelölni, mikor egy vektortérre hivatkozunk,
de ha világos, hogy mi a vektortérbeli elemek közt az additív művelet, és hogy mit jelent egy test elemeivel szorozni,
akkor elhagyjuk a műveletek kijelölését.
A legpontosabb, -- de persze a legkörülményesebb -- jelölés az lenne, hogy pl.
,,tekintsünk egy $\left( V,+,\cdot \right)$ vektorteret az $\mathbb{F}$ test fölött.''
Ehelyett sokszor csak azt mondjuk, hogy
,,legyen $V$ egy vektortér''. 
Ilyenkor a szövegkörnyezetből világosnak kell lennie, hogy mi a test, mit jelent a számmal való szorzás, és mi az összeadás a $V$ halmazon.

\section{Vektortér alterei}
\begin{definition}[altér]\index{vektortér altere}\index{altér}
    Legyen $\left( V,+,\cdot \right)$ egy vektortér az $\mathbb{F}$ test felett.
    Egy $M\subseteq V$ részhalmaz a $V$ vektortér \emph{altere},
    ha $M$ maga is vektorteret alkot a $V$-ben definiált additív művelettel, 
    és a $V$-ben definiált számmal való szorzással.
\end{definition}
\begin{proposition}
    Legyen $V$ egy vektortér, és $M\subseteq V$ egy részhalmaza.
    Az $M$ pontosan akkor altér,
    ha
    \begin{enumerate}
        \item $0\in M$,
        \item $u,v\in M$ esetén $u+v\in M$,
        \item $u\in M$ és $\alpha\in\mathbb{F}$ esetén $\alpha v\in M$.\qedhere
    \end{enumerate}
\end{proposition}
Tetszőleges $V$ vektortérre a $\left\{ 0 \right\}$ és maga $V$ mindig alterek,
ezeket \emph{triviális altereknek} is szokás mondani.\index{triviális altér}

Most két fontos fogalmat vezetünk be.
Egy halmazt tartalmazó legszűkebb altér fogalmát,
és a halmaz lineáris burkának fogalmát. 
Ki fog derülni, hogy a lineáris burok mindig egybeesik a legszűkebb altérrel.
\begin{defprop}[generált altér]\index{generált altér}
    Egy vektortérben, akárhány altér közös része altér.
    Emiatt értelmes a következő definíció.
    Ha $H\subseteq V$ egy részhalmaza a $V$ vektortérnek,
    akkor jelölje 
    $\gen H$ a $H$ halmazt tartalmazó összes alterek metszetét.
    Ezt az alteret nevezzük a $H$ halmaz által \emph{generált altérnek.}
\end{defprop}
\begin{proposition}
    Legyen $V$ egy vektortér.
    Ekkor
    \begin{enumerate}
        \item Minden $H\subseteq V$ halmazra $H\subseteq \gen H$,
        \item Ha $H\subseteq K\subseteq V$, akkor $\gen H\subseteq \gen K$,
        \item Minden $H\subseteq V$ mellett $\gen\left( \gen H \right)=\gen H$.
    \end{enumerate}
    A $\gen H$ a $H$ halmazt tartalmazó alterek közt a legszűkebb.
    Így $H\subseteq V$ pontosan akkor altér, ha $\gen H=H$.
\end{proposition}
\begin{definition}[lineáris kombináció, lineáris burok]\index{lineáris kombináció}\index{lineáris burok}
    Ha adott a $V$ vektortérben véges sok $v_1,\dots,v_r$ vektor,
    akkor a vektortér minden
    \[
        \alpha_1v_1+\dots+\alpha_r v_r
    \]
    alakú vektorát a $v_1,\dots,v_r$ vektorok egy \emph{lineáris kombinációjának} mondjuk.

    Legyen $H\subseteq V$ egy tetszőleges halmaz.
    Ekkor $\lin H$ jelöli $H$ összes véges részhalmazának összes lineáris kombinációinak halmazát,
    azaz
    \[
        \lin H=
        \left\{ \sum_{j=1}^n\alpha_jv_j:n\in\mathbb{N},v_1,\dots,v_n\in H,\alpha_1,\dots,\alpha_n\in\mathbb{F} \right\}
    \]
    Definíció szerint nulla darab vektor lineáris kombinációja a vektortér zérus eleme, 
    tehát $\lin \emptyset=\left\{ 0 \right\}.$
    A $\lin H$ halmazt nevezzük a $H$ halmaz \emph{lineáris burkának}.
\end{definition}
\begin{proposition}
    Egy $V$ vektortér minden $H\subseteq V$ részhalmazának lineáris burka, 
    a $H$ halmazt tartalmazó legszűkebb altér, azaz
    \[
        \lin H=\gen H.\qedhere
    \]
\end{proposition}
\begin{proof}
    Világos, hogy $H\subseteq \lin H$, világos hogy $\lin H$ egy altér, 
    és az is nyilvánvaló, hogy ha $H\subseteq M$ egy tetszőleges altér, 
    akkor $\lin H\subseteq M$.
    Így $\lin H=\gen H$.
\end{proof}
Ha $H$ egy véges halmaz, akkor $\lin H$ kicsit egyszerűbben írható.
Mint arról már a definícióban is szó volt $\lin \emptyset=\left\{ 0 \right\}$.
Ha $H$ egy elemű, akkor $\lin (\left\{ v_1 \right\})=\left\{ \alpha v_1:\alpha\in\mathbb{F} \right\}$.
Ha $H=\left\{ v_1,v_2 \right\}$ két elemű, akkor
$\lin({v_1,v_2})=\left\{ \alpha_1v_1+\alpha_2v_2:\alpha_1,\alpha_2\in\mathbb{F} \right\}$.
Hasonlóan, ha $H=\left\{ v_1,\dots,v_r \right\}$ halmaz $r$ elemből áll akkor elegendő csak az $r$ elemből álló lineáris kombinációkat képezni, azaz
\[
    \lin \left( \left\{ v_1,\dots,v_r \right\} \right)
    =
    \left\{ \sum_{j=1}^r\alpha_jv_j:\alpha_1,\dots,v_r\in\mathbb{F} \right\}.
\]
\begin{definition}[generátorrendszer, végesen generált vektortér]\index{generátorrendszer}\index{végesen generált vektortér}
    Egy vektortér egy $H$ részhalmazáról azt mondjuk, hogy \emph{generálja a vektorteret}
    vagy, hogy $H$ egy \emph{generátorrendszere} $V$-nek, ha 
    \[
        \lin H=V.
    \]
    A $V$ vektorteret \emph{végesen generáltnak} mondunk, ha létezik véges generátorrendszere.
\end{definition}

A generátorrendszer cseréről szóló \ref{le:gencsere}.~lemmának kiemelten fontos szerepe van felépítésünkben.
Egyrészt használjuk majd a Steinitz-lemma\index{Steinitz-lemma} (\ref{le:Steinitz}) igazolásában, 
másrészt ennek segítségével tisztázzuk majd azt a kérdést,
hogy hogyan alakulnak egy vektor ,,koordinátái'', ha a vonatkoztatási rendszert változtatjuk.

\begin{lemma}[generátorrendszer csere]\label{le:gencsere}
    Legyen $\left\{ x_1,\dots,x_m \right\}$ egy generátorrendszere valamely vektortérnek,
    és tegyük fel, hogy valamely $y$ vektorra
    \[
        y=\sum_{j=1}^m\eta_jx_j,
    \]
    ahol $\eta_k\neq 0$ valamely $1\leq k\leq m$ mellett. 
    Ekkor $y$ becserélhető a $k$-adik helyen a generátorrendszerbe, 
    úgy hogy az generátorrendszer maradjon, azaz a
    \[
        \left\{ x_1,\dots,x_{k-1},y,x_{k+1},\dots,x_m \right\}
    \]
    vektorrendszer is generátorrendszer.
\end{lemma}
\begin{proof}
    Fejezzük ki $x_k$-t az $y$-ra felírt formulából:
    \(
    x_k=\frac{1}{\eta_k}y+\sum_{\substack{j=1\\j\neq k}}^m\frac{-1}{\eta_k}\eta_jx_j.
    \)
    Ha $a$ eredetileg 
    \[
        a=\sum_{j=1}^m\alpha_jx_j
    \]
    alakú, akkor $x_k$ helyére betéve, a fent kifejezett formulát és bevezetve a 
    $\delta=\frac{\alpha_k}{\eta_k}$ jelölést, azt kapjuk hogy:
    \begin{multline*}
        a=\alpha_kx_k+\sum_{\substack{j=1\\j\neq k}}^m\alpha_jx_j=
        \\
        =
        \alpha_k
        \left( 
        \frac{1}{\eta_k}y+\sum_{\substack{j=1\\j\neq k}}^m\frac{-1}{\eta_k}\eta_jx_j
        \right)
        +\sum_{\substack{j=1\\j\neq k}}^m\alpha_jx_j
        =
        \frac{\alpha_k}{\eta_k}y+
        \sum_{\substack{j=1\\j\neq k}}^m\left( \alpha_j-\frac{\alpha_k}{\eta_k}\eta_j \right)x_j=
        \\
        =\delta y+
        \sum_{\substack{j=1\\j\neq k}}^m\left( \alpha_j-\delta\eta_j \right)x_j.
    \end{multline*}
    Azt kaptuk tehát, hogy ha egy vektor kifejezhető az eredeti vektorrendszerből az 
    \[
        \left( \alpha_1,\dots,\alpha_m \right) 
    \]
    együtthatókkal, akkor ugyanez a vektor a módosított vektorrendszerből is kifejezhető,
    méghozzá az 
    \[
        \left( 
        \underbrace{\alpha_1-\delta\eta_1}_{1.},
        \underbrace{\alpha_2-\delta\eta_2}_{2.},
        \dots,
        \underbrace{\alpha_{k-1}-\delta\eta_{k-1}}_{k-1.},
        \underbrace{\delta}_{k.},
        \underbrace{\alpha_{k+1}-\delta\eta_{k+1}}_{k+1.},\dots,
        \underbrace{\alpha_m-\delta\eta_m}_{m.}
        \right)
    \]
    együtthatókkal.
\end{proof}
\section{Elimináció}
Szokásos jelölés és a kívánatos szemlélet kialakításában is fontos szerepet játszik a következő táblázat.
Ha $\left\{ x_1,\dots,x_m \right\}$ egy generátorrendszer az azt jelenti, hogy minden $a\in V$ vektor
előáll mint az $x_1,\dots,x_m$ vektorok valamilyen együtthatókkal vett lineáris kombinációja.
Ha tehát $a=\alpha_1x_1+\dots+\alpha_mx_m$, akkor azt a következőképpen fejezzük ki.
\[
    \begin{array}{c|c}
        &a         \\
        \hline
        x_1         &\alpha_1   \\
        \vdots      &\vdots    \\
        x_k         &\alpha_k   \\
        \vdots      &\vdots    \\
        x_m         &\alpha_m   \\
        \hline
    \end{array}
\]
Tekinthető ez egy $m\times 1$ típusú bekeretezett mátrixnak, a hol a sorok címkéi a generátorrendszer elemei,
az egyetlen oszlop címkéje pedig az a vektor, amelynek az előállításáról van szó.

A generátorrendszer csere lemma arról is szólt, hogy ha $y$-t a $k$-adik helyen cseréljük a generátorrendszerbe,
akkor a fenti táblázat hogyan változik.
Azt kaptuk, hogy a 
\begin{eqnarray}\label{alg:G-J}
    \begin{array}{c|cc}
        &y      &a         \\
        \hline
        x_1         &\eta_1  &\alpha_1   \\
        \vdots      &\vdots &\vdots    \\
        x_k         &\boxed{\eta_k}  &\alpha_k   \\
        \vdots      &\vdots &\vdots    \\
        x_m         &\eta_m  &\alpha_m   \\
        \hline
        &\delta &\frac{\alpha_k}{\eta_k}
    \end{array}
    &\implies&
    \begin{array}{c|cc}
        &y      &a         \\
        \hline
        x_1         &0  &\alpha_1-\eta_1\delta   \\
        \vdots      &\vdots &\vdots    \\
        y           &1  &\delta  \\
        \vdots      &\vdots &\vdots    \\
        x_m         &0  &\alpha_m-\eta_m\delta   \\
        \hline
        &&
    \end{array}
\end{eqnarray}
transzformációt kell végrehajtani.
Persze ugyanezt azt egyetlen $a$ vektor helyett kiszámolhatjuk például az $a,b,c$ vektorokra is.
Továbbra is az $y$-t cseréljük a generátorrendszerbe, így a
\begin{eqnarray*}
    \begin{array}{c|cccc}
        &y      &a        &b         &c\\
        \hline
        x_1         &\eta_1  &\alpha_1 &\beta_1 &\gamma_1 \\
        \vdots      &\vdots &\vdots    &\vdots  &\vdots\\
        x_k         &\boxed{\eta_k}  &\alpha_k&\beta_k&\gamma_k   \\
        \vdots      &\vdots &\vdots    &\vdots&\vdots\\
        x_m         &\eta_m  &\alpha_m   &\beta_m&\gamma_m\\
        \hline
        &\delta &\frac{\alpha_k}{\eta_k}&\frac{\beta_k}{\eta_k}&\frac{\gamma_k}{\eta_k}\\
    \end{array}
    &\implies&
    \begin{array}{c|cccc}
        &y      &a         &b        &c\\
        \hline
        x_1         &0  &\alpha_1-\eta_1\delta_a   &\beta_1-\eta_1\delta_b&\gamma_1-\eta_1\delta_c\\
        \vdots      &\vdots &\vdots&\vdots&\vdots    \\
        y           &1  &\delta_a&\delta_b&\delta_c  \\
        \vdots      &\vdots &\vdots &\vdots &\vdots    \\
        x_m         &0  &\alpha_m-\eta_m\delta_a&\beta_m-\eta_m\delta_b&\gamma_m-\eta_m\delta_c   \\
        \hline
        &&
    \end{array}
\end{eqnarray*}
a transzformációt hajtjuk végre.
Most arra figyeljünk, 
hogy végül is a keretben lévő $m\times 4$-es mátrixszal sorműveleteket hajtottunk végre:
\begin{itemize}
    \item 
        Eldöntöttük, hogy az $y,a,b,c$ vektorok közül az $y$-t cseréljük be a generátorrendszerbe,
        mégpedig a $k$-adik helyen. 
        Ezt jeleztük az $y$ oszlopa $k$-adik helyen lévő elemének bekeretezésével. 
        A keretben csak nem zéró szám lehet.
    \item 
        Első lépésként a keretezett számmal osztottuk a $k$-adik sort. Ez az új táblázat $k$-adik sora.
        Pusztán segítségképpen ugyanezt a sort mint egy számolási segédsort az első táblázat alá másoltuk.
    \item 
        Az új táblázat első sora az eredeti első sornak és a segédsor $\eta_1$-szeresének különbsége.
        A második sor az eredeti második sornak és a segédsor $\eta_2$-szeresének különbsége.
        Hasonlóan, $m\neq k$-ra az $m$-edik sor az eredeti $m$-edik sornak és a segédsor $\eta_m$-szeresének különbsége.
\end{itemize}

Látjuk tehát, hogy összesen két fajta sorművelettel alakítottuk a kiindulási mátrixot:
\begin{enumerate}
    \item Egy sort szoroztunk egy nem zérus számmal,
    \item Egy sorhoz hozzáadtuk egy másik sor számszorosát.
\end{enumerate}
Világos, hogy ez ugyanaz, mintha a feladat az lett volna, 
hogy a fenti két sorművelet használva az $y$ oszlopában minden számot el kell tüntetni, 
a $k$-adikat pedig $1$-re kell beállítani.
Ez a \emph{Gauss-Jordan--elimináció}.\index{Gauss-Jordan--elimináció}

\subsection{Homogén lineáris egyenletrendszer}
Az alábbi feladatot lineáris egyenletrendszernek nevezzük.
\[
    \begin{array}{rrlcl}
        a_{1,1}x_1+&\cdots&+a_{1,n}x_n&=& b_1\\
        a_{2,1}x_1+&\cdots&+a_{2,n}x_n&=&b_2\\
        &&&\vdots&\\
        a_{m,1}x_1+&\cdots&+a_{m,n}x_n&=& b_m
    \end{array}
\]
Itt az $n,m\in\mathbb{N}$, az $a_{i,j}\in\mathbb{F}$ testbeli számok előre adottak
minden $i=1,\dots,m$ és minden $j=1,\dots,n$ mellett.
Adottak még az egyenletek jobb oldalát képező $b_i\in\mathbb{F}$ számok.
A feladat megoldása annyit tesz, hogy keressük az $x_1,\dots,x_n$ ismeretlenek 
összes olyan értékét az $\mathbb{F}$ testből,
amelyre fenti egyenletek mind teljesülnek.
Ha a jobboldali számokra $b_i=0$ minden $i=1,\dots,n$ mellett,
akkor 
\emph{homogén lineáris egyenletrendszerről}%
\index{homogén lineáris egyenletrendszer} 
beszélünk,
egyébként a rendszert
\emph{inhomogénnek}%
\index{inhomogén lineáris egyenletrendszer}
mondjuk.
A fenti rendszer együtthatóiból álló
\[
    A=
    \begin{pmatrix}
        a_{1,1}&\cdots&a_{1,n}\\
        a_{2,1}&\cdots&a_{2,n}\\
        \vdots&\ddots&\vdots\\
        a_{m,1}&\cdots&a_{m,n}
    \end{pmatrix}
\]
mátrixot 
\emph{együttható-mátrixnak}%
\index{együttható-mátrix}
mondjuk.

Ha az egyenletek egyikét egy nemzérus számmal szorozzuk,
és az egyenletek egyikéhez hozzáadjuk egy másik egyenlet számszorosát,
akkor a megoldások nem változnak, 
azaz ekvivalens átalakítást hajtunk végre.

Egy Gauss-Jordan eliminációs\index{Gauss--Jordan-eliminácio}  lépésre tekinthetünk úgy is,
mint rögzített $i,j$ mellett a $j$-edik változó kiküszöbölésére
-- azaz eliminációjára -- 
valamennyi nem az $i$-edik sorból,
és az $i$-edik sorban e $j$-edik változó együtthatójának $1$-re állítására.
Valóban,
ha az $i$-edik sort osztjuk az $a_{i,j}\neq 0$ számmal, akkor e sor $j$-edik
eleme $1$-re változik.
Ha a $k$-adik ($k\neq i$) sorból kivonjuk az imént normált sor
$a_{k,j}$-szeresét, akkor az eredmény sor $j$-edik helyén zérust kapunk.
Ha ezt minden $k=1,\dots,m$ mellet kiszámoljuk, akkor éppen egy Gauss-Jordan
eliminációt hajtunk végre az $a_{i,j}$ pivot\footnote{sarok elem}\index{pivot elem} elem választásával.
Az eliminációs lépés hatására az $j$-edik változó az $i$-edik kivételével
minden más sorból eltűnt, és az $i$-edik sorban pontosan $1$ együtthatóval szerepel.

Ugyan ez egy táblázatban megfogalmazva. 
Itt a baloldali mátrix az együttható mátrix, amelynek
a $j$-edik oszlopa van külön kiemelve.
$L_1,\dots,L_m$ jelöli az együttható-mátrix sorait
A jobb oldali mátrix az elimináció eredménye, a megfelelő sorműveletekkel az egyes sorok
címkéiben:
\[
    \begin{array}{ccc|c}
        \cdots&a_{1,j}&\cdots&L_1\\
        &\vdots&       &\vdots\\
        \cdots&\boxed{a_{i,j}}&\cdots&L_i\\
        &\vdots&       &\vdots\\
        \cdots&a_{m,j}&\cdots&L_m
    \end{array}
    \implies
    \begin{array}{ccc|c}
        \cdots&0&\cdots&L_1-a_{1,j}\frac{1}{a_{i,j}}L_i\\
        &\vdots&       &\vdots\\
        \cdots&1&\cdots&\frac{1}{a_{i,j}}L_i\\
        &\vdots&       &\vdots\\
        \cdots&0&\cdots&L_m-a_{m,j}\frac{1}{a_{i,j}}L_i
    \end{array}
\]
Azt kell látnunk, hogy egy eliminációs lépés az $a_{i,j}$ pivot elem választásával pontosan ugyanazt eredményezi, 
mint az együttható-mátrix $j$-edik $\left[ A \right]^j$ oszlopának becserélése a generátorrendszer $i$-edik helyére. 

Az algoritmus célja, hogy
a generátorrendszerbe annyi oszlopot cseréljünk be amennyit csak tudunk, 
de persze egy már korábban becserélt vektort nem cserélünk ki egy újabb oszlopra.
Az algoritmus tehát akkor áll meg,
ha minden oszlopot becseréltünk, 
vagy van ugyan nem becserélt oszlop de annak minden koordinátája zérus az eredeti generátorrendszer nem cserélt helyein.
Ezt utóbbi mondatot egyszerűbben úgy fogalmazhatjuk, 
hogy ha a táblázat minden nem zérus sora egy becserélt vektorhoz tartozik,
akkor véget ér az algoritmus.

A lineáris egyenletrendszerek nyelvére átültetve ez azt jelenti, 
hogy az algoritmus célja a változók eliminálása,
de persze egy sorban csak egy eliminált változó szerepelhet.
Az algoritmus akkor áll meg, 
ha a táblázatnak minden nem zérus sorában van eliminált változó.

Az algoritmus utolsó táblájából a homogén lineáris egyenletrendszer általános megoldása könnyen leolvasható:
Dobjuk el a zérus sorokat, és foglalkozzunk a maradék táblázattal.
Minden sor egy és csak egy eliminált változót tartalmaz.
A többi változó, ha ilyen van egyáltalán egy nem eliminált változóhoz tartozik.
A nem eliminált változók tetszőleges értéket felvehetnek, 
majd minden sorban ebből már egyértelműen kifejezhető az eliminált változók értéke.

Innen érthető, 
hogy a szokásos szóhasználat szerint, az eliminált változókat sokszor
\emph{kötött változónak}\index{kotott@kötött változó}
nevezzük,
és a többi változót 
\emph{szabad változónak}\index{szabad változó}
mondjuk.
A következő tétel rendkívül fontos, még akkor is ha teljesen nyilvánvaló a most tárgyalt algoritmus alapján.
\begin{proposition}
    Ha egy homogén lineáris egyenletrendszerben több ismeretlen van mint egyenlet,
    akkor a rendszernek van nem zéró megoldása.
\end{proposition}
\begin{proof}
    Gauss-Jordan algoritmus\index{Gauss--Jordan-eliminácio}  legutolsó táblázatában
    a kötött változó száma legfeljebb a sorok száma az pedig szigorúan kisebb mint az összes ismeretlenek száma.
    Van tehát szabad változó, amelynek értéke tetszőleges lehet.
\end{proof}
Az teljesen nyilvánvaló, hogy a kötött változók száma és a szabad változók száma azonos a változók számával.
Ebben a pillanatban még nem látszik,
de később ki fog derülni, 
hogy a kötött így a szabad változók száma is független az algoritmus során választott pivot elemektől.

Érdemes a rendszer egy megoldásra úgy gondolni, mint egy $x\in\mathbb{F}^n$ oszlopvektorra,
amelynek $i$-edik koordinátája az $x_i$ változó aktuális értéke.
Ilyen módon az $x_1,\dots,x_n$ pontosan akkor elégíti ki a lineáris egyenletrendszert,
ha
\[
    Ax=b
\]
mátrixegyenlet teljesül, ahol a $b\in\mathbb{F}^n$ az a vektor melynek $i$-edik koordinátája $b_i$.

Érdemes itt konkrét példákkal szemléltetni a homogén lineáris egyenletrendszer általános megoldásának felírását:
\begin{enumerate}
    \item 
        Oldjuk meg a 
        \[
            \systeme{ x+4y+7z=0,
                2x+5y+8z=0,
                3x+6y+8z=0
            }
        \]
        homogén  lineáris egyenletrendszert.
        A Gauss-Jordan algoritmus\index{Gauss--Jordan-eliminácio}  lehet például a következő:
        \[
            \begin{array}{|rrr}
                a&b&c\\
                \hline
                \boxed{1}&4&7\\
                2&5&8\\
                3&6&8\\
                \hline
                \delta&4&7
            \end{array}
            \implies
            \begin{array}{r|rrr}
                &a&b&c\\
                \hline
                a &  1&4&7\\
                &  0&\boxed{-3}&-6\\
                &  0&-6&-13\\
                \hline
                &  0&\delta&2
            \end{array}
            \implies
            \begin{array}{r|rrr}
                &a&b&c\\
                \hline
                a &  1&0&-1\\
                b &  0&1&2\\
                &  0&0&\boxed{-1}\\
                \hline
                &  0&0&\delta
            \end{array}
            \implies
            \begin{array}{r|rrr}
                &a&b&c\\
                \hline
                a &  1&0&0\\
                b &  0&1&0\\
                c &  0&0&1
            \end{array}
        \]
        Ez azt jelenti, hogy az eredeti feladat ekvivalens az
        \[\systeme{x=0,y=0,z=0}\]
        feladattal, amelynek nyilvánvalóan csak a zéró vektor a megoldása.
    \item
        Tekintsük a következő feladatot:
        \[
            \begin{array}{rl}
                x_1+3x_2+4x_3+5x_4-x_5=&0\\
                -2x_1+x_2-x_3+4x_4-5x_5=&0\\
                2x_1+x_2+3x_3+3x_5=&0\\
                3x_1+x_2+4x_3-x_4+5x_5=&0
            \end{array}
        \]
        Az elimináció lehet a következő:
        \begin{multline*}
            \begin{array}{r|rrrrr}
                &a_1&a_2&a_3&a_4&a_5\\
                \hline
                &\boxed{1}&3&4&5&-1\\
                &-2&1&-1&4&-5\\
                &2&1&3&0&3\\
                &3&1&4&-1&5\\
                \hline
                &\delta&3&4&5&-1
            \end{array}
            \implies
            \begin{array}{r|rrrrr}
                &a_1&a_2&a_3&a_4&a_5\\
                \hline
                a_1&1&3&4&5&-1\\
                &0&\boxed{7}&7&14&-7\\
                &0&-5&-5&-10&5\\
                &0&-8&-8&-16&8\\
                \hline
                &0&\delta&1&2&-1
            \end{array}
            \implies
            \\
            \begin{array}{r|rrrrr}
                &a_1&a_2&a_3&a_4&a_5\\
                \hline
                a_1&1&0&1&-1&2\\
                a_2&0&1&1&2&-1\\
                &0&0&0&0&0\\
                &0&0&0&0&0
            \end{array}
        \end{multline*}
        Az eredeti egyenletrendszer tehát ekvivalens a következő egyenletrendszerrel:
        \[
            \begin{array}{rl}
                x_1+x_3-x_4+2x_5=&0\\
                x_2+x_3+2x_4-x_5=&0
            \end{array}
        \]
        Itt $x_1,x_2$ a kötött változók és $x_3,x_4,x_5$ a szabad változók.
        Ezek értéke tetszőleges lehet, mondjuk $x_3=s$, $x_4=r$, $x_5=t$ és ekkor 
        $x_1=-s+r-2t$ valamint $x_2=-s-2r+t$.
        Az általános megoldás ezért
        \[
            \left\{ 
                \begin{pmatrix}
                    -s+r-2t\\
                    -s-2r+t\\
                    s\\
                    r\\
                    t
                \end{pmatrix}
                :s,r,t\in\mathbb{R}
            \right\}
            =
            \left\{ s
                \begin{pmatrix}
                    -1\\-1\\1\\0\\0
                \end{pmatrix}
                +
                r
                \begin{pmatrix}
                    1\\-2\\0\\1\\0
                \end{pmatrix}
                +
                t
                \begin{pmatrix}
                    -2\\1\\0\\0\\1
                \end{pmatrix}
                :s,r,t\in\mathbb{R}
            \right\}.
        \]
        A szabad változók száma tehát $3$, 
        és a rendszer megoldáshalmaza a 
        \[
            \begin{pmatrix}
                -1&1&-2\\
                -1&-2&1\\
                1&0&0\\
                0&1&0\\
                0&0&1
            \end{pmatrix}
        \]
        mátrix oszlopai generált altér.
\end{enumerate}
Egy homogén lineáris egyenletrendszernek a zérus vektor mindig megoldása,
ezt nevezzük triviális megoldásnak.
A Gauss-Jordan eliminációs\index{Gauss--Jordan-eliminácio}  algoritmusból világos a következő gondolat.
A rendszernek pontosan akkor nincs nem triviális megoldása, 
ha nincs szabad változó, azaz minden változó kötött:
\begin{proposition}
    Egy $Ax=0$ homogén lineáris egyenletrendszernek pontosan akkor a triviális megoldás az egyetlen megoldása, 
    ha az eliminációs algoritmusban minden oszlop a generátorrendszerbe cserélhető.
\end{proposition}
\subsection{Mátrix faktorizáció}
Érdemes a Gauss-Jordan eliminációs\index{Gauss--Jordan-eliminácio}  algoritmust az egyenletrendszerek megoldásától függetlenül is szemlélni.
Ide másolom az előző feladat megoldásának táblázatát:
\[
    \begin{array}{r|rrrrr}
        &a_1&a_2&a_3&a_4&a_5\\
        \hline
        a_1&1&0&1&-1&2\\
        a_2&0&1&1&2&-1\\
        &0&0&0&0&0\\
        &0&0&0&0&0
    \end{array}
\]
Ez utolsó táblázat felfedi az eredeti $a_1,a_2,a_3,a_4,a_5$ vektorok közti kapcsolatot.
A táblázat értelmezése szerint
\begin{eqnarray*}
    a_3&=& a_1+a_2\\
    a_4&=& -a_1+2a_2\\
    a_5&=& 2a_1-a_2
\end{eqnarray*}
Ezt mátrixszorzásként interpretálva azt kapjuk, hogy
\[
    \begin{pmatrix}
        1&3&4&5&-1\\
        -2&1&-1&3&-5\\
        2&1&3&0&3\\
        3&1&4&-1&5
    \end{pmatrix}
    =
    \begin{pmatrix}
        1&3\\
        -2&1\\
        2&1\\
        3&1
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        1&0&1&-1&2\\
        0&1&1&2&-1
    \end{pmatrix}
\]
hiszen minden oszlop az $a_1,a_2$ oszlopok lineáris kombinációja.

\begin{definition}
    Legyen $A\in\mathbb{F}^{m\times n}$ mátrix.
    Az $A$ mátrix oszlop- (sor-) vektorterének nevezzük az $A$ oszlopai (sorai) generálta alterét az $\mathbb{F}^m$ ($\mathbb{F}^n$)
    vektortérnek.
\end{definition}
Az előző példa szerint az ottani $4\times 5$ méretű mátrix oszlopvektorterének az $\left\{ a_1,a_2 \right\}$ két elemű rendszer
egy generátorrendszere.
Az alábbi állítás a mátrix szorzás definíciójának következménye.
\begin{proposition}
    Legyen $A\in\mathbb{F}^{m\times n}$ egy nem zérus mátrix.
    \begin{enumerate}
        \item 
            Tegyük fel, hogy $A$ oszlopvektorterének létezik $r$-elemű generátorrendszere.
            Ekkor a generátorrendszer $r$ db vektorát a $B$ mátrix oszlopaiba rendezve egy $m\times r$ mátrixot kapunk.
            Ehhez a $B$ mátrixhoz létezik $C$ mátrix, amely $r\times n$ méretű és 
            \(
            A=B\cdot C.
            \)
        \item
            Most azt tegyük fel, hogy $A$ sorvektorterének létezik $r$-elemű generátorrendszere.
            Ekkor a generátorrendszer $r$ db sorát a $C$ mátrix soraiba rendezve egy $r\times n$ mátrixot kapunk.
            Ehhez a $C$ mátrixhoz létezik $B$ mátrix, amely $m\times r$ méretű és 
            \(
            A=B\cdot C.
            \)\qedhere
    \end{enumerate}
\end{proposition}


\subsection{Inhomogén lineáris egyenletrendszer}
Láttuk, hogy egy $m$ egyenletet és $n$ ismeretlent tartalmazó lineáris egyenletrendszer ekvivalens az 
\[
    Ax=b
\]
feladattal, ahol $A\in\mathbb{F}^{m\times n}$ az együttható-mátrix és $b\in\mathbb{F}^m$
a jobboldalakból alkotott vektor.
A következő egyszerű észrevétel szerint ha az inhomogén rendszernek találunk valahogyan egyetlen megoldását és ismerjük a homogén rendszer általános megoldását,
akkor már az inhomogén rendszer általános megoldása is egyszerűen felírható.
\begin{proposition}
    A fenti jelölések megtartása mellett
    legyen $x_0\in\mathbb{F}^m$ egy tetszőlegesen rögzített megoldása az $Ax=b$ inhomogén lineáris egyenletrendszernek.
    Ekkor
    \[
        \left\{ x:Ax=b \right\}
        =
        \left\{ x_0+z:Az=0 \right\},
    \]
    azaz az inhomogén rendszer megoldáshalmaza, 
    azonos a homogén rendszer megoldás halmazának egy partikuláris megoldással való 
    eltoltjával.
\end{proposition}
\begin{proof}
    Legyen először $x$ egy megoldás, azaz $Ax=b$. 
    Mivel $x_0$ is egy megoldás, ezért $Ax_{0}=b$ is teljesül.
    Persze $x=x_0+\left( x-x_0 \right)=x_0+z$, ahol $z=x-x_0$ egy olyan vektor,
    amelyre $Az=Ax-Ax_0=b-b=0$.

    Másodszor tekintsünk egy $x=x_0+z$ alakú vektort, ahol $Az=0$.
    Ekkor persze $Ax=Ax_0+Az=b+0=b$.
\end{proof}
Egy nem nyilvánvaló következmény, 
hogy akármelyik partikuláris megoldással toljuk is el az
inhomogén rendszer megoldását mindig ugyan azt a halmazt kapjuk, 
emiatt mindegy melyik partikuláris megoldást rögzítettük.

Persze a kérdés, hogy hogyan találunk egyáltalán partikuláris megoldást.
Máshogyan fogalmazva: mikor van egyáltalán megoldása egy inhomogén rendszernek?
\begin{proposition}
    Az $Ax=b$ inhomogén lineáris egyenletrendszernek pontosan akkor van megoldása, 
    ha $b$ előáll mint $A$ oszlopainak valamilyen lineáris kombinációja, 
    azaz a $b$ vektor az $A$ mátrix oszlopvektorteréhez tartozik.
\end{proposition}
Konkrétan adott $A$ és $b$ mellett Gauss-Jordan algoritmussal\index{Gauss--Jordan-eliminácio}  el tudjuk dönteni, hogy
$b$ vektor eleme-e az $A$ mátrix oszlop vektorterének.
Egészítsük ki az $A$ mátrixot az utolsó oszlopában a $b$ vektorral.
Most hajtsuk végre a Gauss-Jordan eliminációt, de úgy hogy pivot-elemet,
csak az első n oszlopból válasszunk, ugyanúgy mintha csak a homogén rendszert oldanánk meg.
Persze minden egyes eliminációs lépésben számoljuk ki az utolsó oszlopvektor elemeit is.
Az algoritmus úgy ér véget, 
hogy az $A$ mátrixnak megfelelő részben minden nem zérus sor tartalmaz eliminált változót.

Ha van olyan sor ahol az első $n$ elem zérus, de az utolsó elem nem zérus, 
akkor nyilván nincs megoldás.
Minden egyéb esetben van megoldás, 
hiszen a csupa zéró sorokhoz tartozó egyenletek az ismeretlenek minden értéke mellett fennállnak,
de maradék a nem zérus sorok esetén a kötött változót
kifejezhetjük a szabad változók tetszőleges -- mondjuk zérus -- értéke mellett.

Az inhomogén rendszernek tehát pontosan akkor van megoldása, 
ha az utolsó oszlop minden nem zérus eleme olyan egyenlethez tartozik, 
ahol szerepel eliminált változó.

Ha a fenti eliminációra, mint a generátorrendszer csere lemma alkalmazására gondolunk,
akkor azt kapjuk, hogy a megoldhatóság szükséges és elegendő feltétele,
hogy a végső táblában az utolsó, a $b$-hez tartozó oszlopnak csak olyan helyeken lehetnek nem zérus tagjai,
amely helyek korábban már a generátorrendszerbe becserélt oszlop vektorokhoz tartoznak,
ami persze nem jelent többet, mint hogy $b$ eleme az $A$ oszlopvektorterének.
Az algoritmus annyiban több, mint a korábban megértett feltétel, 
hogy a megoldható esetben rögtön kapunk egy partikuláris megoldást is.

Nézzünk egy példát.
\[
    \begin{array}{rl}
        x_1+3x_2+4x_3+5x_4-x_5=&6  \\
        -2x_1+x_2-x_3+4x_4-5x_5=&-5  \\
        2x_1+x_2+3x_3+3x_5=&7  \\
        3x_1+x_2+4x_3-x_4+5x_5=&10 
    \end{array}
\]
A Gauss-Jordan algoritmus:\index{Gauss--Jordan-eliminácio} 
\begin{multline*}
    \begin{array}{c|rrrrr|r}
        &a_1&a_2&a_3&a_4&a_5&b\\
        \hline
        &\boxed{1}&3&4&5&-1&6\\
        &-2&1&-1&4&-5&-5\\
        &2&1&3&0&3&7\\
        &3&1&4&-1&5&10\\
        \hline
        &\delta&3&4&5&-1&6
    \end{array}
    \implies
    \begin{array}{c|rrrrr|r}
        &a_1&a_2&a_3&a_4&a_5&b\\
        \hline
        a_1&1&3&4&5&-1&6\\
        &0&7&7&14&-7&7\\
        &0&-5&-5&-10&\boxed{5}&-5\\
        &0&-8&-8&-16&8&-8\\
        \hline
        &0&-1&-1&-2&\delta&-1
    \end{array}
    \implies
    \\
    \begin{array}{c|rrrrr|r}
        &a_1&a_2&a_3&a_4&a_5&b\\
        \hline
        a_1&1&2&3&3&0&5\\
        &0&0&0&0&0&0\\
        a_5&0&-1&-1&-2&1&-1\\
        &0&0&0&0&0&0\\
    \end{array}
\end{multline*}
a rendszernek tehát van megoldása például $x_1=5$, $x_5=-1$, $x_2=0$, $x_3=0$, $x_4=0$.
Az általános megoldás tehát
\[
    \left\{
        \begin{pmatrix}
            5\\0\\0\\0\\-1
        \end{pmatrix}
        +s
        \begin{pmatrix}
            -2\\1\\0\\0\\1
        \end{pmatrix}
        +t
        \begin{pmatrix}
            -3\\0\\1\\0\\1
        \end{pmatrix}
        +r
        \begin{pmatrix}
            -3\\0\\0\\1\\2
        \end{pmatrix}
        :s,t,r\in\mathbb{R}
    \right\}.
\]
\begin{proposition}
    Az $Ax=b$ inhomogén lineáris egyenletrendszernek pontosan akkor van egyetlen 
    megoldása, ha $b$ az $A$ képteréhez tartozik és $A$ minden oszlopa a Gauss-Jordan eliminációs algoritmussal\index{Gauss--Jordan-eliminácio}  a generátorrendszerbe cserélhető.
\end{proposition}
Ebben az esetben persze az algoritmusból adódó partikuláris megoldás az egyetlen megoldás.

A következő állításban azt gondoljuk meg, hogy ha kevesebb ismeretlene mint sora van egy rendszernek, akkor megválasztható olyan jobb oldali vektor, amellyel az inhomogén lineáris egyenletrendszernek nincs megoldása.
\begin{proposition}
    Legyen $A\in\mathbb{F}^{m\times r}$ olyan mátrix, ahol $r<m$.
    Ekkor létezik $b\in\mathbb{F}^m$ vektor, 
    amellyel felírt $Ax=b$ inhomogén lineáris egyenletrendszernek már nincs megoldása.
\end{proposition}
\begin{proof}
    Tegyük fel tehát, hogy kevesebb ismeretlen van mint sor.
    Mikor az $A$ mátrixra alkalmazott Gauss-Jordan algoritmus\index{Gauss--Jordan-eliminácio}  véget ér, 
    akkor minden nem zérus sorban pontosan egy eliminált ismeretlen van.
    Eszerint minden egyes sor vagy zéró sor, 
    vagy van benne eliminált ismeretlen.
    Mivel kevesebb ismeretlen van mint sor, 
    ezért legalább egy zéró sornak lennie kell.
    Ha a $k$-adik sor csak nullákat tartalmaz, akkor az a $b\in\mathbb{F}^m$ vektor,
    amelynek a $k$-adik koordinátája nem zérus, de minden más koordináta zérus meg is felel.
\end{proof}
\subsection{Inverz mátrix}
Gondoljunk arra, hogy milyen módszerrel automatizálhatnánk a Gauss-Jordan algoritmust.\index{Gauss--Jordan-eliminácio} 
Mivel sorműveletekről van szó, természetes gondolat, hogy az $A\in\mathbb{F}^{m\times n}$
mátrix transzformálásához $m\times m$ méretű mátrixokat használjunk,
amelyekkel balról szorozzuk $A$-t.
Valóban, ha $m\times m$ méretű identitás mátrix 
\begin{enumerate}
    \item 
        $i$-edik sorát szorozzuk egy $\delta$ számmal,
        akkor egy olyan mátrixot kapunk, 
        amellyel való balszorzása $A$-nak éppen az $A$ mátrix 
        $i$-edik sorát szorozza $\delta$-val.
    \item
        $k$-adik sorából kivonjuk az $i$-edik sor $\delta$-szorosát,
        akkor egy olyan mátrixot kapunk, 
        amellyel való balszorzása $A$-nak éppen az $A$ mátrix 
        $k$-adik sorából vonja ki az $i$-edik sor $\delta$-szorosát.
    \item
        $k$-adik és $j$-edik sorát felcseréljük,
        akkor egy olyan mátrixot kapunk, 
        amellyel való balszorzása $A$-nak éppen az $A$ mátrix 
        $k$-adik és $j$-edik sorát cseréli fel.
\end{enumerate}

Tekintsünk most, egy $m\times m$ méretű négyzetes mátrixot.
Tegyük fel, hogy a mátrix minden oszlopát a generátorrendszerbe cserélhetjük
a szokásos Gauss-Jordan eliminációs algoritmus\index{Gauss--Jordan-eliminácio}  során.
Láttuk korábban, hogy ez azt jelenti, hogy az $Ax=b$ inhomogén lineáris egyenletrendszer
minden $b$ vektor mellett egyértelműen megoldható.

Legyen $\left\{ e_1,\dots,e_m \right\}$ az $\mathbb{F}^m$ szokásos generátorrendszere,
azaz $e_k$-nak éppen a $k$-adik koordinátája 1, a többi zérus.
Ha $B\in\mathbb{F}^m$ az a mátrix, 
amelynek $k$-edik oszlopa az $Ax=e_k$ inhomogén lineáris egyenletrendszer egyetlen $x_k\in\mathbb{F}^m$ megoldása,
akkor a mátrix szorzás definíciója szerint 
\[
    AB=I
\]
teljesül.
A $B$ mátrixot tehát $m$ darab $m$ lépéses Gauss-Jordan eliminációval\index{Gauss--Jordan-eliminácio}  meg tudnánk határozni.

Egyszerűbb, 
ha egyetlen eliminációs algoritmus használunk, de arra az $m\times 2m$ méretű mátrixra,
amelyet úgy kapunk, hogy az a $A$ mátrix jobboldalához illesztjük az $I\in\mathbb{F}^{m\times m}$ identitás mátrixot.
Az elimináció során, 
$A$ minden sorát a generátorrendszerbe cseréljük, de persze a jobbra illesztett identitás
mátrix sorait is transzformáljuk.
Feltevésünk szerint $m$ lépés után áll le az algoritmus.
Ekkor egy  olyan táblázat van előttünk, amelynek első $m$ oszlopa egy olyan négyzetes
mátrixot alkot, amelynek minden oszlopa és minden sora egyetlen 1-est tartalmaz a többi elem zérus. 
Vegyük észre, 
hogy egy ilyen mátrix a sorok felcserélésével az identitás mátrixszá transzformálható.

Az $Ax=e_k$ egyenlet megoldása a jobboldali $m\times m$ mátrix $k$-adik oszlopából olvasható le. 
Ha például az első sor $j$-edik elemén van 1-es, 
akkor az azt jelenti, hogy az első sorba elimináltuk az $j$-edik ismeretlent, 
tehát a megoldás $j$-edik koordinátáját tartalmazza az $m+k$-adik oszlop első eleme.

Ha tehát úgy cseréljük fel a táblázat sorait, 
hogy a bal oldali $m\times m$ méretű mátrix váljon az identitás mátrixszá, 
akkor az $Ax=e_k$ egyenlet egyetlen partikuláris megoldása jelentkezik az egész táblázat
$m+k$ adik oszlopában.
Mivel a keresett $B$ mátrixnak éppen ez a $k$-adik oszlopa,
ezért maga a $B$ mátrix áll a táblázat jobboldali $m\times m$ méretű részén.

Most meggondoljuk, hogy $BA=I$ is fennáll.
Az iménti algoritmussal az $[A|I]$ mátrixot transzformáltuk a $[I|B]$ mátrixszá.
A transzformáció során a Gauss-Jordan eliminációt használtuk,\index{Gauss--Jordan-eliminácio}  amely a szakasz elején említett 1) és 2) típusú mátrixokkal mint balszorzásokkal hajtható végre.
Az algoritmus végén még sorokat is felcseréltünk, ami egy 3) típusú balszorzást jelent.
Ha az 1) vagy 2) vagy 3) típusú mátrixot \emph{elemi mátrixnak}\index{elemi mátrix}
nevezünk, 
akkor úgy fogalmazhatunk, 
hogy van véges sok $P_1,\dots,P_r\in\mathbb{F}^{m\times m}$ elemi mátrix, amelyre
\(
(P_r\cdots P_1)\left[ A|I \right]=\left[ I|B \right].
\)
Ekkor persze $(P_r\cdots P_1)A=I$ és $P_r\cdots P_1=B$,
amiből már következik, hogy
\[
    BA=I.
\]

\begin{definition}[nem szinguláris mátrix]
    Egy $A\in\mathbb{F}^{m\times m}$ négyzetes mátrixot 
    \emph{invertálhatónak}\index{invertálható mátrix} vagy 
    \emph{regulárisnak} vagy \emph{nem szingulárisnak} nevezünk,
    ha létezik $B\in\mathbb{F}^{m\times m}$ négyzetes mátrix, 
    amelyre
    \[
        AB=BA=I
    \]
    Mivel egy egységelemes gyűrűben ha van inverz, akkor csak egyetlen egy van,
    ezért adott $A$ invertálható mátrixhoz csak egy $B$ mátrix van, amely kielégíti a fenti definíciót.
    Ezt a $B$ mátrixot nevezzük az $A$ inverzének és $A^{-1}=B$ módon jelöljük.
\end{definition}

\begin{proposition}
    Legyen $A\in\mathbb{F}^{m\times m}$ mátrix. Az alábbi feltevések ekvivalensek:
    \begin{enumerate}
        \item
            $A$ invertálható.
        \item 
            Létezik $B\in\mathbb{F}^{m\times m}$ mátrix, amelyre
            \begin{math}
                AB=I.
            \end{math}
        \item 
            Gauss-Jordan eliminációval\index{Gauss--Jordan-eliminácio}  az $A$ minden oszlopa a generátorrendszerbe cserélhető.
    \end{enumerate}
    Ha a fenti feltevések egyike (ergo mindegyike) fennáll, 
    akkor az 2)-ben szereplő $B$ mátrixból csak egy van, 
    mégpedig az $A^{-1}$ inverz mátrix.
\end{proposition}
\begin{proof}
    Körben igazolunk:
    \begin{description}
        \item[$1.\Rightarrow 2.$] Nyilvánvaló.
        \item[$2.\Rightarrow 3.$] 
            Tegyük fel -- indirekt --, hogy már $r$ transzformáció után a Gauss-Jordan algoritmus megáll, ahol $r<m$.
            Ez azt jelenti, hogy az $A$ oszlop-vektorterének van $r$ elemű generátorrendszere.
            E generátorrendszer vektorait mint oszlopokat egy $A_1$ mátrixba téve egy $m\times r$ mátrixot kapunk.
            Mivel az oszlopok lineáris burkában az $A$ valamennyi oszlopa szerepel,
            van olyan $r\times m$ méretű $A_2$ mátrix, amelyre 
            \(
            A_1A_2=A.
            \)
            Azt kaptuk tehát, hogy
            \[
                A_1\left( A_2B \right)=\left( A_1A_2 \right)B=AB=I.
            \]
            Ebből az következik,
            hogy tetszőleges $b\in\mathbb{F}^{m}$ vektor mellett az
            \begin{math}
                x=A_2Bb\in\mathbb{F}^{r}
            \end{math}
            vektor megoldása az $A_1x=b$ inhomogén lineáris egyenletrendszernek,
            ami ellentmondás, hiszen $A_1$ mátrixnak kevesebb oszlopa van mint sora.
        \item[$3.\Rightarrow 1.$] Éppen ezt igazoltuk a szakasz elején.
    \end{description}
    Ha a feltevések fennállnak, akkor
    \[
        A^{-1}=A^{-1}I=A^{-1}\left( AB \right)=\left( A^{-1}A \right)B=IB=B.\qedhere
    \]
\end{proof}
Ha tehát $A$ és $B$ négyzetes mátrixok, amelyekre $AB=I$,
akkor 2) szerint $A$ invertálható, és $A^{-1}=B$, ezért 
$BA=A^{-1}A=I$ is fennáll.
Ahogyan azt a mátrixok bevezetésekor \apageref{pg:kommutal}. oldalon megígértük,
most megmutattuk azt, hogy a négyzetes mátrixok gyűrűjében ha két mátrix szorzata a gyűrű
egységeleme, akkor ezek a mátrixok kommutálnak.\index{kommutáló mátrixok}\index{kommutál}

Egy konkrét példa megoldásával ismételjük át a szakasz elején megértett algoritmust.
A feladat, hogy keressük meg az 
\(
A=
\begin{pmatrix}
    1&1&1&0\\
    0&3&1&2\\
    2&3&1&0\\
    1&0&2&1
\end{pmatrix}
\)
mátrix inverzét!
Az inverz létezésének szükséges és elegendő feltétele, hogy 4 lépést tudjunk végrehajtani az eliminációs algoritmusban.
Egy lehetséges megoldás a következő:
\begin{multline*}
    \begin{array}{cccc|cccc}
        \boxed{1}&1&1&0 & 1&0&0&0\\
        0&3&1&2 & 0&1&0&0\\
        2&3&1&0 & 0&0&1&0\\
        1&0&2&1 & 0&0&0&1\\
        \hline
        \delta&1&1&0&1&0&0&0
    \end{array}\Rightarrow
    \begin{array}{cccc|cccc}
        1&1&1&0 & 1&0&0&0\\
        0&3&1&2 & 0&1&0&0\\
        0&1&-1&0 & -2&0&1&0\\
        0&-1&1&\boxed{1} & -1&0&0&1\\
        \hline
        0&-1&1&\delta&-1&0&0&1
    \end{array}\Rightarrow
    \\
    \begin{array}{cccc|cccc}
        1&1&1&0 & 1&0&0&0\\
        0&5&\boxed{-1}&0 & 2&1&0&-2\\
        0&1&-1&0 & -2&0&1&0\\
        0&-1&1&1 & -1&0&0&1\\
        \hline
        0&-5&\delta&0&-2&-1&0&2
    \end{array}\Rightarrow
    \begin{array}{cccc|cccc}
        1&6&0&0 & 3&1&0&-2\\
        0&-5&1&0 & -2&-1&0&2\\
        0&\boxed{-4}&0&0 & -4&-1&1&2\\
        0&4&0&1 & 1&1&0&-1\\
        \hline
        0&\delta&0&0&1&\frac{1}{4}&-\frac{1}{4}&-\frac{1}{2}
    \end{array}\Rightarrow
    \\
    \begin{array}{cccc|cccc}
        1&0&0&0 & -3&-\frac{1}{2}&\frac{3}{2}&1\\
        0&0&1&0 & 3&\frac{1}{4}&-\frac{5}{4}&-\frac{1}{2}\\
        0&1&0&0 & 1&\frac{1}{4}&-\frac{1}{4}&-\frac{1}{2}\\
        0&0&0&1 & -3&0&1&1
    \end{array}\Rightarrow
    \begin{array}{cccc|cccc}
        1&0&0&0 & -3&-\frac{1}{2}&\frac{3}{2}&1\\
        0&1&0&0 & 1&\frac{1}{4}&-\frac{1}{4}&-\frac{1}{2}\\
        0&0&1&0 & 3&\frac{1}{4}&-\frac{5}{4}&-\frac{1}{2}\\
        0&0&0&1 & -3&0&1&1
    \end{array}
\end{multline*}
Kaptuk hát, hogy az $A$ mátrix inverze az
\begin{math}
    A^{-1}=\frac{1}{4}
    \begin{pmatrix}
        -12&-2&6&4\\
        4&1&-1&-2\\
        12&1&-5&-2\\
        -12&0&4&4
    \end{pmatrix}
\end{math}
mátrix.

Picit kevesebb helyet foglal az algoritmus, ha lusták vagyunk a generátorrendszerbe már becserélt 
oszlopok kiírására. 
Ekkor érdemes kiírni a sorok és oszlopok címkéjét, nehogy eltévedjünk.
Az előző feladat így alakul:
\begin{multline*}
    \begin{array}{r|cccc|cccc}
        &a_1&a_2&a_3&a_4&e_1&e_2&e_3&e_4\\
        \hline
        e_1&\boxed{1}&1&1&0 & 1&0&0&0\\
        e_2&0&3&1&2 & 0&1&0&0\\
        e_3&2&3&1&0 & 0&0&1&0\\
        e_4&1&0&2&1 & 0&0&0&1\\
        \hline
        &\delta&1&1&0&1&0&0&0
    \end{array}\Rightarrow
    \begin{array}{r|ccc|cccc}
        &a_2&a_3&a_4&e_1&e_2&e_3&e_4\\
        \hline
        a_1&1&1&0 & 1&0&0&0\\
        e_2&3&1&2 & 0&1&0&0\\
        e_3&1&-1&0 & -2&0&1&0\\
        e_4&-1&1&\boxed{1} & -1&0&0&1\\
        \hline
        &-1&1&\delta&-1&0&0&1
    \end{array}\Rightarrow
    \\
    \begin{array}{r|cc|cccc}
        &a_2&a_3&e_1&e_2&e_3&e_4\\
        \hline
        a_1& 1& 1&  1& 0& 0& 0\\
        e_2& 5& \boxed{-1}&  2& 1& 0& -2\\
        e_3& 1&-1& -2& 0& 1& 0\\
        a_4&-1& 1& -1& 0& 0& 1\\
        \hline
        &-5&\delta&-2&-1&0&2
    \end{array}\Rightarrow
    \begin{array}{r|c|cccc}
        &a_2&e_1&e_2&e_3&e_4\\
        \hline
        a_1&           6&   3& 1& 0& -2\\
        a_3&          -5&  -2&-1& 0&  2\\
        e_3&\boxed{-4}  &  -4&-1& 1&  2\\
        a_4&           4&   1& 1& 0& -1\\
        \hline
        &\delta      &   1&\frac{1}{4}&-\frac{1}{4}&-\frac{1}{2}
    \end{array}\Rightarrow
    \\
    \begin{array}{r|cccc}
        &e_1&e_2&e_3&e_4\\
        \hline
        a_1 & -3&-\frac{1}{2}&\frac{3}{2}&1\\
        a_3 & 3&\frac{1}{4}&-\frac{5}{4}&-\frac{1}{2}\\
        a_2 & 1&\frac{1}{4}&-\frac{1}{4}&-\frac{1}{2}\\
        a_4 & -3&0&1&1
    \end{array}\Rightarrow
    \begin{array}{r|cccc}
        &e_1&e_2&e_3&e_4\\
        \hline
        a_1 & -3&-\frac{1}{2}&\frac{3}{2}&1\\
        a_2 & 1&\frac{1}{4}&-\frac{1}{4}&-\frac{1}{2}\\
        a_3 & 3&\frac{1}{4}&-\frac{5}{4}&-\frac{1}{2}\\
        a_4 & -3&0&1&1
    \end{array}
\end{multline*}

\section{Lineárisan független rendszerek}
\begin{definition}[lineárisan összefüggő vektorrendszer]\index{lineárisan összefüggő rendszer}
    Egy véges $\left\{ y_1,\dots,y_n \right\}$ vektorrendszert \emph{lineárisan összefüggőnek}
    mondunk, ha van olyan vektora, amely kifejezhető a többi vektor lineáris kombinációjaként.
\end{definition}
Úgy is fogalmazhatnánk, hogy az $\left\{ y_1,\dots,y_n \right\}$ rendszer pontosan akkor
lineárisan összefüggő, ha létezik $1\leq k\leq n$ index, amelyre
\[
    y_k\in\lin\left\{ y_1,\dots,y_{k-1},y_{k+1},\dots,y_n \right\}.
\]
Nyilvánvaló példa, hogy ha a vektorrendszer a $0$ vektort tartalmazza, 
akkor lineárisan összefüggő. 
Hasonlóan, ha ugyanaz a vektor többször szerepel a vektorrendszerben, a rendszer szintén összefüggő.
Olyan vektorrendszerre, amelyre nem igaz a lineárisan összefüggőség definíciója példa egy egyedüli nem zérus vektor,
de az $\left\{  \right\}$ üres rendszer is.

\begin{proposition}
    Legyen $\left\{ y_1,\dots,y_n \right\}$ vektorrendszer rögzítve a $V$ vektortérben.
    A vektorrendszerre tett alábbi feltevések egymással ekvivalensek.
    \begin{enumerate}
        \item Lineárisan összefüggő;
        \item Van olyan elem a vektortérben, amely nem csak egyféleképpen áll elő mint az $y_1,\dots,y_n$
            vektorok lineáris kombinációja,\\
            azaz formálisabban:
            létezik z\in V, amelyre $z=\sum_{j=1}^n\xi_jy_j$ és $z=\sum_{j=1}^n\eta_jy_j$
            és létezik $1\leq k\leq n$, amelyre $\xi_k\neq\eta_k$.
        \item Vannak olyan nem mind zérus $\alpha_1,\dots,\alpha_n$ skalárok, amelyekkel
            \[
                \sum_{j=1}^n\alpha_jy_j=0.\qedhere
            \]
    \end{enumerate}
\end{proposition}
\begin{proof}
    Körben bizonyítunk.
    \begin{description}
        \item[$1.\Rightarrow 2.$] 
            Tegyük fel, hogy $y_k=\sum_{j=1}^{k-1}\eta_jy_j+\sum_{j=k+1}^n\eta_jy_j$.
            Ekkor az alábbi együttható rendszerek
            \[
                \left( \alpha_1,\dots,\alpha_{k-1},0,\alpha_{k+1},\dots,\alpha_n \right)
                \qquad
                \left( 0,\dots,0,1,0,\dots,0 \right)
            \]
            a $k$-adik helyen biztosan különböznek, 
            hiszen $0\neq 1$, 
            és mind a két együttható rendszerrel képzett lineáris kombináció ugyanazt az $y_k$ vektort eredményezi.
        \item[$2.\Rightarrow 3.$]
            Világos, hogy 
            \[
                0=z-z=
                \sum_{j=1}^n\left( \xi_j-\eta_j \right)y_j
            \]
            és a $k$-adik skalár nem zérus.
        \item[$3.\Rightarrow 1.$]
            Tegyük fel most, hogy 
            \(
            \sum_{j=1}^n\alpha_jy_j=0,
            \)
            és, hogy $\alpha_k\neq 0.$
            Ekkor 
            \[
                y_k=\sum_{\substack{=1\\j\neq k}}^n-\frac{1}{\alpha_k}\alpha_jy_j
            \]
            azaz a $k$-adik vektor tekinthető mint a többi vektor valamely lineáris kombinációja.
    \end{description}
    Ezt kellett belátni. 
\end{proof}
Fontos észrevétel a következő.
\begin{proposition}
    Minden, 
    valamely lineárisan összefüggő vektorrendszert tartalmazó vektorrendszer maga is lineárisan összefüggő.
\end{proposition}
\begin{definition}
    Egy nem feltétlen véges vektorrendszert lineárisan összefüggőnek nevezünk,
    ha van véges részrendszere, amely lineárisan összefüggő.
\end{definition}
\begin{definition}[lineárisan független vektorrendszer]\index{lineárisan független rendszer}
    Egy vektorrendszer \emph{lineárisan független}, ha nem lineárisan összefüggő.
\end{definition}
Így egy nem véges vektorrendszer akkor lineárisan független, ha minden véges részrendszere is az.
Egy véges vektorrendszer lineárisan függetlenségét, pedig a következő egymással ekvivalens állítások karakterizálják.
\begin{proposition}
    Legyen $\left\{ y_1,\dots,y_n \right\}$ vektorrendszer rögzítve a $V$ vektortérben.
    A vektorrendszerre tett alábbi feltevések egymással ekvivalensek.
    \begin{enumerate}
        \item Lineárisan független;
        \item A vektorrendszer lineáris burkában minden elem egyetlen egyféleképpen áll elő,
            mint az $y_1,\dots,y_n$ vektorok lineáris kombinációja.
        \item Az $y_1,\dots,y_n$ vektoroknak csak a \emph{triviális lineáris kombinációja}\index{triviális lineáris kombináció} zérus,
            azaz
            \[
                \sum_{j=1}^n\alpha_jy_j=0\text{ esetén }\alpha_1=\alpha_2=\dots=\alpha_n=0.\qedhere
            \]
    \end{enumerate}
\end{proposition}
\begin{proof}
    Nyilvánvaló a lineáris összefüggés karakterizációjából.
\end{proof}
A lehető legszűkebb lineárisan független rendszer az $\left\{  \right\}$ üres vektorrendszer,
amelynek egyetlen eleme sincs.
A lehető legbővebb generátorrendszer az egész vektortér. 
Ennél sokkal érdekesebb és fontosabb a lineárisan független rendszerek közül a lehető legbővebbet keresni,
és generátorrendszerek közül a lehető legszűkebbet keresni.
\begin{definition}[maximális lineárisan független-- és minimális generátorrendszer]\index{maximális lineárisan független rendszer}\index{minimális generátorrendszer}
    Egy lineárisan független rendszert \emph{maximális lineárisan független rendszernek} nevezünk,
    ha nem lehet bővíteni úgy, hogy lineárisan független maradjon.

    Egy generátorrendszert \emph{minimális generátorrendszernek} mondunk, ha nem lehet szűkíteni úgy,
    hogy generátorrendszer maradjon.
\end{definition}
\begin{proposition}
    Legyen $\left\{ x_1,\dots,x_m \right\}$ egy vektorrendszere a $V$ vektortérnek.
    Az alábbi feltevések ekvivalensek.
    \begin{enumerate}
        \item A vektorrendszer maximális lineárisan független rendszer.
        \item A vektorrendszer egyszerre lineárisan független és generátorrendszer.
        \item A vektorrendszer minimális generátorrendszer.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Az alábbi lépéseket követjük.
    \begin{itemize}
        \item[1.\Rightarrow 2.]
            Ha a vektorrendszer nem lenne generátorrendszer is,
            akkor a lineáris burkán kívül lenne egy vektor.
            Ezt a vektorrendszerhez illesztve, a vektorrendszer egy valódi lineárisan független bővítését kapjuk,
            ami ellentmond a maximalitás feltételének.
        \item[3.\Rightarrow 2.]
            Ha a vektorrendszer egyik eleme a többi elem lineáris kombinációja,
            akkor azt az elemet elhagyva is generátorrendszert kapunk,
            ami ellentmond a minimalitás feltételének.
        \item[2.\Rightarrow 1.]
            Ha nem lenne maximális a lineárisan független tulajdonságra nézve,
            akkor létezne egy vektor a lineáris burkán kívül is,
            ami ellentmond a generátorrendszer tulajdonságnak.
        \item[2.\Rightarrow 3.]
            Mivel a vektorrendszer egyik eleme, sincs a többi lineáris burkában,
            ezért egyetlen elemet sem hagyhatunk el a generátorrendszer tulajdonság megtartásával,
            ami azt jelenti, hogy ez egy minimális generátorrendszer.\qedhere
    \end{itemize}
\end{proof}
Egy a zéró vektort tartalmazó vektorrendszer persze lineárisan összefüggő, 
és egy nem zérus vektorból álló egyelemű vektorrendszer lineárisan független.
A következő állítás sokszor teszi kényelmessé a gondolatmenetünket. 
\begin{proposition}
    Legyen $\left\{ y_1,\dots,y_n \right\}$ egy olyan legalább két elemű vektorrendszer,
    amelynek első eleme nem a zérus vektor, tehát $y_1\neq 0$.
    A vektorrendszer pontosan akkor lineárisan összefüggő,
    ha létezik olyan eleme, 
    amely pusztán az előző elemek lineárisan kombinációja.

    Formálisabban: akkor és csak akkor, 
    ha 
    $\exists k\quad 2\leq k\leq n : y_k\in\lin\left\{ y_1,\dots,y_{k-1} \right\}$
\end{proposition}
\begin{proof}
    Tegyük fel, hogy a vektorrendszer lineárisan összefüggő.
    Ekkor van olyan a zérus vektort eredményező lineáris kombinációja
    \(
    \alpha_1y_1+\dots+\alpha_ny_n=0,
    \)
    ahol nem az összes együttható nulla.
    Legyen $k$ a lineáris kombinációban a legnagyobb nem nulla együttható indexe.
    Világos, hogy $k\neq 1$, 
    hiszen $y_1\neq 0$.
    Persze a $k$ feletti együtthatók mind nullák,
    emiatt
    \[
        \alpha_1y_1+\dots+\alpha_ky_k=0.
    \]
    Itt már $\alpha_k\neq 0$, tehát $y_k$ kifejezhető az előző vektorok segítségével:
    \[
        y_k=
        \frac{-1}{\alpha_k}\alpha_1y_1+\dots+\frac{-1}{\alpha_{k-1}}\alpha_{k-1}y_{k-1}.\qedhere
    \]
\end{proof}

\begin{lemma}[független rendszer csere]\label{le:fgtlncsere}
    Legyen $\left\{ y_1,\dots,y_n \right\}$ egy lineárisan független rendszere valamely vektortérnek,
    és tegyük fel, hogy a tér valamely $x$ vektorára
    \[
        x=\sum_{j=1}^n\xi_jy_j,
    \]
    ahol $\xi_k\neq 0$ az egyik $1\leq k\leq n$ mellett. 
    Ekkor $y$ becserélhető a $k$-adik helyen a független rendszerbe 
    úgy, hogy az független maradjon, azaz az
    \[
        \left\{ y_1,\dots,y_{k-1},x,y_{k+1},\dots,y_n \right\}
    \]
    vektorrendszer is lineárisan független.
\end{lemma}
\begin{proof}
    Megmutatjuk, hogy a cserélt rendszernek egyedül a triviális lineáris kombinációja zérus.
    Legyen tehát
    \[
        0
        =\sum_{\substack{j=1\\j\neq k}}^n\eta_jy_j+\eta_kx
        =\sum_{\substack{j=1\\j\neq k}}^n\eta_jy_j+\sum_{j=1}^n\eta_k\xi_jy_j
        =\sum_{\substack{j=1\\j\neq k}}^n\left( \eta_j+\eta_k\xi_j \right)y_j+\eta_k\xi_ky_k.
    \]
    Mivel az eredeti rendszer lineárisan független,
    ezért az utóbbi lineáris kombináció minden együtthatója a test null eleme.
    A $k$-adikkal kezdve $\eta_k\xi_k=0$, $\xi_k\neq 0$, így $\eta_k=0$.
    Ezt a nem $k$-adik együtthatókba visszahelyettesítve
    már azt kapjuk, hogy $\eta_j=0$ minden $j\neq k$ mellett is.
\end{proof}
\chapter{A Steinitz-lemma}\index{Steinitz-lemma} 
\scwords Független- és generátorrendszerek elemszáma közti kapcsolat vezet a a bázis és a dimenzió fogalmához.
A félév legfajsúlyosabb állítása, az egyszerű bizonyítás ellenére.
A pontos megfogalmazás előtt emlékezzünk a generátorrendszer cseréjére vonatkozó \aref{le:gencsere}. lemmára.
\begin{SL}\index{Steinitz--lemma}
    Legyenek $n$ és $m$ nem negatív egészek.
    Tegyük fel, hogy az $\left\{ y_1,\dots,y_n \right\}$ egy lineárisan független rendszer,
    és az
    $\left\{ x_1,\dots,x_m \right\}$ egy generátorrendszer.
    Ekkor
    \begin{enumerate}
        \item $n\leq m$ és;
        \item az $x_1,\dots,x_m$ vektorok alkalmas átindexelésével kapott
            \(
            \left\{ y_1,\dots,y_n,x_{n+1},\dots,x_m \right\}
            \)
            vektorrendszer is generátorrendszer.%
            \footnote{Úgy kell a jelöléseket érteni, hogy az $n=0$, de az $n=m$ eset is lehetséges. 
                Az $n=0$ esetben az $y$-okkal jelölt vektorok egyike sem,
                míg az $n=m$ esetben az $x$-el jelölt vektorok egyike sem szerepel az 
                \(
                \left\{ y_1,\dots,y_n,x_{n+1},\dots,x_m \right\}
                \)
            vektorrendszer elemei közt.}%
            \qedhere
    \end{enumerate}
    \label{le:Steinitz}
\end{SL}
\begin{proof}
    Legyen $k$ a legnagyobb a $\left\{ 0,\dots,n \right\}$ egészek közül, amelyre
    \begin{enumerate}
        \item $k\leq m$, és
        \item az $x_1,\dots,x_m$ vektorok alkalmas átindexelésével az
            \[
                \left\{ y_1,\dots,y_k,x_{k+1},\dots,x_m \right\}\tag{\dag}
            \]
            vektorrendszer is generátorrendszer.
    \end{enumerate}
    Ilyen $k$ biztosan van van, hiszen $k=0$ triviálisan jó.
    Összesen azt kell meggondolnunk, hogy $k=n$.
    Ha $k<n$ lenne, 
    \begin{itemize}
        \item 
            akkor létezne $y_{k+1}$ vektor.
            No de, ez az $y_{k+1}$ nem szerepel az $\left\{ y_1,\dots,y_k \right\}$ lineáris burkában,
            ami (\dag) generátorrendszer volta miatt csak úgy lehetséges, 
            hogy $k\neq m$, azaz $k<m$, ergo $k+1\leq m$.
        \item
            \Aref{le:gencsere}. lemma szerint a (\dag) vektorrendszerben az $y_{k+1}$ vektor 
            avval az $x$-el
            --- a generátorrendszer tulajdonság megtartásával is --- 
            kicserélhető, 
            amely $x$ szerepel az $y_{k+1}$ vektornak a (\dag)-beli
            vektorokkal képzett lineáris kombinációjában. 
    \end{itemize}
    Ez ellentmondás, hiszen $k$ a legnagyobb olyan szám, 
    amelyre a bizonyítás elején szereplő 1. és 2. feltételek egyszerre állnak fenn.
\end{proof}
\section{Rang-tétel}
\begin{definition}[mátrix feszítőrangja]
    Legyen $A\in\mathbb{F}^{n\times m}$ egy tetszőleges nem zérus mátrix.
    Azt mondjuk, hogy feszítő rangja $r$, ha $r$ a legkisebb olyan pozitív egész, amelyre $A$ előáll
    \[
        A=BC
    \]
    alakban, ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times n}$.
\end{definition}
Világos, hogy tetszőleges nemzérus négyzetes mátrixra ez jól definiált és $1\leq r \leq \min\{n,m\}$.
A rang-tételnek a szokásosnál egy kicsit erősebb formája következik.

\begin{proposition}[Rang-tétel]
    Minden nemzérus mátrixban 
    a maximális lineárisan független oszloprendszerek 
    és a max\-i\-má\-lis lineárisan független sorrendszerek azonos elemszámúak, 
    és ez a szám azonos a mátrix feszítőrangjával.
\end{proposition}
\begin{proof}
    Jelölje $r$ az $A\in\mathbb{F}^{n\times m}$ mátrix feszítőrangját.
    Legyen $r_c$ a mátrix egyik rögzített maximális lineárisan független oszloprendszerének elemszáma.
    \begin{itemize}
        \item 
            Ezen oszlopokat egy $B\in\mathbb{F}^{n\times r_c}$ mátrixba téve 
            -- a maximalitás miatt -- létezik olyan $C\in\mathbb{F}^{r_c\times m}$ mátrix, 
            amelyre $A=BC$, azaz $r\leq r_c$.
        \item
            Most tekintsünk egy tetszőleges olyan
            \(
            A=BC
            \)
            felbontást, 
            ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$.
            Jelölje $W$ a $B$ mátrix oszlopai lineáris burkát. 
            Az $A$ mátrix fent rögzített maximális lineárisan független oszloprendszere egy lineárisan független rendszer a 
            $W$ vektortérben,
            és $B$ oszlopai pedig egy generátorrendszer ugyanebben a vektortérben,
            így a Steinitz-lemma\index{Steinitz-lemma}  szerint $r_c\leq r$.
    \end{itemize}
    Evvel megmutattuk, hogy bármely két maximális lineárisan független oszloprendszer azonos elemszámú, és számuk megegyezik a mátrix feszítőrangjával.


    Legyen $r_w$ az $A$ mátrix egyik rögzített maximális lineárisan független sorrendszerének elemszáma.
    \begin{itemize}
        \item 
            Ezen sorokat egy $C\in\mathbb{F}^{r_w\times m}$ mátrixba téve 
            -- a maximalitás miatt -- létezik olyan $B\in\mathbb{F}^{n\times r_w}$ mátrix, 
            amelyre $A=BC$, azaz $r\leq r_w$.
        \item
            Most tekintsünk egy tetszőleges olyan
            \(
            A=BC
            \)
            felbontást, 
            ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$.
            Jelölje most $V$ a $C$ mátrix sorai lineáris burkát. 
            Az $A$ mátrix fent rögzített maximális lineárisan független sorrendszere egy lineárisan független rendszer e
            $V$ vektortérben,
            és $C$ sorai pedig egy generátorrendszert alkotnak ugyanebben a $V$ vektortérben,
            így a Steinitz-lemma szerint $r_w\leq r$.
    \end{itemize}
    Evvel azt is megmutattuk, 
    hogy bármely két maximális lineárisan független sorrendszer azonos elemszámú, 
    és számuk megegyezik a mátrix feszítőrangjával.
\end{proof}
Ha $A\in\mathbb{F}^{n\times m}$ mátrix, amelynek feszítőrangja $r$, 
akkor $A$ bármelyik maximális lineárisan független oszloprendszerének $r$ az elemszáma.
Ezeket az oszlopokat egy $B\in\mathbb{F}^{n\times r}$ mátrixba rendezve az $A$ oszlopvektortének
egy generátorrendszerét kapjuk, 
így létezik $B\in\mathbb{F}^{r\times m}$ mátrix, amelyre $AB=A$ teljesül.
Mivel két mátrix szorzata diádok összegeként is felírható, \index{diad@diád}
úgy azt kaptuk, hogy egy $r$ feszítőrangú mátrix felírható mint $r$ darab diád,
tehát 1 rangú mátrix, összegeként. 
Itt a diádokat alkotó oszlop rendszer és sorrendszer is lineárisan független.
A következő gondolat szerint több is igaz.
\begin{proposition}
    Tegyük fel, hogy az $A\in\mathbb{F}^{n\times m}$ mátrix, amelynek feszítőrangja $r$ előáll
    \[
    A=\sum_{j=1}^rb_j\cdot c_j
    \]
    $r$ darab diád összegének alakjában, 
    ahol $b_j\in\mathbb{F}^n$ oszlop vektorok és $c_j\in\mathbb{F}^m$ sor vektorok.
    Ekkor a diádok oszlopaiból illetve soraiból alkotott 
    \[
        \left\{ b_j:j=1,\dots,r \right\} \text{ és }
        \left\{ c_j:j=1,\dots,r \right\}
    \] 
    rendszerek lineárisan független rendszereket alkotnak.
\end{proposition}
\begin{proof}
    Legyen a $B$ mátrix $j$-edik oszlopa $b_j$ és a $C$ mátrix $j$-edik sora $c_j$.
    Világos, hogy $B\in\mathbb{F}^{n\times r}, C\in\mathbb{F}^{r\times m}$ és 
    $A=BC$.
    Ha $B$ oszlopai nem alkotnának lineárisan független rendszert, akkor 
    lenne $B=B_1B_2$ előállítás, 
    ahol $B_1\in\mathbb{F}^{n\times s}, B_2\in\mathbb{F}^{s\times r}$,
    ahol $s<r$.
    Ekkor persze
    \[
        A=BC=\left( B_1B_2 \right)C=B_1\left( B_2C \right)
    \]
    Itt $B_1$ mátrixnak $s$ oszlopa van, a
    $B_2C$ mátrixnak $s$ sora van,
    ami ellentomond annak, hogy $A$ feszítőrangja $r$.
    A sorrendszer lineárisan függetlensége is hasonlóan adódik.
\end{proof}

\section{Dimenzió}
A Steinitz-lemma következményeképpen:\index{Steinitz-lemma} 
\begin{corollary}
    Egy vektortérben bármely két véges egyszerre lineárisan független és egyszerre generátorrendszer elemszáma azonos.
    Konkrétabban, ha
    \[
        \left\{ x_1,\dots,x_m \right\} \text{ és } \left\{ y_1,\dots,y_n \right\}
    \]
    lineárisan független generátorrendszerek, akkor $n=m$.
    \label{co:baziselemszam}
\end{corollary}
\begin{definition}[végesen generált vektortér]\index{végesen generált vektortér}
    Egy vektorteret \emph{végesen generáltnak} nevezünk,
    ha létezik véges elemszámú generátorrendszere.
\end{definition}
Teljesen világos, hogy ha van egy vektortérben véges generátorrendszer,
akkor van minimális generátorrendszer is, azaz van a térben lineárisan független generátorrendszer.
Ezt rögzítjük a következőekben.
\begin{proposition}
    Minden végesen generált vektortérnek van olyan vektorrendszere, 
    amely egyszerre lineárisan független és generátorrendszer.
    \label{pr:bazisletezik}
\end{proposition}
\begin{proof}
    Tekintsünk egy véges generátorrendszert.
    Ha minden elem kívül esik a többi elem lineáris burkában, akkor a rendszer lineárisan független, és készen is vagyunk.
    Ha van olyan elem, amely a többi elem lineáris burkában van, akkor dobjuk el ezt az elemet, és tekintsük, a most már
    eggyel kevesebb elemből álló vektorrendszert. 
    Világos, hogy ez is generátorrendszer marad.

    Folytassuk az eljárást.
    Mivel véges sok vektor van az eredeti generátorrendszerben az algoritmus előbb-utóbb megáll,
    ami azt jelenti, hogy olyan generátorrendszert kapunk, 
    ahol már minden elem a többi lineáris burkán kívül van,
    ergo lineárisan független.
\end{proof}
A lineárisan független generátorrendszerek olyan sűrűn fordulnak elő a tárgyalásban,
hogy rövidebb külön nevet adni nekik.
\begin{definition}[bázis]\index{bázis}
    Egy vektorrendszert \emph{bázisnak} nevezünk, ha ez egyszerre lineárisan független és generátorrendszer.
\end{definition}
\Aref{pr:bazisletezik}. állítást tehát úgy fogalmazhatjuk, hogy végesen generált vektortérnek van bázisa,
és hasonlóan \aref{co:baziselemszam}. következmény pedig azt jelenti, 
hogy egy vektortérben bármely két bázis azonos elemszámú.
Ez utóbbi tény ad értelmet a következő definíciónak:
\begin{proposition}
    Egy végesen generált vektortérről azt mondjuk, hogy $n$ dimenziós, vagy $n$ a dimenzió száma,
    ha a vektortérben van $n$ elemű bázis.
\end{proposition}
Fontos látni, hogy éppen azt gondoltuk meg, hogy \emph{minden végesen generált vektortérben van bázis}, 
\footnote{Ez nem végesen generált vektorterekre is igaz, de itt nem igazoljuk, 
viszont később sem használjuk.}
és \emph{bármely két bázis pontosan annyi vektorból áll mint a tér dimenziója.}
A végesen generált vektortereket sokszor szinonimaként \emph{véges dimenziósnak} is mondjuk.\index{véges dimenziós}

Az eddigiek összefoglalásaként is tekinthető a következő állítás.
\begin{proposition}
    Tekintsünk egy $m$-dimenziós vektorteret, és abban egy $m$-elemű
    $\left\{ x_1,\dots,x_m \right\}$
    vektorrendszert.
    E vektorrendszerre tett alábbi feltevések ekvivalensek.
    \begin{enumerate}
        \item Lineárisan független;
        \item Maximális lineárisan független rendszer;
        \item Generátorrendszer;
        \item Minimális generátorrendszer;
        \item Bázis.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Az első négy feltétel ekvivalenciájával kezdünk.
    \begin{itemize}
        \item[1.\Rightarrow 2.]
            Mivel a tér $m$-dimenziós, ezért van $m$-elemű generátorrendszere,
            így a Steinitz-lemma szerint nincs $m$-nél több elemet tartalmazó lineárisan független
            rendszere, ergo bármely $m$ elemet tartalmazó lineárisan független rendszer maximális is.
        \item[2.\Rightarrow 3.]
            Láttuk korábban.
        \item[3.\Rightarrow 4.]
            Mivel a tér $m$-dimenziós, ezért van $m$-elemű lineárisan független rendszere,
            így a Steinitz-lemma szerint nincs $m$-nél kevesebb elemet tartalmazó generátorrendszere,
            ergo bármely $m$ elemet tartalmazó generátorrendszer rendszer minimális is.
        \item[4.\Rightarrow 1.]
            Láttuk korábban.
    \end{itemize}

    Az első négy feltétel tehát ugyanazt jelenti. 
    Így ha 1.-et feltesszük, akkor 3. is fennáll, ami azt jelenti, hogy 1. feltétel és 5. feltétel is ekvivalensek.
\end{proof}\index{Steinitz-lemma} 
A Steinitz-lemma kulcs szerepet játszott dimenzió fogalmának megértésében,
hiszen a bázis elemszáma nem lehetne a tér dimenziója, anélkül hogy tudnánk a tényt: 
bármely két bázis azonos elemszámú! 
Márpedig egy vektortérben nagyon sok bázis van. 
A Steinitz-lemma 2. pontja segít ennek megértéséhez.

\begin{proposition}
    Egy végesen generált vektortér bármely lineárisan független rendszere kiegészíthető bázissá.
    \label{pr:lfgtenbazissa}
\end{proposition}
\begin{proof}
    Tegyük fel, hogy a tér $m$ dimenziós, ami azt jelenti, hogy van 
    \[
        \left\{ x_1,\dots,x_m \right\}
    \]
    $m$ elemű lineárisan független generátorrendszere.
    Legyen $\left\{ y_1,\dots,y_n \right\}$ egy lineárisan független.
    A Steinitz-lemma szerint ez a rendszer kiterjeszthető egy 
    \[
        \left\{ y_1,\dots,y_n,x_{n+1},\dots,x_m \right\}
    \]
    generátorrendszerré, ami persze bázis is.
    Ezt kellett belátni. 
\end{proof}

Meggondoltuk tehát, hogy bármely véges generátorrendszerből elhagyható néhány elem úgy, 
hogy a rendszer lineárisan független generátorrendszerré váljon,
és hasonlóan bármely lineárisan független rendszerhez, hozzátehető néhány elem úgy, hogy a
rendszer lineárisan független generátorrendszerré váljon.

A független rendszerek cseréjéről (\ref{le:fgtlncsere}.) és a generátor rendszerek cseréjéről (\ref{le:gencsere}.) szóló lemmák 
együttes alkalmazásaként kapjuk a következő lemmát.

\begin{lemma}[bázis csere]\label{le:baziscsere}
    Legyen $\left\{ x_1,\dots,x_m \right\}$ egy bázisa valamely vektortérnek,
    és tegyük fel, 
    hogy egy $y$ vektorra
    \[
        y=\sum_{j=1}^m\eta_jx_j,
    \]
    ahol $\eta_k\neq 0$ valamely $1\leq k\leq m$ mellett. 
    Ekkor $y$ becserélhető a $k$-adik helyen a bázisba, 
    úgy hogy az is bázis maradjon, azaz az
    \[
        \left\{ x_1,\dots,x_{k-1},y,x_{k+1},\dots,x_m \right\}
    \]
    vektorrendszer is egy bázis.
\end{lemma}

\begin{proposition}
    Egy végesen generált altér minden altere is végesen generált.
    Ha $M\subseteq V$ egy altér és $V$ végesen generált,
    akkor $\dim M\leq \dim V$.
\end{proposition}
\begin{proof}
    Mivel $V$-ben nincs tetszőleges nagy lineárisan független rendszer,
    ezért $M$-ben van véges elemszámú maximális lineárisan független rendszer,
    amiről tudjuk, hogy egyben generátorrendszer is.

    Van tehát $V$-ben is $M$-ben is bázis.
    Mivel az $M$-beli bázis egyben lineárisan független rendszer $V$-ben,
    ezért a Steinitz-lemma\index{Steinitz-lemma}  szerint $\dim M\leq\dim V$.
\end{proof}
\chapter{Koordinátázás}
\scwords A koordináta-tér fogalmát vezetjük be. 
Látni fogjuk, hogy véges dimenziós vektortérre ,,lényegében'' az egyetlen példa,
az $\mathbb{F}^n$ tér.
\section{Lineáris operátor fogalma}
\begin{definition}[lineáris operátor]
    Legyenek $V,W$ ugyanazon $\mathbb{F}$ test feletti vektorterek.
    Az $A:V\to W$ függvényt \emph{lineáris operációnak}\index{lineáris operáció} mondjuk,
    ha az
    \begin{displaymath}
        A\left( \alpha x+\beta y \right)=
        \alpha A\left( x \right)+\beta A\left( y \right)
    \end{displaymath}
    azonosság teljesül minden $x,y\in V$ és minden $\alpha,\beta\in\mathbb{F}$ mellett.

    Ha $V=W$, akkor a \emph{lineáris transzformáció}\index{lineáris transzformáció} kifejezést is használjuk,
    ha pedig $W=\mathbb{F}$, 
    akkor szokásos még a \emph{lineáris funkcionál}\index{lineáris funkcionál} szó összetétel is.

    A $V\to W$ összes lineáris operációk halmazát $L\left( V,W \right)$ módon jelöljük.
    A $V=W$ esetben a rövidség kedvéért csak $L\left( V \right)$-t írunk.
\end{definition}
Láttuk korábban, hogy ha $A$ egy $m\times n$ méretű mátrix,
akkor $x\in\mathbb{F}^n$ oszlop vektor mellett az
\[
    x\mapsto A\cdot x,\
\]
mátrix szorzás egy $\mathbb{F}^n\to\mathbb{F}^m$ lineáris operációt definiál.
Hasonlóan,
ha most $x\in\mathbb{F}^m$ sorvektort jelöl akkor az
\[
    x\mapsto x\cdot A
\]
szorzás egy $\mathbb{F}^m\to\mathbb{F}^n$ lineáris operációt jelent.

\begin{defprop}
    Legyen $A\in L\left( V,W \right)$ egy lineáris operáció.
    Ennek értékkészlete egy altér, 
    amelyet $\im A$ módon jelölünk.
    Azon pontok halmaza, 
    amelyeket $A$ operátor a $W$ tér zérus elemébe képez egy alteret alkotnak,
    amelyet $\ker A$ módon jelölünk.

    Az $L\left( V,W \right)$ lineáris operációk halmaza a szokásos függvény műveletekkel vektorteret alkot.
\end{defprop}
A jelölések tehát:
\[
    \im A=\left\{ y\in W:\text{létezik } x\in V, A(x)=y \right\},
    \qquad
    \ker A=
    \left\{ x\in V:A\left( x \right)=0 \right\}.
\]
\begin{proposition}
    Egy lineáris leképezés pontosan akkor injektív, ha $\ker A=\left\{ 0 \right\}$.
    Egy injektív lineáris leképezés inverz függvénye egy $\im A\to V$ lineáris operátor.
    Az $A\in L\left( V,W \right)$ injektív lineáris leképezés inverzét $A^{-1}$ módon jelöljük.

    Lineáris leképezések kompozíciója is lineáris.
\end{proposition}
Tekinthetünk az $A$ mátrixra mint az $x\mapsto A\cdot x$ lineáris operátorra.
Ennek inverze az $x\mapsto A^{-1}\cdot x$ operátor,
ahol $A^{-1}$ az inverz mátrixot jelöli, hiszen
$Ax=y$ pontosan akkor teljesül, ha $x=A^{-1}y$.

Itt jegyezem meg, hogy a tradíciókat követve egy $A$ lineáris operáció esetén az $x$ vektor
$A\left( x \right)$ képét a zárójeleket elhagyva $Ax$ módon írjuk.
Hasonlóan az $A\circ B$ kompozíciót is $AB$ módon jelöljük.

\begin{proposition}
    Legyen $A\in L\left( V,W \right)$ egy lineáris operáció.
    Ekkor
    \begin{enumerate}
        \item ha $A$ injektív és $\left\{ y_1,\dots,y_m \right\}\subseteq V$
            lineárisan független,
            akkor 
            $\left\{ Ay_1,\dots,Ay_m \right\}$ is lineárisan független.
        \item ha $A$ szürjektív és $\left\{ x_1,\dots,x_n \right\}\subseteq V$
            generátorrendszer,
            akkor 
            $\left\{ Ax_1,\dots,Ax_n \right\}$ is generátorrendszere $W$-nek.
        \item ha $A$ bijekció és $\left\{ e_1,\dots,e_n \right\}\subseteq V$
            bázis,
            akkor 
            $\left\{ Ae_1,\dots,Ae_n \right\}$ is bázis $W$-ben.
            \qedhere
    \end{enumerate}
\end{proposition}
\begin{definition}[izomorf vektorterek]\index{izomorf vektorterek}\index{izomorfizmus}
    Egy $A:V\to W$ függvényt \emph{izomorfizmusnak}\index{izomorfizmus} mondunk,
    ha $A$ lineáris bijekció.
    Az ugyanazon test feletti $V,W$ vektortereket egymással \emph{izomorf vektortérnek} mondjuk,
    ha létezik $A:V\to W$ izomorfizmus.
\end{definition}
Gondoljuk meg, hogy az azonos test feletti vektorterek izomorfizmusa egy ekvivalencia reláció,
azaz (1) minden vektortér izomorf saját magával; (2) ha $V$ izomorf $W$-vel,
akkor $W$ is $V$-vel; (3) ha $V_1$ és $V_2$ izomorfak, továbbá ha $V_2$ és $V_3$ izomorfak, akkor 
$V_1$ és $V_3$ is izomorfak.
\begin{proposition}
    Minden az $\mathbb{F}$ test feletti véges dimenziós $V$ vektortér izomorf az $\mathbb{F}^{\dim V}$
    koordináta-térrel.
\end{proposition}
\begin{proof}
    Rögzítsünk egy $\left\{ e_1,\dots,e_n \right\}$ bázist $V$-ben.
    Definiálja $\Psi:\mathbb{F}^n\to V$ a következő függvényt:
    \[
        \begin{pmatrix}
            \alpha_1\\ \vdots\\ \alpha_n
        \end{pmatrix}
        \mapsto\sum_{j=1}^n\alpha_je_j
    \]
    Könnyű számolással ellenőrizhető,
    hogy $\Psi$ egy lineáris operáció,
    a bázis lineárisan függetlenségét használva adódik $A$ injektivitása,
    a szürjektivitás pedig a bázis generátorrendszer tulajdonságát használva igazolható.
\end{proof}
\begin{definition}[koordináta]\index{koordináta}
    Legyen $\left\{ e_1,\dots,e_n \right\}$ egy bázisa a $V$ vektortérnek.
    Tudjuk, hogy minden $v\in V$ vektor egyetlen egyféleképpen, de előáll mint a bázisvektorok egy lineáris kombinációja.
    Ha ez $v=\sum_{j=1}^n\alpha_je_j$, akkor az 
    \begin{math}
        \begin{pmatrix}
            \alpha_1\\ \vdots\\ \alpha_n
        \end{pmatrix}
        \in
        \mathbb{F}^n
    \end{math}
    vektort az $x$ vektor $\left\{ e_1,\dots,e_n \right\}$ bázisban felírt koordináta-vektorának nevezzük.
\end{definition}
Pont azt mutattuk meg, hogy egy rögzített bázis mellett az a leképezés,
amely egy vektorhoz hozzárendeli a bázisban felírt koordinátáit, egy izomorfizmus
a vektortér és a koordináta-tere közt.
Az eddigiek összefoglalásaként kapjuk az alábbi állítást.
\begin{proposition}
    Legyenek $V$ és $W$ ugyanazon test feletti végesen generált vektorterek.
    E két vektortér pontosan akkor izomorf, ha azonos dimenziósak.
\end{proposition}
\begin{proof}
    Ha $V$ izomorf $W$-vel, akkor létezik köztük izomorfizmus.
    No de izomorfizmus bázist bázisra visz, ami azt jelenti, hogy azonos elemszámú
    bázisa van mindkét térnek.

    Ha $\dim V=\dim W=n$, akkor mindkét vektortér izomorf $\mathbb{F}^n$ vektortérrel,
    ergo egymással is izomorfak.
\end{proof}
Fontos látni, hogy a vektor koordináta-vektora függ a bázis megválasztásától.
Akár ha csak a bázisban az elemek sorrendjét megváltoztatjuk,
már akkor is változik a vektort koordináta-vektora.
\Aref{le:gencsere}., \aref{le:fgtlncsere}.  és \aref{le:baziscsere}. lemmák összefoglalásaként kapjuk,
hogy milyen módon változnak egy rögzített vektor koordinátái, 
ha a bázisban egy elemet megváltoztatunk.
\begin{lemma}[bázis transzformáció]\label{le:bazistrafo}
    Legyen $\left\{ x_1,\dots,x_m \right\}$ egy bázisa valamely vektortérnek,
    és tegyük fel, 
    hogy egy $y$ olyan vektor
    \(
        y=\sum_{j=1}^m\eta_jx_j,
    \)
    ahol $\eta_k\neq 0$ valamely $1\leq k\leq m$ mellett. 
    Láttuk $y$ becserélhető a $k$-adik helyen a bázisba, 
    tehát az
    \(
        \left\{ x_1,\dots,x_{k-1},y,x_{k+1},\dots,x_m \right\}
    \)
    vektorrendszer is bázis marad.
    Tegyük fel továbbá, hogy $a\in V$ vektor koordinátái ismertek az eredeti bázisban,
    \(
        a=
        \sum_{j=1}^m\alpha_j x_j.
    \)
    Ekkor ugyanez az $a$ vektor az új bázisban kifejezve
    \(
        a=
        \sum_{\substack{j=1,\\j\neq k}}^n\left( \alpha_j-\eta_j\delta \right)x_j
        +\delta y_k,
    \)
    ahol $\delta=\frac{\alpha_k}{\eta_k}$.

    Egy táblázatban összefoglalva:
\begin{eqnarray}\label{alg:bazistrafo}
    \begin{array}{c|cc}
        &y      &a         \\
        \hline
        x_1         &\eta_1  &\alpha_1   \\
        \vdots      &\vdots &\vdots    \\
        x_k         &\boxed{\eta_k}  &\alpha_k   \\
        \vdots      &\vdots &\vdots    \\
        x_m         &\eta_m  &\alpha_m   \\
        \hline
        &\delta &\frac{\alpha_k}{\eta_k}
    \end{array}
    &\implies&
    \begin{array}{c|c}
                    &a                       \\
        \hline
        x_1         &\alpha_1-\eta_1\delta   \\
        \vdots      &\vdots                  \\
        y           &\delta                  \\
        \vdots      &\vdots                  \\
        x_m         &\alpha_m-\eta_m\delta   \\
        \hline
        \phantom{\delta}&
    \end{array}
\end{eqnarray}
\end{lemma}

\chapter{Alterek Minkowski-összege és direkt összege}\index{Minkowski-összeg}
\scwords Az alterek struktúráját vizsgáljuk a fejezetben.
\section{Minkowski-összeg}
\begin{definition}\index{Minkowski-összeg}
    Legyen $V$ egy $\mathbb{F}$ test feletti vektortér és $H_1, H_2\subseteq V$ részhalmazok.
    E két halmaz \emph{összegén} vagy \emph{Minkowski-összegén}
    az
    \[
        H_1+H_1=\left\{ a+b:a\in H_1,b\in H_2 \right\}
    \]
    halmazt értjük.
    Hasonlóan, $\alpha\in\mathbb{F}$ szám mellett jelölje
    \[
        \alpha H_1=\left\{ \alpha a:a\in H_1 \right\}
    \]
    az $\alpha$ szám és $H_1$ halmaz szorzatát.
\end{definition}
Világos, hogy ha például $H_1$ üres, akkor $H_1+H_2$ is, és a $\alpha H_1$ halmazok is üresek.

Az alábbi tulajdonságok, a vektortér axiómák közvetlen következményei.
Azokat a tulajdonságokat foglaljuk össze, amelyeket a vektortér axiómákból meg tudunk menteni
a Minkowski-összegre és a számmal való szorzásra.
\begin{proposition}\label{pr:Minkowski}
    Legyenek $A,B\subseteq V$ a $V$ vektortér részhalmazai,
    továbbá $\alpha,\beta\in\mathbb{F}$ számok.
    Ekkor 
    \begin{enumerate}
        \item $A+B=B+A$;
        \item $\left( A+B \right)+C=A+\left( B+C \right)$;
        \item $A+\left\{ 0 \right\}=A$;
        \item $\alpha\left( A+B \right)=\alpha A+\beta B$;
        \item $\left( \alpha+\beta \right) A\subseteq \alpha A+\beta A$%
            \footnote{
                Egy $A\subseteq V$ halmazt \emph{affin halmaznak}\index{affin halmaz}
                szokás mondani, 
                ha zárt az $1$ összegű lineáris kombinációra 
                (\emph{affin kombinációra}\index{affin kombináció})
                nézve, 
                tehát ha a $\lambda A+\left( 1-\lambda \right)A\subseteq A$ 
                tartalmazás teljesül minden $\lambda\in\mathbb{F}$
                mellett.
                Ha $A$ affin halmaz, 
                akkor a fordított egyenlőség is igaz feltéve, hogy $\alpha+\beta\neq 0$.
                Ugyanis
                $\alpha A+\beta A
                =
                \left( \alpha+\beta \right)\left( \frac{1}{\alpha+\beta}\left( \alpha A+\beta A \right) \right)
                =
                \left( \alpha+\beta \right)\left( \frac{\alpha}{\alpha+\beta}A+\frac{\beta}{\alpha+\beta}A \right)
                \subseteq 
                \left( \alpha+\beta \right)A$.
            };
        \item $\alpha\left( \beta A \right)=\left( \alpha\beta \right)A$;
        \item $1\cdot A=A$;
    \end{enumerate}
    Igaz továbbá, hogy ha $A\neq \emptyset$, akkor $A+V=V$ és $0\cdot A=\left\{ 0 \right\}$.
\end{proposition}
Ha $A\subseteq V$ legalább két elemű, 
akkor $A+\left( -A \right)\neq\left\{ 0 \right\}$, ami azt mutatja hogy a fenti 5. tartalmazás
általában nem teljesülhet egyenlőségre.

Az alábbi állítás csak átfogalmazása az altér definíciójának.
\begin{proposition}
    A $V$ vektortér $N\subseteq V$ részhalmaza pontosan akkor altér $V$ a vektortérben,
    ha 
    (a) $N\neq \emptyset$ és az 
    (b) $\alpha N+\beta N\subseteq N$ tartalmazás minden $\alpha,\beta\in\mathbb{F}$
    mellett fennáll.
\end{proposition}

Látható, hogy egy affin halmaz eltoltja affin marad,
hiszen ha 
 $N$ egy altér $v\in V$ egy vektor,
akkor
$
\lambda\left( x+N \right)+\left( 1-\lambda \right)\left( x+N \right)
=
x+\lambda N+\left( 1-\lambda \right)N
\subseteq
x+N
$.
Mivel egy altér nyilvánvalóan affin halmaz is, 
ezért egy altér eltoltja is affin halmaz. 
Még speciálisabban egy lineáris egyenletrendszer megoldáshalmaza is fontos példa affin halmazra.

Láttuk korábban, hogy alterek közös része is altér.
Ezt úgy interpretálhatjuk, hogy a legbővebb olyan altér,
amelyet két előre megadott altér tartalmaz ezen alterek halmazelméleti közös része.
Felmerül, hogy mi a legszűkebb olyan altér, amely mindkét előre megadott alteret tartalmazza.
A válasz kézenfekvő, hiszen éppen ez a generált altér fogalma.
Ha $H_1,H_2\subseteq V$ tetszőleges halmazok, akkor 
$\lin\left\{ H_1\cup H_2 \right\}$ a $H_1$ és $H_2$ halmazokat tartalmazó legszűkebb altér.
Amennyiben altereket tartalmazó legszűkebb alteret keresünk,
akkor többet is állíthatunk:
\begin{proposition}
    Legyenek $M,N\subseteq V$ alterek.
    Ekkor 
    $\lin(M\cup N)=M+N$,
    tehát az alterek Minkowski-összege\index{Minkowski-összeg} is altér, sőt ez az $M$ és $N$ altereket tartalmazó
    legszűkebb altér.
\end{proposition}
\begin{proof}
    Ha $R$ egy altér, amelyre $M\subseteq R$ és $N\subseteq R$ is fennáll,
    akkor $M+N\subseteq R+R\subseteq R$, emiatt 
    $M+N\subseteq \gen(M\cup N)$.
    Másrészt világos, hogy $M+N$ egy az $M\cup N$ halmazt tartalmazó altér, 
    Így azt kaptuk, hogy 
    \begin{math}
        \gen(M\cup N)\subseteq M+N.
    \end{math}
    Figyelembe véve, hogy $\gen(M\cup N)=\lin(M\cup N)$, 
    készen is vagyunk.
\end{proof}

A következő állítás szerint,
ha előre adott két altér, akkor ezek összegének dimenziója mindig azonos
a két alteret tartalmazó legszűkebb altér dimenziójának,
és a két altér által tartalmazott legbővebb altér  dimenziójának összegével.
\begin{proposition}\label{pr:drosszeg2}
    Legyenek $M$ és $N$ a $V$ vektortér végesen generált alterei.
    Ekkor az $M+N$ altér is végesen generált, 
    továbbá
    \[
        \dim\left( M+N \right)=\dim M+\dim N-\dim \left( M\cap N \right).\qedhere
    \]
\end{proposition}
\begin{proof}
    Világos, hogy $M$ egy generátorrendszerének és $N$ egy generátorrendszerének egyesítése
    generátorrendszere az $M+N$ altérnek is.

    Legyen most $\left\{ p_1,\dots,p_r \right\}$ bázis $M\cap N$-ben.
    Mivel egy lineárisan független rendszer kiegészíthető egy bázissá,
    ezért legyen
    $\left\{ m_1,\dots,m_s,p_1,\dots,p_r \right\}$ bázis $M$-ben, 
    és
    $\left\{ p_1,\dots,p_r,n_1,\dots,n_t \right\}$ bázis $N$-ben,
    ahol $r,s,t$ nem negatív egészek.%
    \footnote{
        Figyeljünk arra, hogy $r=0$ is lehet, ha $M$ és $N$ diszjunktak.
        Ekkor $\left\{  \right\}$ a bázis a $\left\{ 0 \right\}$ altérben,
        azaz nincs egyetlen $p$-sem. 
    Az összes $r$ elemet tartalmazó szumma ilyenkor üres, tehát értéke zérus.
    }
    Nyilvánvaló, hogy az
    \[
        \left\{ m_1,\dots,m_s,p_1,\dots,p_r,n_1,\dots,n_t \right\}\tag{\dag}
    \]
    rendszer egy véges generátorrendszere az $M+N$ altérnek.
    Most megmutatjuk,
    hogy a $(\dag)$ rendszer lineárisan független is.
    Legyen ehhez
    \[
        \sum_{j=1}^s\alpha_j m_j+\sum_{j=1}^r\gamma_j p_j+\sum_{j=1}^t\beta_jn_j=0.\tag{\ddag}
    \]
    Ekkor persze 
    \(
        \sum_{j=1}^s\alpha_j m_j =
        -\sum_{j=1}^r\gamma_j p_j
        -\sum_{j=1}^t\beta_jn_j
        \in
        M\cap N.
    \)
    Léteznek tehát $\delta_1,\dots,\delta_r$ számok, 
    amelyekre
    \(
        \sum_{j=1}^s\alpha_j m_j=
        \sum_{j=1}^r\delta_j p_j,
    \)
    emiatt a $(\ddag)$ egyenlőség a következő módon alakul:
    \[
        \sum_{j=1}^r\left( \delta_j+\gamma_j \right)p_j+\sum_{j=1}^t\beta_jn_j=0.
    \]
    No de, $\left\{ p_1,\dots,p_r,n_1,\dots,n_r \right\}$ lineárisan független,
    ezért itt minden együttható zérus.
    Speciálisan $\beta_j=0$ minden $j=1,\dots,t$ mellett.
    Ezt $(\ddag)$-be visszaírva kapjuk, hogy
    \[
        \sum_{j=1}^s\alpha_j m_j+\sum_{j=1}^r\gamma_j p_j=0,
    \]
    amiből az $\left\{ m_1,\dots,m_s,p_1,\dots,p_r \right\}$ rendszer lineárisan függetlenségét használva kapjuk,
    hogy $\alpha_1=\dots=\alpha_s=\gamma_1=\dots=\gamma_t=0$.


    Megmutattuk tehát, hogy $(\dag)$ vektorrendszer bázisa $M+N$-nek.
    Mivel ennek elemszáma
    \begin{math}
        s+r+t=\left( s+r \right)+\left( r+t \right)-r,
    \end{math}
    ezért készen is vagyunk.
\end{proof}
\section{Direkt összeg}
Szokásos szóhasználat alterek esetén, hogy két alteret \emph{diszjunktnak}\index{diszjunkt alterek}
mondunk,
ha metszetük csak a vektortér zérus elemét tartalmazza.
\begin{definition}[direkt összeg]
    Legyenek az $M_1,\dots,M_s\subseteq V$ alterek a $V$ vektortér alterei ($s\geq 2$).
    Azt mondjuk, hogy az $N\subseteq V$ altér az $M_1,\dots,M_s$ alterek \emph{direkt összege}\index{direkt összeg},
    ha
    \begin{enumerate}
        \item $\sum_{j=1}^sM_j=N$,
        \item $\left( \sum_{\substack{j=1\\j\neq k}}^sM_j \right)\cap M_k
            =
            \left\{ 0 \right\}$ 
            minden $k=1,\dots,s$ mellett.
    \end{enumerate}
    Ekkor a $N$-et $N=M_1\oplus\dots\oplus M_s$ módon jelöljük.
\end{definition}
Külön érdemes figyelni az $s=2$, tehát csak két altér direkt összegének esetére.
Ilyenkor a fenti definíció azt jelenti, 
hogy az $M_1$ és $M_2$ diszjunkt alterekre $N=M_1+M_2$.
\begin{proposition}
    Legyen az $M_1,\dots,M_s,N\subseteq V$ a $V$ vektortér altere. 
    Az $N=M_1\oplus\dots\oplus M_s$ pontosan akkor teljesül,
    ha 
    \begin{enumerate}
        \item minden $k=1,\dots,s$ mellett $M_k\subseteq N$, és
        \item 
            minden $v\in N$ vektorhoz létezik egyetlen $v_j\in M_j, j=1,\dots,s$ 
            vektor, amelyre
            \[
                v=\sum_{j=1}^sv_j.\qedhere
            \]
    \end{enumerate}
\end{proposition}
\begin{proof}
    Tegyük fel, hogy $M$ direkt összege az $M_1,\dots,M_s$ altereknek.
    Minden szóba jövő $k$-ra $M_k\subseteq\sum_{j=1}^sM_j=M.$
    Mivel $M=M_1+\dots+M_s$, ezért minden $v\in M$-hez létezik $v_j\in M_j$, hogy 
    $v=v_1+\dots+v_s$.
    Most tegyük fel, hogy $v$ előáll $v=u_1+\dots+u_s$ alakban is, 
    ahol $u_j\in V_j$ minden $j=1,\dots,s$ mellett.
    Ekkor minden rögzített $k=1,\dots,s$ mellett
    \[
        u_k-v_k=\sum_{\substack{j=1\\j\neq k}}^s\left( v_j-u_j \right).
    \]
    No de, a baloldali vektor $M_k$-beli, 
    míg a jobboldali vektor $\sum_{\substack{j=1\\j\neq k}}^sM_j$-beli.
    A feltétel szerint ilyen vektor egyedül a zérus vektor, 
    amiből $u_k=v_k$ következik. 
    Mivel ezt minden $k=1,\dots,s$ mellett elismételhetjük,
    ezért az előállítás egyértelműségét be is láttuk.

    Megfordítva,
    most azt tegyük fel, hogy a két feltétel fennáll.
    Ekkor 1. szerint $M_1+\dots+M_s\subseteq N+\dots+N=N$,
    és 2. szerint $N\subseteq M_1+\dots+M_s$,
    amiből már
    \begin{math}
        \sum_{j=1}^sM_j=N
    \end{math}
    következik is.
    Most képzeljük el, 
    hogy valamely $k$ mellett $v_k\neq 0, v_k\in M_k$ 
    és 
    $v_k=\sum_{\substack{j=1\\j\neq k}}^sv_j$, ahol $v_j\in M_j$ minden $j=1,\dots,s$ mellett.
    Ekkor 
    \[
        v_k-\sum_{\substack{j=1\\j\neq k}}^sv_j
        =
        0
        =
        \sum_{j=1}^s 0
    \]
    az $N$ vektortér zérus elemének két különböző előállítása,
    hiszen a $k$-adik vektortérbeli elem a baloldalon a $v_k$ nem zérus vektora $M_k$-nak,
    míg a jobboldalon a zérus vektor szerepel $M_k$-ból (is).
\end{proof}
Érdemes észrevenni, 
hogy a fenti állítás 2. feltétele nem más mint az alábbi két feltétel együttesének tömör megfogalmazása:
\emph{
\begin{itemize}
    \item[2a.] $N=M_1+\dots+M_s$,
    \item[2b.] $\sum_{j=1}^sv_j=0$, $v_j\in M_j$ esetén minden $j=1,\dots,s$ mellett $v_j=0$.
\end{itemize}
}
Így a következő állítást is meggondoltuk:
\begin{proposition}
    Legyen az $M_1,\dots,M_s,N\subseteq V$ a $V$ vektortér altere. 
    Az $N=M_1\oplus\dots\oplus M_s$ pontosan akkor teljesül,
    ha 
    \begin{enumerate}
        \item $M_k\subseteq N$ minden $k=1,\dots,s$ mellett;
        \item[2a.] $N=M_1+\dots+M_s$;
        \item[2b.] $\sum_{j=1}^sv_j=0$, $v_j\in M_j$ esetén minden $j=1,\dots,s$ mellett $v_j=0$.
            \qedhere
    \end{enumerate}
\end{proposition}
Ha tehát csak az $M_1,\dots,M_s$ alterek adottak,
akkor annak szükséges és elegendő feltétele, 
hogy ezeknek a direktösszege értelmezhető legyen éppen a fenti 2b feltétel.
Ebben az esetben az $N=\sum_{j=1}^sM_j$ altér lesz az $M_1,\dots,M_s$ alterek
\[
    N=M_1\oplus\dots\oplus M_s
\]
direktösszege.

Megint csak a két vektortér speciális esetére figyelve 
azt igazoltuk, hogy $N$ pontosan akkor az $M_1$ és $M_2$ direkt összege,
ha $M_1$ és $M_2$ olyan alterei $N$-nek, hogy $N$ minden eleme előáll,
de csak egyféleképpen egy $M_1$ és egy $M_2$-beli vektor összegeként.
\begin{proposition}\label{pr:drosszeg}
    Legyenek az $M_1,\dots,M_s\subseteq V, (s\geq 2)$ alterek a $V$ vektortér olyan végesen generált alterei,
    amelyekre 
    \(
        N=M_1\oplus\dots\oplus M_s.
    \)
    Ekkor 
    \(
        \dim N=
        \dim M_1+\dots+\dim M_s.
    \)
\end{proposition}
\begin{proof}
    Az $s=2$ eset éppen \aref{pr:drosszeg2}. állítás,
    hiszen a $\left\{ 0 \right\}$ triviális altér nulla dimenziós.
    Most tegyük fel, hogy $s$-nél kisebb számokra igaz az állítás, 
    és lássuk be $s$-re. $s>2.$
    Legyen $M=\sum_{j=1}^{s-1}M_j.$
    Világos, hogy $M=M_1\oplus\dots\oplus M_{s-1}$ és $M\oplus M_s=N$.
    A már igazolt $s=2$ eset és az indirekt feltevés szerint
    \[
        \dim N
        =
        \dim M+\dim M_s
        =
        \sum_{j=1}^{s-1}\dim M_j+\dim M_s
        =
        \sum_{j=1}^{s}\dim M_j.\qedhere
    \]
\end{proof}
Az állításból azonnal adódik,
hogy ha az $M_1, M_2, \dots,M_s$ alterek bázisait, egy közös vektorrendszerbe tesszük,
akkor az így kapott vektorrendszer az $N$ tér egy bázisává válik.
Világos ugyanis, hogy az egyesített rendszer $N$-nek generátorrendszere,
és az elemeinek száma \aref{pr:drosszeg}.~állítás szerint éppen $\dim N$.
Azt kaptuk tehát, hogy ez egy minimális generátorrendszere $N$-nek,
ergo egy bázis.

A fenti állítás konkrét alkalmazhatóságához nagy segítség a most következő észrevétel.
\begin{proposition}\label{pr:dirosszegelozo}
    Adottak az $M_1,\dots,M_s\subseteq V$ alterek, ahol $s\geq 2$.
    Az alábbi feltételek ekvivalensek:
    \begin{enumerate}
        \item Minden $k=1,\dots,s$ mellett
            $M_k\cap\left( \sum_{\substack{j=1\\j\neq k}}^sM_j \right)=\left\{ 0 \right\}$.
        \item Minden $k=2,\dots,s$ mellett
            $M_k\cap\left( \sum_{\substack{j=1}}^{k-1}M_j \right)=\left\{ 0 \right\}$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    Mivel $k\geq 2$ mellett
    $
    \sum_{\substack{j=1}}^{k-1}M_j
    \subseteq
    \sum_{\substack{j=1\\j\neq k}}^sM_j,
    $
    ezért $1.\Rightarrow 2.$ nyilvánvaló.

    Most azt mutatjuk meg, hogy ha 1. feltétel nem igaz, akkor 2. feltétel sem áll.
    Tegyük fel tehát, 
    hogy van olyan $1\leq k\leq s$,
    amelyre 
    $M_k\cap\left( \sum_{\substack{j=1\\j\neq k}}^sM_j \right)\neq\left\{ 0 \right\}$.
    Ha $k=s$, akkor készen is vagyunk.
    Legyen emiatt a továbbiakban $k<s$.
    Eszerint léteznek $v_j\in M_j (j=1,\dots,s)$ vektorok, amelyekre
    \[
        0\neq v_k=
        \sum_{j=1}^{k-1}v_j
        +
        \sum_{j=k+1}^{s}v_j.
    \]
    Legyen $t$ a legnagyobb olyan szám, amelyre $k\leq t\leq s$ és $v_t\neq 0$.
    Ha $k=1$, akkor a jobb oldali első szumma üres, emiatt $t>1$.
    Ha viszont $k>1$, akkor $1<k\leq t$.
    Mindkét esetben látjuk tehát, hogy $t>1$.
    Ekkor az
    \[
        v_k=
        \sum_{j=1}^{k-1}v_j
        +
        \sum_{j=k+1}^{t}v_j.
    \]
    azonosságot átrendezve kapjuk, hogy 
    \[
        -v_t
        =
        \sum_{j=1}^{k-1}v_j
        -
        v_k
        +
        \sum_{j=k+1}^{t-1}v_j.
    \]
    Itt a bal oldali nem zérus vektor $M_t$-beli, 
    a jobb oldali vektor $\sum_{j=1}^{t-1}M_j$-beli, 
    ergo valamely $1<t\leq s$ mellett találtunk az $M_t\cap\left( \sum_{j=1}^{t-1}M_j \right)$
    altérben is egy nem zérus elemet.
\end{proof}
Az 1. feltétel előnye, hogy ennek alapján teljesen világos, hogy az 
$M_1\oplus\dots\oplus M_s$ direkt összeg nem függ az alterek sorrendjétől,
2. előnye pedig,
hogy ha képeznünk kell az $M_1,\dots,M_s$ alterek direkt összegét, 
akkor rekurzívan járhatunk el: 
ha a korábbi lépésben ellenőriztük, hogy az $M_1\oplus\dots\oplus M_{n-1}$ értelmes, akkor 
a következő lépésben csak azt kell ellenőriznünk, 
hogy $M_n$ diszjunkt az első $n-1$ altér direkt összegétől.

\section{Direkt kiegészítő}
\begin{proposition}\index{direkt kiegészítő}
    Legyen $V$ egy véges dimenziós vektortér, és $M\subseteq V$ egy altér.
    Ekkor létezik $N\subseteq V$ altér,
    amelyre $M\oplus N=V$.
    Egy ilyen alteret az $M$ altér \emph{direkt kiegészítőjének} nevezünk.
    Az $N$ altér valamennyi direkt kiegészítője egymással izomorf.
\end{proposition}
Egy altérnek sok-sok direkt kiegészítője lehet.
Például tekintsük a folytonos függvények $C\left[ 0,1 \right]$ vektorterét $\mathbb{R}$ felett,
és legyen $N$ az az altér, amely azon $f\in C\left[ 0,1 \right]$ függvényeket tartalmazza, 
amelyekre $f\left( 0 \right)=0$.
Látható, hogy tetszőleges $g\in C\left[ 0,1 \right]$, függvényre, amelyre $g\left( 0 \right)\neq 0$,
az $g$ által generált egy dimenziós altér direkt kiegészítője az $N$ altérnek.%
\footnote{
    Ha véges dimenziós példára vágyunk, 
    helyettesítsük $C\left[ 0,1 \right]$-et a legfeljebb $n$-ed fokú 
    $\mathbb{R}$ feletti polinomok $n+1$ dimenziós vektorterével.
}
\chapter{Vektortér faktortere}
\scwords A vektortér konstrukciók végéhez érkeztünk.
Láttuk, hogy alterek közösrésze, összege is vektorteret alkot.
A faktortér az utolsó vektortér konstrukciós eljárásunk.
\begin{definition}
    Legyen $V$ egy vektortér és $M\subseteq V$ egy adott altér.
    Definiáljuk a $\sim$ relációt a vektortér elemei felett:
    $x\sim y$ pontosan akkor, ha $x-y\in M$.
\end{definition}
    Látható, hogy $\sim$ ekvivalencia reláció,
    hiszen 
    \begin{enumerate}
        \item reflexív,
            ugyanis $x-x=0\in M$ minden $x\in V$-re,
        \item szimmetrikus,
            ugyanis $x-y\in M$ mellett $y-x=-\left( x-y \right)\in -M=M$,
        \item tranzitív,
            ugyanis $x-y\in M$ és $y-z\in M$ esetén $x-z=\left( x-y \right)+\left( y-z \right)\in M+M=M$
    \end{enumerate}
    Legyen adott $x\in V$ mellett az $x$ elemet tartalmazó ekvivalencia osztály $M_x$,
    azaz 
    \[
        M_x=
        \left\{ u\in V:u\sim x \right\}.
    \]
    Tudjuk, hogy az összes ekvivalencia osztályok $\left\{ M_x:x\in V \right\}$ halmazrendszere
    a $V$ egy partícióját alkotja, 
    ami azt jelenti, hogy $V=\cup_{x\in V}M_x$; 
    ha valamely $x,y\in V$ mellett $M_x\cap M_y\neq\emptyset$,
    akkor $M_x=M_y$;
    és minden $x\in V$ mellett $M_x\neq\emptyset$.

    Most azt gondoljuk meg, hogy minden egyes ekvivalencia osztály tekinthető úgyis mint,
    bármelyik elemével való eltoltja az $M$ vektortérnek.
    Speciálisan az is adódik, 
    hogy az ekvivalencia osztályok affin\index{affin halmaz} halmazok.
    \begin{proposition}
        A fenti jelölések mellett $M_x=u+M$ minden $u\in M_x$ mellett.
        Speciálisan, minden $x\in V$ mellett $M_x=x+M$.
    \end{proposition}
    \begin{proof}
        Megmutatjuk, hogy $M_x\subseteq u+M$.
        Legyen $v\in M_x$ tetszőleges.
        Ekkor $v\sim x$, $u\sim x$, ezért $v\sim u$.
        Ez azt jelenti, hogy $v-u\in M$, amiből már adódik, 
        hogy $v\in u+M$.

        Megfordítva, most igazoljuk, hogy $u+M\subseteq M_x$.
        Legyen tehát $v\in u+M$.
        Ekkor $v-u\in M$, ergo $v\sim u$.
        No de $u\sim x$ is fel van téve, 
        emiatt $v\sim x$, azaz $v\in M_x$.
    \end{proof}
    Most definiálni szeretnénk az ekvivalencia osztályok halmazán összeadás és egy testbeli elemmel való
    szorzás műveletet.
    Az összeadás művelet legyen a korábban definiált Minkowski--összeg.\index{Minkowski-összeg}
    Ez valóban művelet az ekvivalencia osztályokra megszorítva, 
    hiszen ha $M_{x_1}$ és $M_{x_2}$ két ekvivalencia osztály,
    akkor
    \[
        M_{x_1}+M_{x_2}=\left( x_1+M \right)+\left( x_2+M \right)
        =
        x_1+x_2+M+M=
        \left( x_1+x_2 \right)+M
        =
        M_{x_1+x_2},\tag{\dag}
    \]
    azaz két ekvivalencia osztály Minkowski--összege is egy ekvivalencia osztály.
    Sőt, az is adódik, hogy 
    \emph{
        egy-egy vektorhoz tartozó ekvivalencia osztályok Minkowski--összege
        azonos e két vektor összegéhez tartozó ekvivalencia osztállyal.}

    A skalárral való szorzás már nem ilyen egyszerű,
    hiszen $0\cdot M_x=\left\{ 0 \right\}$, de ez utóbbi halmaz általában nem egy ekvivalencia osztály,
    ezért az ekvivalencia osztályok halmazán a skalárral való szorzás nem egy művelet.
    \begin{definition}[ekvivalencia osztály számszorosa]
        Legyen $\alpha\in\mathbb{F}$ egy szám és $M_{x}$ egy ekvivalencia osztály.
        Definiálja
        \[
            \alpha\ast M_x=
            \begin{cases}
                \alpha M_x&\text{, ha }\alpha\neq 0\\
                M&\text{, ha }\alpha=0.
            \end{cases}\qedhere
        \]
    \end{definition}
    Egyrészt vegyük észre,
    hogy $\alpha\neq 0$ mellett
    $\alpha M_x=\alpha\left( x+M \right)=\alpha x+\alpha M=\alpha\cdot x+M=M_{\alpha\cdot x}$,
    másrészt azt lássuk, 
    hogy az $\alpha=0$ esetben
    $M=0+M=M_0=M_{0\cdot x}$, ami azt jelenti, hogy a fenti definícióra az
    \[
        \alpha\ast M_x=M_{\alpha\cdot x}\tag{\ddag}
    \]
    azonosság is teljesül minden $x\in V$ vektorra és minden $\alpha\in\mathbb{F}$ számra.
    Kiderült tehát, hogy a most bevezetett $\ast$ skalárral való szorzást alkalmazva
    egy ekvivalencia osztálynak egy számmal való szorzata egy ekvivalencia osztály lesz,
    sőt
    \emph{egy vektorhoz tartozó ekvivalencia osztály $\alpha$ szorosa azonos e vektor $\alpha$-szorosához tartozó
    ekvivalencia osztállyal}.
    \begin{proposition}[faktortér]
    Legyen $V$ egy az $\mathbb{F}$ test feletti vektortér,
    és $M\subseteq V$ egy rögzített altér.
    Legyen $\sim$ a $V$ vektortér elemein értelmzett reláció,
    amelyre $x\sim y$, ha $x-y\in M$ ekvivalencia reláció.
    Láttuk, hogy ez ekvivalencia reláció.
    Jelölje
    \[
        V/M=\left\{ M_x:x\in V \right\}
    \]
    az ekvivalencia osztályok halmazát.
    Definiáljuk a $V/M$ halmazrendszer elemei mint halmazok közt
    a Minkowski--összeget és a skalárral való szorzást:
    \[
        M_{x_1}+M_{x_2}=M_{x_1+x_2},
        \qquad
        \text{és}
        \qquad
        \alpha\ast M_x=M_{\alpha\cdot x}.
    \]
    Ekkor az itt bevezetett $(V/M,+,\ast)$ struktúra az $\mathbb{F}$ test feletti vektorteret alkot.
    Ezt a vektorteret nevezzük a $V$ vektortér $M$ szerinti \emph{faktorterének}\index{faktortér}.
\end{proposition}
\begin{proof}
    Meggondoltuk már, hogy a Minkowski--összeg és a $\ast$ szorzás eredménye egy ekvivalencia osztály.
    Most vegyük sorra a vektortér axiómákat:
    Az
    \begin{enumerate}
        \item[1.] $M_{x_1}+M_{x_{2}}=M_{x_2}+M_{x_1}$;
        \item[2.] $\left( M_{x_1}+M_{x_2} \right)+M_{x_3}=
            M_{x_1}+\left( M_{x_2}+M_{x_3} \right)$;
    \end{enumerate}
    axiómák tetszőleges halmazokra, nem csak ekvivalencia osztályokra is fennállnak.
    
    Az ekvivalencia osztályok közt $M_0=M$ neutrális elem,
    hiszen minden $x\in V$ mellett $M_x+M=\left( x+M \right)+M=x+\left( M+M \right)=x+M=M_x$.
    \begin{enumerate}
        \item[3.] Minden $M_x$ ekvivalencia osztályra $M_x+M=M_x$;
        \item[4.] Minden $M_x$ ekvivalencia osztályra $M_x+M_{-x}=M$,\\
            hiszen 
            $M_x+M_{-x}=M_{x+\left( -x \right)}=M_0=M$, 
            felhasználva a már igazolt $\dag$ azonosságot.
    \end{enumerate}
    A $\ast$ szorzás és az összeadás kapcsolatát leíró aximák ellenőrzéséhez használjuk
    $(\dag)$ és $(\ddag)$ azonosságokat:
    \begin{enumerate}
        \item[5.] $\left( \alpha+\beta \right)\ast M_x=\alpha\ast M_x+\beta\ast M_x$,\\
            hiszen 
            $\left( \alpha+\beta \right)\ast M_x
            =
            M_{\left( \alpha+\beta \right)x}
            =
            M_{\alpha x+\beta x}
            =
            M_{\alpha x}+M_{\beta x}
            =
            \alpha\ast M_x+\beta\ast M_x$;
        \item[6.] $\alpha\ast\left( M_{x_1}+M_{x_2} \right)
            =
            \alpha\ast M_{x_1}+\alpha\ast M_{x_2}$,\\
            hiszen
            $
            \alpha\ast\left( M_{x_1}+M_{x_2} \right)
            =
            \alpha\ast\left( M_{x_1+x_2} \right)
            =
            M_{\alpha(x_1+x_2)}
            =
            M_{\alpha x_1+\alpha x_2}
            =
            M_{\alpha x_1}+M_{\alpha x_2}
            =
            \alpha\ast M_{x_1}+\alpha\ast M_{x_2}$;
        \item[7.] $
            \left( \alpha\beta \right)\ast M_{x}=
            \alpha\ast\left( \beta\ast M_{x} \right)$,\\
            hiszen 
            $
            \left( \alpha\beta \right)\ast M_{x}
            =
            M_{\left( \alpha\beta \right)x}
            =
            M_{\alpha\left( \beta x \right)}
            =
            \alpha\ast M_{\beta x}
            =
            \alpha\ast\left( \beta\ast M_x \right)
            $;
        \item[8.]
            $
            1\ast M_{x}=M_{x},
            $
            \\hiszen
            $
            1\ast M_{x}=M_{1\cdot x}=M_x.
            $
    \end{enumerate}
    Ezt kellett belátni. 
\end{proof}
\begin{proposition}
    Legyen $M$ a $V$ vektortér egy altere. 
    Definiálja $\varphi:V\to V/M$ az $x\mapsto M_x$ függvényt.
    \begin{enumerate}
        \item Ekkor $\varphi:V\to V/M$ egy szürjektív lineáris operáció.
        \item Legyen most $N$ az $M$ egy direkt kiegészítője,\index{direkt kiegészítő}
            azaz $M\oplus N=V$.
            Definiálja $\Phi$ a $\varphi$ megszorítását az $N$ direkt kiegészítőre.
            Ekkor $\Phi:N\to V/M$ egy izomorfizmus.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    1. A ($\dag$) és ($\ddag$) szerint
    $ 
    \varphi\left( \alpha x_1+\beta x_2 \right)
    =
    M_{\alpha x_1+\beta x_2}
    =
    M_{\alpha x_1}+M_{\beta x_2}
    =
    \alpha\ast M_{x_1}+\beta\ast M_{x_2}
    =
    \alpha\ast\varphi\left( x_1 \right)+\beta\ast\varphi\left( x_2 \right)
    $,
    ami éppen azt jelenti, hogy $\varphi$ egy lineáris operáció.
    Ha $A\in V/M$ egy ekvivalencia osztály, és $x\in A$ tetszőleges eleme,
    akkor $\varphi\left( x \right)=M_x=A$, ergo $\varphi$ szürjekció.

    2. Világos, hogy $\Phi$ lineáris leképezés leszűkítéseként maga is lineáris.
    
    Most nézzük, hogy $\Phi$ is szürjekció marad.
    Ha $A\in V/M$ egy ekvivalencia osztály,
    akkor létezik $x\in V$, amelyre $\varphi\left( x \right)=A$.
    No de $x=u+v$ alakú, ahol $u\in M$ és $v\in N$, és $M=M_0$ az $V/M$ vektortér neutrális eleme.
    Így
    $
    A=\varphi\left( x \right)=\varphi\left( u+v \right)=\varphi\left( u \right)+\varphi\left( v \right)
    =M+\varphi\left( v \right)=\varphi\left( v \right).
    $

    Az injektiv tulajdonsághoz legyen $v\in \ker\Phi$, 
    azaz $v\in N$, amelyre $\varphi\left( v \right)=M$.
    Ez a $\varphi$ definíciója miatt azt jelenti, hogy $M_v=M$, azaz $v\in M$.
    Azt kaptuk tehát, hogy $v\in N\cap M=\left\{ 0 \right\}$, 
    ami csak úgy lehetséges,
    hogy $v=0$.
    Megmutattuk tehát, hogy $\ker\Phi=\left\{ 0 \right\}$,
    ami $\Phi$ injektivitását jelenti.
\end{proof}
Az állítás legelső következménye, 
hogy az $M$ altér bármely direkt kiegészítője\index{direkt kiegészítő} izomorf a $V/M$ faktortérrel,
emiatt bármely direkt kiegészítő izomorf bármely direkt kiegészítővel is.
Feltéve, hogy 
létezik direkt kiegészítő, izomorfiától eltekintve csak egyetlen egy létezik.
De van-e, mindig direkt kiegészítő? 
Véges dimenziós esetben már láttuk az igenlő választ.
Nem véges dimenziós vektorterekre is igaz az állítás, de itt nem igazoljuk.
\begin{definition}[co-dimenzió]\index{co-dimenzió}
    Azt mondjuk, hogy a $V$ vektortér $M$ altere \emph{$k$ co-dimenziós},
    ha létezik $k$-dimenziós direkt kiegészítője $M$-nek,
    azaz létezik olyan $N$ altere $V$-nek, amelyre $M\oplus N=V$
    és $\dim N=k$.
    A tényt,
    hogy $M$ altérnek létezik $k$-dimenziós direkt kiegészítője
    $\codim M=k$ módon jelöljük.
\end{definition}
\begin{proposition}
    Tegyük fel, hogy $M$ a $V$ vektortér olyan altere,
    amelynek van véges dimenziós direkt kiegészítője.
    Ekkor
    \[
        \codim M=\dim(V/M).\qedhere
    \]
\end{proposition}
\begin{proof}
    Válasszunk $N$ véges dimenziós alterét $V$-nek, 
    amelyre $V=M\oplus N$.
    Láttuk, hogy $V/M$ és $N$ izomorf vektorterek,
    emiatt $V/M$ is végesen generált, és
    $\dim V/M=\dim N=\codim M$.
\end{proof}
\begin{proposition}
    Legyenek $V,W$ vektorterek,
    továbbá $A\in L\left( V,W \right)$ egy olyan lineáris operáció,
    amelyre az $\im A\subseteq W$ végesen generált.
    Ekkor a $\ker A$ altérnek van végesen generált direkt kiegészítője, és
    \[
        \codim\left( \ker A \right)=\dim\left( \im A \right).\qedhere
    \]
\end{proposition}
    \begin{proof}
        Legyen $\im A$ egy bázisa $\left\{ Ax_1,\dots,Ax_r \right\}$.
        Definiálja $N=\lin\left\{ x_1,\dots,x_r \right\}$.
        Ha $x\in\ker A\cap N$, akkor valamely $\alpha_j$ skalárokkal
        $x=\sum_{j=1}^r\alpha_jx_j$ és $Ax=0$.
        Így $0=\sum_{j=1}^r\alpha_jAx_j$, ami csak $\alpha_1=\dots=\alpha_r=0$
        esetben lehetséges az $\left\{ Ax_1,\dots,Ax_r \right\}$ rendszer lineárisan függetlensége szerint.

        Ha $x\in V$ tetszőleges vektor, akkor $Ax\in\im A$, emiatt
        $Ax$ valamely skalárokkal $Ax=\sum_{j=1}^r\alpha_jAx_j$ alakú.
        Világos, hogy így $x-\sum_{j=1}^r\alpha_jx_j\in\ker A$,
        ami igazolja, hogy $V=\ker A+N$.

        Láttuk tehát, hogy $V=\ker A\oplus N$, azaz $\ker A$-nak valóban találtunk véges dimenziós direkt kiegészítőjét.
        Az is világos, hogy $\codim\left( \ker A \right)=\dim N=r=\dim\left( \im A \right)$.
    \end{proof}
Magától értetődik a következő észrevétel.
\begin{proposition}
Ha $A\in L\left( V,\mathbb{F} \right)$ egy nem zérus lineáris funkcionál a $V$ vektortéren,
akkor $\ker A$ egy 1 co-dimenziós altere $V$-nek.
\end{proposition}
Nyilvánvaló következmény még a rang-defektus--tétel is.
\begin{proposition}[rang-defektus--tétel]
    Legyen $V$ egy véges dimenziós vektortér,
    és $W$ egy tetszőleges vektortér, 
    továbbá $A\in L\left( V,W \right)$ egy lineáris operáció.
    Ekkor
    $\im A$ is véges dimenziós, és
    \[
        \dim\left( \ker A \right)+\dim\left( \im A \right)=\dim V.\qedhere
    \]
\end{proposition}

Ha $V$ véges dimenziós és $A\in L\left( V,W \right)$ egy lineáris operáció,
akkor láttuk, hogy
\[
\dim\left( \im A \right)
=
\codim\left( \ker A \right)
=
\dim\left( V/\ker A \right).
\]
Tudjuk, hogy azonos dimenziójú vektorterek egy mással izomorfak,
emiatt $\im A$ izomorf $V/\ker A$ faktortérrel.
A következő állítás ennek a gondolatnak a nem véges dimenziós esetre vonatkozó általánosítása,
de véges dimenzióban is érdekes, hiszen konkrét izomorfiát mutatunk.
\begin{proposition}
    Legyenek $V,W$ ugyanazon test feletti vektorterek,
    és $A\in L\left( V,W \right)$ egy lineáris operáció.
    Ekkor a
    \(
        V/\ker A
    \)
    faktortér izomorf az $\im A$ vektortérrel.
\end{proposition}
\begin{proof}
    Jelölje $M=\ker A$ a $V$ vektortér alterét.
    Definiálni szeretnénk egy $\Omega:\im A\to V/M$ izomorfizmust.
    Ha $Ax_1=y=Ax_2$, akkor $x_1-x_2\in M$, azaz $x_1\sim x_2$, ergo $M_{x_1}=M_{x_2}$.
    Emiatt $y\in\im A$ mellett
    \[
        y\mapsto M_x, \text{ ahol } Ax=y
    \]
    jól definiálja az $\Omega:\im A\to V/M$ függvényt.
    
    1. $\Omega$ lineáris operáció:
    Legyen $Ax_1=y_1$ és $Ax_2=y_2$.
    Ekkor 
    $A\left( \alpha x_1+\beta x_2 \right)=\alpha y_1+\beta y_2$, így
    \[
    \Omega\left( \alpha y_1+\beta y_2 \right)
    =
    M_{\alpha x_1+\beta x_2}
    =
    \alpha\ast M_{x_1}+\beta\ast M_{x_2}
    =
    \alpha\ast \Omega\left( y_1 \right)+\beta\ast\Omega\left( y_2 \right).
    \]

    2. $\Omega$ szürjekció:
    Ha $H\in V/M$ egy ekvivalencia osztály, 
    akkor tetszőleges $x\in H$ mellett legyen $y=Ax$.
    Ekkor
    \[
        \Omega\left( y \right)=M_x=H,
    \]
    amivel azt igazoltuk, 
    hogy minden ekvivalencia osztály egy $\im A$ beli vektor $\Omega$ képe.


    3. $\Omega$ injekció:
    Legyen $y\in\ker\Omega$, azaz $y\in W$, amelyre $\Omega\left( y \right)$ a $V/M$ faktortér neutrális eleme,
    azaz 
    $\Omega\left( y \right)=M$.
    Ez $\Omega$ definíciója szerint csak úgy lehet, ha $M=M_x$, ahol $Ax=y$.
    No de $M=M_0$, így $y=A0=0$.
    Megmutattuk tehát, hogy $\ker \Omega=\left\{ 0 \right\}$,
    ami éppen $\Omega$ injektivitása.
\end{proof}
\chapter*{Rang-tétel mégegszer}

\begin{definition}[rang, oszloprang, sorrang, feszítőrang]\index{vektorrendszer rangja}\index{oszloprang}\index{sorrang}\index{feszítőrang}
    Egy véges vektorrendszer \emph{rangján} a vektorrendszer generálta altér dimenzióját értjük.
    Egy mátrix \emph{oszloprangján} a mátrix oszlopai mint vektorrendszer rangját értjük.
    Egy mátrix \emph{sorrangján} a mátrix sorai mint vektorrendszer rangját értjük.
    Ha $A\in\mathbb{F}^{n\times m}$ nemzérus mátrix,
    akkor legkisebb olyan $r$ számot, amelyre
    létezik $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$ mátrix úgy, hogy 
    $A=BC$ szorzatfelbontás teljesül,
    az $A$ mátrix \emph{feszítőrangjának} nevezzük.

    Jelölések: Ha $\left\{ x_1,\dots,x_m \right\}$ a szóban forgó vektorrendszer, akkor
    \[
        \rank\left\{ x_1,\dots,x_m \right\}=\dim\lin\left\{ x_1,\dots,x_m \right\}
    \]
    Ha $A\in\mathbb{F}^{n\times m}$ egy mátrix,
    akkor 
    \[
        \crank{A}=\rank\left\{ [A]^{j}:j=1,\dots,m \right\},
        \quad
        \rrank{A}=\rank\left\{ [A]_k:k=1,\dots,n\right\},
    \]
    továbbá $\srank{A}$ jelöli a feszítőrangját $A$-nak.
\end{definition}
\begin{proposition}(Rang-tétel)\label{pr:rang}
    Tetszőleges test feletti tetszőleges mátrix mellett, a fent bevezetett három rang-koncepció azonos.

    Formálisabban: Minden $A\in\mathbb{F}^{n\times m}$ mellett
    \[
        \crank{A}=\srank{A}=\rrank{A}\qedhere
    \]
\end{proposition}
\begin{proof}
    Induljunk ki a feszítőrang fogalmából.
    Legyen $r=\srank{A}$, és 
    \[
        A=BC,\tag{\dag}
    \]
    ahol $B\in\mathbb{F}^{n\times r},C\in\mathbb{F}^{r\times m}$.
    Azt mutatjuk meg, hogy ekkor $B$ oszloprendszere minimális generátorrendszere $A$ oszlop-vektorterének
    és $C$ sorrendszere minimális generátorrendszere $A$ sor-vektorterének.

    Vegyük észre, hogy $\srank{A}\leq \crank{A}$.
    Ugyanis ha az $A$ mátrix oszlop-vektorterének veszünk egy tetszőleges választott
    $b_1,\dots,b_k$ generátorrendszerét, 
    akkor van $B\in\mathbb{F}^{n\times k}$ és $C\in\mathbb{F}^{k\times m}$ mátrix, hogy $A=BC$.
    Az $r=\srank{A}$ szám az ilyen $k$ számok legkisebbike, tehát valóban $r\leq\crank{A}$.
    \\
    Most tekintsük a (\dag)-ben rögzített szorzatot.
    Jelölje $W$ a $B$ mátrix és $V$ az $A$ mátrix oszlopvektorterét.
    Mivel $BC$ oszlopai $B$ oszlopainak lineáris kombinációja, 
    ezért $A$ minden oszlopa beleesik $W$-be, 
    így az $A$ oszlopainak lineáris burka is részhalmaza $W$-nek,
    azaz 
    \[
        V\subseteq W.
    \]
    A $B$ mátrixnak $r$ darab oszlopa van, 
    tehát $\dim W\leq r$.
    Látjuk tehát, hogy 
    \[\dim W\leq r\leq\crank{A}=\dim V,
    \]
    ami csak úgy lehetséges, 
    hogy $V=W$.
    A $B$ oszlopai tehát $V$-nek is generátorrendszerét alkotják,
    és $r$ minimalitása szerint egy elem sem elhagyható a generátorrendszer tulajdonság
    elvesztése nélkül.

    A sorokra vonatkozó indoklás analóg.
    Először is $\srank{A}\leq \rrank{A}$.
    Ugyanis ha az $A$ mátrix sorvektorterének veszünk egy tetszőleges választott
    $c_1,\dots,c_k$ generátorrendszerét, 
    akkor van $B\in\mathbb{F}^{n\times k}$ és $C\in\mathbb{F}^{k\times m}$ mátrix, hogy $A=BC$.
    Az $r=\srank{A}$ szám az ilyen $k$ számok legkisebbike, tehát valóban $r\leq\rrank{A}$.
    \\
    Most tekintsük a (\dag)-ben rögzített szorzatot.
    Jelölje $W$ a $C$ mátrix és $V$ az $A$ mátrix sorvektorterét.
    Mivel $BC$ sorai $C$ sorainak lineáris kombinációja, 
    ezért $A$ minden sora $W$-be esik,
    így az $A$ sorainak lineáris burka is részhalmaza $W$-nek,
    azaz 
    \[
        V\subseteq W.
    \]
    A $C$ mátrixnak $r$ sora van, 
    tehát $\dim W\leq r$.
    Látjuk tehát, hogy 
    \[\dim W\leq r\leq\crank{A}=\dim V,
    \]
    ami csak úgy lehetséges, 
    hogy $V=W$.
    A $C$ oszlopai tehát $V$-nek is generátorrendszerét alkotják,
    és $r$ minimalitása szerint egy elem sem elhagyható a generátorrendszer tulajdonság
    elvesztése nélkül.

    Ezt kellett belátni. 
\end{proof}
\begin{definition}[mátrix rangja]\index{matrix@mátrix rangja}
    Mivel a sorrang, az oszloprang, a feszítőrang minden mátrix mellett azonos,
    ezért a továbbiakban a közös értékre a \emph{rang} szót is használjuk.\footnote{Lásd: \parencite{Wardlaw2005}}
\end{definition}
\begin{note}
    Érdemes a rang-tétel következő összegzését megjegyezni.
    Legyen $A\in\mathbb{F}^{n\times m}$ mátrix, amelynek $r$ a rangja.
    Ekkor létezik $A=BC$ felbontása, ahol $B\in\mathbb{F}^{n\times r},C\in\mathbb{F}^{r\times m}$.
    Ez a felbontás persze nem egyértelmű, hiszen $A$ oszlop-vektorterének nagyon sok bázisa van.
    Viszont minden ilyen felbontásban $B$ oszloprendszere az $A$ oszlop-vektorterének, 
    míg $C$ sorrendszere az $A$ sorvektorterének minimális generátorrendszerét, ergo bázisát alkotja.
\end{note}
Következményképpen érdemes meggondolni a mátrix és inverzének felcserélhetőségére vezető állítást.
\begin{proposition}
    Legyenek $A,B\in\mathbb{F}^{n\times n}$ négyzetes mátrixok, amelyekre AB=I.
    Ekkor BA=I is teljesül.
\end{proposition}
\begin{proof}
    Az identitás mátrix rangja nyilván $n$.
    E mátrix feszítőrangjának definíciójára gondolva, 
    az előző megjegyzés szerint $A$ oszlopai $\mathbb{F}^n$ lineárisan független rendszerét alkotják.
    Vegyük észre, hogy a mátrix szorzás asszociativitását is kihasználva
    \[
        A\left( BA-I \right)=A\left( BA \right)-AI=\left( AB \right)A-AI=IA-AI=A-A=0.
    \]
    Na most, 
    ha $BA\neq I$ lenne, 
    akkor a $BA-I$ mátrixnak lenne egy olyan nem zérus oszlopa, 
    melynek elemeivel mint együtthatókkal képzett lineáris kombinációja az $A$ oszlopainak 
    a zéró vektort eredményezi.
    Ez persze ellentmond az $A$ oszloprendszer lineáris függetlenségének,
    tehát $BA=I$ valóban fennáll.%
    \footnote{%
        A feszítőrang fogalmának ismerete nélküli -- talán még elemibb -- bizonyítás: \parencite{doi:10.4169/college.math.j.48.5.366}%
    }%
\end{proof}
\begin{defprop}[invertálható mátrix]
    Legyen $A\in\mathbb{F}^{n\times n}$ egy négyzetes mátrix.
    Az alábbi feltételek egymással ekvivalensek.
    \begin{enumerate}
        \item Van egyetlen olyan $B\in\mathbb{F}^{n\times n}$ mátrix,
            amelyre $AB=I$,
        \item Van olyan $B\in\mathbb{F}^{n\times n}$ mátrix,
            amelyre $AB=I$,
        \item Van egyetlen olyan $B\in\mathbb{F}^{n\times n}$ mátrix,
            amelyre $BA=I$,
        \item Van olyan $B\in\mathbb{F}^{n\times n}$ mátrix,
            amelyre $BA=I$,
        \item $\rank A=n$,
        \item $A$ oszlopai lineárisan független rendszer alkotnak,
        \item $A$ sorai lineárisan független rendszert alkotnak.
    \end{enumerate}
    Ha a fenti feltételek egyike (ergo mindegyike) fennáll, 
    akkor azt mondjuk, hogy $A$ mátrix \emph{invertálható}\index{invertálható mátrix}.
    Szinonimaként használjuk még a \emph{nemszinguláris}\index{nemszinguláris mátrix}, 
    vagy az \emph{reguláris}\index{reguláris mátrix} szavakat.
    Ha egy mátrix nem invertálható, akkor \emph{szingulárisnak}\index{szinguláris mátrix} nevezzük.

    Egy invertálható négyzetes mátrix esetén azt az egyetlen $B$ mátrixot, amelyre
    \(
    AB=I
    \)
    fennáll az $A$ inverzének mondjuk, és $A^{-1}=B$-vel jelöljük.
    Világos, hogy
    \[
        AA^{-1}=I=A^{-1}A,\quad (A^{-1})^{-1}=A.\qedhere
    \]
\end{defprop}
\begin{proof}
    A 2., 4., 5., 6., 7. állítások ekvivalenciája nyilvánvaló az előzőek szerint.
    Ha $AB=I=AC$, akkor $A\left( B-C \right)=0$ így $A$ oszloprendszere lineáris függetlensége 
    miatt $B=C$. 
    Ezzel $2.\Rightarrow 1.$ implikációt is beláttuk.
    Az 1. és 3. feltevések ekvivalenciája az előző állítás miatt teljesül.
\end{proof}
\chapter*{Elemi fogalmak mégegyszer}
Tegyük fel, hogy, hogy ismerjük az alábbi fogalmakat:
\begin{enumerate}
    \item Vektorrendszer lineáris függetlensége;
    \item Altér, lineáris burok, generátorrendszer;
    \item Mátrix szorzás,
    \item Gauss-Jordan elimináció.\index{Gauss--Jordan-eliminácio}  Pontosan azt tesszük fel, hogy ha $Q$ egy négyzetes mátrix, melynek oszlopai
        lineárisan függetlenek, akkor az elemi sor műveletekkel az identikus mátrixszá transzformálható.
        Mivel az elemi sorműveletek, bal-szorzások alkalmas mátrixszal, 
        azt kapjuk, hogy létezik $P$ mátrix, amelyre $PQ=I$.
\end{enumerate}
Amit határozottan kerülünk, és azt tesszük fel, hogy nem ismerjük,
\begin{enumerate}
    \item Bázisok fogalma,
    \item Különböző bázisok azonos számossága,
    \item determináns.
\end{enumerate}
Külön probléma a mátrix rangjának definíciója.
Nem definiálhatom, 
mint az oszlop vagy sorvektortér dimenzióját, hiszen a felépítés jelen szintjén még nincs bázis.
Természetesen azt sem tudjuk még, hogy a maximális lineárisan független sor- vagy oszloprendszer választástól függetlenül mindig azonos elemszámú.
A mátrix feszítő rangja viszont definiálható.
\begin{definition}[mátrix feszítőrangja]
    Legyen $A\in\mathbb{F}^{n\times m}$ egy tetszőleges nem zérus mátrix.
    Azt mondjuk, hogy feszítő rangja $r$, ha $r$ a legkisebb olyan pozitív egész, amelyre $A$ előáll
    \[
        A=BC
    \]
    alakban, ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times n}$.
\end{definition}
Világos, hogy tetszőleges nemzérus négyzetes mátrixra ez jól definiált és $1\leq r \leq \min\{n,m\}$.
A rang-tételnek a szokásosnál egy kicsit erősebb formáját lehet megfogalmazni a dimenzió fogalmának bevezetése nélkül,
ami a lenti 3. állítás.

\begin{proposition}
    Az alábbi állítások egymás következményei:
    \begin{enumerate}
        \item Homogén lineáris egyenletrendszernek, amelynek több ismeretlene van, mint egyenlete
            mindig létezik nem triviális megoldása.
        \item Lineárisan független vektorrendszer elemszáma nem nagyobb mint egy generátorrendszer elemszáma.
        \item Minden nemzérus mátrixban 
            a maximális lineárisan független oszloprendszerek 
            és maximális lineárisan független sorrendszerek azonos elemszámúak, 
            és ez a szám egybeesik a mátrix feszítőrangjával.
        \item
            Legyen $A,B\in\mathbb{F}^{n\times n}$ négyzetes mátrixok.
            Ekkor $AB=I$ esetén $BA=I$ is fennáll.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}[1. \Rightarrow 2.]
    Legyen $\left\{ y_1,\dots,y_n \right\}$ egy generátorrendszer,
    és $\left\{ x_1,\dots,x_m \right\}$ olyan vektorrendszer a vektortérben, ahol $m>n$.
    Meg kell mutatnunk, hogy ez utóbbi egy lineárisan összefüggő.
    Világos, hogy minden $1\leq k\leq m$ mellett
    \[
        x_k=\sum_{j=1}^n\alpha_{j,k}y_j.
    \]
    Egyelőre tetszőleges $\xi_1,\dots,\xi_m$ együtthatók mellett
    \begin{eqnarray}
        \sum_{k=1}^m\xi_kx_k=
        \sum_{k=1}^m\sum_{j=1}^n\xi_k\alpha_{j,k}y_j=
        \sum_{j=1}^n\left( \sum_{k=1}^m\alpha_{j,k}\xi_k \right)y_j
        \label{eq:sys}
    \end{eqnarray}
    Tekintsük az $\left( \alpha_{j,k} \right)$ együtthatók generálta
    homogén lineáris egyenletrendszert. 
    Itt $j=1,\dots,n$ és $k=1,\dots,m$.
    Mivel $m>n$, ezért az ismeretlenek száma több mint az egyenletek száma.
    Létezik tehát nem triviális megoldás\index{triviális megoldás}, azaz léteznek nem mind nulla
    $\xi_1,\dots,\xi_m$ számok, amelyekre minden $j=1,\dots,n$ esetén
    \[
        \sum_{k=1}^m\alpha_{j,k}\xi_k=0.
    \]
    Találtunk tehát az $\left\{ x_1,\dots,x_m \right\}$ vektorrendszernek egy
    nem triviális, 
    de a zéró vektort eredményező,
    lineáris kombinációját (\ref{eq:sys}).
\end{proof}
\begin{proof}[2.\Rightarrow 3.]
    Jelölje $r$ az $A\in\mathbb{F}^{n\times m}$ mátrix feszítőrangját.
    Legyen $r_c$ a mátrix egyik rögzített maximális lineárisan független oszloprendszerének elemszáma.
    \begin{itemize}
        \item 
            Ezen oszlopokat egy $B\in\mathbb{F}^{n\times r_c}$ mátrixba téve 
            -- a maximalitás miatt -- létezik olyan $C\in\mathbb{F}^{r_c\times m}$ mátrix, 
            amelyre $A=BC$, azaz $r\leq r_c$.
        \item
            Most tekintsünk egy tetszőleges olyan
            \(
            A=BC
            \)
            felbontást, 
            ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$.
            Jelölje $W$ a $B$ mátrix oszlopai lineáris burkát. 
            Az $A$ mátrix fent rögzített maximális lineárisan független oszloprendszere egy lineárisan független rendszer a 
            $W$ vektortérben,
            és $B$ oszlopai pedig egy generátorrendszer ugyanebben a vektortérben.
            A 2. állitás szerint $r_c\leq r$.
    \end{itemize}
    Evvel megmutattuk, hogy bármely két maximális lineárisan független oszloprendszer azonos elemszámú, és számuk megegyezik a mátrix feszítőrangjával.


    Legyen $r_w$ az $A$ mátrix egyik rögzített maximális lineárisan független sorrendszerének elemszáma.
    \begin{itemize}
        \item 
            Ezen sorokat egy $C\in\mathbb{F}^{r_w\times m}$ mátrixba téve 
            -- a maximalitás miatt -- létezik olyan $B\in\mathbb{F}^{n\times r_w}$ mátrix, 
            amelyre $A=BC$, azaz $r\leq r_w$.
        \item
            Most tekintsünk egy tetszőleges olyan
            \(
            A=BC
            \)
            felbontást, 
            ahol $B\in\mathbb{F}^{n\times r}$ és $C\in\mathbb{F}^{r\times m}$.
            Jelölje most $V$ a $C$ mátrix sorai lineáris burkát. 
            Az $A$ mátrix fent rögzített maximális lineárisan független sorrendszere egy lineárisan független rendszer e
            $V$ vektortérben,
            és $C$ sorai pedig egy generátorrendszert alkotnak ugyanebben a $V$ vektortérben.
            A 2. állitás szerint $r_w\leq r$.
    \end{itemize}
    Evvel azt is megmutattuk, 
    hogy bármely két maximális lineárisan független sorrendszer azonos elemszámú, 
    és számuk megegyezik a mátrix feszítőrangjával.
\end{proof}
\begin{proof}[3.\Rightarrow 4.]
    Tegyük fel, hogy $AB=I$.
    Az identitás mátrix rangja nyilván $n$.
    A 3. állitás miatt a feszítőrang is $n$.
    Ha $A$ oszlopai nem lennének lineárisan függetlenek,
    akkor lenne $A=CD$ felbontás, ahol $C\in\mathbb{F}^{n \times r}$ és $D\in\mathbb{F}^{r\times n}$ valamely $r<n$ mellett.
    Ekkor persze $I=AB=\left( CD \right)B=C\left( DB \right)$ is teljesülne, 
    ahol $DB\in\mathbb{F}^{r\times n}$ ellentmondva az identitás mátrix feszítőrangja definíciójának.
    Világos, hogy
    \[
        A\left( BA-I \right)=
        A\left( BA \right)-AI=
        \left( AB \right)A-AI=IA-AI=0.
    \]
    Figyelembe véve, hogy $A$ oszlopai lineáris függetlenek, ez csak úgy lehetséges, ha $BA-I$ a zéró mátrix, ergo $BA=I$.
\end{proof}
\begin{proof}[4.\Rightarrow 1.]
    Legyen $A\in\mathbb{F}^{n\times m}$ a homogén lineáris egyenletrendszer együttható mátrixa,
    ahol $n$ az egyenletek száma, $m$ az ismeretlenek száma.
    Azt kell megmutatnunk, hogy az oszloprendszer lineárisan összefüggő.
    Ha független lenne, akkor
    egészítsük ki e mátrixot alulról $m-n$ darab csupa nullákat tartalmazó sorral.
    Mivel $m>n$ ezért a kiegészített $Q\in\mathbb{F}^{m\times m}$ mátrix legalsó sora csak nullát tartalmaz.
    Mivel $A$ oszlopai lineárisan függetlenek, ezért $Q$ oszlopai is azok.
    Emiatt létezik $P\in\mathbb{F}^{m\times m}$ mátrix, amelyre $PQ=I$.
    A 3. állitás szerint $I=QP$ is teljesül, 
    ami azt jelenti, hogy $I$ utolsó sora a csupa nullákat tartalmaz, ami ellentmondás.
    Beláttuk tehát, hogy $A$ oszlopai lineárisan összefüggőek, azaz az eredeti egyenletrendszernek van nem triviális megoldása.
\end{proof}

%%\part{E-dúr hegedűverseny No. 1, Op. 8, RV 269, ,,La primavera''}
\part{Tavasz}
\chapter{Mátrixok és lineáris operációk}
\scwords Mátrixok és lineáris operációk kapcsolatát vizsgáljuk.
Azt már a mátrixok bevezetésekor is láttuk, hogy egy $n\times m$ méretű mátrix egyben tekinthető valamely
$\mathbb{F}^m\to\mathbb{F}^n$ lineáris operációnak olyan módon,
hogy az egy $x\in\mathbb{F}^m$ oszlopvektorhoz a mátrix szorzás definíciójának megfelelően
az $A\cdot x\in\mathbb{F}^n$ oszlopvektort rendeli.
Ebben a fejezetben a mátrixok ezen interpretációját erősítjük tovább.

\section{Rang-defektus--tétel következménye}
Láttuk, hogy tetszőleges $A\in L\left( V,W \right)$ lineáris operáció mellett
\[
    \nu\left( A \right)+\rho\left( A \right)=\dim V,
\]
ahol $\nu\left( A \right)=\dim\left( \ker A \right)$ az $A$ defektusa,
$\rho\left( A \right)=\dim\left(\im A \right)$ az $A$ rangja.
Ebből adódóan, ha $A\in L\left( V \right)$ egy lineáris transzformáció,
akkor $\nu\left( A \right)=0$ és $\rho\left( A \right)=\dim V$ egymással ekvivalens feltevések.

Itt az első feltétel pontosan $A$ injektivitását, 
míg a második feltétel pontosan $A$ szürjektivitását jelenti.
Azt látjuk tehát, 
hogy lineáris transzformációk esetén a transzformáció injektivitása és szürjektivitása egyszerre teljesül, 
vagy egyszerre nem teljesül:
\begin{proposition}
    Legyen az $V$ egy véges dimenziós vektortér, és $A\in L\left( V \right)$ egy lineáris transzformáció.
    \begin{enumerate}
        \item 
            Ekkor az alábbi feltételek egymással ekvivalensek.
        \begin{enumerate}
            \item $A$ injektív;
            \item $A$ szürjektív;
            \item $A$ vektortér--izomorfizmus.
            \item Létezik $B\in L\left( V \right)$ lineáris transzformáció, amelyre $A\circ B=I$.
        \end{enumerate}
        \item
            Ha a fenti d) fennáll, akkor $B\circ A=I$ is teljesül.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Az első három pont ekvivalens voltát már meggondoltuk.

    Ha $A\circ B=I$, akkor $A$ szürjektív, hiszen az $y\in V$ vektor elő áll,
    mit a $By$ vektor $A$ képe.
    
    Megfordítva, ha $A$ szürjektív, akkor a) szerint injektív is, van tehát $V\to V$ inverze.
    De lineáris függvény inverze is lineáris, 
    így a $B=A^{-1}$ jelölést bevezetve találtunk $B\in L\left( V \right)$
    transzformációt, amelyre $B\circ A=I$.

    Most tegyük fel, hogy valamely $B\in L\left( V \right)$-re $A\circ B=I$.
    Ekkor a) szerint $A$ injektív is, ergo $\ker A=\left\{ 0 \right\}$.
    No de minden $x\in V$ mellett
    \[
        A\left( (B\circ A)x-x \right)
        =
        A\left( B\left( Ax \right) \right)-Ax
        =
        \left( A\circ B \right)\left( Ax \right)-Ax
        =
        I\left( Ax \right)-Ax
        =
        Ax-Ax=0,
    \]
    ezért $(B\circ A)x-x\in\ker A=\left\{ 0 \right\}$. 
    $(B\circ A)x=x$ minden $x\in V$ mellett, ami éppen azt jelenti, hogy $B\circ A=I$ is fennáll.
\end{proof}
Kiderült tehát, hogy ugyanúgy mint amikor mátrixok regularitásáról volt szó, 
az $A\in L\left( V \right)$ lineáris transzformáció pontosan akkor injektív, 
ha van olyan $B\in L\left( V \right)$ lineáris transzformáció, 
amelyre $AB=I$ (vagy $BA=I$) teljesül.
Ekkor $A^{-1}=B$.

\section{Mátrixok tere mint koordináta-tér}
Világos, hogy adott $\mathbb{F}$ test feletti $n\times m$ méretű mátrixok az $\mathbb{F}$ feletti
vektorteret alkotnak.
Az is világos, hogy ha $A_{i,j}$ jelöli azt az $n\times m$ méretű mátrixot,
amelynek minden helyén $0$ van az $i$-edik sor $j$-edik helyének kivételével, ahol $1$ szerepel,
akkor az 
\[
    \left\{ A_{i,j}:i=1,\dots,n;j=1,\dots,m\right\}
\]
mátrixok rendszere bázisa az $\mathbb{F}^{n\times m}$ térben.
Így persze $\dim \mathbb{F}^{n\times m} = n\cdot m$.

Most áttérünk az $L\left( V,W \right)$ lineáris operációk vektorterének vizsgálatára.
Először azt gondoljuk meg, hogy lineáris transzformáció egy bázison tetszőlegesen és egyértelműen előírható.
\begin{proposition}\label{pr:bazisonegyertelmu}
    Legyenek $V$ és $W$ ugyanazon test feletti vektorterek.
    Legyen $V$-ben a $\left\{ v_1,\dots,v_m \right\}$ egy bázis, 
    és rögzítsünk $W$-ben egy tetszőleges $m$ elemű $\left\{ w_1,\dots,w_m \right\}$ vektorrendszert.

    Ekkor létezik egyetlen egy $A\in L\left( V,W \right)$ lineáris operáció,
    amelyre $Av_j=w_j$ minden $j=1,\dots,m$ esetén.
\end{proposition}
\begin{proof}
    Definiáljuk az $A:V\to W$ függvényt a következőképpen.
    Minden $v\in V$ egyértelműen áll elő mint $v=\sum_{j=1}^m\alpha_jv_j$ alakban.
    Egy ilyen $v$ mellett legyen
    \[
        A\left( v \right)=\sum_{j=1}^m\alpha_jw_j.
    \]
    Világos, hogy $A:V\to W$ függvény jól definiált, hiszen a rögzített bázisban minden elem előáll és egyetlen egyféleképpen
    áll elő mint a bázis elemek egy lineáris kombinációja.
    Megmutatjuk, hogy az így definiált $A$ függvény egy lineáris operáció.
    Legyen $x_1=\sum_{j=1}\alpha_jv_j$ és
    $x_2=\sum_{j=1}\beta_jv_j$.
    Tetszőleges $\alpha,\beta\in\mathbb{F}$ mellett
    \[
        \alpha x_1+\beta x_2=
        \sum_{j=1}^m\left( \alpha\alpha_j+\beta\beta_j \right)v_j,
    \]
    tehát $A$ definíciója szerint
    \[
        A\left( \alpha x_1+\beta x_2 \right)=
        \sum_{j=1}^m\left( \alpha\alpha_j+\beta\beta_j \right)w_j
        =
        \alpha\sum_{j=1}^m\alpha_jw_j+
        \beta\sum_{j=1}^m\beta_jw_j=
        \alpha A\left( x_1 \right)+\beta A\left( x_2 \right).
    \]
    Ez éppen $A$ függvény linearitását jelenti.
    Az is világos, hogy $v_j=0v_1+\ldots+1v_j+\ldots +0v_m$, tehát az $A$ függvény definíciója értelmében
    \[
        Av_j=w_j
    \] valóban fennáll minden $j=1,\dots,m$ mellett.

    Most tegyük fel, hogy $B\in L\left( V,W \right)$ szintén teljesíti a $Bv_j=w_j$ feltételeket.
    Ekkor tetszőleges $v\in V$ mellett, ha $v=\sum_{j=1}^m\alpha_jv_j$ alakú,
    akkor az $A$ definíciója és $B$ linearitása miatt
    \[
        Av=\sum_{j=1}^m\alpha_jw_j=
        \sum_{j=1}^m\alpha_jBv_j=
        B\left( \sum_{j=1}^m\alpha_jv_j \right)=Bv,
    \]
    ami éppen azt jelenti, hogy $A=B$.
\end{proof}
\begin{proposition}
    Legyenek az $\left\{ e_1,\dots,e_m \right\}\subseteq V$ és az
    $\left\{ f_1,\dots,f_n \right\}\subseteq W$ bázisok rögzítve.
    Definiáljuk valamely rögzített $i\in\left\{ 1,\dots,n \right\}$ és valamely rögzített
    $j\in \left\{ 1,\dots,m \right\}$ mellett az $A_{i,j}\in L\left( V,W \right)$ lineáris operációt az
    \begin{equation}
        A_{i,j}\left( e_k \right)=\delta_{j,k}f_i
        \label{eq:aij}
    \end{equation}
    azonosságokkal, ahol $k=1,\dots,m$.
    Ekkor az $\left\{ A_{i,j}: i=1,\dots,n;j=1,\dots,m \right\}$
    lineáris operációk rendszere egy bázisa az $L\left( V,W \right)$ vektortérnek,
    ezért 
    \[
        \dim L\left( V,W \right)=n\cdot m.\qedhere
    \]
\end{proposition}
\begin{proof}
    Azt kell először is látni, hogy (\ref{eq:aij}.) azonosságok összesen az előző \ref{pr:bazisonegyertelmu}.~állítás
    alkalmazását írják elő rögzített $i,j$ mellett az 
    \begin{math}
        \left\{ e_1,\dots,e_j,\dots,e_m \right\}
    \end{math}
    és az
    \begin{math}
        \left\{ 0,\dots,0,f_i,0,\dots,0 \right\}
    \end{math}
    két pontosan $m$ elemű vektorrendszerre.
    Ezt úgy is fogalmazhatjuk, hogy az $A_{i,j}$ az a lineáris operáció,
    amely a bázis minden nem $j$-edik elemét zérusra viszi, és a $j$-edik bázis elemet
    pedig $f_i$-re.
    \Aref{pr:bazisonegyertelmu}. állítás szerint vannak ilyen $A_{i,j}$ lineáris transzformációk,
    és minden $i,j$ pár mellett csak egyetlen egy van.

    Most megmutatjuk, hogy az $\left\{ A_{i,j}:i=1,\dots,n;j=1,\dots m \right\}$ lineárisan független
    rendszer.
    Legyenek az $\alpha_{i,j}\in\mathbb{F}$ számok olyanok,
    amelyekre 
    \begin{math}
        \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}=0.
    \end{math}
    Ekkor tetszőleges $k\in\left\{ 1,\dots,m \right\}$ esetén
    \[
        0=
        \left( \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j} \right)e_k
        =
        \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}e_k 
        =
        \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}\delta_{j,k}f_i
        =
        \sum_{i=1}^n\alpha_{i,k}f_i.
    \]
    No de az $\left\{f_1,\dots,f_n  \right\}$ egy lineárisan független rendszer, 
    ergo csak a  triviális lineáris kombinációja zérus, 
    ergo $\alpha_{i,k}=0$ minden $i=1,\dots,n$ mellett.
    Persze ez minden $k$ mellett megismételhető, tehát azt kaptuk, hogy valamennyi $\alpha_{i,j}$ együttható
    zérus.

    Most azt mutatjuk meg, hogy az $\left\{ A_{i,j}:i=1,\dots,n;j=1,\dots m \right\}$ egy
    generátorrendszere az $L\left( V,W \right)$ vektortérnek.
    Legyen $A\in L\left( V,W \right)$ egy rögzített lineáris operáció.
    Definiáljuk az $\alpha_{i,j}$ számokat, 
    mint az $Ae_{j}\in W$ vektor $\left\{ y_1,\dots,y_n \right\}$ bázisban felírt koordináta-vektorának 
    $j$-edik elemét. 
    Magyarul
    \[
        Ae_j=
        \sum_{i=1}^n\alpha_{i,j}f_i.
    \]
    Ekkor minden $k\in\left\{ 1,\dots,m \right\}$ mellett
    \[
        \left( \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j} \right)e_k
        =
        \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}e_k 
        =
        \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}\delta_{j,k}f_i
        =
        \sum_{i=1}^n\alpha_{i,k}f_i
        =Ae_k.
    \]
    Ez azt jelenti, hogy az $A$ és az 
    \begin{math}
        \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}
    \end{math}
    lineáris operátorok az $\left\{ e_1,\dots,e_m \right\}$ bázis minden elemén megegyeznek.
    Láttuk, egy bázison felvett értékek már egyértelműen meghatározzák a lineáris operációt,
    ezért e két lineáris operáció is azonos.
    Találtunk tehát $\alpha_{i,j}$ számokat, amelyekre
    \begin{math}
        A=
        \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j},
    \end{math}
    ami azt jelenti, hogy $A$ előáll mint az $A_{i,j}$ függvények valamely lineáris kombinációja.
\end{proof}
Érdemes későbbre is eltennünk magunkban, 
hogy hogyan találtunk az adott $A$ operátorhoz azon $\alpha_{i,j}$ számokat, 
amelyekre az
\[
        A=
        \sum_{i=1}^n\sum_{j=1}^m\alpha_{i,j}A_{i,j}
        =
        \sum_{j=1}^m\sum_{i=1}^n\alpha_{i,j}A_{i,j}
\]
egyenlőség teljesül: 
$\alpha_{i,j}$ a $j$-edik bázis elem $A$ képének $i$-edik koordinátája.
Ugyanez a koordináta-vektor fogalmával: 
Az $Ae_j\in W$ vektornak az $\left\{ y_,\dots,y_n \right\}$ bázisban felírt koordináta-vektorának az elemei 
adják az $\alpha_{1,j},\alpha_{2,j},\dots,\alpha_{n,j}$ számokat, formálisabban:
\[
    \left[ Ae_j \right]_{\left\{ y_1,\dots,y_n \right\}}
    =
    \begin{pmatrix}
        \alpha_{1,j}\\ \alpha_{2,j}\\ \vdots \\ \alpha_{n,j}
    \end{pmatrix}.
\]

A következő gondolat előtt arra kell emlékeznünk, 
hogy minden véges dimenziós vektortér izomorf a koordináta-terével.
Láttuk, hogy $\dim L\left( V,W \right)=\dim V\cdot\dim W$.
De mi lesz $L\left( V,W \right)$ koordináta-tere?
A válaszhoz rögzítenünk kell a bázis elemek egy sorrendjét.
\begin{definition}[lineáris operátor mátrixa]
    (\Aref{eq:aij}.) azonosságokkal definiált $A_{i,j}$ lineáris operátorokat állítsuk a következő sorrendbe:
    \[
    \{ 
        \underbrace{A_{1,1},A_{2,1},\dots,A_{n,1}}_{j=1},
        \underbrace{A_{1,2},A_{2,2},\dots,A_{n,2}}_{j=2},
        \underbrace{A_{1,3},A_{2,3},\dots,A_{n,3}}_{j=3},
        \underbrace{\dots,\dots,\dots,\dots,\dots}_{j=4,\dots,m-1},
        \underbrace{A_{1,m},A_{2,3},\dots,A_{n,m}}_{j=m}
    \}
    \]
    Láttuk, hogy a fent konstruált $\alpha_{i,j}$ számokkal 
    \[
        A=
        \sum_{j=1}^m\sum_{i=1}^n\alpha_{i,j}A_{i,j}.
    \]
    Ez azt jelenti, hogy az $A\in L\left( V,W \right)$ függvénynek a fenti bázisban felírt koordináta-vektora
    az $\alpha_{i,j}$ elemekből a fenti sorrendben képzett oszlopvektor, azaz
    \[
    \left[ A \right]=
    \begin{pmatrix}
        \alpha_{1,1}\\ \alpha_{2,1}\\ \vdots \\ \alpha_{n,1}\\
        \alpha_{1,2}\\ \alpha_{2,2}\\ \vdots \\ \alpha_{n,2}\\
        \alpha_{1,3}\\ \alpha_{2,3}\\ \vdots \\ \alpha_{n,3}\\
        \vdots \\ \vdots\\
        \alpha_{1,m}\\ \alpha_{2,m}\\ \vdots \\ \alpha_{n,m}
    \end{pmatrix}\tag{\dag}
    \]
    Mint minden vektortér így az $L\left( V,W \right)$ is izomorf a koordináta-terével, ergo $L\left( V,W \right)$ izomorf 
    az $\mathbb{F}^{n\cdot m}$ vektortérrel.

    Érdemes a koordinátatér elemeit, azaz az $n\cdot m$ elemből álló oszlopvektorokat $n$ koordinátánként megtörni,
    és evvel egy $n\times m$-es mátrixba rendezni.
    Ezt a jelölést alkalmazva az $A$ lineáris operációnak a fenti bázisban felírt koordináta vektora az az 
    $n\times m$-es mátrix,
    amelyre
    \[
    [A]=
    \begin{pmatrix}
        \alpha_{1,1}&\alpha_{1,2}&\alpha_{1,3}&\dots&\alpha_{1,m}\\
        \alpha_{2,1}&\alpha_{2,2}&\alpha_{2,3}&\dots&\alpha_{2,m}\\
        \alpha_{3,1}&\alpha_{3,2}&\alpha_{3,3}&\dots&\alpha_{3,m}\\
        \vdots      &\vdots      &\vdots      &\ddots&\vdots\\
        \alpha_{n,1}&\alpha_{n,2}&\alpha_{n,3}&\dots&\alpha_{n,m}
    \end{pmatrix}.
    \]
    Ezt a mátrixot nevezzük az 
    $A\in L\left( V,W \right)$ \emph{lineáris operáció mátrixának}\index{lineáris operáció mátrixa}
    az előre rögzített 
    $\left\{e_1,\dots,e_m  \right\}\subseteq V$ és $\left\{ f_1,\dots,f_m \right\}$ bázisok mellett.
\end{definition}
Meggondoltuk tehát, hogy a kapcsolat lineáris operáció és mátrixa között azonos avval a kapcsolattal,
ami általában egy vektor és koordinátái közt van.

Nagyon fontos, hogy egy lineáris operáció mátrixát fel tudjuk írni, emiatt összefoglalom az eddigieket.
\begin{proposition}
    Tegyük fel, hogy $A\in L\left( V,W \right)$ egy lineáris operáció.
    Rögzítsük a $V$ és a $W$ vektortér egy-egy bázisát.
    Legyen tehát $\{e_1,\dots,e_m\}\subseteq V$ egy bázis és
    $\left\{ f_1,\dots,f_n \right\}\subseteq W$ egy bázis.
    Az $A$ operációnak a fenti rögzített bázisokban felírt mátrixa az az
    $n\times m$ méretű 
    $[A]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ f_1,\dots,f_n \right\}}}$ 
    mátrix,
    \footnote{Ha világos, hogy mely bázisokat rögzítjük akkor a nehézkes
    $[A]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ f_1,\dots,f_n \right\}}}$
    jelölés helyett csak $[A]$-t írunk.
    Persze mindig tudnunk kell, hogy az $A$ lineáris operátor mely bázisokban felírt mátrixáról van szó.
    }
    amelynek $j$-edik oszlopa az $Ae_j$ vektor $\left\{ y_1,\dots,y_n \right\}$ bázisban felírt koordinátája.
    Formálisabban:
    \[
        [A]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ f_1,\dots,f_n \right\}}}^j
        =
        \left[ Ae_j \right]_{\left\{ y_1,\dots,y_n \right\}}.
        \qedhere
    \]
\end{proposition}
Speciálisan, ha $V=W$, akkor $A:V\to V$ lineáris transzformációról beszélünk.
Amikor egy lineáris transzformáció mátrixáról van szó, 
akkor az mindig úgy értendő, hogy a $V$ vektortérnek mint az értelmezési tartománynak és a
$V$ vektortérnek mint értékkészletnek is ugyanazt a bázisát választjuk.
Ekkor tehát $A$ lineáris transzformáció 
$[A]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ e_1,\dots,e_m \right\}}}$ 
mátrixára:
\footnote{
    A nehézkes 
    $[A]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ e_1,\dots,e_m \right\}}}$ 
    helyett egyszerűbben
    $[A]_{\left\{ e_1,\dots,e_m \right\}}$, vagy még inkább ha a szövegkörnyezetből nyilvánvaló, hogy mely bázisra gondolunk,
    akkor csak a $[A]$ jelölést használjuk.
}
\[
        [A]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ e_1,\dots,e_m \right\}}}^j
        =
        \left[ Ae_j \right]_{\left\{ e_1,\dots,e_m \right\}}.
\]

Meggondoltuk tehát, hogy az $L\left( V,W \right)$ lineáris operátorok vektortere izomorf az
$\mathbb{F}^{\dim W\times \dim V}$ mátrixok vektorterével. 
Az izomorfizmus tehát az a leképezés, 
amely egy lineáris transzformációhoz hozzárendeli annak 
-- valamely előre megadott bázisokban felírt -- 
mátrixát.
Emiatt persze $A,B\in L\left( V,W \right)$ és $\alpha,\beta\in\mathbb{F}$ mellett
\begin{equation}\label{eq:vt}
    \left[ \alpha A+\beta B \right]=\alpha\left[ A \right]+\beta\left[ B \right].
\end{equation}
Izomorf struktúrák közt nem érdemes különbséget tenni, viszont vigyáznunk kell arra,
hogy olyan fogalmakat definiáljunk, amelyek invariánsak az izomorfiára.
Például lineáris operáció rangja definíció szerint a képtere dimenziója, 
mátrix rangja definíció szerint a feszítőrang, azaz a legkisebb 
$r$ szám amelyre a mátrix felírható $n\times r$ és egy $r\times m$ méretű mátrix szorzataként.
Látni fogjuk, hogy lineáris operátornak és mátrixának rangja azonos.
\begin{proposition}
    Legyen $A\in L\left( V,W \right)$ lineáris operátor és $x\in V$ egy vektor.
    Rögzítsük az $\left\{ e_1,\dots,e_m \right\}\subseteq V$ és az $\left\{ f_1,\dots,f_n \right\}\subseteq W$ bázisokat.
    Ekkor 
    \[
        [Ax]_{\left\{ f_1,\dots,f_n \right\}}
        =
        [A]_{\substack{\left\{ e_1,\dots,e_n \right\}\\ \left\{ f_1,\dots,f_n \right\}}}
        \cdot 
        [x]_{\left\{ e_1,\dots,e_m \right\}}.
    \]
    Emiatt $\im A$ és $\im[A]$ egymással izomorf alterek, hasonlóan $\ker A$ és $\ker[A]$ egymással
    izomorf vektorterek, így dimenziójuk is azonos.
    Konkrétan $A$ lineáris operátornak és az $[A]$ mátrixának a rangja is defektusa is azonos.\index{rang}\index{defektus}
\end{proposition}
\begin{proof}
    Legyen 
    \begin{math}
        [x]_{\left\{ e_1,\dots,e_m \right\}}
        =
        \begin{pmatrix}
            \xi_1\\ \xi_2 \\ \vdots \\ \xi_m
        \end{pmatrix}.
    \end{math}
    Ez azt jelenti, 
    hogy $x=\sum_{j=1}^m\xi_je_j$, így 
    $Ax=\sum_{j=1}^m\xi_jAe_j$.
    Véve az $Ax$ vektor $\left\{ f_1,\dots,f_n \right\}$ bázisban felírt koordináta-vektorát 
    \begin{multline*}
        [Ax]_{\left\{ f_1,\dots,f_n \right\}}
        =
        \left[ \sum_{j=1}^n\xi_jAe_j \right]_{\left\{ f_1,\dots,f_n \right\}}
        =
        \sum_{j=1}^n\xi_j[Ae_j]_{\left\{ f_1,\dots,f_n \right\}}
        =
        \sum_{j=1}^n\xi_j[A]_{\substack{\left\{ e_1,\dots,e_n \right\}\\ \left\{ f_1,\dots,f_n \right\}}}^j
        =\\
        [A]_{\substack{\left\{ e_1,\dots,e_n \right\}\\ \left\{ f_1,\dots,f_n \right\}}}
        \cdot 
        [x]_{\left\{ f_1,\dots,f_n \right\}}.
    \end{multline*}

    Ebből persze már könnyen adódik, ahogy $Ax=0$ ekvivalens $[A][x]=0$ feltétellel, emiatt 
    $\ker A$ és $\ker [A]$ izomorfak, 
    így $\nu(A)=\nu([A])$.

    Hasonlóan $\im A$ és $\im [A]$ is izomorfak.
    A rangtétel szerint $[A]$ mátrix rangja megegyezik az oszlopvektorai generálta altérben lévő maximális lineárisan
    független rendszer elemszámával.
    Mivel egy vektor pontosan akkor van $[A]$ képterében, ha előáll mint az oszlopai lineáris kombinációja,
    ezért $[A]$ rangja azonos $[A]$ képterének dimenziójával, 
    így $\rho(A)=\rho([A])$.
\end{proof}

\section{Lineáris operátorok szorzata}
Emlékezzünk arra, hogy a lineáris operáció mátrixát úgy kaptuk, hogy a koordinátáit $n$ elemenként
megtörve az oszlop vektort egy mátrixszá rendeztük át.
Ennek az átrendezésnek az igazi értelme, hogy ilyen módon a mátrix szorzás művelet a lineáris operátorok kompozíciójával
kapcsolódik össze.
\begin{proposition}
    Legyenek $V,Z,W$ ugyanazon test feletti véges dimenziós vektorterek,
    $B \in L\left( V,Z \right)$ és $A \in L\left( Z,W \right)$.
    Rögzítsük az 
    \[
        \left\{ e_1,\dots,e_m \right\}\subseteq V,\quad
        \left\{ z_1,\dots,z_r \right\}\subseteq Z,\quad
        \left\{ f_1,\dots,f_n \right\}\subseteq W
    \]
    bázisokat.
    Jelölje $C=A\circ B$ a kompozíció lineáris operátort.

    Ekkor $C$ mátrixa az $A$ és $B$ mátrixának szorzata.
    Formálisabban:
    \[
        \left[ C \right]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ f_1,\dots,f_n \right\}}}
        =
        \left[ A \right]_{\substack{\left\{ z_1,\dots,z_r \right\}\\ \left\{ f_1,\dots,f_n \right\}}}
        \cdot
        \left[ B \right]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ z_1,\dots,z_r \right\}}}.\qedhere
    \]
\end{proposition}
\begin{proof}
    Először is ellenőrizzük, hogy értelmes-e az állításban felírt formula.
    $\left[ A \right]$ mérete $n\times r$, $\left[ B \right]$ mérete $r\times m$.
    Így az $[A]\cdot[B]$ szorzat értelmes és a szorzás eredménye egy $n\times m$ mátrix.
    A $[C]$ szintén egy $n\times m$ méretű mátrix, 
    így a bal és a jobb oldal összehasonlítása is értelmes.

    Már csak azt kell meggondolni, 
    hogy a bal oldali mátrix minden eleme azonos a jobb oldali szorzatmátrix megfelelő elemével.
    Az $\alpha,\beta,\gamma$ szimbólumokkal jelöljük az $[A],[B], [C]$ mátrixok megfelelő elemeit.
    Ez azt jelenti, hogy
    \[
        Be_j=\sum_{k=1}^r\beta_{k,j}z_k,\quad
        Az_k=\sum_{i=1}^n\alpha_{i,k}f_i,\quad
        Ce_j=\sum_{i=1}^n\gamma_{i,j}f_i.
    \]
    Így azt kapjuk, hogy minden $j=1,\dots,m$ mellett
    \begin{multline*}
        \sum_{i=1}^n\gamma_{i,j}f_i
        =
        Ce_j
        =A\left( Bej \right)
        =
        \sum_{k=1}^r\beta_{k,j}Az_k
        =
        \sum_{k=1}^r\beta_{k,j}\left( \sum_{i=1}^n\alpha_{i,k}f_i \right)
        =\\
        \sum_{k=1}^r\sum_{i=1}^n\beta_{k,j}\alpha_{i,k}f_i
        =
        \sum_{i=1}^n\sum_{k=1}^r\alpha_{i,k}\beta_{k,j}f_i
        =
        \sum_{i=1}^n\left( \sum_{k=1}^r\alpha_{i,k}\beta_{k,j} \right)f_i.
    \end{multline*}
    No de az $\{f_1,\dots,f_n\}$ rendszer lineárisan független,
    tehát a lineáris burkában minden elem előállítása egyértelmű,
    ami éppen azt jelenti, hogy minden $i=1,\dots,n$ és minden $j=1,\dots,m$ mellett
    \[
        [C]_{i,j}=\gamma_{i,j}
        =\sum_{k=1}^r\alpha_{i,k}\beta_{k,j}
        =\left( \left[ A \right]\cdot\left[ B \right] \right)_{i,j}.\qedhere
    \]
\end{proof}
Azért, hogy teljes legyen az analógia a lineáris operátorok kompozíciója és a mátrixok szorzása műveletek közt,
a lineáris operátorok $A\circ B$ kompozícióját is $A\cdot B$ módon, vagy még egyszerűbben $AB$ módon jelöljük.
A jelölést a szóhasználat is követi:
\begin{definition}[lineáris transzformációk szorzata]
    A lineáris operátorok kompozíció műveletét \emph{szorzatnak}\index{lineáris transzformációk szorzata} is mondjuk.
\end{definition}
\noindent Ilyen módon ha $A$ és $B$ két olyan lineáris operátor, amelynek kompozíciója -- azaz szorzata -- értelmes,
akkor a mátrixaik szorzata is értelmes, továbbá
\begin{equation}\label{eq:gyuru}
    [AB]=[A][B].
\end{equation}
\begin{definition}[lineáris operátor hatványai]
    A lineáris transzformációk a bevezetett szorzás művelettel egységelemes gyűrűt alkotnak,
    ahol az egységelem az
    $I:V\to V$ az identitás operáció.

    Legyen $A\in L\left( V \right)$ egy lineáris transzformáció.
    Ennek $0$-dik hatványát definiálja $A^0=I$,
    Ha valamely $n$ nem negatív egész mellett $A^n$ már definiált,
    akkor legyen $A^{n+1}=AA^n$.
\end{definition}
Világos, hogy ha $n,m$ nem negatív egészek, akkor $A^{n+m}=A^n\cdot A^m$.
Az is nyilvánvaló, hogy az $I$ identitás operációnak akár melyik bázisban felírt mátrixa ugyanaz a mátrix,
mégpedig az identitás mátrix (ahol minden elem zérus, kivétel a diagonális elemek, amelyek értéke 1.)
\begin{proposition}
    Legyen $A\in L\left( V \right)$ egy lineáris transzformáció, és legyen rögzítve a tér egy bázisa,
    amelyben felírjuk $A$ transzformáció $[A]$ mátrixát.
    Ekkor 
    \(
        \left[ A^n \right]=\left[ A \right]^n
    \),
    továbbá $A$ pontosan akkor izomorfizmus, ha $[A]$ reguláris\index{reguláris mátrix} és 
    \begin{math}
        \left[ A^{-1} \right]
        =
        \left[ A \right]^{-1}.
    \end{math}
\end{proposition}
\begin{proof}
    A (\ref{eq:gyuru}.) azonosságot alkalmazva $B=A$ mellett nyilvánvaló indukcióval kapjuk az
    $\left[ A^n \right]=\left[ A \right]^n$
    azonosságot.

    Világos, hogy $A$ pontosan akkor izomorfizmus, 
    ha létezik $B\in L\left( V \right)$, amelyre $AB=I$.
    No de ez ekvivalens avval, hogy $[A][B]=[I]$, ami pedig a szükséges és elegendő feltétele 
    $[A]$ mátrix invertálhatóságának.
    Ekkor $[A]^{-1}=[B]=\left[ A^{-1} \right]$.
\end{proof}
A (\ref{eq:vt}.) és a (\ref{eq:gyuru}.) szerint egy lineáris operátorhoz hozzárendeli valamely báziskban
felírt mátrixát egy olyan bijekció, ami tartja a gyűrű műveleteket.
Az $L\left( V \right)$ és a $F^{\dim V\times \dim V}$ tehát olyan egységelemes (nem kommutatív) gyűrűk,
amelyek közt van a gyűrű műveleteket tartó bijekció (gyűrű-izomorfizmus)\index{gyűrű-izomorfizmus}.

\chapter{Általános bázis-transzformáció}
\scwords Egy vektor koordinátái függnek a bázis megválasztásától.
Az elemi bázistranszformáció arra szolgál hogy felírjuk az új bázisban a vektor koordinátáit,
amikor az új bázis és a régi bázis csak egyetlen vektorban különbözik.
A fejezetben általánosabban oldjuk meg a problémát, mikor 
az új bázis és a régi bázis elemei tetszőlegesen különbözhetnek.

Vegyük fel a $V$ vektortér egy-egy bázisát.
Nevezzük az $\left\{ e_1,\dots,e_n \right\}$ bázist régi bázisnak,
és nevezzük az $\left\{ f_1,\dots,f_n \right\}$ bázis elemeit új bázisnak.
A kérdések a következők:
\begin{enumerate}
    \item Ha ismerjük egy $x\in V$ vektor régi bázisra vonatkozó koordinátáit, 
        akkor hogyan számítható ugyanennek a vektornak az új bázisban felírt koordináta-vektora?
    \item Ha ismerjük egy $A\in L\left( V \right)$ lineáris transzformációnak a régi bázisban felírt mátrixát,
        akkor hogyan számolható ki az $A$-nak valamely új bázisban felírt mátrixa?
    \item Ha ismerjük egy $A\in L\left( V, W \right)$ lineáris operációnak a régi bázis páron felírt mátrixát,
        akkor hogyan számítható $A$-nak az valamely új bázis páron felírt mátrixa?
\end{enumerate}
\begin{definition}
    Legyenek az 
    $\left\{ e_1,\dots,e_n \right\}$  és
    $\left\{ f_1,\dots,f_n \right\}$ bázisok rögzítve.
    Tekintsük azt a $B$ lineáris transzformációt, 
    amelyre $Be_j=f_j$ teljesül minden $j=1,\dots,n$ mellett.
    Ezt a lineáris transzformációt nevezzük az
    $\left\{ e_1,\dots,e_n \right\}$ bázisról az 
    $\left\{ f_1,\dots,f_n \right\}$ bázisra való \emph{áttérés transzformációjának}\index{az@áttérés transzformáció}.
\end{definition}
Világos, hogy ha $B$ az áttérés transzformáció, akkor ennek 
a régi bázisban felírt $[B]$ mátrixa egy olyan $n\times n$ méretű mátrix,
amelynek $j$-edik oszlopa a $Be_j=f_j$ vektornak a régi bázisban felírt koordinátája.
Mivel $B$ szürjektív, ergo injektiv is, tehát $B$ egy izomorfizmus,
ennek megfelelően a $\left[ B \right]$ mátrix reguláris mátrix, azaz létezik
$\left[ B \right]^{-1}$ inverze.
\footnote{
Könnyen látható, hogy $B$-nek a régi és az új bázisban felírt mátrixa azonos, de ez később egyszerű következményként is adódik.}
\section{Vektor koordinátái az új bázisban}
\begin{proposition}
    Legyen az $\left\{ e_1,\dots,e_m \right\}$ a régi bázis,
    és $\left\{ f_1,\dots,f_n \right\}$ az új bázis.
    Jelölje $B$ a régi bázisról az új bázisra való áttérést
    Ekkor tetszőleges $x\in V$ vektor mellett
    \[
        \left[ x \right]_\uj
        =
        \left[ B \right]_\rgi^{-1}
        \cdot
        \left[ x \right]_\rgi.\qedhere
    \]
\end{proposition}
\begin{proof}
    Legyen 
    $
    \left[ x \right]_\uj
    =
    \begin{pmatrix}
        \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n
    \end{pmatrix}.
    $
    Ez azt jelenti, hogy $x=\sum_{j=1}^n\alpha_jf_j$.
    Így felírva az $x$ vektornak a régi bázisra vonatkozó koordináta-vektorát
    \[
        \left[ x \right]_\rgi
        =\left[ \sum_{j=1}^n\alpha_jf_j \right]_\rgi
        =\sum_{j=1}^n\alpha_j\left[ f_j \right]_\rgi
        =\sum_{j=1}^n\alpha_j\left[ B \right]_\rgi^j
        =\left[ B \right]_\rgi\cdot \left[ x \right]_\uj.
    \]
    A kívánt azonosságot kapjuk, 
    ha balról szorozzuk mindkét oldalt $\left[ B \right]^{-1}_\rgi$ inverz mátrixszal.
\end{proof}

\section{Lineáris operátorok mátrixa új bázis párban}
\begin{proposition}
    Legyen $A\in L\left( V,W \right)$ lineáris operáció
    és tegyük fel, hogy ismerjük $A$ mátrixát az 
    $\left\{ e_1,\dots,e_m \right\}\subseteq V$ és az
    $\left\{ f_1,\dots,f_n \right\}\subseteq W$ régi bázisokban.
    Legyenek az $\left\{ v_1,\dots,v_m \right\}\subseteq V$
    és a $\left\{ w_1,\dots,w_n \right\}\subseteq W$ az új bázisok.
    Definiálja $B\in L\left( V \right)$ a $V$ tér áttérés lineáris transzformációját
    és $D\in L\left( W \right)$ a $W$ tér áttérés transzformációját.
    Ekkor 
    \[
        \left[ A \right]_{\uj}=
        \left[ D \right]_{\rgi}^{-1}\left[ A \right]_{\rgi}\left[ B \right]_{\rgi}.\qedhere
    \]
\end{proposition}
\begin{proof}
    Azt mutatjuk meg, hogy a bal oldali mátrix és a jobb oldali szorzat mátrix megfelelő oszlopai megegyeznek.
    A $j$-edik oszlopra:
    \begin{multline*}
        \left[ A \right]_{\uj}^j
        =
        \left[ A \right]_{\substack{\left\{ v_1,\dots,v_m \right\}\\ \left\{ w_1,\dots,w_n \right\}}}^j
        =
        \left[ Av_j \right]_{\left\{ w_1,\dots,w_n \right\}}
        \\
        =
        \left[ D \right]^{-1}_{\left\{ f_1,\dots,f_n \right\}}\cdot
            \left[ Av_j \right]_{\left\{ f_1,\dots,f_n \right\}}
        =
        \left[ D \right]^{-1}_{\left\{ f_1,\dots,f_n \right\}}\cdot
            \left[ A \right]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ f_1,\dots,f_n \right\}}}
            \cdot
            \left[ v_j \right]_{\left\{ e_1,\dots,e_m \right\}}
        \\
        =
        \left[ D \right]^{-1}_{\left\{ f_1,\dots,f_n \right\}}\cdot
            \left[ A \right]_{\substack{\left\{ e_1,\dots,e_m \right\}\\ \left\{ f_1,\dots,f_n \right\}}}
            \cdot
        \left[ B \right]_{\left\{ e_1,\dots,e_m \right\}}^j
        \\
        =
            \left[ D \right]_\rgi^{-1}
            \left[ A \right]_\rgi
            \left[ B \right]_\rgi^j
        =
        \left( \left[ D \right]^{-1}_\rgi\left[ A \right]_\rgi\left[ B \right]_\rgi \right)^j.
    \end{multline*}
    Közben használtuk a mátrix szorzás művelet asszociativitását,
    és azt a tényt, 
    hogy tetszőleges $\left[ E \right],\left[ F \right]$ mátrixok mellett
    $\left( \left[ E \right]\left[ F \right] \right)^j=\left[ E \right]\left( \left[F  \right]^j \right)$.
\end{proof}
\section{Lineáris transzformáció mátrixa az új bázisban}
\begin{proposition}
    Legyen $A\in L\left( V \right)$ egy lineáris transzformáció.
    Ekkor 
    \[
        \left[ A \right]_{\uj}
        =
        \left[ B \right]^{-1}_\rgi\
            \left[ A \right]_\rgi
            \left[ B \right]_\rgi.\qedhere
    \]
\end{proposition}
\begin{proof}
    A lineáris operátorokra vonatkozó állítás speciális esete,
    mikor $W=V$ és $D=B$.
\end{proof}
A fenti állítás minden $A\in L\left( V \right)$ lineáris transzformáció mellett igaz.
Speciálisan, ha alkalmazzuk az $A=B$ esetre, akkor látjuk, 
hogy az áttérés mátrixa azonos, ha az új bázisban, vagy a régi bázisban írjuk fel.
\begin{proposition}
    Legyen $B$ az áttérés lineáris transzformáció.
    Ekkor
    \[
        \left[ B \right]_\rgi=\left[ B \right]_\uj
        \quad\text{és persze}\quad
        \left[ B^{-1} \right]_\rgi=\left[ B^{-1} \right]_\uj.\qedhere
    \]
\end{proposition}

\chapter{Invariáns alterek}
\begin{definition}
    Legyen $A\in L\left( V \right)$ egy lineáris transzformációja a $V$ vektortérnek,
    és $M\subseteq V$ a tér egy altere.
    Az mondjuk, hogy $M$ egy \emph{invariáns altere}\index{invariáns altér}
    $V$-nek, ha minden $v\in M$ mellett $Av\in M$ is teljesül.
\end{definition}
Ha nem világos, hogy mely lineáris transzformációról van szó,
akkor azt is mondjuk, hogy az $M$ altér $A$-invariáns, vagy azt,
hogy az $M$ altér invariáns az $A$ transzformációra nézve.

Úgy is fogalmazhatnánk, hogy az $M$ altér akkor invariáns altér,
ha az $A$ transzformációnak az $M$-re való $A|_M$ megszorítása az $M$
altér egy lineáris transzformációja, azaz
\[
    A|_M\in L\left( M \right).
\]

Nyilvánvaló példa invariáns alterekre $M$ és $\{0\}$.
Tetszőleges $A$ lineáris transzformáció mellett $\ker A$ és $\im A$ mindig invariáns alterek.
Ugyanis, ha $u\in\ker A$, akkor $A\left( Au \right)=A0=0$, tehát $Au\in\ker A$.
Az $\im A$ altér esete még egyszerűbb: A tér minden elemének képe $\im A$-ban van, speciálisan persze ez $\im A$ elemeire is igaz.

A célunk, hogy a teret előállítsuk lehetőleg minél alacsonyabb dimenziós terek direktösszegeként.

\begin{definition}[legszűkebb invariáns altér]
    Legyen $A\in L\left( V \right)$ egy lineáris transzformáció.
    Világos, hogy akárhány $A$-ra nézve invariáns altér metszete is $A$-invariáns altér,
    így egy $H\subseteq V$ halmazt tartalmazó
    \emph{legszűkebb $A$-invariáns altér}\index{legszűkebb invariáns altér}
    a $H$ halmazt tartalmazó $A$-invariáns alterek közös része.
    Formálisan
    \[
        \lin\left( H;A \right)
        =
        \bigcap_{
            \substack{
                H\subseteq N\\ 
                N\text{altér }\\ 
                N\text{invariáns}}
            }N.\qedhere
    \]
\end{definition}
A legérdekesebb eset, amikor $H$ egy elemű halmaz.
A $H=\left\{ v \right\}$ esetben a kissé nehézkes 
$\lin\left( \left\{ v \right\};A \right)$ helyett egyszerűbben $\lin\left( v;A \right)$-t írunk.

\begin{proposition}
    Egy $A\in L\left( V \right)$ lineáris transzformációra és egy $v\in V$ vektorra
    \[
        \lin\left( v;A \right)
        =
        \lin\left\{ v,Av,A^2v,\dots \right\}.\qedhere
    \]
\end{proposition}
\begin{proof}
    Mivel $\lin\left( v;A \right)$ egy $v$-t tartalmazó $A$-invariáns altér,
    ezért $\left\{ v,Av,\dots,A^kv,\dots \right\}\subseteq \lin\left( v;A \right)$,
    amiatt
    \[
        \lin\left\{ v,Av,A^2v,\dots \right\}
        \subseteq
        \lin\left( v;A \right).
    \]
    Most vegyük észre, hogy 
    \begin{math}
        \lin\left\{ v,Av,A^2v,\dots \right\}
    \end{math}
    is egy $v$-t tartalmazó $A$-invariáns altér és $\lin\left( v;A \right)$ ilyenek közt a legszűkebb, ezért
    \[
        \lin\left( v;A \right)
        \subseteq
        \lin\left\{ v,Av,A^2v,\dots \right\}.\qedhere
    \]
\end{proof}
\begin{lemma}
    Tegyük fel, hogy egy $A\in L\left( V \right)$ lineáris transzformációra és egy 
    $v\in V$ vektorra a 
    \[
        \left\{ v,Av,\dots,A^kv \right\}
    \]
    lineárisan összefüggő ($k\geq 1$).
    Ekkor minden $n\geq k$ mellett
    \[
        A^nv\in\lin\left\{ v,Av,\dots,A^{k-1}v \right\}.\qedhere
    \]
    \label{le:of}
\end{lemma}
\begin{proof}
    Ha $v=0$ akkor az állítás nyilvánvaló.
    Ha $v\neq 0$, akkor a $\left\{ v \right\}$ rendszer lineárisan független.
    Legyen $t$ a legkisebb olyan szám, hogy a rendszerhez $A^tv$-t hozzávéve az lineárisan összefüggővé válik.
    Ilyen módon tehát
    \[
        \left\{ v,Av,\dots,A^{t-1}v \right\} \text{ lineárisan független}\tag{\dag}
    \]
    de
    \[
        \left\{ v,Av,\dots,A^{t-1}v, A^tv \right\} \text{ lineárisan összefüggő.}\tag{\ddag}
    \]
    Világos, hogy $1\leq t\leq k$.
    Jelölje $N=\lin\left\{ v, Av,\dots,A^{t-1}v \right\}$.

    Most indukcióval megmutatjuk, hogy minden $m\geq 0$ számra
    \[
        A^{t+m}v\in N.
    \]

    Ha $m=0$, akkor $A^tv\in N$, hiszen a fenti (\ddag) lineárisan összefüggő rendszerre,
    az egyik elem előáll mint az előző elemek lineáris kombinációja.
    No de ez elem csak az utolsó lehet, 
    hiszen az utolsó elem nélküli (\dag) rendszer még lineárisan független.

    Most tegyük fel, hogy $A^{t+m}v\in N$ és lássuk be, hogy $A^{t+m+1}v\in N$ is teljesül.
    Ezek szerint
    \begin{math}
        A^{t+m}v=\sum_{j=0}^{t-1}\alpha_jA^jv
    \end{math}
    alakú. Erre $A$-t alkalmazva
    \begin{displaymath}
        A^{t+m+1}v
        =
        \sum_{j=0}^{t-1}\alpha_jA^{j+1}v
        =
        \sum_{j=1}^{t}\alpha_{j-1}A^{j}v
        =
        \left( \sum_{j=1}^{t-1}\alpha_{ j-1}A^{j}v \right)+\alpha_{t-1}A^tv\in N+N=N.\qedhere
    \end{displaymath}
\end{proof}
\begin{proposition}
    Legyen $V$ egy véges dimenziós vektortér.
    $A\in L\left( V \right)$ lineáris transzformáció,
    és $v\in V$ egy $v\neq 0$ vektor.
    Ekkor létezik egyetlen $1\leq k\leq \dim V$ szám,
    amelyre 
    \[
        \left\{ v,Av,\dots,A^{k-1}v \right\} \text{ lineárisan független}\tag{\dag}
    \]
    de
    \[
        \left\{ v,Av,\dots,A^{k-1}v, A^kv \right\} \text{ lineárisan összefüggő.}\tag{\ddag}
    \]
    A fenti (\dag) rendszer bázisa $\lin\left( v;A \right)$-nak.
\end{proposition}
\begin{proof}
    Először a $k$ szám konstrukciója:
    Mivel $v\neq 0$, ezért $\left\{ v \right\}$ lineárisan független.
    Ha $\left\{ v,Av \right\}$ lineárisan összefüggő,
    akkor $k=1$ és készen vagyunk.
    Egyébként tekintsük a $\left\{ v,Av,A^2v \right\}$ rendszert. 
    Ha ez lineárisan összefüggő, akkor $k=2$-vel készen vagyunk.
    Ha ez lineárisan független, akkor tekintsük a 
    $\left\{ v,Av,A^2v,A^3v \right\}$ vektorrendszert.
    Ha lineárisan összefüggő, akkor $k=3$ és készen vagyunk,
    ha lineárisan független, akkor folytatjuk egy újabb elem hozzá vételével.
    Az eljárás előbb-utóbb megáll a vektorrendszer összefüggővé válásával,
    hiszen a Steinitz-lemma \index{Steinitz-lemma} szerint egy véges dimenziós vektortérben legfeljebb
    $\dim V$ elemszámú lineárisan független vektorrendszer van.

    Mivel $\lin\left( v;A \right)$ egy $v$-t tartalmazó $A$-invariáns altér,
    ezért 
    \[
        \left\{ v,Av,\dots,A^{k-1}v \right\}
        \subseteq 
        \lin\left( v;A \right).
    \]
    Az előző lemma szerint
    \[
        \left\{ v,Av,A^2v,\dots \right\}
        \subseteq
        \lin\left\{ v,Av,\dots,A^{k-1}v \right\}
    \]
    ezért
    \[
        \lin\left( v;A \right)
        =
        \lin\left\{ v,Av,A^2v,\dots \right\}
        \subseteq
        \lin\left\{ v,Av,\dots,A^{k-1}v \right\}
        \subseteq
        \lin\left( v;A \right).
    \]
    Ez azt jelenti, hogy a (\dag) vektorrendszer egy lineárisan független generátorrendszere 
    a $\lin\left( v;A \right)$ térnek, tehát valóban bázisa is.
\end{proof}
A $\lin\left( v;A \right)$ tehát a fenti $k$-ra egy $k$-dimenziós altér,
amelynek bázisa $\left\{A^{k-1}v,\dots,Av,v \right\}$.
A játék kedvéért írjuk fel az $A|_{\lin\left( v;A \right)}$ transzformáció mátrixát 
ebben a bázisban:
\[
    \begin{array}{r|ccccc}
        &A^kv&A^{k-1}v&A^{k-2}v&\dots&Av\\
        \hline
        A^{k-1}v&\alpha_{k-1}&1&0&\dots\dots&0\\
        A^{k-2}v&\alpha_{k-2}&0& 1&\dots&\vdots\\
        \vdots  &\vdots      &\vdots&\ddots&\ddots&0\\
        Av&\alpha_1&0&0&\dots&1\\
        v&\alpha_0&0&0&\dots&0
    \end{array},
    \text{ ahol }A^kv=\sum_{j=0}^{k-1}\alpha_jA^jv.
\]
\section{Transzformációk sajátértéke}
A célunk, hogy a lehető legalacsonyabb dimenziós invariáns altereket találjunk.
Így persze az 1 dimenziós invariáns alterek a legérdekesebbek.
Ez vezet a sajátérték fogalmához.
\begin{definition}[sajátérték, sajátvektor, spektrum]
    Legyen $A\in L\left( V \right)$.
    Azt mondjuk, hogy a $\lambda\in\mathbb{F}$ szám az $A$ lineáris transzformáció
    \emph{sajátértéke}\index{sajátérték},
    ha létezik $v\in V$, $v\neq 0$ vektor, amelyre
    \[
        Av=\lambda v
    \]
    teljesül.
    
    Ha $\lambda$ egy sajátértéke $A$-nak,
    akkor az összes olyan nem zérus $v$ vektort, 
    amelyre $Av=\lambda v$ fennáll az $A$ transzformáció $\lambda$ sajátértékéhez tartozó
    \emph{sajátvektorainak}\index{sajátvektor} nevezzük.

    Egy $A$ lineáris transzformáció összes sajátértékeinek halmazát az
    $A$ \emph{spektrumának}\index{spektrum} nevezzük, és
    $\sigma\left( A \right)$-val jelöljük.
\end{definition}
A spektrum tehát az $\mathbb{F}$ test azon részhalmaza, amelyre
\[
    \sigma\left( A \right)
    =
    \left\{ 
        \lambda\in\mathbb{F}:\exists v\in V, v\neq 0, Av=\lambda v
    \right\}
\]
teljesül.
Világos, hogy ha $\lambda$ egy sajátértéke $A$-nak, akkor a $\lambda$-hoz tartozó
sajátvektorok halmaza éppen a 
\[
    \ker\left( A-\lambda I \right)
\]
altér nem zérus elemei.
Emiatt a fenti alteret a $\lambda$ sajátértékhez tartozó \emph{sajátaltérnek}\index{sajátaltér}
mondjuk.

Vegyük észre, 
hogy $v\in V$ vektor pontosan akkor sajátvektora $A$-nak, ha $\dim\left( \lin\left( v;A \right) \right)=1$.
Hasonlóan az is könnyű, hogy $\lambda$ pontosan akkor sajátértéke $A$-nak,
ha az $A-\lambda I$ transzformáció szinguláris.

\chapter{Transzformációk polinomjai}

\begin{definition}[transzformáció polinomja]
    Legyen $V$ az $\mathbb{F}$ test feletti vektortér,
    $A\in L\left( V \right)$ egy lineáris transzformáció, és
    legyen $p\in\mathbb{F}\left[t \right]$ egy az $\mathbb{F}$ test feletti 
    polinom,
    amely $p\left( t \right)=
    \sum_{j=0}^n\alpha_jt^j$
    alakú.
    Definiálja $p\left( A \right)\in L\left( V \right)$
    az $A$ transzformáció $p$ polinomját
\[
    p\left( A \right)
    =
    \sum_{j=0}^n\alpha_jA^j.\qedhere
\]
\end{definition}
\begin{proposition}[Számolási szabályok]
    Legyen $p,q\in\mathbb{F}\left[ t \right]$ polinomok,
    $A\in L\left( V \right)$ lineáris transzformáció.
    \begin{enumerate}
        \item Ha $r=p+q$, akkor $r\left( A \right)=p\left( A \right)+q\left( A \right)$.
        \item Ha $r=pq$, akkor $r\left( A \right)=p\left( A \right)q\left( A \right)$.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Az első állítás azért teljesül, mert
    \(
        \left( \alpha A^k+\beta A^k \right)
        =
        \left( \alpha+\beta \right)A^k.
    \)

    A második állításhoz azt vegyük észre, hogy
    \(
        A^kA^l=A^{k+l}.
    \)
    Így, amikor összegyűjtjük, hogy a $q\left( A \right)p\left( A \right)$
    kompozícióban, mi lesz $A^j$ együthatója, 
    akkor azt kapjuk,
    hogy 
    \begin{displaymath}
        \left( \sum_{\substack{k,l\\k+l=j}}\alpha_k\beta_l \right)A^j
        =
        \left( \sum_{k=0}^j\alpha_k\beta_{j-k} \right)A^j.
    \end{displaymath}
    Vegyük észre, hogy \aref{def:polmuveletek}. definíció szerint az
    $r$ szorzat polinomban is a fenti zárójelben lévő szám a 
    $t^j$ tag együtthatója.
\end{proof}
Tudjuk, hogy lineáris transzformációk szorzata függ azok sorrendjétől.
Ugyanúgy mint mátrixokra, két lineáris transzformációt \emph{kommutálónak}\index{kommutál}
mondunk, ha szorzatuk a szorzás sorrendjétől független.
Például az $I$ identitás minden transzformációval kommutál.
Azt is láttuk, hogy ha $AB=I$, akkor $BA=I$ is fennáll, azaz $A$ és $B$ kommutálnak.
Nagyon fontos, de nyilvánvaló küvetkezménye a fenti számolási szabálynak,
hogy ha $p,q$ tetszőleges polinomok, akkor a $p\left( A \right)$ és $q\left( A \right)$ egymással kommutáló lineáris transzformációk lesznek, hiszen az 
$\mathbb{F}\left[ t \right]$ egy kommutatív gyűrű, 
azért ha $r=pq$, akkor $qp=r$,
ergo 
\[
    p\left( A \right)q\left( A \right)=r\left( A \right)=q\left( A \right)p\left( A \right).
\]
Tehát meggondoltuk, hogy
\begin{proposition}\index{kommutáló transzformációk}
    Lineáris transzformáció polinomjai egymással kommutálnak.\index{kommutál}
\end{proposition}
Polinomok segítségével sok-sok új invariáns alteret kapunk.
\begin{proposition}
    Tetszőleges $p$ polinomra és $A\in L\left( V \right)$ 
    lineáris transzformáció mellett 
    $\ker p\left( A \right)$ és $\im p\left( A \right)$ is invariáns alterek.
\end{proposition}
\begin{proof}
    Legyen előszőr $v\in\ker p\left( A \right)$.
    Ekkor, mivel $A$ és $p\left( A \right)$ kommutálnak
    \[
        p\left( A \right)Av=Ap\left( A \right)v=A0=0,
    \]
    ami pont azt jelenti, hogy $Av\in\ker p\left( A \right)$.

    Legyen most $v\in\im p\left( A \right)$,
    azaz
    $v=p\left( A \right)x$ valamely $x\in V$ mellett.
    Ekkor, mivel $A$ és $p\left( A \right)$ kommutálnak
    \[
        Av=Ap\left( A \right)x=p\left( A \right)\left( Ax \right),
    \]
    amiből már látszik, hogy $Av\in\im p\left( A \right)$.
\end{proof}
Speciálisan ez igaz az $t-\lambda$ polinomra is, 
amikor $\lambda$
egy sajátértéke $A$-nak. Tehát a $\lambda$ sajátértékhez tartozó 
$\ker \left( A-\lambda I \right)$ sajátaltér egy 1 dimenziós invariáns altere
$A$-nak.\index{saját altér}
\begin{definition}
    Azt mondjuk, hogy az $A$ lineáris transzformáció a $p$ polinom gyöke,
    ha $p\left( A \right)=0$.
\end{definition}
Mielőtt tovább lépünk érdemes visszagondolnunk arra,
hogy egy test feletti polinomgyűrű egy főideálgyűrű.\index{főideál-gyűrű}\index{főideál}
Láttuk ugyanis 
-- \ref{pr:pgyurufoidealgyuru}. állítás --, 
hogy minden nem csak a $\left\{ 0 \right\}$ elemet tartalmazó ideálnak 
van egyetlen legkisebb fokú és normált eleme, 
ami egyben az ideál generáló eleme is.

\section{Kis minimál polinom}
\begin{proposition}
    Legyen $A\in L\left( V \right)$ a $V$ véges dimenziós vektortér egy lineáris transzformációja,
    és legyen $v\in V$ egy rögzített vektor.
    Tekintsük az 
    $\mathbb{F}\left[ t \right]$ polinom gyűrű következő részhalmazát.
    \[
        J_{A,v}
        =
        \left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)v=0 \right\}
    \]
    Ez a halmaz egy ideálja $\mathbb{F}\left[ t \right]$-nek, 
    amelynek van legfeljebb $\dim V$-ed fokú, de nem konstans zérus polinomja.
\end{proposition}
\begin{proof}
    Ha $p,q\in J_{A,v}$, 
    akkor 
    $\left( p+q \right)(A)v=p\left( A \right)v+q\left( A \right)v=0+0=0$,
    azaz $p+q\in J_{A,v}$. 
    Ha most $p\in J_{A,v}$ és $h$ egy tetszőleges polinom,
    akkor 
    $
    \left( hp \right)(A)v=h\left( A \right)p\left( A \right)v=h\left( A \right)0=0,
    $
    azaz $hp\in J_{A,v}$. 
    Megmutattuk tehát, hogy $J_{A,v}$ egy ideálja a polinom gyűrűnek.

    Jelölje $n=\dim V$ és tekintsük az $n+1$ elemű
    $\left\{ v,Av,\dots,A^nv \right\}$ vektorrendszert.
    Mivel a Steinitz-lemma szerint \index{Steinitz-lemma} 
    $n+1$ vektor egy $n$-dimenziós vektortérben lineárisan összefüggő,
    ezért van
    $\alpha_0,\dots,\alpha_n\in\mathbb{F}$ nem mind zérus szám, hogy
    $\sum_{j=0}^n\alpha_jA^jv=0$.
    Ha tehát $p$ jelöli a $p\left( t \right)=\sum_{j=0}^n\alpha_jt^j$ polinomot,
    akkor
    $p\left( A \right)v=0$, azaz $p\in J_{A,v}$ és $-\infty<\deg p\leq n$.
\end{proof}
Ha például $v=0$, akkor $J_{A,0}=\mathbb{F}\left[ t \right]$, azaz minden polinom
az ideálhoz tartozik.
Ha $v\neq 0$, 
akkor a $J_{A,v}$ ideálnak nincs nulladfokú polinomja, 
hiszen $p(t)=c, (c\neq 0)$ mellett
$p\left( A \right)v=(cI)v=cv\neq 0$.

\begin{definition}
    Legyen $A\in L\left( V \right)$ a $V$ véges dimenziós vektortér egy lineáris transzformációja,
    és legyen $v\in V$ egy rögzített vektor.
    Láttuk, hogy
    \[
        J_{A,v}
        =
        \left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)v=0 \right\}
    \]
    az $\mathbb{F}\left[ t \right]$ gyűrű egy ideálja, amely nem csak a konstans zéruspolinomot tartalmazza.
    Tudjuk, hogy az $\mathbb{F}\left[ t \right]$ polinomgyűrű egy főideál-gyűrű,\index{főideál}\index{főideál-gyűrű}
    van tehát egyetlen normált polinomja $J_{A,v}$-nek, amely generálja $J_{A,v}$.
    Ez a $J_{A,v}$ legkisebb fokú, normált polinomja.
    Ezt a polinomot nevezzük az $A$ transzformáció, $v$ vektorhoz tartozó
    \emph{kis minimál polinomjának}\index{kis minimál polinom}.
\end{definition}

\begin{proposition}
Ha  $n$ az $A$ transzformáció $v$ vektorhoz tartozó kis minimál polinomja,
akkor
\begin{enumerate}
    \item $n$ normált polinom,
    \item $n\left( A \right)v=0$,
    \item ha $p\in\mathbb{F}\left[ t \right], p\neq 0$, amelyre 
        $p\left( A \right)v=0$, akkor $n|p$,
    \item $\deg n\leq\dim V$.\qedhere
\end{enumerate}
\end{proposition}
\begin{proof}
    Definíció szerint $n$ azon $p$ polinomok közül, 
    amelyek normáltak, és $p\left( A \right)v=0$, a legalacsonyabb fokú.
    Láttuk hogy ilyen polinom csak egy van, es erre a polinomra
    \[
        J_{A,v}=J\left( n \right)=\left\{ hn:h\in\mathbb{F}\left[ t \right] \right\}.
    \]
    Ezt kellett belátni. 
\end{proof}
\begin{proposition}
    Legyen $v\neq 0$ és tegyük fel, hogy $p\left( A \right)v=0$ valamely normált,
    irreducibilis $p$ polinomra.
    Ekkor $p$ a $v$ vektorhoz tartozó kis minimál polinom.
\end{proposition}
\begin{proof}
    Ha maga $\deg p=1$, akkor készen vagyunk, hiszen nem zérus vektornak minimálpolinomja legalább első fokú.
    Ha $\deg p>1$ és $p$ nem egyezne az $n$ kis minimál polinommal,
    akkor $\deg n<\deg p$ lenne, és mivel $n$ generálja a $J_{A,v}$ ideált,
    ezért $p=nh$ alakú lenne, ahol $h$ is legalább első fokú.
    Így $p$ két legalább első fokú polinom szorzata, azaz reducibilis lenne.
\end{proof}

A következő állítás módszert ad a kis minimál polinom meghatározására,
amely mindig használható.
\begin{proposition}
    Legyen $A\in L\left( V \right)$ és $v\neq 0.$
    Mivel $V$ egy véges dimenziós altér,
    ezért létezik $1\leq k \leq \dim V$,
    hogy 
    $\left\{ v,Av,\dots,A^{k-1}v \right\}$ lineáris független,
    de $\left\{ v,Av,\dots,A^{k-1}v,A^{k}v \right\}$ lineárisan összefüggő.
    Ekkor léteznek $\alpha_0,\dots,\alpha_{k-1}\in\mathbb{F}$
    számok, amelyekre
    \[
        A^{k}v=\sum_{j=0}^{k-1}\alpha_jA^{j}v
    \]
    Az $A$ operátor ezen $v$ vektorhoz tartozó kis minimál polinomja
    \[
        n\left( t \right)=
        t^k-\alpha_{k-1}t^k-\dots-\alpha_1t-\alpha_0.\qedhere
    \]
\end{proposition} 
\begin{proof}
    Láttuk, hogy $\left\{ v,Av,\dots,A^{k-1}v \right\}$ bázisa $\lin\left( v;A \right)$ altérnek.
    Világos, hogy $A^kv\in\lin\left( v;A \right)$, emiatt a kívánt előállítás valóban létezik.
    Ezt átrendezve kapjuk, hogy $n$ valóban olyan normált polinom, amelyre $n\left( A \right)v=0$ fennáll.
    No de $k$-nal alacsonyabb fokú ilyen polinom csak a konstans zérus polinom lehet,
    hiszen $\left\{ v,Av,\dots,A^{k-1}v \right\}$ lineárisan független.
    Azt láttuk tehát, hogy $n$ a legalcsonyabb fokú nem zérus eleme $J_{A,v}$-nek,
    tehát $n$ generálja a főideált.
\end{proof}

\section{Minimál polinom}
\begin{proposition}
    Legyen $A\in L\left( V \right)$ a $V$ véges dimenziós vektortér egy lineáris transzformációja.
    Tekintsük az 
    $\mathbb{F}\left[ t \right]$ polinom gyűrű következő részhalmazát.
    \[
        J_{A}
        =
        \left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)=0 \right\}
    \]
    Ez a halmaz egy ideálja $\mathbb{F}\left[ t \right]$-nek, 
    amelynek van legfeljebb $\left( \dim V \right)^2$-ed fokú, 
    de nem konstans zérus polinomja.
\end{proposition}
\begin{proof}
    Ha $p,q\in J_{A}$, 
    akkor 
    $\left( p+q \right)(A)=p\left( A \right)+q\left( A \right)=0+0=0$,
    azaz $p+q\in J_{A}$. 
    Ha most $p\in J_{A}$ és $h$ egy tetszőleges polinom,
    akkor 
    $
    \left( hp \right)(A)=h\left( A \right)p\left( A \right)=h\left( A \right)0=0,
    $
    azaz $hp\in J_{A}$. 
    Megmutattuk tehát, hogy $J_{A}$ egy ideálja a polinom gyűrűnek.

    Jelölje $n=\dim V$ és tekintsük az $n^2+1$ elemű
    $\left\{ I,A,\dots,A^{n^2} \right\}$ rendszerét az $L\left( V \right)$ vektortérnek.
    Mivel a Steinitz-lemma szerint \index{Steinitz-lemma} 
    $n^2+1$ vektor egy $n^2$-dimenziós vektortérben lineárisan összefüggő,
    ezért van
    $\alpha_0,\dots,\alpha_{n^2}\in\mathbb{F}$ nem mind zérus szám, hogy
    $\sum_{j=0}^{n^2}\alpha_jA^j=0$.
    Ha tehát $p$ jelöli a $p\left( t \right)=\sum_{j=0}^{n^2}\alpha_jt^j$ polinomot,
    akkor
    $p\left( A \right)=0$, azaz $p\in J_{A}$ és $-\infty<\deg p\leq n^2$.
\end{proof}
\begin{definition}[minimál polinom]\index{minimál polinom}
    Legyen $A\in L\left( V \right)$ a $V$ véges dimenziós vektortér egy lineáris transzformációja.
    Láttuk, hogy
    \[
        J_{A}
        =
        \left\{ p\in\mathbb{F}\left[ t \right]:p\left( A \right)=0 \right\}
    \]
    az $\mathbb{F}\left[ t \right]$ gyűrű egy ideálja, amely nem csak a konstans zéruspolinomot tartalmazza.
    Tudjuk, hogy az $\mathbb{F}\left[ t \right]$ polinomgyűrű egy főideál-gyűrű,\index{főideál}\index{főideál-gyűrű}
    van tehát egyetlen normált polinomja $J_{A}$-nek, amely generálja $J_{A}$.
    Ez a $J_{A}$ legkisebb fokú, normált polinomja, 
    amit $A$ transzformáció \emph{minimál polinomjának} nevezünk.
\end{definition}
Ha $V\neq\left\{ 0 \right\}$, ergo $\dim V\geq 1$,
akkor a $J_{A}$ ideálnak nincs nulladfokú polinomja, 
hiszen $p(t)=c, (c\neq 0)$ mellett
$p\left( A \right)=cI\neq 0$, 
tehát legalább egy dimenziós tér egy lineáris transzformációjának a minimál polinom legalább első fokú.

\begin{proposition}
    Ha  $m\in\mathbb{F}\left[ t \right]$ polinom akkor és csak akkor 
    az $A\in L\left( V \right)$ transzformáció minimál polinomja, 
    ha
\begin{enumerate}
    \item $m$ normált polinom,
    \item $m\left( A \right)=0$,
    \item ha $p\in\mathbb{F}\left[ t \right], p\neq 0$, amelyre 
        $p\left( A \right)=0$, akkor $m|p$.
\end{enumerate}
A minimál polinom fokszámára:%
\footnote{Kis vártatva kiderül, hogy $\deg m\leq \dim V$ is igaz.}%
$\deg m\leq(\dim V)^2$.
\end{proposition}
\begin{proof}
    Definíció szerint $m$ azon $p$ polinomok közül, 
    amelyek normáltak, és $p\left( A \right)=0$, a legalacsonyabb fokú.
    Láttuk hogy ilyen polinom csak egy van, es erre a polinomra
    \[
        J_{A}=J\left( m \right)=\left\{ hm:h\in\mathbb{F}\left[ t \right] \right\}.
    \]
    Ezt kellett belátni. 
\end{proof}
\begin{proposition}
    Legyen $V$ legalább 1 dimenziós, 
    és tegyük fel, hogy $p\left( A \right)=0$ valamely normált,
    irreducibilis $p$ polinomra.
    Ekkor $p$ az $A$ minimál polinomja.
\end{proposition}
\begin{proof}
    Ha maga $\deg p=1$, akkor készen vagyunk, hiszen a minimálpolinom legalább első fokú.
    Ha $\deg p>1$ és $p$ nem egyezne az $m$ minimál polinommal,
    akkor $\deg m<\deg p$ lenne, és mivel $m$ generálja a $J_{A}$ ideált,
    ezért $p=mh$ alakú lenne, ahol $h$ is legalább első fokú.
    Így $p$ két legalább első fokú polinom szorzata, azaz reducibilis lenne.
\end{proof}

A következő állítás módszert ad a minimál polinom meghatározására.
\begin{proposition}
    Legyen $A\in L\left( V \right)$, ahol $\dim V\geq 1$.
    Mivel $\dim L\left( V \right)=\left( \dim V \right)^2$ egy véges dimenziós vektortér,
    ezért létezik $1\leq k \leq (\dim V)^2$,
    hogy 
    $\left\{ I,A,\dots,A^{k-1} \right\}$ lineáris független,
    de $\left\{ I,A,\dots,A^{k-1},A^{k} \right\}$ lineárisan összefüggő.
    Ekkor léteznek $\alpha_0,\dots,\alpha_{k-1}\in\mathbb{F}$
    számok, amelyekre
    \[
        A^{k}=\sum_{j=0}^{k-1}\alpha_jA^{j}.
    \]
    Ekkor az $A$ operátor minimál polinomja.
    \[
        m\left( t \right)=
        t^k-\alpha_{k-1}t^k-\dots-\alpha_1t-\alpha_0.\qedhere
    \]
\end{proposition} 
\begin{proof}
    Mivel a $k+1$ elemű rendszer lineárisan összefüggő és a $k$ elemű rendszer lineárisan független, 
    ezért a kívánt előállítás valóban létezik.
    Ezt átrendezve kapjuk, hogy $m$ valóban olyan normált polinom, amelyre $m\left( A \right)=0$ fennáll.
    No de $k$-nál alacsonyabb fokú ilyen polinom csak a konstans zérus polinom lehet,
    hiszen $\left\{ I,A,\dots,A^{k-1} \right\}$ lineárisan független.
    Azt láttuk tehát, hogy $m$ a legalcsonyabb fokú nem zérus eleme $J_{A}$-nek,
    tehát az $m$ polinom generálja a főideált.\index{főideál}
\end{proof}
A minimál polinom algoritmikus meghatározásához a kis minimál polinom is használhatók.
\begin{proposition}
    Legyen az $A\in L\left( V \right)$ lineáris transzformáció minimál polinomja $m$.
    Tegyük fel, hogy az $\left\{ e_1,\dots,e_n \right\}$ egy bázisa $V$-nek,
    és $p_1,\dots,p_n$ rendre a bázis elemekhez tartozó kis minimál polinomok.
    Ekkor a $p_1,\dots,p_n$ polinomok legkisebb közös többszöröse az $m$ minimal polinom.
    \index{legkisebb közös többszörös}
\end{proposition}
\begin{proof}
    Tetszőleges $v\in V$ mellett, ha $p_v$ a kis minimál polinom,
    akkor $p_v|m$, hiszen $m\left( A \right)$ a konstans zérus transzformáció.
    Emiatt $m$ egy közös többszöröse a $p_j$ kis minimál polinomoknak.
    Most tegyük fel, hogy egy $p$ polinom többszöröse a $p_j$ kis minimál polinomoknak.
    Világos, hogy minden $j$ mellett
    \[
        p\left( A \right)e_j
        =
        h_j\left( A \right)\left( p_j\left( A \right)e_j \right)
        =
        h_j\left( A \right)0
        =
        0.
    \]
    Mivel egy lineáris transzformáció egy bázison egyértelműen meghatározott, ezért
    $p\left( A \right)=0$.
    Ekkor persze $m|p$, 
    azaz $m$ valóban  a kis minimálpolinomok legkisebb közös többszöröse.
\end{proof}
\begin{proof}[Egy másik bizonyítás]\index{főideál}
    Mivel a $p\left( A \right)$ lineáris transzformáció a bázison egyértelműen meghatározott, 
    ezért $p\in J_A$ pontosan akkor teljesül,
    ha $p\in J_{A,e_i}$ a bázis minden $e_i$ elemére.
    Így 
    \[
        J\left( d \right)
        =
        \cap_{i=1}^nJ(p_i)
        =
        \cap_{i=1}^nJ_{A,e_i}
        =
        J_A
        =
        J(m).
    \]
    No de, a bal oldali ideál $d$ generáló eleme 
    -- \aref{pr:lkkt}. állítás szerint --
    a $p_1,\dots,p_n$ polinomok legkisebb közös többszöröse.
    Mivel egy főidálnak csak egy normált generáló eleme van, 
    ezért a $d$ legkisebb közös többszörös azonos a jobb oldali ideált generáló $m$ minimálpolinommal.
\end{proof}
\section{Sajátvektorok és diagonalizálhatóság}\index{sajátvektor}\index{diagonalizálhatóság}

\begin{proposition}
    Tegyük fel, hogy $p$ legalább elsőfokú osztója az $A$ transzformáció minimál polinomjának.
    Ekkor $p\left( A \right)$ szinguláris\index{szinguláris}.
\end{proposition}
\begin{proof}
    Jelölje $m$ a minimál polinomot,
    és $m=pq$.
    Így $\deg q<\deg m$. Persze $m\left( A \right)=p\left( A \right)q\left( A \right)$,
        így ha $p\left( A \right)$ reguláris lenne,
        akkor 
        \[
            q\left( A \right)=p\left( A \right)^{-1}m\left( A \right)=0
        \]
        Ebből persze $m|q$, így $\deg m\leq \deg q$, ami ellentmondás.
\end{proof}
\begin{proposition}[sajátérték és minimál polinom]\index{sajátérték}
    Legyen $A\in L\left( V \right)$ lineáris transzformációja a $V$ véges dimenziós 
    vektortérnek, és $\lambda\in\mathbb{F}$ egy szám, valamint $m$ az $A$ minimál
    polinomja.
    Az alábbi feltevések ekvivalensek:
    \begin{enumerate}
        \item $\lambda$ sajátértéke $A$-nak,
        \item $\ker (A-\lambda I)\neq \left\{ 0 \right\}$,
        \item $A-\lambda I$ szinguláris,
        \item létezik $v\in V$, 
            amelyre a $v$-hez tartozó kis minimálpolinomja $A$-nak $p_v(t)=t-\lambda$.
        \item $t-\lambda|m\left( t \right)$,
        \item $m\left( \lambda \right)=0.$\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Az első három pont ekvivalenciája nyilvánvaló,
    majd a $3\implies 4\implies 5\implies 6\implies 5\implies 3$
    utat érdemes követni.
    Az utolsó lépéshez mutattuk meg az előző állítást.
\end{proof}
Az egyik legfontosabb definícióhoz érkeztünk.
\begin{definition}[diagonalizálható transzformáció]
    Az $A\in L\left( V \right)$ lineáris transzformációt \emph{diagonalizálhatónak}\index{diagonalizálhatóság}
    mondjuk, 
    ha van a térnek olyan bázisa,
    amelyben a transzformáció mátrixa diagonális alakú,\index{diagonális alakú mátrix}
    azaz a fődiagonálisán kívül minden elem zérus.
\end{definition}
Az $\left[ A \right]$ négyzetes mátrix tehát pontosan akkor diagonális alakú, 
ha minden $i\neq j$ mellett $\left[ A \right]_{i,j}=0.$
Ez azt jelenti hogy a $j$-edik bázis elem képének a nem $j$-edik koordinátája zérus,
azaz az $e_j$ bázis vektorra $Ae_j=\lambda e_j$ áll fenn,
valamely $\lambda\in\mathbb{F}$ számmal.
Ez éppen azt jelenti, 
hogy az $e_j$ bázisvektor egy sajátvektor.
Nyilvánvaló tehát, 
hogy diagonalizálhatóság szükséges és elegendő módon megragadható a sajátvektor fogalmának segítségével.
\begin{proposition}[diagonalizálhatóság]
    Az $A\in L\left( V \right)$ egy lineáris transzformáció
    pontosan akkor diagonalizálható,
    ha van térnek csupa sajátvektorokból álló bázisa.

    Ebben az esetben a transzformációnak a $\left\{ v_1,\dots,v_n \right\}$ sajátvektorokban felírt mátrixának
    $j$-edik diagonális eleme, 
    éppen az a $\lambda_j$ sajátértéke $A$-nak,
    amelyre $Av_j=\lambda_jv_j$.
\end{proposition}
Mivel a 
\begin{math}
    \begin{pmatrix}
        -t&1\\-1&-t
    \end{pmatrix}
\end{math}
valós test feletti mátrix minden valós $t$ mellett reguláris, 
ezért a 
\begin{math}
    \begin{pmatrix}
        0&1\\-1&0
    \end{pmatrix}
\end{math}
mátrix egy nagyon egyszerű példa olyan mátrixra, amelynek spektruma üres,
így persze nem diagonalizálható.

Csak a játék kedvéért, ha az
\begin{math}
    \begin{pmatrix}
        0&1\\0&0
    \end{pmatrix}
\end{math}
mátrixot tekintjük $\mathbb{R}$ felett, 
azt kapjuk, hogy ez sem diagonalizálható.
Van ugyan egyetlen sajátértéke $\lambda=0$,
de az ehhez a sajátértékhez tartozó $\ker A$ sajátaltér egy dimenziós,
emiatt nincs a térben két lineárisan független sajátvektor.

A pozitív példa kedvéért nézzük az 
\begin{math}
    A
    =
    \begin{pmatrix}
        1&1\\1&1
    \end{pmatrix}
\end{math}
mátrixot. 
Mind az $\mathbb{R}$,
mind a $\mathbb{C}$ test felett diagonalizálható,
hiszen $\sigma\left( A \right)=\left\{ 0,2 \right\}$, továbbá
a $\lambda=0$-hoz tartozó sajáteltérre 
$\ker A=\lin\left\{ 
    \begin{pmatrix}
        1\\-1
    \end{pmatrix}
\right\},$
míg a $\lambda=2$ sajátértékhez tartozó sajátaltérre
\begin{math}
    \ker\left( A-2I \right)
    =
    \lin\left\{ 
        \begin{pmatrix}
            1\\1
        \end{pmatrix}
    \right\}.
\end{math}
Világos, hogy a
\begin{math}
    B
    =
    \left\{ 
        \begin{pmatrix}
            1\\-1
        \end{pmatrix},
        \begin{pmatrix}
            1\\1
        \end{pmatrix}
    \right\}
\end{math}
vektorrendszer egy sajátvektorokból álló bázisa a két dimenziós vektortérnek.
A fenti bázisban a transzformáció mátrixa
\begin{math}
    \begin{pmatrix}
        0&0\\
        0&2
    \end{pmatrix}.
\end{math}
Mivel az erre a bázisra való áttérés mátrixa
\begin{math}
    B=
    \begin{pmatrix}
        1&1\\
        -1&1
    \end{pmatrix},
\end{math}
ezért az új bázisra való áttérést formuláját használva
\[
    [B]^{-1}[A]_{\rgi}[B]=
    \frac{1}{2}
    \begin{pmatrix}
        1&-1\\
        1&1
    \end{pmatrix}
    \begin{pmatrix}
        1&1\\1&1
    \end{pmatrix}
    \begin{pmatrix}
        1&1\\
        -1&1
    \end{pmatrix}
    =
    \begin{pmatrix}
        0&0\\
        0&2
    \end{pmatrix}
    =[A]_{\uj}.
\]
Ezt úgy fejezzük ki, hogy az
\begin{math}
    \begin{pmatrix}
        1&1\\
        -1&1
    \end{pmatrix}
\end{math}
izomorfizmus diagonalizálja az 
\begin{math}
    \begin{pmatrix}
        1&1\\1&1
    \end{pmatrix}
\end{math}
mátrixot.
\begin{proposition}\label{pr:svlinfgtlen}
    Különböző sajátértékekhez tartozó sajátvektorok rendszere lineárisan független.

    Formálisabban:
    Legyen $\left\{ \lambda_1,\dots,\lambda_s \right\}\subseteq \sigma\left( A \right)$
    a sajátérékek páronként különböző elemekből álló rendszere,
    és legyen $\left\{ v_1,\dots,v_s \right\}\subseteq V$ sajátvektorok olyan rendszere,
    amelyre $v_j\in\ker\left( A-\lambda_jI \right)$ minden $j=1,\dots,s$ mellett.
    Ekkor a sajátvektorok rendszere lineárisan független.
\end{proposition}
\begin{proof}
    A sajátvektorok száma, azaz $s$ szerinti indukció.
    Ha $s=1$, akkor készen is vagyunk,
    hiszen egy sajátvektor egy nem zérus vektor.

    Most tegyük fel, hogy igaz az állítás sajátvektorok $s$-nél kevesebb elemből álló rendszerére, 
    és lássuk be sajátvektorok olyan $s$ elemű rendszerére, amelyek különböző sajátértékhez tartoznak.
    Az indukciós feltevés szerint tehát $\left\{ v_1,\dots,v_{s-1} \right\}$ lineárisan független.
    Ha $\left\{ v_1,\dots,v_{s-1},v_s \right\}$ lineárisan összefüggő lenne,
    akkor valamely $\alpha_1,\dots,\alpha_{s-1}$ számokkal
    \[
        v_s=\sum_{j=1}^{s-1}\alpha_jv_j
    \]
    lenne. 
    No de
    \begin{equation*}
        \sum_{j=1}^{s-1}\lambda_s\alpha_jv_j
        =
        \lambda_sv_s
        =
        Av_s
        =
        \sum_{j=1}^{s-1}\alpha_jAv_j
        =
        \sum_{j=1}^{s-1}\alpha_j\lambda_jv_j,
    \end{equation*}
    ami a az első $s-1$ elem lineárisan függetlensége szerint csak úgy lehetséges,
    ha minden $j=1,\dots s-1$ mellett 
    \begin{math}
        \lambda_s\alpha_j
        =
        \alpha_j\lambda_j
    \end{math},
    ergo
    \begin{math}
        \alpha_j\left( \lambda_s-\lambda_j \right)
        =
        0.
    \end{math}
    Mivel itt különböző sajátértékekről van szó,
    ezért minden szóba jövő $j$ mellett $\alpha_j=0$.
    Ebből $v_s=0$ következik, 
    ami ellentmond annak, hogy $v_s$ egy sajátvektor.
\end{proof}
Egy $n$-dimenziós térben $n$-elemű lineárisan független rendszer generátorrendszer is, így
azonnali következmény a diagonalizálhatóság egy elegendő feltétele:
\begin{proposition}[diagonalizálhatóság elegendő feltétele]
    Tegyük fel, hogy az $A\in L\left( V \right)$ lineáris transzformációnak annyi különböző sajátértéke van, 
    mint a $V$ vektortér dimenziója.
    Ekkor $A$ diagonalizálható.
\end{proposition}
Az identitás mátrix példája mutatja, hogy a feltétel elegendő, de nem szükséges.
Mivel $n$-dimenziós térben legfeljebb $n$ elemű linárisan független rendszer van,
ezért kapjuk, hogy a spektrumnak több eleme nem lehet, mint a tér dimenziója:
\begin{proposition}
    Legyen $A\in L\left( V \right)$ lineáris transzformáció.
    Ekkor $A$-nak legfeljebb $\dim V$ darab különböző sajátértéke lehet.
\end{proposition}

\begin{definition}[geometriai multiplicitás]
    Ha $A\in L\left( V \right)$ egy lineáris transzformáció, 
    és $\lambda\in\sigma\left( A \right)$ annak egy sajátérétke,
    akkor az $A-\lambda I$ sajátaltér defektusát, 
    tehát az $\ker\left( A-\lambda I \right)$ altér dimenzióját,
    a $\lambda$ sajátérték \emph{geometriai multiplicitásának}\index{geometriai multiplicitás}
    mondjuk.
\end{definition}
\begin{proposition}
    Legyen $A\in L\left( V \right)$ transzformáció, és
    $\left\{ \lambda_1,\dots,\lambda_s \right\}\subseteq\sigma\left( A \right)$,
    a spektrum különböző elemei.
    Jelölje $M_j=\ker\left( A-\lambda_j I \right)$.
    Ekkor értelmes az $M_1\oplus\dots\oplus M_s$ direktösszeg.
\end{proposition}
\begin{proof}
    Megmutatjuk, hogy a $\sum_{j=1}^sM_j$ összegben minden elem előállítása egyértelmű.
    Ehhez elég azt belátni, hogy $\sum_{j=1}^sv_j=0$, $v_j\in M_j$ csak úgy lehetséges, ha
    minden $j=1,\dots,s$ mellett $v_j=0$.
    Tegyük fel tehát, hogy valamely $v_j\in M_j$ vektorokra 
    \[
        \sum_{j=1}^sv_j=0.
    \]
    Ez egy olyan lineáris kombináció, amelyben minden vektor együtthatója 1.
    Emiatt a $\left\{ v_1,\dots,v_s \right\}$ vektorrendszer nem zérus vektorai is összefüggő rendszert alkotnak, 
    feltéve hogy vannak ilyenek.
    No de egy $v_j\in M_j$ vektor ha nem zérus, akkor egy sajátvektor.
    Tehát ha a vektorrendszerben lenne nem zérus elem, 
    akkor találnánk különböző sajátértékekhez tartozó sajátvektorok egy lineárisan összefüggő rendszerét, 
    ami \aref{pr:svlinfgtlen}.~állítás szerint nem lehetséges.
\end{proof}
Meggondoltuk tehát,
hogy ha páronként különböző $\left\{ \lambda_1,\dots,\lambda_s \right\}\subseteq\sigma\left( A \right)$ sajátértékekből indulunk ki,
és egyesítjük a $\ker\left( A-\lambda_j I \right)$ sajátalterek egy-egy bázisait,
akkor az így összetett vektorrendszer az
\[
    \ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)
\]
altér egy -- sajátvektorokból álló -- bázisa.
Rögzítsük is ezt a fontos gondolatot, 
amelyet a feladatok megoldása során sokszor használjuk majd.
\begin{proposition}
    A különböző sajátértékekhez tartozó sajátelterek bázisainak egyesítése a sajátalterek direktösszegének egy bázisa.
\end{proposition}
\begin{proposition}[diagonalizálhatóság]\label{pr:diagkar}
    Legyen $A\in L\left( V \right)$ lineáris transzformáció.
    Jelölje $\left\{ \lambda_1,\dots,\lambda_s \right\}=\sigma\left( A \right)$
    az $A$ spektrumát, 
    azaz valamennyi különböző sajátértékét.
    Az alábbi feltevések ekvivalensek.
    \begin{enumerate}
        \item Az $A$ sajátértékei geometriai multiplicitásának összege $\dim V$,
        \item 
            \begin{math}
                \ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)=V,
            \end{math}
        \item Minden vektor előáll mint sajátvektorok összege,
        \item Az $A$ diagonalizálható lineáris transzformáció.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Mivel a $\ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)\subseteq V$ 
    tartalmazás mindig fennáll,
    ezért a 2. feltétel ekvivalens avval, hogy
    \[
        \sum_{j=1}^s\dim\left( \ker\left( A-\lambda_j I \right) \right)
        =
        \dim\left( 
        \ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)
        \right)
        =\dim V,
    \]
    ami éppen a geometriai multiplicitásra vonatkozó feltétel.
    Így az első két feltevés ekvivalenciáját megértettük.


    Mivel a $\ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)\subseteq V$ 
    tartalmazás mindig fennáll,
    ezért a 2. feltétel ekvivalens avval, hogy
    \[
        \sum_{j=1}^s\ker\left( A-\lambda_j I \right)=V,
    \]
    ami éppen a 3. feltétel.
    Így a 2. és a 3. feltételek ekvivalenciáját is megértettük.

    Ha 3., ezért 2. fennáll, akkor az egyes $\ker\left( A-\lambda_j I \right)$ sajátelterek bázisait egyesítve a 
    $V$ tér egy sajátvektorokból álló bázisát kapjuk, 
    ergo az $A$ diagonalizálható transzformáció.
    Megfordítva, 
    ha $A$ diagonalizálható, 
    akkor van sajátvektorokból álló bázisa, 
    így minden vektor előáll mint sajátvektorok összege.
    Evvel a 3. és a 4. feltételek ekvivalenciáját is igazoltuk.
\end{proof}
\chapter{Transzformációk redukálása}
\begin{proposition}
    Tekintsünk egy $A\in L\left( V \right)$ lineáris transzformációt,
    amelynek minimál polinomja $m\in\mathbb{F}\left[ t \right]$.
    Tegyük fel, hogy $m$ előáll mint a $p,q$ egymással relatív prím, normált 
    polinomok
   \[
       m=pq
   \]
   szorzata.
   Ekkor a tér szétesik a 
   $\ker p\left( A \right)$ és a 
   $\ker q\left( A \right)$ invariáns alterei direktösszegére, azaz
   \[
       \ker p\left( A \right)\oplus
       \ker q\left( A \right)=
       V.
   \]
   Ha $A_1=A_{|\ker p\left( A \right)}$ és
   $A_2=A_{|\ker q\left( A \right)}$, akkor $A_1$ minimál polinomja $p$ és $A_2$ minimál polinomja $q$.
\end{proposition}
\begin{proof}
    Mivel $p$ és $q$ relatív prímek, ezért a Bezout-azonosság szerint van $f,g\in\mathbb{F}\left[ t \right]$ polinom, amelyekre
    \[
        fp+gq=1.
    \]
    Emiatt persze minden $x\in V$ mellett 
    \[
        f\left( A \right)p\left( A \right)x
        +
        g\left( A \right)q\left( A \right)x=Ix=x.\tag{\dag}
    \]

    Ha $x\in\ker p\left( A \right)\cap\ker q\left( A \right)$,
    akkor 
    $p\left( A \right)x=0=q\left( A \right)x$, tehát $x=0$, ami azt jelenti, hogy
    $\ker p\left( A \right)$ és 
    $\ker q\left( A \right)$ diszjunkt alterek.
    \\
    Vegyük észre, hogy a
    $f\left( A \right)p\left( A \right)x\in\ker q\left( A \right)$,
    és hasonlóan 
    $g\left( A \right)q\left( A \right)x\in\ker p\left( A \right)$.
    Ez azt jelenti, hogy $\ker q\left( A \right)+\ker p\left( A \right)=V$ azonosság is fennáll.
    Megmutattuk tehát, hogy $V$ előáll mint a $\ker p\left( A \right)$ és a 
    $\ker q\left( A \right)$ alterek direktösszege.

    Világos, hogy minden $u\in\ker p\left( A \right)$ mellett
    $p\left( A_1 \right)u
    =
    p\left( A \right)u=0$.

    Ha $h$ egy másik olyan polinom, amelyre $h\left( A_1 \right)=0\in L\left( \ker p\left( A \right) \right)$,
    akkor mivel a direktösszegre vonatkozó állítást már igazoltuk
    \[
        (hq)\left( A \right)x=
        h\left( A \right)q\left( A \right)\left( x_1+x_2 \right)
        =
        q\left( A \right)h\left( A \right)x_1
        +
        h\left( A \right)q\left( A \right)x_2
        =0+0
        =0,
    \]
    ahol $x=x_1+x_2$, $x_1\in \ker p\left( A \right)$ és 
    $x_2\in\ker q\left( A \right)$.
    Látjuk tehát, hogy az $A$ transzformáció a $hq$ polinomnak is gyöke,
    emiatt $m|hq$.
    Mivel $p|m$, ezért 
    \[p|hq\] is fennáll.
    Most újra használjuk, hogy a $p$ és a $q$ polinomok relatív prímek,
    így azt kapjuk, hogy $p|h.$
    Megmutattuk, hogy a $J_{A_1}$ ideált a $p$ normált polinom generálja,
    ami éppen azt jelenti, hogy $p$ az $A_1$ transzformáció minimál polinomja.
    Az $A_2$ minimál polinomja $q$ állitás igazolása a fentivel analóg.
\end{proof}
\begin{proposition}\label{pr:redukcio-primszorzat}
    Tekintsünk egy $A\in L\left( V \right)$ lineáris transzformációt,
    amelynek minimál polinomja $m\in\mathbb{F}\left[ t \right]$.
    Tegyük fel, hogy $m$ előáll mint a páronként relatív prím
    normált polinomok
    \[
       m=p_1p_2\dots p_n
    \]
    szorzata.
    Jelölje minden $i=1,\dots,n$ mellett $V_i=\ker p_i\left( A \right)$
    invariáns alteret, és $A_i=A|V_i$ megszorítást.
    Világos, hogy $A_i\in L\left( V_i \right)$.
    Ekkor
    \begin{enumerate}
        \item $V=V_1\oplus\dots\oplus V_n$;
        \item $p_i$ az $A_i$ transzformáció minimál polinomja minden szóba jövő
            $i=1,\dots,n$ mellett.\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    A polinomok $n$ száma szerinti teljes indukcióval igazolunk.
    Az $n=1$ eset triviális, de $n=2$ éppen az előző állítás.

    Most tegyük fel, hogy az állítás $n$-nél kevesebb polinom szorzatára igaz, 
    és lássuk be $n$-re.
    Feltehető tehát, hogy $n\geq 3$.
    Legyen $p=p_1,\dots,p_{n-1}$.
    Világos, hogy $p$ és $p_n$ relatív prímek, hiszen ha $d$ irreducibilis osztója
    $p$-nek és $p_n$-nek, 
    akkor $d$ prím tulajdonsága szerint $d|p_i$ valamely $i<n$-re,
    tehát $d|p_i$ és $d|p_n$. 
    A feltevés szerint ilyen csak a konstans polinom lehetséges, 
    ami valóban igazolja, hogy $p$ és $p_n$ relatív prímek.
    Persze 
    \[
        m=pp_n.
    \]
    Alkalmazhatjuk tehát az előző állítást, azaz
    \[
        V=\ker p\left( A \right)\oplus V_n,
    \]
    továbbá $p$ a minimál polinomja az $A|\ker p\left( A \right)$-nak
    és persze $p_n$ minimál polinomja $A_n$-nek.

    Alkalmazzuk most az indukciós feltevést a $\ker p\left( A \right)$ vektortérre.
    Ott az $A_{|\ker p\left( A \right)}$ lineáris transzformáció $p$ minimál polinomja
    előáll mint $n-1$ páronként relatív prím polinom szorzata:
    \[
        p=p_1,\dots,p_{n-1}.
    \]
    Világos tehát, hogy 
    \begin{enumerate}
        \item 
            $\ker p\left( A \right)=V_1\oplus\dots\oplus V_{n-1}$ és
        \item $p_i$ az $A_i$ minimál polinomja minden $i=1,\dots,n-1$ mellett.
    \end{enumerate}
    Teljesül tehát a $V_1,\dots,V_{n-1},V_n$ alterekre, 
    hogy mind diszjunkt -- az itt adott sorrendben -- az előzőek összegétől,
    és a Minkowski-összegük az egész $V$ vektortér.\index{Minkowski-összeg}
    \Aref{pr:dirosszegelozo}. állítás szerint tehát $V=V_1\oplus\dots\oplus V_n$.
\end{proof}
\section{Sajátvektorok, minimálpolinom és diagonalizálhatóság}
Alkalom nyílik, 
hogy a lineáris transzformáció diagonalizálhatóságát karakterizáló \aref{pr:diagkar}.~állítást
tovább bővítsük, a minimál polinom szerepének hangsúlyozásával.
\begin{proposition}[diagonalizálhatóság]
    Legyen $A\in L\left( V \right)$ lineáris transzformáció.
    Jelölje $\left\{ \lambda_1,\dots,\lambda_s \right\}=\sigma\left( A \right)$
    az $A$ spektrumát, 
    azaz valamennyi különböző sajátértékét.
    Az alábbi feltevések ekvivalensek.
    \begin{enumerate}
        \item Az $A$ sajátértékei geometriai multiplicitásának összege $\dim V$;
        \item 
            \begin{math}
                \ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)=V
            \end{math};
        \item Minden vektor előáll mint sajátvektorok összege;
        \item Az $A$ diagonalizálható lineáris transzformáció;
        \item Az $A$ transzformáció minimál polinomja
            \[
                m\left( t \right)=
                \prod_{j=1}^s\left( t-\lambda_j \right).
            \qedhere
            \]
    \end{enumerate}
\end{proposition}
\begin{proof}
    Ha az $A$ transzformáció diagonalizálható,
    akkor a diagonálisában a sajátértékei vannak.
    Írjuk fel tehát a 
    \begin{math}
                \prod_{j=1}^s\left( A-\lambda_j I \right)
    \end{math}
    transzformáció mátrixát abban a bázisban, 
    amelyben $A$ mátrixa is diagonális.
    Az eredmény egy diagonális mátrix,
    és ha felírjuk ezt mint az $[A-\lambda_j I]$ mátrixok szorzatát,
    akkor minden diagonális pozició az egyik szorzó mátrixban zérus,
    ergo a szorzat mátrix is a zéró mátrix.
    A fenti polinomnak tehát az $A$ transzformáció gyöke.
    Mivel minden sajátérték a minimál polinom gyöke, 
    ezért a fenti polinom a legalacsonyabb fokú normált polinom, 
    amelynek gyöke $A$.
    \footnote{
        Egy másik érv: $t-\lambda_j$ a $\lambda_j$ sajátértékhez tartozó sajátvektor kis minimál polinomja. 
        Mivel a sajátvektorok egy bazist alkotnak,
        ezért ezen polinomok legkisebb közös többszöröse a minimál polinom.
        Persze $\dim V$ darab elsőfokú polinom normált polinom legkisebb közös többszöröse,
        ezek közül a különbözők szorzata.
    }

    Megfordítva,
    legyen $p_j\left( t \right)=t-\lambda_j$ minden $j=1,\dots,s.$
    Ekkor $m=p_1\cdots p_s$ páronként relatív prím, normált polinomok szorzata,
    ezért az éppen igazolt \ref{pr:redukcio-primszorzat}.~állítás szerint 
    \begin{equation*}
        V=
        \ker p_1\left( A \right)\oplus\dots\oplus\ker p_j\left( A \right)
        =
        \ker\left( A-\lambda_1 I \right)\oplus\dots\oplus\ker\left( A-\lambda_s I \right)
    \end{equation*}
    Ezt kellett belátni. 
\end{proof}
\section{Redukálás: az általános eset}
Mivel minden polinom előáll mint néhány irreducibilis polinom szorzata,
ezért \ref{pr:redukcio-primszorzat}.~állítás így is fogalmazható.
\begin{proposition}\label{pr:redukcio-primfelbontas}
    Legyen $A\in L\left( V \right)$ lineáris transzformáció minimál polinomja
    $m$.
    Tudjuk, hogy $m$ egyértelműen áll elő
    \[
        m
        =
        p_1^{m_1}p_2^{m_2}\cdots p_r^{m_r},
    \]
    alakban, 
    ahol $p_1,\dots,p_r$ egymástól páronként különböző normált, irreducibilis polinomok.
    Jelölje $V_i=\ker p_i^{m_i}\left( A \right)$ 
    jelölje $A_i=A_{|V_i}$ minden $i=1,\dots,r$ mellett.
    Ekkor
    \begin{enumerate}
        \item $V=V_1\oplus\dots\oplus V_r$ és,
        \item $A_i$ minimál polinomja $p_i^{m_i}$ minden $i=1,\dots,r$ mellett.\qedhere
    \end{enumerate}
\end{proposition}
A konklúzió tehát az, 
hogy elég olyan transzformációkkal foglalkoznunk,
amelyek minimál polinomja egy irreducibilis polinom valamely egész kitevős hatványa.
Az algebra alaptétele szerint a komplex számtest feletti irreducibilis polinom csak elsőfokú polinom lehet.
Abban a speciális esetben tehát,
amikor a $\mathbb{C}$ komplex számtest feletti vektortereket\index{komplex számtest feletti vektortér} vizsgálunk,
ez azt jelenti, 
hogy elég ha olyan $A\in L\left( V \right)$ transzformációval foglalkozunk, 
amelynek $m$ minimál polinomjára
\[
    m\left( t \right)=(t-\alpha)^n
\]
teljesül valamely $\alpha\in\mathbb{C}$ komplex szám és $n\in\mathbb{N}$ egész mellett.
Ilyen $A$ transzformációra, ha $B$ jelöli a $B=A-\alpha I$ lineáris transzformációt,
akkor a $B$ olyan, 
hogy valamely egész kitevős hatványa a konstans zérus transzformáció.
Ez vezet majd a \emph{nillpotens}\index{nillpotens transzformáció} fogalmához.
Ha felírjuk valamely bázisban egy ilyen $B$ nillpotens transzformáció mátrixát,
akkor $A$ mátrixa is könnyen adódik $B$ mátrixából.
Ehhez csak az $\alpha [I]$ mátrixot kell $\left[ B \right]$-hez adni,
ami praktikusan nem jelent többet, 
mint hogy a $\left[ B \right]$ diagonális elemeket kell az $\alpha$ komplex számmal megemelni.
A fenti gondaloton alapul a \emph{Jordan-normálak}\index{Jordan-normálalak} fogalma. 

\chapter{Redukálás irreducibilis minimál polinom esetén}
\scwords A legegyszerűbb eset, mikor a minimálpolinom elsőfokú irreducibilis polinomok szorzata.

\begin{proposition}\label{pr:irred_redukcio}
    Legyen $A\in L\left( V \right)$ egy lineáris transzformációja a $V$ véges dimenziós vektortérnek,
    az $m\in\mathbb{F}\left[ t \right]$ egy $k$-adfokú, irreducibilis polinom, amelyre $m\left( A \right)=0$.
    Ekkor
    \begin{enumerate}
        \item minden $v\neq 0$ mellett 
            \begin{math}
                \dim\lin\left( v;A \right)=k;
            \end{math}
        \item
            minden $v\in V$ vektor és minden $K\subseteq V$ invariáns altér mellett
            $\lin\left( v;A \right)\cap K=\left\{ 0 \right\}$ vagy 
            $\lin\left( v;A \right)\subseteq K$;
        \item
            a $V$ altér nulla dimenziós,
            vagy $k$ dimenziós,
            vagy $k$ dimenziós $A$ invariáns alterek direktösszege.
            Pontosabban,
            ha $\dim V>0$, akkor létezik $r\geq 1$ szám, 
            és léteznek $v_1,\dots,v_r$ vektorok, amelyekre
            \[
                \lin\left( v_1;A \right)\oplus\dots\oplus \lin\left( v_r;A \right)=V.
                \qedhere
            \]
    \end{enumerate}
\end{proposition}
\begin{proof}[Bizonyítás (1.)]
    Legyen adott $v\neq 0$ vektor mellett $p_v$ a $v$-hez tartozó kis minimál polinom.
    Mivel $m\left( A \right)v=0$, ezért $p_v|m$.
    No de $m$ irreducibilis, ezért $m=p_v$.
    Ekkor viszont
    \[
        k=\deg m=\deg p_v
        =
        \dim\lin\left( v;A \right).\qedhere
    \]
\end{proof}
\begin{proof}[Bizonyítás (2.)]
    Ha $v=0$, akkor az állítás nyilvánvaló. 
    A továbbiakban emiatt $v\neq 0$.
    Most tegyük fel, hogy $x\in\lin\left( v;A \right)\cap K$ és $x\neq 0$.
    Ekkor
    \[
        \lin\left( x;A \right)
        \subseteq
        \lin\left( v;A \right)\cap K
        \subseteq
        \lin\left( v;A \right)
    \]
    No de, 
    a bal és jobb oldali altér azonos dimenziós alterek,
    emiatt fent mindenütt egyenlőség van.
    Speciálisan 
    $\lin\left( v;A \right)=\lin\left( v;A \right)\cap K$,
    ami éppen azt jelenti, 
    hogy 
    $\lin\left( v;A \right)\subseteq K.$
\end{proof}
\begin{proof}[Bizonyítás (3.)]
    Először is gondoljuk meg, hogy invariáns alterek Minkowski--összege is invariáns altér marad.
    Ha $V$ nem tartalmaz nem zérus vektort, akkor $\dim V=\left\{ 0 \right\}$.

    Ha $v_1\in V$ egy nem zérus vektor,
    akkor jelölje $V_1=\lin\left( v_1;A \right)$.
    Ha $V=V_1$, 
    akkor $V$ egy $k$-dimenziós vektortér.

    Ha $V\neq V_1$, akkor van van $v_2\in V\setminus V_1$.
    Mivel $V_1$ egy invariáns altér, 
    ezért a már igazolt állítás szerint bevezetve a $V_2=\lin\left( v_2;A \right)$
    jelölést $V_2\cap V_1=\left\{ 0 \right\}$.
    Értelmes tehát venni e két invariáns altér direktösszegét.
    Ha $V=V_1\oplus V_2$, akkor $V$ előállt két $k$ dimenziós invariáns alterének direktösszegeként.

    Ha $V\neq V_1\oplus V_2$, akkor van van $v_3\in V\setminus (V_1\oplus V_2)$.
    Mivel $V_1\oplus V_2$ egy invariáns altér, 
    ezért a már igazolt állítás szerint bevezetve a $V_3=\lin\left( v_3;A \right)$
    jelölést $V_3\cap (V_1\oplus V_2)=\left\{ 0 \right\}$.
    Értelmes tehát venni e három invariáns altér direktösszegét, hiszen
    a $V_1,V_2,V_3$ alterek ebben a sorrendben véve olyanok, 
    hogy mind diszjunkt az előzőek összegétől.
    Ha $V=V_1\oplus V_2\oplus V_2$, 
    akkor $V$ előállt három $k$ dimenziós invariáns alterének direktösszegeként.

    Ha $V\neq V_1\oplus\dots\oplus V_t$, valamely $t\geq 2$ mellett,
    akkor van van $v_{t+1}\in V\setminus (V_1\oplus\dots\oplus V_t)$.
    Mivel $V_1\oplus\dots\oplus V_t$ egy invariáns altér, 
    ezért a már igazolt állítás szerint bevezetve a $V_{t+1}=\lin\left( v_{t+1};A \right)$
    jelölést $V_{t+1}\cap (V_1\oplus\dots\oplus V_t)=\left\{ 0 \right\}$.
    Értelmes tehát venni ezen $t+1$ invariáns altér direktösszegét, hiszen
    a $V_1,V_2,V_3,\dots,V_t,V_{t+1}$ alterek ebben a sorrendben véve olyanok, 
    hogy mind diszjunkt az előzőek összegétől.
    Ha $V=V_1\oplus\dots\oplus V_{t+1}$, 
    akkor $V$ előállt $t+1$ darab $k$ dimenziós invariáns alterének direktösszegeként.

    Az eljárás előbb utóbb a $V$ vektortér véges dimenziós volta miatt megáll.
\end{proof}
\subsection{Következmény}
Világos, hogy 
\(
\left\{ 
A^{k-1}v_1, A^{k-2}v_1,v_1,\dots,Av_1,v_1 
\right\}
\)
bázisa a $\lin\left\{ v_1;A \right\}$
invariáns altérnek.
Ha ebben a bázisban felírjuk a transzformáció mátrixát, 
akkor az első oszlopban vannak a minimálpolinom együtthatóinak ellentettjei,
a diagonális feletti $1$-ek, vannak és minden más elem zérus:
\[
    \begin{pmatrix}
        -\alpha_{k-1}&1&0&\dots&0\\
        -\alpha_{k-2}&0&1&\dots&0\\
        \vdots&\vdots&\ddots&\ddots&\vdots\\
        -\alpha_{1}&0&0&\dots&1\\
        -\alpha_{0}&0&0&\dots&0
    \end{pmatrix}
\]
Meggondoltuk tehát, hogy ha a transzformáció $m$ minimálpolinomja irreducibilis és
\[m\left( t \right)
=\alpha_0+\alpha_1t+\dots+\alpha_{k-1}t^{k-1}+t^k
\]
alakú, akkor a térnek van olyan bázisa, amelyben a transzformáció mátrixa a fenti mátrix
diagonális elrendezésű $r$ darab másolatából áll, ahol $rk=\dim V$.

Házi feladatként gondoljuk meg, hogy minden nem konstans $m\left( t \right)\in\mathbb{F}\left[ t \right]$ 
polinomhoz létezik egy $\mathbb{F}$ feletti vektortér, és azon egy lineáris transzformáció, 
amelynek minimálpolinomja éppen $m$.
({\footnotesize Segítség: Nézzük és csodáljuk a fenti mátrixot.})
\subsubsection{Illusztráció}
\textit{%
Bontsuk lehető legalacsonyabb invariáns alterek direktösszegére az alábbi $\mathbb{R}$
feletti vektortéren értelmezett lineáris transzformáció értelmezési tartományát,
és írjuk fel $A$ mátrixát a lehető legegyszerűbb módon.
A transzformáció definíciója egy 
$\left\{
u_1,u_2,u_3,u_4
\right\}
$
bázis felett a következő:
\[
    A
    \left( 
    \alpha u_1
    +
    \beta u_2
    +
    \gamma u_3
    +
    \delta u_4
    \right)
    =
    \left( -2\alpha + 3\gamma \right)u_1
    +
    \left( -2\alpha-\beta+3\gamma+\delta \right)u_2
    +
    \left( -\alpha+\gamma \right)u_3
    +
    \left( -\alpha-\beta+3\gamma \right)u_4.
\]
}
\begin{proof}[Megoldás]
Írjuk fel az operátor mátrixát:
\(
    A=
    \begin{pmatrix}
        -2&0&3&0\\
        -2&-1&3&1\\
        -1&0&1&0\\
        -1&-1&3&0
    \end{pmatrix}.
\)
Ha a sajátvektorokat keressük látjuk, hogy nincs valós sajátérték.
Keressük tehát a minimálpolinomot a bázis egyes elemeihez tartozó kis minimálpolinomok 
meghatározásával.
\[
    \begin{array}{r|cc}
        &Au_1&A^2u_1\\
        \hline
        u_1&-2&1\\
           &\framebox{-2}&2\\
           &-1&1\\
           &-1&1\\
        \hline
        &\delta&-1
    \end{array}
    \qquad
    \begin{array}{r|c}
           &A^2u_1\\
        \hline
        u_1&-1\\
       Au_1&-1\\
           & 0\\
           & 0\\
           \hline
           & 
    \end{array}
    \qquad\qquad
    \begin{array}{r}
        \left\{ u_1,Au_1 \right\}\mbox{ lineárisan független, de}\\
        A^2u_1+Au_1+u_1=0,\\
        \mbox{ezért } p_1\left( t \right)=t^2+t+1.
    \end{array}
\]
\[
    \begin{array}{r|cc}
        &Au_2&A^2u_2\\
        \hline
           &0 &0\\
        u_2&-1&0\\
           & 0&0\\
           &\framebox{-1}&1\\
        \hline
        &\delta&-1
    \end{array}
    \qquad
    \begin{array}{r|c}
           &A^2u_2\\
        \hline
           & 0\\
        u_2&-1\\
           & 0\\
       Au_2&-1\\
           \hline
           & 
    \end{array}
    \qquad\qquad
    \begin{array}{r}
        \left\{ u_2,Au_2 \right\}\mbox{ lineárisan független, de}\\
        A^2u_2+Au_2+u_2=0,\\
        \mbox{ezért } p_2\left( t \right)=t^2+t+1.
    \end{array}
\]
\[
    \begin{array}{r|cc}
        &Au_3&A^2u_3\\
        \hline
           &\framebox{3}&-3\\
           &3&-3\\
        u_3&1&-2\\
           &3&-3\\
        \hline
        &\delta&-1
    \end{array}
    \qquad
    \begin{array}{r|c}
           &A^2u_3\\
        \hline
       Au_3&-1\\
           & 0\\
        u_3&-1\\
           &0\\
           \hline
           & 
    \end{array}
    \qquad\qquad
    \begin{array}{r}
        \left\{ u_3,Au_3 \right\}\mbox{ lineárisan független, de}\\
        A^2u_3+Au_2+u_3=0,\\
        \mbox{ezért } p_3\left( t \right)=t^2+t+1.
    \end{array}
\]
\[
    \begin{array}{r|cc}
        &Au_4&A^2u_4\\
        \hline
           &0&0\\
           &\framebox{1}&-1\\
           &0&0\\
        u_4&0&-1\\
        \hline
        &\delta&-1
    \end{array}
    \qquad
    \begin{array}{r|c}
           &A^2u_4\\
        \hline
           &0\\
       Au_4&-1\\
           & 0\\
        u_4&-1\\
           \hline
           & 
    \end{array}
    \qquad\qquad
    \begin{array}{r}
        \left\{ u_4,Au_4 \right\}\mbox{ lineárisan független, de}\\
        A^2u_4+Au_4+u_4=0,\\
        \mbox{ezért } p_4\left( t \right)=t^2+t+1.
    \end{array}
\]
Ez azt jelenti, hogy a minimálpolinom az 
$m\left( t \right)=t^2+t+1$ másodfokú, az $\mathbb{R}$ test felett irreducibilis polinom.
A mátrix, tehát két darab
\(
\begin{pmatrix}
    -1,1\\
    -1,0
\end{pmatrix}
\)
mátrix diagonális elrendezésű partíciója.
Mivel $Au_4=u_2$ az első invariáns altér bázisa lehet például
$
\left\{ 
u_2,u_4
\right\}
$. Minden olyan nem zérus vektorra, amely nincs e két vektor lineáris burkában,
az
$
\left\{ Av,v \right\}$
rendszer egy invariáns direktkiegészítőt definiál.
Pont erről szól \aref{pr:irred_redukcio}.~állítás.
Ilyen módon például az
\(
\left\{ 
    u_2,u_4,Au_1,u_1
\right\}
\)
bázisban a transzformáció mátrixa
\(
\begin{pmatrix}
    -1&1&0&0\\
    -1&0&0&0\\
    0&0&-1&1\\
    0&0&-1&0
\end{pmatrix}
\)
alakú.
Emlékezve az általános bázis transzformációra azt kaptuk, hogy
\[
\begin{pmatrix}
    -1&1&0&0\\
    -1&0&0&0\\
    0&0&-1&1\\
    0&0&-1&0
\end{pmatrix}
=
    \begin{pmatrix}
        0&0&-2&1\\
        1&0&-2&0\\
        0&0&-1&0\\
        0&1&-1&0
    \end{pmatrix}^{-1}\cdot
    \begin{pmatrix}
        -2&0&3&0\\
        -2&-1&3&1\\
        -1&0&1&0\\
        -1&-1&3&0
    \end{pmatrix}\cdot
    \begin{pmatrix}
        0&0&-2&1\\
        1&0&-2&0\\
        0&0&-1&0\\
        0&1&-1&0
    \end{pmatrix}.\qedhere
\]
\end{proof}
\section{Irreducibilis polinommal képzett magtér redukálása}
Legyen most $p$ egy tetszőleges $k$-ad fokú irreducibilis polinom,
és $A\in L(V)$ egy lineáris transzformáció.
Tekintsük a $V_1=\ker p\left( A \right)$ invariáns alteret, amelyre szorítsuk meg az $A$
transzformációt, azaz $A_1=A|V_1$.
Világos, hogy $p\left( A_1 \right)=0\in L\left( V_1 \right)$, 
ezért alkalmazhatjuk a fent igazolt \ref{pr:irred_redukcio}.~állítást
a $V_1$ altérre és az $A_1$ transzformációra.
\begin{proposition}
    Legyen $V$ egy véges dimenziós vektortér, $A\in L\left( V \right)$ egy lineáris transzformáció,
    és $p\in\mathbb{F}\left[ t \right]$ egy irreducibilis polinom.
    Ekkor:
    \begin{enumerate}
        \item Minden $v\neq 0$, $v\in\ker p\left( A \right)$ mellett
            \(
                \dim\lin(v;A)=\deg p;
            \)
        \item A $\ker p\left( A \right)$ altér nulla dimenziós,
            vagy $k$ dimenziós, vagy $k$ dimenziós invariáns alterek direktösszege.
            Pontosabban fogalmazva
            ha $\nu\left( A \right)>0$, akkor létezik $r\geq 1$ szám,
            és léteznek $v_1,\dots,v_r$ vektorok, amelyekre
            \[
                \lin\left( v_1;A \right)\oplus\dots\oplus \lin\left( v_r;A \right)
                =
                \ker p\left( A \right).
                \qedhere
            \]
    \end{enumerate}
\end{proposition}
Összefoglalhatjuk azt az esetet, 
mikor a minimálpolinom különböző irreducibilis polinomok szorzata.
Ez éppen \aref{pr:redukcio-primszorzat}.~állítás esete.
\begin{proposition}
    Tekintsünk egy $A\in L\left( V \right)$ lineáris transzformációt,
    amelynek minimál polinomja $m\in\mathbb{F}\left[ t \right]$.
    Tegyük fel, 
    hogy $m$ előáll mint a különböző normált, irreduciblis polinomok elsőfokú hatványainak
    \[
       m=p_1p_2\dots p_s
    \]
    szorzata.
    Ekkor $V$ előáll mint néhány -- de legalább egy-egy darab --
    $\deg p_1,\deg p_2,\dots,\deg p_s$ dimenziós minimális invariáns alterének direktösszege.
\end{proposition}
\begin{proof}
    \Aref{pr:redukcio-primfelbontas}.~állításban láttuk, 
    hogy $V=\ker p_1\left( A \right)\oplus\dots\oplus\ker p_s\left( A \right)$ alakú.
    Minden egyes $p_j$ a minimál polinom legalább lesőfokú osztója, így $\ker p_j\left( A \right)\neq\left\{ 0 \right\}$.
    Az előző állítás szerint minden $j$ mellett $\ker p_j\left( A \right)$ egy $\deg p_j$
    dimenziós invariáns altér, vagy néhány ilyen direktösszege.
    Mivel ez minden $j$ mellett igaz, ezért $\ker p_j\left( A \right)$ felbontását a $V$ felbontásába helyettesítve készen is vagyunk.
\end{proof}
\subsection{A komplex és a valós eset}
Most az tegyük fel, hogy a $V$ vektortér a $\mathbb{C}$ komplex számtest feletti vektortér.
Ekkor az $A\in L\left( V \right)$ lineáris transzformáció minimál polinomja egy komplex együtthatós polinom.
Az algebra alaptétele szerint irreducibilis normált polinom csak elsőfokú, azaz $t-\lambda$ alakú lehet,
tehát az előző állítás feltétele most abba megy át, hogy $A$ minimál polinomja
\[
    m\left( t \right)=\left( t-\lambda_1 \right)\left( t-\lambda_2 \right)\dots\left( t-\lambda_s \right)
\]
alakú, ahol $\lambda_1,\dots,\lambda_s$ az $m$ különböző gyökei.
Tudjuk viszont, hogy a minimál polinom gyökei a sajátértékek,
ezért a feltételt egyszerűbben úgy is fogalmazhatjuk,
a minimál polinom gyökei egyszeresek, vagy ami ugyanaz: 
a sajátértékek a minimál polinom egyszeres gyökei.
\begin{proposition}
    Tegyük fel, hogy $V$ egy $\mathbb{C}$ feletti vektortér és $A\in L\left( V \right)$ egy lineáris transzformáció.
    $A$ pontosan akkor diagonalizálható,
    ha minimál polinomja gyökei egyszeres multiplicitásúak.
\end{proposition}
\begin{proof}
    Ha a sajátértékek a minimál polinomnak egyszeres multiplicitású gyökei,
    akkor fennállnak az előző tétel feltételei.
    Így $V$ előáll mint néhány egy dimenziós invariáns alterének direktösszege,
    ami azt jelenti, hogy van sajátvektorokból álló bázisa, ergo diagonalizálható.

    Megfordítva, ha $A$ diagonalizálható, akkor a diagonálisban a sajátértékei vannak.
    Ha $\left\{ \lambda_1,\dots,\lambda_s \right\}$ a diagonális különböző elemei,
    akkor az $A$ minimál polinomja nyilvánvalóan
    \[
        m\left( t \right)
        =
        \prod_{j=1}^s\left( t-\lambda_j \right)
    \]
    alakú.
    Ez persze olyan polinom, 
    amelyben minden gyöktényező csak egyszer szerepel.
\end{proof}
Mivel a valós test feletti irreducibilis polinomok első-- vagy másodfokúak, 
ezért a valós esetben is szép állítást kapunk.
\footnote{Ha nem is annyira szépet mint a komplex esetben.}
\begin{proposition}
    Tegyük fel, hogy $V$ egy $\mathbb{R}$ feletti vektortér és $A\in L\left( V \right)$.
    A $V$ pontosan akkor bontható fel egy vagy két dimenziós minimális $A$-invariáns alterek direktösszegére,
    ha az $A$ transzformáció $m$ 
    minimál polinomjának felbontásában minden irreducibilis polinom az első hatványon szerepel.
\end{proposition}
\begin{proof}
    A komplex esettel analóg.
\end{proof}
\chapter{A minimál polinom fokszámáról}
\scwords A minimál polinom definiálásakor csak annyit láttunk,
hogy legfeljebb $n^2$ fokú polionom mindig konstruálható,
amelynek a transzformáció gyöke, ahol $n$ a tér dimenziója.
Ebben a fejezetben látni fogjuk, hogy a fenti gondolat nagyon lényegesen erősíthető.
Azt mutatjuk meg, 
hogy a minimál polinom fokszáma nem lehet a tér dimenziójánál magasabb.
\begin{lemma}
    Legyen $B\in L\left( V \right)$ lineáris transzformáció,
    tegyük fel, hogy a $v_1,\dots,v_r$ vektorok mindegyikére $B^mv_j=0$, 
    de a 
    \[
        \left\{ B^{m-1}v_1,B^{m-1}v_2,B^{m-1}v_3,\dots,B^{m-1}v_r \right\}
    \]
    vektorrendszer lineárisan független. 
    Ekkor a 
    \[
        \left\{ 
        \begin{matrix}
            v_1 & v_2 & v_3 & \dots & v_r\\
            Bv_1 & Bv_2 & Bv_3 & \dots & Bv_{r}\\
           B^2v_1 & B^2v_2 & B^2v_3 & \dots & B^2v_r\\
           \vdots & \vdots & \vdots & \ddots & \vdots \\
           B^{t}v_1 & B^{t}v_2 & B^{t}v_3 & \dots & B^{t}v_r\\
           \vdots & \vdots & \vdots & \ddots & \vdots\\
           B^{m-1}v_1 & B^{m-1}v_2 & B^{m-1}v_3 & \dots & B^{m-1}v_r
        \end{matrix}
        \right\}
    \]
    vektorrendszer is lineárisan független.
    \label{le:nillp-fgtlen}
\end{lemma}
\begin{proof}
    Tekintsük ezen vektorok egy
    \[
        \sum_{j=0}^{m-1}\sum_{k=1}^r\alpha_{j,k}B^jx_k=0
    \]
    lineáris kombinációját.
    Meg kell mutatnunk, hogy az összes $\alpha_{j,k}=0$.
    Ha $m-1\geq t\geq 0$, az első olyan index, amelyre van $\alpha_{t,k}\neq 0$
    együttható, akkor a $t$-edik index előtt, 
    minden együttható zérus, ergo
    \[
        \sum_{j=t}^{m-1}\sum_{k=1}^r\alpha_{j,k}B^jx_k=0
    \]
    Erre alkalmazva a $B^{m-t-1}$ transzformációt azt kapjuk, hogy
    \[
        0
        =
        B^{m-t-1}0
        =
        \sum_{j=t}^{m-1}\sum_{k=1}^r\alpha_{j,k}B^{j+m-t-1}x_k
        =
        \sum_{k=1}^r\alpha_{t,k}B^{t+m-t-1}x_k
        =
        \sum_{k=1}^r\alpha_{t,k}B^{m-1}x_k.
    \]
    No de az $\left\{ B^{m-1}x_1,\dots,B^{m-1}x_k \right\}$ egy lineárisan független rendszer,
    amiből már következik, 
    hogy minden $\alpha_{t,k}=0$,
    ami ellentmondásban van a $t$ index definíciójával.
\end{proof}
\begin{proposition}
    Legyen $B\in L\left( V \right)$ lineáris transzformációra $B^m=0$.
    Ekkor $\dim V\geq m\rho\left( B^{m-1} \right)$.
    \label{th:nilldim}
\end{proposition}
\begin{proof}
    Létezik tehát olyan $\left\{ B^{m-1}v_1,B^{m-1}v_2,\dots,B^{m-1}v_r \right\}$
    lineárisan független vektorrendszer, ahol $r=\rho(B^{m-1})$ a $B^{m-1}$ transzformáció képterének dimenziója.
    \Aref{le:nillp-fgtlen}.~lemma szerint a térben van egy $m\cdot r$ elemű lineárisan független vektorrendszer,
    ergo a tér legalább $mr=m\rho\left( B^{m-1} \right)$ dimenziós.
\end{proof}
\begin{proposition}
    Legyen $A\in L\left( V \right)$ minimálpolinomja 
    $
    p^{m}\left( t \right)
    $
    alakú,
    ahol $p$ egy irreducibilis polinom, melynek foka $k$.
    Ekkor a tér legalább $m\cdot k$ dimenziós 
    (, azaz a minimálpolinom foka legfeljebb a tér dimenziója).
    \label{th:minpol1}
\end{proposition}
\begin{proof}
    Először nézzük a trivialitásokat.
    Ha $p$ a konstans 1 polinom, akkor a tér csak a 0 vektort tartalmazza,
    tehát mind a minimálpolinom fokszáma, mind a tér dimenziója zérus.
    Ha $\deg p\geq 1$, és $m=1$, akkor a minimálpolinom irreducibilis, 
    de van $v\in V$ nem zérus vektor.
    Láttuk, hogy ilyenkor 
    \[
        \dim\lin\left( v;A \right)=\deg p=k,
    \]
    amiből persze következik, hogy a tér legalább $k$ dimenziós.
    \footnote{Sőt még azt is láttuk, hogy néhány -- lehet, hogy csak egy -- $k$ dimenziós invariáns altér direktösszege.}

    Most nézzük az érdekes esetet, 
    mikor $\deg p\geq 1$ és $m\geq 2$.
    Mivel a 
    \begin{math}
        p^{m-1}\left( t \right)
    \end{math}
    a minimálpolinomnál alacsonyab fokú de nem zérus polinom,  
    ezért létezik $v\in V$, melyre $B=p\left( A \right)$ jelöléssel
    \(
        B^{m-1}v\neq 0.
    \)
    Persze e vektor a $\ker B=\ker p\left( A \right)$ 
    egy eleme, 
    és $\ker p\left( A \right)$-ban minden nem zérus elem generálta invariáns altér éppen $k$-dimenziós,
    ezért 
    \[
        \lin\left\{ B^{m-1}v;A \right\}=
        \lin\left\{ 
            B^{m-1}v,
            AB^{m-1}v,
            A^2B^{m-1}v,
            \ldots,
            A^{k-1}B^{m-1}v
        \right\}
    \]
    pontosan $k$ dimenziós.
    Persze $\left\{ B^{m-1}v,B^{m-1}Av,B^{m-1}A^2v,\dots,B^{m-1}A^{k-1}v \right\}\subseteq \im B^{m-1}$,
    ergo 
    \[
        \rho\left( B^{m-1}\right)\geq k.
    \]
    Alkalmazva $B$-re az imént igazolt \ref{th:nilldim} tételt kapjuk a kívánt
    \(
        \dim V\geq m\rho\left( B^{m-1} \right)\geq m\cdot k
    \)
    becslést.
\end{proof}
\begin{proposition}
    Tetszőleges lineáris transzformáció minimálpolinomjának foka legfeljebb a tér dimenziója.
    \label{th:minpol}
\end{proposition}
\begin{proof}
    Legyen 
    $
    m\left( t \right)
    =
    p_1^{m_1}\left( t \right)\cdot
    p_2^{m_2}\left( t \right)\cdots
    p_r^{m_r}\left( t \right)
    $
    a minimálpolinom relativ prím, irreducibilis polinomok hatványaiként való faktorizációja.
    Tudjuk, hogy ekkor 
    \[
        V
        =
        \ker p_1^{m_1}\left( A \right)
        \oplus
        \ker p_2^{m_2}\left( A \right)
        \oplus\dots\oplus
        \ker p_r^{m_r}\left( A \right).
    \]
    Az $A|_{\ker p_i^{m_i}\left( A \right)}$ minimálpolinomja $p_i^{m_i}\left( t \right)$, így az előző tétel szerint minden egyes $i$ index mellett
    $
    m_i\cdot k_i\leq \dim(\ker p_i^{m_i}\left( A \right)).
    $
    Világos, hogy
%    \begin{IEEEeqnarray}{+c+c*}
    \[
        \deg m
        =
        \sum_{i=1}^rk_i\cdot m_i
        \leq
        \sum_{i=1}^r\dim(\ker p_i^{m_i}\left( A \right))
        =
        \dim V.\qedhere
    \]
%    \end{IEEEeqnarray}
\end{proof}
Később ki fog derülni,
hogy a transzformáció karakterisztikus polinomjának, amely pontosan $\dim V$-ed fokú,
is mindig gyöke a transzformáció.
Ez az úgynevezett Cayley-Hamilton--tétel.\index{Cayley--Hamilton}
\chapter{Nilpotens transzformációk}
\section{Hatvány függvény alakú minimálpolinom}
\begin{definition}[nillpotens transzformáció]
    Egy $A\in L\left( V \right)$ lineáris transzformációt 
    \emph{nillpotensnek} mondjuk, ha
    létezik $k\in\mathbb{N}$, melyre $A^k=0$.
    Ha $A$ egy nillpotens transzformáció, akkor azt a legkisebb $m$ számot, melyre $A^m=0$ a \emph{nillpotencia rendjének} nevezzük.
\end{definition}

Például egy 5 dimenziós téren könnyen definiálhatunk első-, másod-, harmad-, negyed, és ötöd-rendű nilpotens transzformációkat.
De van-e mondjuk hatod-rendű nillpotens transzformáció ezen öt dimenziós vektortéren?
Az első észrevétel adja a negatív választ.
\begin{proposition}
    Legyen $B$ egy $m$-ed rendben nillpotens operátor.
    Ekkor létezik $v\in V$ vektor, 
    melyre $B^{m-1}v\neq 0$.
    Minden ilyen $v$ vektorra a
    \[
        \left\{ v,B,B^2v,\cdots,B^{m-1}v \right\}
    \]
    $m$ elemű vektorrendszer lineárisan független.
    
    Emiatt ha a $V$ vektortérnek van $m$-edrendben nillpotens lineáris transzformációja, akkor $\dim V\geq m$,
    azaz a nillpotencia rendje legfeljebb a tér dimenziója.
    \label{th:fgtlen}
\end{proposition}
\begin{proof}
    Mivel $B^{m-1}\neq 0$, ezért valóban létezik $v\in V$ vektor,
    melyre $B^{m-1}v\neq 0$.
    Persze ekkor a $\left\{ B^{m-1}v \right\}$ egy elemet tartalmazó rendszer lineárisan független,
    ezért \aref{le:nillp-fgtlen}.~lemma szerint a tételbeli rendszer is lineárisan független.
\end{proof}
\begin{proposition}
    Egy lineáris transzformáció pontosan akkor nillpotens, 
    ha valamely $m\leq\dim V$ mellett a minimálpolinomja
    $m\left( t \right)=t^m$ alakú.
\end{proposition}
\begin{proof}
    Tegyük fel először, hogy $B$ lineáris transzformáció $m$-ed rendben 
    nillpotens.
    Legyen $p\left( t \right)=t^m$.
    Ekkor $p\in J_A$, így a minimál polinom $p$ osztója.
    Másrészt \aref{th:fgtlen}.~állítás szerint van a térnek olyan vektora,
    amelyhez tartalmazó kis minimál polinom is $p$.
    Így $p$ osztója a minimál polinomnak, ergo azonos vele.

    Megfordítva, ha $m\left( t \right)=t^m$ a minimálpolinomja $B$-nek,
    akkor $B^m=0$ és ez $m$-nél kisebb kitevőre nem teljesülhet.
    Ez éppen azt jelenti, hogy $B$ transzformáció $m$-ed rendben nillpotens.
\end{proof}

\section{Nilpotens operátorok redukálása}
Az alábbi lemmának nincs köze a transzformációk redukálásához.
Arra kell emlékeznünk, hogy egy véges dimenziós vektortérben minden altérnek van direkt kiegészítője.
\begin{lemma}
    Legyenek $V_1$ és $V_2$ diszjunkt alterei a véges dimenziós $W$ vektortérnek.
    Ekkor $V_1$-nek létezik $V_2$ alteret tartalmazó direkkiegészítője,
    azaz
    létezik $K\subset W$ altér, amelyre $V_2\subseteq K$ és $V_1\oplus K=V.$
    \label{felb0}
\end{lemma}
\begin{proof}
    Jelölje $V=V_1+V_2$. 
    Világos, hogy $V$ altér $W$-ben.
    Legyen $L$ a direktkiegészítője, azaz $V\oplus L=W$.
    Ha $K=V_2+L$, akkor $K$ olyan altér $W$-ben,
    amelyre $V_2\subseteq K$, $K\cap V_1=\left\{ 0 \right\}$, valamint 
    $V_1+K=W$.
\end{proof}
\begin{lemma}
    Legyen a $W$ véges dimenziós vektortérnek $H,K_0,\overline{K}$ altere.
    Tegyük fel, hogy 
    \begin{enumerate}
        \item $H\cap K_0=\left\{ 0 \right\}$;
        \item $H+\overline{K}=W$;
        \item $K_0\subseteq\overline{K}$.
    \end{enumerate}
    Ekkor létezik $K$ altere $W$-nek, melyre
    \begin{enumerate}
        \item $K_0\subseteq K\subseteq\overline{K}$ és
        \item $K$ direkt kiegészítője $H$-nak, azaz 
            $H\oplus K=W$.\qedhere
    \end{enumerate}
    \label{lem:felb}
\end{lemma}
\begin{proof}
    Világos, hogy $H\cap\overline{K}\subseteq\overline{K}$ és $K_0\subseteq\overline{K}$ diszjunkt alterek
    $\overline{K}$-ban, hiszen
    \[
        \left( H\cap\overline{K} \right)\cap K_0\subseteq
        H\cap K_0=\left\{ 0 \right\}.
    \]
    Alkalmazzuk az előző (\ref{felb0}) lemmát a $\overline{K}$ altérben.
    Létezik tehát $K_0\subseteq K\subseteq \overline{K}$ altér $\overline{K}$-ban, 
    amelyre 
    \[
        \left( H\cap\overline{K} \right)\oplus K=\overline{K}.
    \]
    Most megmutatjuk, hogy $H$ és $K$ diszjunkt alterek:
    \[
        H\cap K=H\cap\left( K\cap \overline{K} \right)=\left( H\cap\overline{K} \right)\cap K=\left\{ 0 \right\}.
    \]
    Most azt mutatjuk meg, hogy $H+K=W$.
    Ugyanis
    \[
        W=H+\overline{K}
        =
        H+\left( H\cap\overline{K}+K \right)
        =
        \left( H+H\cap\overline{K}\right)+K 
        =
        H+K.
    \]
    Ez éppen azt jelenti, hogy $K$ direkt kiegészítője $H$-nak.
\end{proof}
\begin{proposition}[nillpotens operátorok felbontása]
    Legyen $B\in L\left( W \right)$ egy $m$-ed rendben nillpotens lineáris transzformáció.
    Ekkor minden olyan $v\in V$ vektorhoz, 
    amelyre $B^{m-1}v\neq 0$, a $\lin\left\{ v;B \right\}$ invariáns altérnek van invariáns altér 
    direkt kiegészítője.

    Formálisabban: létezik $K\subseteq W$ invariáns altér, amelyre
    \(
        \lin\left\{ v,Bv,\dots,B^{m-1}v \right\}\oplus K = W.
    \)
    \label{th:nilpfelb}
\end{proposition}
\begin{proof}
    A nillpotens transzformáció rendje szerinti teljes indukció.
    Ha $m=1$, akkor $B=0$, de a konstans zérus operátorra nézve minden altér invariáns,
    így a tétel összesen annyit állít, hogy egy nem zérus $v$ vektor generálta egy dimenziós altérnek van direkt kiegészítője.

    Tegyük fel, hogy igaz az állítás minden vektortér legfeljebb $m-1$-ed rendben nillpotens transzformációjára.
    Legyen tehát $m>1$ és $B$ egy a $W$ vektortéren értelmezett $m$-ed rendben nillpotens lineáris transzformáció.
    Rögzítsünk egy $v\in W$ elemet, amelyre $B^{m-1}v\neq 0$.
    Tekintsük az $\im B$ invariáns alteret.
    Világos, hogy $B|_{\im B}$ egy lineáris transzformáció az $\im B$ vektortéren.
    Az is világos, hogy $B|_{\im B}$ egy $m-1$-rendben nillpotens lineáris transzformáció, hiszen 
    minden $u\in\im B$ mellett $B^{m-1}u=0$.
    Azt is vegyük észre, hogy ezek szerint $v\notin\im B$.

    Alkalmazhatjuk tehát az indukciós feltevést $B|_{\im B}\in L\left( \im B \right)$ mellett a
    $Bv$ vektorra.
    Persze $B^{m-2}Bv=B^{m-1}v\neq 0$.
    Létezik tehát $K_0\subseteq\im B$ a $B$-re is invariáns altér, amelyre
    \[
        \lin\left\{ Bv,B^2v,\dots,B^{m-1}v \right\}\oplus K_0 = \im B.
    \]

    Most megmutatjuk, hogy $\lin\left\{ v;B \right\}\cap K_0=\left\{ 0 \right\}.$
    Ugyanis, ha 
    $
    x=\sum_{k=0}^{m-1}\alpha_kB^kv\in K_0\subseteq\im B
    $, akkor $\alpha_0v\in\im B$,
    ami csak úgy lehetséges, hogy $\alpha_0=0$.
    Ezek szerint $x\in\lin\left\{ Bv;B \right\}$ altérnek melynek direkt kiegészítője $K_0$.
    Ez persze csak úgy lehetséges, hogy $x=0$.

    Definiálja 
    \[
        \overline{K}=\left\{ x\in W:Bx\in K_0 \right\}.
    \]
    Mivel $K_0$ egy altér, ezért $\overline{K}$ is az.
    Mivel a $K_0$ altér $B$-invariáns, azért teljesül a $K_0\subseteq \overline{K}$ tartalmazás.

    Most megmutatjuk, hogy $\lin\left\{ v;B \right\}+\overline{K}=W$.
    Válasszunk egy $u\in W$ vektort.
    Persze $Bu\in\im B$, ezért előáll
    \[
        Bu
        =
        \sum_{k=1}^{m-1}\alpha_kB^kv +k_0
        =
        B\left( 
        \sum_{k=0}^{m-2}\alpha_{k+1}B^kv
        \right)
        + k_0
    \]
    alakban, ahol $k_0\in K_0$.
    Ebből azt látjuk, hogy 
    $
    B
    \left( 
    u-
    \sum_{k=0}^{m-2}\alpha_{k+1}B^kv
    \right)
    \in
    K_0,
    $
    ami persze $\overline{K}$ definícióját figyelembe véve azt jelenti, hogy
    \(
    u-
    \sum_{k=0}^{m-2}\alpha_{k+1}B^kv
    \in
    \overline{K}.
    \)
    Előállítottuk tehát az $u$ vektort egy $\lin\left\{ v;B \right\}$-beli és egy 
    $\overline{K}$ beli összegeként.

    Alkalmazhatjuk tehát \aref{lem:felb}.~lemmát.
    Így létezik $K_0\subseteq K\subseteq\overline{K}$ altér, melyre $\lin\left\{ v;B \right\}\oplus K=W.$
    Persze ha $u\in K\subseteq\overline{K}$, akkor $Bu\in K_0\subseteq K$, ergo $K$ egy $B$-invariáns direkt kiegészítője a
    $v$-t tartalmazó legszűkebb $B$ invariáns altérnek.
    Ezt kellett belátni. 
\end{proof}
Ahhoz, hogy megkapjuk az egész vektorteret $v$-invariáns altérként vagy ilyenek direktösszegeként, az előző tételt
kell rekurzívan alkalmaznunk.
A $W$ vektortér véges dimenziós volta garantálja, hogy a rekurzió véget ér.

Persze $B|_K$ a $K$ altér lineáris transzformációja, 
ami $m\geq n_2\geq 1$ rendben nillpotens.
Ha alkalmazzuk a fenti tételt, akkor kapjuk, 
hogy létezik $v_2\in K, v_2\neq 0$ elem és létezik $K_2\subseteq K$ a $B$-re nézve invariáns altér,
amelyre
\[
    \lin\left\{ v_2,Bv_2,\dots,B^{n_2-1}v_2 \right\}
    \oplus
    K_2=K.
\]
Itt persze $\dim K_2<\dim K$, hiszen a baloldali első invariáns altér legalább egydimenziós.
Az első két lépést összefoglalva:
\[
    \lin\left\{ v_1;B \right\}
    \oplus
    \lin\left\{ v_2;B \right\}
    \oplus
    K_2=W,
\]
Az eljárást folytatva minden lépésben legalább eggyel csökken a kiegészítő invariáns altér dimenziója.
Végül a vektortér előáll néhány, mondjuk $r$ darab $B$-re invariáns altér direktösszegeként:
\[
    \lin\left\{ v_1;B \right\}
    \oplus
    \lin\left\{ v_2;B \right\}
    \oplus
    \ldots
    \lin\left\{ v_r;B \right\}
    =W.
\]
\begin{proposition}[nillpotens transzformáció redukálása]
    Legyen $B\in L\left( W \right)$ egy $m$-ed rendben nilpotens transzformáció.
    Ekkor léteznek olyan $v_1,v_2\dots,v_r\in W$ vektorok és 
    léteznek olyan $m=n_1\geq n_2\geq \dots\geq n_r\geq 1$ pozitív egészek,
    amelyekre
    \[
        \lin\left\{ v_1,Bv_1,\dots,B^{n_1-1}v_1 \right\}
        \oplus
        \lin\left\{ v_2,Bv_2,\dots,B^{n_2-1}v_2 \right\}
        \oplus
        \ldots
        \oplus
        \lin\left\{ v_r,Bv_r,\dots,B^{n_r-1}v_r \right\}
        =
        W.
    \]
    Emiatt az egyes invariáns alterek bázisainak egyesítésével kapott vektorrendszer bázisa $W$-nek:
    \[
        \left\{ B^{n_1-1}v_1,\dots,Bv_1,v_1 \right\}
        \cup
        \left\{ B^{n_2-1}v_2,\dots,Bv_2,v_2 \right\}
        \cup
        \ldots
        \cup
        \left\{ B^{n_r-1}v_r,\dots,Bv_r,v_r \right\}
        \qedhere
    \]
\end{proposition}
\section{Egyértelműség}
Azt mutatjuk meg, hogy a normálalakban $r=\nu\left( B \right)$,
és az $n_1,n_2,\dots,n_r$ számok is a $B$ nilpotens transzformáció altal egyertelműen meghatározottak.
Ez azt jelenti, hogy minden nillpotens transzformációnak csak egyetlen normálalakja van.
\begin{lemma}
    Legyen $A\in L\left( V \right)$ lineáris transzformáció, és $v\in V$ olyan vektor, amelyre
    $A^mv=0$, de $A^{m-1}v\neq 0$.
    Ekkor 
    \begin{enumerate}
        \item 
        $\left\{ v,Av,\dots,A^{m-1}v \right\}$ lineárisan független,
        így $\lin\left\{ v;A \right\}=\lin\left\{ v,Av,\dots,A^{m-1}v \right\}$;
        \item
        Minden $0\leq l\leq m$ mellett
        \begin{math}
            \nu\left( A^l|\lin\left\{ v;A \right\} \right)=l
        \end{math} 
        és
        \begin{math}
            \rho\left( A^l|\lin\left\{ v;A \right\} \right)=m-l.
        \end{math}\qedhere
    \end{enumerate}
\end{lemma}
\begin{proof}
    Ha $l=m$, akkor mindkét állítás triviálisan teljesül. Legyen tehát $0\leq l<m$.
    Az $A^l$ transzformáció az $\left\{ A^{m-l}v,\dots,A^{m-1}v \right\}$ 
    lineárisan független vektorrendszert nullára viszi,
    így $l\leq\nu\left( A^l|\lin\left\{ v;A \right\} \right)$.
    A maradékot, a 
    $\left\{ v,\dots,A^{m-l-1}v \right\}$ vektorrendszert pedig az 
    $\left\{ A^lv,\dots,A^{m-1}v \right\}$ lineárisan független rendszerre képezi.
    Így
    $m-l\leq \rho\left( A^l |\lin\left\{ v;A \right\}\right)$,
    amiből
    \[
        l\leq\nu\left( A^l|\lin\left\{ v;A \right\} \right)=m-\rho\left( A^l|\lin\left\{ v;A \right\} \right)\leq l.\qedhere
    \]
\end{proof}
\begin{lemma}
    Legyen $A\in L\left( V \right)$ és tegyük fel, hogy a $V$ vektortér előáll a $K_1,K_2$ invariáns alterei
    direktösszegeként, azaz 
    \(
        V=K_1\oplus K_2.
    \)
    Ekkor 
    \begin{math}
        \rho\left( A \right)=\rho\left( A|K_1 \right)+\rho\left( A|K_2 \right)
    \end{math}
    és
    \begin{math}
        \nu\left( A \right)=\nu\left( A|K_1 \right)+\nu\left( A|K_2 \right).
    \end{math}
    \qedhere
\end{lemma}
\begin{proof}
    Világos, hogy $A\left( V \right)=A\left( K_1 \right)+A\left( K_2 \right)$, és az
    invariancia szerint $A\left( K_1 \right)\cap A\left( K_2 \right)\subseteq K_1\cap K_2=\left\{ 0 \right\}$.
    Így persze
    \(
        A\left( V \right)=A\left( K_1 \right)\oplus A\left( K_2 \right),
    \)
    és
    \[
       \rho\left( A \right)=
       \dim\left( A\left( V \right) \right)
       =
       \dim A\left( K_1 \right)+
       \dim A\left( K_2 \right)
       =
       \rho\left( A|K_1 \right)+\rho\left( A|K_2 \right).
    \]
    Ebből már
    \[
        \nu\left( A \right)
        =
        \dim\left( V \right)-\rho\left( A \right)
        =\dim\left( K_1 \right)+\dim\left( K_2 \right)-\rho\left( A|K_1 \right)-\rho\left( A|K_2 \right)
        =
        \nu\left( A|K_1 \right)+\nu\left( A|K_2 \right)
    \]
    könnyen adódik.
\end{proof}

\begin{proposition}[A nillpotens felbontás egyértelműsége]
    Legyen $A\in L\left( V \right)$ egy lineáris transzformáció.
    Tegyük fel, hogy valamely $\left\{ v_1,\dots,v_r \right\}$ vektorrendszerre és valamilyen 
    $m_1\geq m_2\geq \dots\geq m_r$ pozitív számokra
    $A^{m_k-1}v_k\neq 0$, de $A^{m_k}v_k=0$ fennáll minden $k=1,\dots,r$, továbbá
    \[
        V=\lin\left\{ v_1;A \right\}\oplus \dots \oplus\lin\left\{ v_r;A \right\}.
    \]
    Tegyük fel még azt is, 
    hogy valamely másik $\left\{ w_1,\dots,w_s \right\}$ vektorrendszerre és valamely más
    $n_1\geq n_2\geq \dots\geq n_s$ pozitív számokra
    $A^{n_k-1}v_k\neq 0$, de $A^{n_k}v_k=0$ fennáll minden $k=1,\dots,s$ mellett, és
    \[
        V=\lin\left\{ w_1;A \right\}\oplus \dots \oplus\lin\left\{ w_s;A \right\}.
    \]
    Ekkor
    \begin{enumerate}
        \item $A$ nillpotens lineáris transzformációja $V$-nek;
        \item Ha $m$ jelöli a nillpotencia rendjét, akkor
            \(
                m_1=m=n_1
            \);
        \item A transzformáció $\nu\left( A \right)$ defektusára
            \(
                r=\nu\left( A \right)=s
            \);
        \item Valamennyi $k=1,\dots,r$ esetén
            \(
                m_k=n_k
            \).\qedhere
    \end{enumerate}
\end{proposition}
\begin{proof}
    Az első két állítás nyilvánvaló, mivel a rendezettség szerint 
    $A^{m_1}|_{\lin\left\{ v_k;A \right\}}=0$,
    minden $k=1,\dots,r$ mellett

    A harmadik állításhoz:
    \[
        \nu\left( A \right)=\sum_{k=1}^r\nu\left( A|\lin\left\{ v_k;A \right\} \right)=
        \sum_{k=1}^r1=r.
    \]
Ugyanígy a másik direktösszeg felbontásból kapjuk, hogy $\nu\left( A \right)=s$.

A negyedik állítás.
Tegyük fel -- indirekt --, hogy $m_k=n_k$ nem teljesül minden $k=1,\dots,r$ számra.
Legyen $1<t\leq r$ az a legkisebb szám, amelyre $m_t\neq n_t$.
Ezek szerint $k=1,\dots,t-1$ mellett $m_k=n_k$, de $m_t\neq n_t$.
Feltehető, hogy $m_t>n_t$. 
Ekkor tehát
\[
    m_1=n_1\geq m_2=n_2\geq\dots\geq m_{t-1}=n_{t-1}\geq m_t>n_t.
\]
Ekkor az első direktösszeg felbontásban a $t$-nél magasabb indexű tagokat elhagyva
\[
    \rho\left( A^{n_t} \right)
    \geq
    \left( \sum_{k=1}^{t-1}\rho\left( A^{n_t}|\lin\left\{ v_k;A \right\} \right) \right)
    +\rho\left( A^{n_t}|\lin\left\{ v_t;A \right\} \right)
    =
    \left( \sum_{k=1}^{t-1}(m_k-n_t) \right)+m_t-n_t.
\]
Hasonlóan, a második direktösszeg felbontásban a $t$-edik, és a $t$-nél magasabb indexű rangok zérók, így
\[
    \rho\left( A^{n_t} \right)
    =
    \sum_{k=1}^{t-1}\rho\left( A^{n_t}|\lin\left\{ w_k;A \right\} \right)
    =
    \sum_{k=1}^{t-1}(n_k-n_t)
    =
    \sum_{k=1}^{t-1}(m_k-n_t),
\]
ami ellentmond $m_t>n_t$ feltételnek.
\end{proof}
Az alábbiakban a nillpotens felbontási tételt annak egyértelműségével együtt foglaljuk össze.
\begin{proposition}[nillpotens transzformáció normálakja]\label{pr:nillpnormal}
    Legyen $B\in L\left( W \right)$ egy $m$-ed rendben nilpotens transzformáció.
    Jelölje $r=\nu\left( B \right)$ a $B$ defektusát.
    Ekkor léteznek olyan $v_1,v_2\dots,v_r\in W$ vektorok és 
    létezik pozitív egészek egyetlen olyan $m=n_1\geq n_2\geq \dots\geq n_r\geq 1$ véges sorozata,
    amelyekre
    \[
        \lin\left\{ v_1,Bv_1,\dots,B^{n_1-1}v_1 \right\}
        \oplus
        \lin\left\{ v_2,Bv_2,\dots,B^{n_2-1}v_2 \right\}
        \oplus
        \ldots
        \oplus
        \lin\left\{ v_r,Bv_r,\dots,B^{n_r-1}v_r \right\}
        =
        W.
    \]
    Emiatt a
    \[
        \left\{ B^{n_1-1}v_1,\dots,Bv_1,v_1 \right\}
        \cup
        \left\{ B^{n_2-1}v_2,\dots,Bv_2,v_2 \right\}
        \cup
        \ldots
        \cup
        \left\{ B^{n_r-1}v_r,\dots,Bv_r,v_r \right\}
        \tag{\dag}
    \]
    vektorrendszer bázisa $W$-nek.

    Ha ebben a bázisban felírjuk $B$ mátrixát, 
    akkor $r$ darab diagonálisan elhelyezkedő részmátrixból álló mátrixot kapunk. 
    Az első $n_1\times n_1$ méretű, 
    a második $n_2\times n_2$ méretű, \dots, az utolsó $n_r\times n_r$ méretű.
    Minden ilyen blokkban csak a (felső) mellék diagonális elemei nem nullák.
    A mellék diagonális elemei 1-esek.
    Minden más elem zérus.
    Ezt a mátrixot nevezzük a $B$ nillpotens transzformáció \emph{normálalakjának}.
    \index{nillpotens transzformáció normálalakja}
\end{proposition}
Adott $1\leq j\leq r$ mellett tehát, $\mathbf{B}_j\in \mathbb{F}^{n_j\times n_j}$
a fenti $j$-edik invariáns altérre leszorított $B$ leképezésnek a 
\begin{math}
    \left\{ B^{n_j-1}v_j,\dots,Bv_j,v_j \right\}
\end{math}
bázison felírt mátrixa:
\[
    \mathbf{B}_j
    =
    \begin{pmatrix}
        0&1&0&\dots& 0 &0&0\\
        0&0&1&\dots& 0 &0&0\\
        \vdots&\vdots&\ddots&\ddots&\vdots&\vdots&\vdots\\
        0&0&\dots&0& 1 &0&0\\
        0&0&\dots&0& 0 &1&0\\
        0&0&\dots&\dots& 0 &0&1\\
        0&0&\dots&\dots& 0 &0&0
    \end{pmatrix}.
\]
Így az egész $V$ vektortéren értelmezett $B$ lineáris transzformáció normálakja 
--
azaz a $B$-nek (\dag) bázisban felírt mátrixa 
-- 
a fenti tipusú mátrixok diagonális alakú elrendezésével adódik:
\begin{displaymath}
    [B]=
    \begin{pmatrix}
    \begin{matrix}
        0&1&0&\dots& 0 &0&0\\
        0&0&1&\dots& 0 &0&0\\
        \vdots&\vdots&\ddots&\ddots&\vdots&\vdots&\vdots\\
        0&0&\dots&0& 1 &0&0\\
        0&0&\dots&0& 0 &1&0\\
        0&0&\dots&\dots& 0 &0&1\\
        0&0&\dots&\dots& 0 &0&0
    \end{matrix}&&&\\
    &
    \begin{matrix}
        0&1&0&\dots& 0 &0&0\\
        0&0&1&\dots& 0 &0&0\\
        \vdots&\vdots&\ddots&\ddots&\vdots&\vdots&\vdots\\
        0&0&\dots&0& 1 &0&0\\
        0&0&\dots&0& 0 &1&0\\
        0&0&\dots&\dots& 0 &0&1\\
        0&0&\dots&\dots& 0 &0&0
    \end{matrix}
    &&\\
    &&\ddots&\\
    &&&
    \begin{matrix}
        0&1&0&\dots& 0 &0&0\\
        0&0&1&\dots& 0 &0&0\\
        \vdots&\vdots&\ddots&\ddots&\vdots&\vdots&\vdots\\
        0&0&\dots&0& 1 &0&0\\
        0&0&\dots&0& 0 &1&0\\
        0&0&\dots&\dots& 0 &0&1\\
        0&0&\dots&\dots& 0 &0&0
    \end{matrix}
    \end{pmatrix}.
\end{displaymath}
Hangsúlyozni szeretném, hogy az egyértelműség szerint a $\mathbf{B}_j$ blokkok száma, és azok mérete is csak
a nillpotens transzformációtól függ.
Több bázis is lehetséges, amelyben a nillpotens transzformáció normálalakú, 
de nem csak hogy minden normálalakban azonos számú blokk van ($B$ defektusa), 
de a blokkok mérete is azonos.
A nillpotens felbontásban szereplő direktösszeadandó alterek páronként izomorfak egymással.
\section{Illusztrációk}
\subsection{Egyetlen invariáns altér}
Írjuk fel a $W$ vektortéren értelmezett lineáris transzformáció normál alakját,
ahol az $\left\{ u_1,u_2,u_3,u_4 \right\}$ bázisban
\[
    B\left( \alpha u_1+\beta u_2+\gamma u_3+\delta u_4 \right)=
    -\left( \gamma + \delta \right)u_1
    +\gamma u_2
    -\left( \alpha+\beta+\gamma \right)u_3
    +\left( \alpha+\beta+\gamma+\delta \right)u_4.
\]
A $B$ mátrixa a fent rögzített bázisban:
\[
    \begin{pmatrix}
    0 & 0 & -1&-1\\
    0 & 0 & 1 & 0\\
   -1 & -1& -1& 0\\
    1 & 1 & 1 & 1
    \end{pmatrix}.
\]
Mivel két azonos oszlop is van, ezért a defektus legalább egy.
Számoljuk ki a minimálpolinomot!
Az $u_1$ bázis elemhez
\(
    \begin{array}{c|cccccc}
           & u_1 & Bu_1 & B^2u_1 & B^3u_1 & B^4u_1\\
        \hline
        u_1&   1 &    0 &      0 &  -1&   0\\
        u_2&   0 &    0 &     -1 &   1&   0\\
        u_3&   0 &   -1 &      1 &   0&   0\\
        u_4&   0 &    1 &      0 &   0&   0
    \end{array}.
\)
Mivel az első négy oszlop lineárisan független,
ezért az $u_1$-hez tartozó kis minimálpolinom $p_1=t^4.$
Mivel tudjuk, hogy a minimálpolinom legfeljebb 4-ed fokú, és 
$p_1$ osztója, ezért csak $m\left( t \right)=t^4$ lehetséges,
ezért $B$ transzformáció 4-ed rendben nillpotens.
Éppen most számoltuk ki, hogy $B^3u_1\neq 0$, ezért 
az $u_1$-et tartalmazó legszűkebb $B$ invariáns altér az egész $W$,
tehát
\[
    \lin\left\{ u_1,Bu_1,B^2u_1,B^3u_1 \right\}=W
\]
és a $\left\{ B^3u_1,Bu_1,Bu_1,u_1 \right\}$ bázisban $B$ mátrixa
\[
    \begin{pmatrix}
        0&1&0&0\\
        0&0&1&0\\
        0&0&0&1\\
        0&0&0&0
    \end{pmatrix}
\]
alakú.
Emlékezzünk, hogy az új bázisra való áttérés mátrixa egyszerűen az új bázis
elemeiből mint oszlopokból alkotott mátrix, ami azt jelenti, hogy
\[
    \begin{pmatrix}
    -1&0&0&1\\
    1&-1&0&0\\
    0&1&-1&0\\
    0&0&1&0
    \end{pmatrix}^{-1}
    \begin{pmatrix}
    0 & 0 & -1&-1\\
    0 & 0 & 1 & 0\\
   -1 & -1& -1& 0\\
    1 & 1 & 1 & 1
    \end{pmatrix}
    \begin{pmatrix}
    -1&0&0&1\\
    1&-1&0&0\\
    0&1&-1&0\\
    0&0&1&0
    \end{pmatrix}.
\]
\subsection{Két invariáns altér (3- és 1-dimenziós)}
Most a játék kedvéért, keressük meg a fenti  felbontást és bázist, egy konkrét esetben.
Legyen $W$ egy négy dimenziós vektortér az $\left\{ u_1,u_2,u_3,u_4 \right\}$ rögzült bázissal.
A $B\in L\left( W \right)$ lineáris transzformáció definíciója a báziselemek segítségével:
\[
    B\left( \alpha u_1+\beta u_2 +\gamma u_3+\delta u_4 \right)
    =
    \left( \gamma-\delta \right)u_1+
    \gamma u_2+
    \delta u_3.
\]

Első lépésként keressük meg a minimálpolinomot.
A transzformáció mátrixa
\[
    B=
    \begin{pmatrix}
        0&0&1&-1\\
        0&0&1&0\\
        0&0&0&1\\
        0&0&0&0
    \end{pmatrix}.
\]
Az $u_1$-minimálpolinom $p_1\left( t \right)=t$, 
az $u_2$-minimálpolinom $p_2\left( t \right)=t$.
A harmadik kis minimálpolinomhoz írjuk fel $u_3$ hatványait:
\(
    \begin{array}{c|cccc}
           & u_3 & Bu_3 & B^2u_3\\
        \hline
        u_1&   0 &    1 &      0\\
        u_2&   0 &    1 &      0\\
        u_3&   1 &    0 &      0\\
        u_4&   0 &    0 &      0
    \end{array}.
\)
Persze az első két oszlop lineárisan független, így $p_3\left( t \right)=t^2$.
Hasonlóan az $u_4$ vektor $B$ hatványait felírva:
\(
    \begin{array}{c|ccccc}
           & u_4 & Bu_4 & B^2u_4 & B^3u_4\\
        \hline
        u_1&   0 &   -1 &      1 &      0\\
        u_2&   0 &    0 &      1 &      0\\
        u_3&   0 &    1 &      0 &      0\\
        u_4&   1 &    0 &      0 &      0
    \end{array}.
\)
Az első három oszlop ránézésre független, emiatt a minimálpoliniom $p_4\left( t \right)=t^3$.
\\
Emlékszünk, hogy a minimálpolinom a $p_1,p_2,p_3,p_4$ legkisebb közös többszöröse,
ergo $m\left( t \right)=t^3$, és $B$ egy harmadrendben nilpotens transzformáció.

Írjuk fel a fenti tételben szereplő invariáns direktfelbontást.
Mivel a $B$ rangja ránézésre 2, így a magtere 2 dimenziós, ergo két invariáns altér direktösszege $W$, amiből
az egyik 3 dimenziós, így a másik csak egy dimenziós lehet, ezért azt csak a magtér egyik eleme generálhatja!
A három dimenziós altér lehet például a $\lin\left\{ u_4;B \right\}$, 
hiszen éppen az imént láttuk, hogy $B^2u_4\neq 0$.
A fenti \ref{th:nilpfelb} Tétel éppen azt állítja, hogy ennek az altérnek van olyan $K$ invariáns altér direkt kiegészítője, 
aminek van olyan bázisa, amely annyi magtérbeli elemet tartalmaz, ami a $B|_K$ nilpotens operátor rendje, tehát legalább 1.
Így most nyilvánvaló, hogy az
egydimenziós invariáns alteret generálhatja bármely olyan eleme a $\ker A$ magtérnek,
amely lineárisan független $B^2u_4$-től.
Például $u_1$.\footnote{Vagy bármi, ami $\alpha u_1+\beta u_2$ alakú, ahol $\alpha\neq\beta$.}
Ilyen módon
\[
    \lin\left\{ u_4,Bu_4,B^2u_4 \right\}\oplus
    \lin\left\{ u_1 \right\}
    =
    W,
\]
és az 
\(
    \left\{ 
    B^2u_4,Bu_4,u_4,u_1
    \right\}
\)
vektorok olyan bázisát adják a térnek, melyben $B$ mátrixa
\[
    \begin{pmatrix}
        0&1&0&0\\
        0&0&1&0\\
        0&0&0&0\\
        0&0&0&0
    \end{pmatrix}
\]
alakú.
\section{A nillpotens felbontási tétel nélkül?}
Legyen $B\in L\left( V \right)$ egy $m$-edrendben nillpotens operátor.
Világos, hogy $\nu\left( B \right)\geq 1$, 
hiszen egyébként $B$ valamennyi hatványa is reguláris maradna.

Válasszuk ki a $\ker A$ egy 
$\left\{ e_1,\dots,e_r \right\}$ bázisát, 
ahol a rövidség kedvéért $r=\nu\left( A \right)$.
Most minden $i=1,\dots,r$ mellett legyen $m_i$ a legnagyobb olyan $k$ egész, amelyre a
$B^{k-1}x=e_i$ egyenletnek még van megoldása.
Világos, hogy $1\leq m_i\leq m$.
Az általánosság elvesztése nélkül feltehető, hogy 
$m\geq m_1\geq m_2\geq \dots \geq m_r$, hiszen a bázis elemeket az $m_i$ számok
csökkenő sorrendjében átindexelhetjük.
Jelölje $v_i\in V$ egy tetszőleges megoldását $B^{m_i-1}x=e_i$-nek.
Világos tehát, hogy minden $i=1,\dots,r$ mellett
\[
    B^{m_i-1}v_i=e_i \quad\text{ és }\quad B^{m_i}v_i=0
\]
Írjuk egy táblázat legalsó sorába a $\ker B$ kiválaszott bázis elemeit,
majd föléjük a megfelelő csökkenő $B$ hatványokat.%
\footnote{Tipográfiai probléma e táblázat áttekinthető leírása.
A lényeg, hogy a legalsó sorban lévő vektorok vannak csak biztosan egy sorban. A 2. oszlopnak $m_2\leq m_1$
eleme van, tehát a $v_1$ akkor és csak akkor esik egy sorba $v_2$ vel, ha $m_1=m_2$. Egyébként $v_2$ lejjebb van mint $v_1$. Képzeljük úgy a táblázatot, mint monoton fogyó elemszámú oszlopok, alulra zárt összeségét, úgy hogy a legnagyobbal kezdem, stb.}
    \[
        \begin{matrix}
            v_1         & \vdots       & \vdots       & \dots  & \vdots\\
            Bv_1        & v_2          & \vdots       & \dots  & \vdots\\
           B^2v_1       & Bv_2         & v_3          & \dots  & v_r\\
           \vdots       & \vdots       & \vdots       & \ddots & \vdots \\
           B^{m_1-k}v_1 & B^{m_2-k}v_2 & B^{m_3-k}v_3 & \dots  & B^{m_r-k}v_r\\
           \vdots       & \vdots       & \vdots       & \ddots & \vdots\\
           B^{m_1-1}v_1 & B^{m_2-1}v_2 & B^{m_3-1}v_3 & \dots  & B^{m_r-1}v_r
        \end{matrix}
    \]
Felmerül, hogy a fenti vektorrendszer bázis, evvel a nillpotens felbontási tételt megkerülve 
kapnánk a nillpotens normálalak igazolását.

Sajnos nem feltétlen bázis a fenti vektorrendszer.
A lineárisan függetlenség a korábbi technikával igazolható -- tegyük ezt meg! --, de a rendszer nem mindig
generátorrendszer.

Tekintsük például a
\[
    B=
    \begin{pmatrix}
        0&1&0&0\\
        0&0&0&0\\
        0&0&0&0\\
        0&0&0&0
    \end{pmatrix}
\]
mátrixát. 
Jelölje most $\left\{ e_1,e_2,e_3,e_4 \right\}$ a térnek azt  a bázisát, amelyben 
a fenti mátrixot felírtuk.
Világos, hogy $\left\{ e_4,e_3+e_4,e_1+e_3+e_4 \right\}$ vektorrendszer a $\ker B$ egy olyan bázisa,
amelynek egyik eleme sem esik $B$ képterébe, hiszen egyik elem sem az $e_1$ skalárszorosa.
Ilyen módon a fent konstruált vektorrendszer csak három elemű lesz, ergo nem bázisa a négy 
dimenziós térnek.

A nillpontens felbontási tétel éppen azt mondja, hogy a $\ker B$-nek van olyan alkalmasan megválasztott bázisa, amelyre a fenti konstrukcióban kapott vektorrendszer a térnek generátorrendszere, ergo bázisa.
Persze amikor konkrétan a normálalakot konstruáljuk, akkor elegendő olyan bázisát keresni $\ker B$-nek, 
amelyből kiindulva a fenti vektorrendszer elemeinek száma a tér dimneziójával egyezik meg.
Az így kapott rendszer persze bázis lesz.
\chapter{Jordan--normálalak}
\section{Egy gyöktényezős minimál polinom}
\begin{definition}[Jordan-normálalak]
Tegyük fel, hogy az $A\in L\left( V \right)$ lineáris transzformáció
minimál polinomja 
    \[
    \left( t-\lambda \right)^m
    \]
    alakú, valamely $\lambda\in\mathbb{F}$ és $m$ pozitív egész mellett.
    Világos, hogy $B=A-\lambda I$ éppen $m$-edrendben nillpotens.
    Alkalmazaható tehát $B$-re \aref{pr:nillpnormal}.~állítás.
    Jelölje tehát $r=\nu\left( B \right)$ a $B$ defektusát.
    Így léteznek olyan $v_1,v_2\dots,v_r\in W$ vektorok és 
    létezik pozitív egészek egyetlen olyan $m=n_1\geq n_2\geq \dots\geq n_r\geq 1$ véges sorozata,
    amelyekre a
    \[
        \left\{ B^{n_1-1}v_1,\dots,Bv_1,v_1 \right\}
        \cup
        \left\{ B^{n_2-1}v_2,\dots,Bv_2,v_2 \right\}
        \cup
        \ldots
        \cup
        \left\{ B^{n_r-1}v_r,\dots,Bv_r,v_r \right\}
        \tag{\dag}
    \]
    vektorrendszer bázisa $W$-nek. 
    
    A $B$ transzformációnak ebben a bázisban felírt mátrixa olyan, hogy
    a szuper diagonálisán kívük minden elem zérus,
    a szuper diagonálisban $n_1-1$ db $1$-es, aztán egy zérus, majd $n_2-1$ db $1$-es aztán egy zérus, stb.
    Adjuk a $B=A-\lambda I$ mátrixához a $\lambda I$ mátrixot diagonális mátrixot!
    Ekkor kapjuk az $A$-nak a $(\dag)$ bázisban felírt mátrixát.
    Ezt a mátrixot nevezzük az $A$ transzformáció \emph{Jordan--normálalakjának}\index{Jordan--normálalak}.
\end{definition}
Adott $1\leq j\leq r$ mellett, 
a $j$-edik invariáns altérre leszorított $A$ leképezésnek a 
\begin{math}
    \left\{ B^{n_j-1}v_j,\dots,Bv_j,v_j \right\}
\end{math}
bázison felírt mátrixát \emph{Jordan--blokknak}\index{Jordan--blokk} mondjuk.
\[
    \begin{pmatrix}
        \lambda&1&0&\dots& 0 &0&0\\
        0&\lambda&1&\dots& 0 &0&0\\
        \vdots&\vdots&\ddots&\ddots&\vdots&\vdots&\vdots\\
        0&0&\dots&\lambda& 1 &0&0\\
        0&0&\dots&0& \lambda&1&0\\
        0&0&\dots&\dots& 0 &\lambda&1\\
        0&0&\dots&\dots& 0 &0&\lambda
    \end{pmatrix}.
\]
Így az egész $V$ vektortéren értelmezett $A$ lineáris transzformáció normálakja 
--
azaz a $A$-nak (\dag) bázisban felírt mátrixa 
-- 
a fenti tipusú Jordan--blokkok, diagonális alakú elrendezésével adódik:
\begin{displaymath}
    [A]=
    \begin{pmatrix}
    \begin{matrix}
        \lambda&1&0&\dots& 0 &0&0\\
        0&\lambda&1&\dots& 0 &0&0\\
        \vdots&\vdots&\ddots&\ddots&\vdots&\vdots&\vdots\\
        0&0&\dots&\lambda& 1 &0&0\\
        0&0&\dots&0& \lambda&1&0\\
        0&0&\dots&\dots& 0 &\lambda&1\\
        0&0&\dots&\dots& 0 &0&\lambda
    \end{matrix}&&&\\
    &\!\!\!
    \begin{matrix}
        \lambda&1&0&\dots& 0 &0&0\\
        0&\lambda&1&\dots& 0 &0&0\\
        \vdots&\vdots&\ddots&\ddots&\vdots&\vdots&\vdots\\
        0&0&\dots&\lambda& 1 &0&0\\
        0&0&\dots&0& \lambda&1&0\\
        0&0&\dots&\dots& 0 &\lambda&1\\
        0&0&\dots&\dots& 0 &0&\lambda
    \end{matrix}
    &&\\
    &&\ddots&\\
    &&&
    \begin{matrix}
        \lambda&1&0&\dots& 0 &0&0\\
        0&\lambda&1&\dots& 0 &0&0\\
        \vdots&\vdots&\ddots&\ddots&\vdots&\vdots&\vdots\\
        0&0&\dots&\lambda& 1 &0&0\\
        0&0&\dots&0& \lambda&1&0\\
        0&0&\dots&\dots& 0 &\lambda&1\\
        0&0&\dots&\dots& 0 &0&\lambda
    \end{matrix}
    \end{pmatrix}
\end{displaymath}
Hangsúlyozni szeretném, hogy az egyértelműség szerint a Jordan blokkok száma, és azok mérete is csak
az $A$ transzformációtól függ.
Több bázis is lehetséges, amelyben a transzformáció Jordan--normálalakú, 
de nem csak hogy minden normálalakban azonos számú Jordan--blokk van ($A-\lambda I$ defektusa), 
de a blokkok mérete is azonos.
A felbontásban szereplő direktösszeadandó alterek páronként izomorfak egymással.

%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix             %
%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\renewcommand{\appendixpagename}{Függelékek}\renewcommand{\appendixtocname}{\appendixpagename}
\appendixpage




\chapter{A komplex számokról}
Az algebra alaptétele, és a komplex számtest egyértelműsége. Elsősorban \parencite{MR1415833} és \parencite{10.2307/3647746} alapján
\section{Lineáris algebrai megközelítés}
Ha $\left\{ e_1,\dots,e_n \right\}$ bázisa egy $\mathbb{C}$ feletti $V$ vektortérnek
és $m\left( t \right)\in\mathbb{C}\left[ t \right]$ egy pontosan $n$-ed fokú, 
normált polinom,
például
$m\left( t \right)=\alpha_0+\alpha_1 t+\dots+\alpha_{n-1}t^{n-1}+t^{n}$,
akkor definiálja az $A\in L\left( V \right)$ lineáris transzformációt
\[
    A\left( e_k \right)=
    \begin{cases}
        e_{k+1}&, \text{ ha } 1\leq k \leq n-1\\
        -\sum_{j=0}^{n-1}\alpha_j e_{j+1}&, \text{ ha } k=n.
    \end{cases}
\]
Világos, hogy $A^je_1=e_{j+1}$ tetszőleges $0\leq j\leq n-1$ mellett.
Ebből azonnal következik, hogy 
\begin{enumerate}
    \item 
    $\left\{ e_1, Ae_1,A^2e_1,\dots,A^{n-1}e_1\right\}$ rendszer lineárisan független,
    \item
    $\left\{ e_1, Ae_1,A^2e_1,\dots,A^{n-1}e_1,A^ne_1\right\}$ rendszer már lineárisan összefüggő,
    hiszen
    $A^ne_1=AA^{n-1}e_1=Ae_n=-\sum_{j=0}^{n-1}\alpha_j A^je_1$.
\end{enumerate}
Látjuk tehát, hogy $m$ a legalacsonyabb fokú normált polinom, melyre
$m\left( A \right)e_1=0$,
így az $e_1$ vektorhoz tartozó kis minimálpolinom éppen $m$.
Mivel a tér $n$-dimenziós, ezért $m$ egyben minimálpolinomja is $A$-nak.

Persze egy lineáris transzformációnak egy szám pontosan akkor sajátértéke,
ha az a minimálpolinomjának gyöke,
emiatt 
az algebra alaptétele a következőképpen is fogalmazható:
\begin{center}
\emph{Minden komplex vektortér feletti lineáris transzformációnak van sajátvektora.}
\end{center}

Jelölje $\rho\left( A \right)$ az $A$ transzformáció rangját, 
azaz a képtér dimenzióját,
és $\nu\left( A \right)$ a defektust, azaz a magtér dimenzióját.
Jól ismert, hogy ha $A\in L\left( V \right)$ egy lineáris transzformáció
és $p\left( t \right)\in\mathbb{F}\left[ t \right]$ egy polinom, akkor 
$\ker p\left( A \right)$ és
$\im p\left( A \right)$ is $A$-ra invariáns alterek.
Két lineáris trafóról azt mondjuk, hogy \emph{kommutálnak},\index{kommutál}
ha $A_1A_2=A_2A_1.$
Könnyen látható, hogy ha $A_1$ és $A_2$ kommutálnak, akkor tetszőleges két $p,q$
polinom mellett $p\left( A_1 \right)$ és $q\left( A_2 \right)$ is kommutálnak.\index{kommutál}
\begin{lemma}
    Legyenek az $A_1,A_2\in L\left( V \right)$ kommutáló lineáris transzformációk,
    valamint $p,q\in\mathbb{F}\left[ t \right]$ polinomok.
    Ekkor $\ker p\left( A_1 \right)$ és
    $\im p\left( A_1 \right)$ is invariáns alterek $q\left( A_2 \right)$-re.
\end{lemma}
\begin{proof}
    Elég megmutatni, hogy $\ker\left( A_1 \right)$-re és
    $\im\left( A_1 \right)$-re invariáns $A_2$, hiszen $p\left( A_1 \right)$ 
    és $q\left( A_2 \right)$ is kommutálnak.
    No de, az
    \(
        A_1 A_2 x=A_2A_1x=A_20=0
    \)
    szerint a magra,
    és $u=A_1v$ jelöléssel az
    \(
        A_2u=A_2 A_1v =A_1A_2v 
    \)
    azonosságból a képre vonatkozó állítás következik.
\end{proof}
\begin{lemma}
    \label{le:indukcio}
    Legyen a $d>1$ pozitív egész rögzítve. 
    Tegyük fel, hogy az $\mathbb{F}$ test rendelkezik avval a tulajdonsággal,
    hogy minden az $\mathbb{F}$ feletti $d$-vel nem osztható dimenziós vektortér tetszőleges
    lineáris trafójának van sajátvektora.
    Ekkor minden olyan $\mathbb{F}$ feletti vektortérre, 
    amelynek dimenziója $d$-vel nem osztható igaz, 
    hogy bármely két kommutáló lineáris transzformációjának van közös sajátvektora is.\index{kommutál}
\end{lemma}
\begin{proof}
    A tér dimenziója szerinti indukció.
    Egy egy dimenziós tér minden nem nulla vektora sajátvektora tetszőleges lineáris transzformációjának,
    így persze bármely két egy dimenziós téren értelmezett lineáris transzformációnak is van közös
    sajátvektora.
    Tegyük fel, 
    hogy az állítás igaz minden legfeljebb $n$-dimenziós vektortérre
    és tekintsünk egy olyan $\mathbb{F}$ feletti vektorteret, 
    amely éppen $n$-dimenziós és $d$ nem osztója $n$-nek.
    Jelölje $A_1$ és $A_2$ a szóban forgó két lineáris transzformációt.
    A feltétel szerint mondjuk $A_1$-nek van sajátvektora, 
    így valamely $\mu\in\mathbb{F}$ mellett 
    $\nu\left( A_1-\mu I \right)>0.$
    Ha az 
    $\nu\left( A_1-\mu I \right)=n$, akkor a tér minden vektora sajátvektora $A_1$-nek,
    így mivel $A_2$-nek is van sajátvektora, ezért ez közös sajátvektoruk is.
    Ha 
    $\nu\left( A_1-\mu I \right)<n$, akkor 
    $\nu\left( A_1-\mu I \right)+\rho\left( A_1-\mu I \right)=n$ miatt az
    $K=\ker\left( A_1-\mu I \right)$ és a 
    $L=\im\left( A_1-\mu I \right)$ 
    valódi alterek dimenziójának egyike nem osztható $d$-vel.
    No de $A_2$ és $A_1$ invariáns $K$-ra is és $L$-re is
    az előző lemma miatt, 
    így alkalmazhatjuk az indukciós feltevést $K$ és $L$ közül 
    a $d$-vel nem osztható dimenziós altérre,
    amely garantálja az $A_1$ és $A_2$ közös sajátvektorát.
\end{proof}

Mivel a karakterisztikus polinom gyökei a sajátértékek, és mivel egy $n$-dimenziós
téren értelmezett lineáris transzformációnak pontosan $n$-edfokú a karakterisztikus polinomja,
ezért a Bolzano-tétel szerint egy $\mathbb{R}$ feletti páratlan dimenziós vektortér lineáris transzformációjának van sajátvektora. 
A fenti lemma tehát kommutáló transzformációk esetében közös sajátvektort garantál 
páratlan dimenziójú valós vektortér felett.

\begin{proposition}
    Egy páratlan dimenziós komplex vektortér 
    minden lineáris transzformációjának van sajátvektora.
\end{proposition}
 
\begin{proof}
    Legyen $V$ a $\mathbb{C}$ feletti vektortér, $n$ páratlan szám a dimenziója,
    $A\in L\left( V \right)$ a transzformáció.
    Jelölje $\mathcal{H}=\left\{ A\in L\left( V \right):A=A^* \right\}$ 
    az önadjungált transzformációkat.
    Világos, hogy $\mathcal{H}$ egy valós, $n^2$ dimenziós vektortér. 
    Minden $C\in L\left( V \right)$ lineáris transzformáció előáll
    \[
        C=
        \frac{C+C^*}{2}+
        i\frac{C-C^*}{2i}
    \]
    alakban, ahol persze 
    $\frac{1}{2}\left( C+C^* \right)$ és
    $\frac{1}{2i}\left( C-C^* \right)$ is önadjungált transzformációk.
    A továbbiakban rögzített $A\in L\left( V \right)$ mellett jelölje
    $L_1,L_2:\mathcal{H}\to\mathcal{H}$ függvényeket.
    \[
        L_1\left( B \right)=
        \frac{AB+BA^*}{2}
        \text{ és }
        L_2\left( B \right)=
        \frac{AB-BA^*}{2i}
    \]
    Világos, hogy $L_1$ és $L_2$ lineáris transzformációk a $\mathcal{H}$ valós
    vektortéren, 
    amelyekre minden $B\in \mathcal{H}$ mellett
    \[
        AB=L_1\left( B \right)+iL_2\left( B \right).
    \]
    E két operátor felcserélhető, hiszen tetszőleges $B\in\mathcal{H}$ mellett, ugyanis
    \begin{eqnarray*}
        L_1\circ L_2\left( B \right)=
        \frac{1}{4i}\left( A\left( AB-BA^* \right)+\left( AB-BA^* \right)A^* \right)=
        \frac{1}{4i}\left( A^2B+BA^{*2} \right)&&\\
        L_2\circ L_1\left( B \right)=
        \frac{1}{4i}\left( A\left( AB+BA^* \right)-\left( AB+BA^* \right)A^* \right)=
        \frac{1}{4i}\left( A^2B+BA^{*2} \right)&.&
    \end{eqnarray*}
    Alkalmazhatjuk az $n^2$ páratlan dimenziós valós vektortérre az előző lemmát.
    Létezik $B\in\mathcal{H}$ nem a konstans zéró transzformáció és
    létezik $\alpha_1,\alpha_2$ valós szám,
    melyekre 
    \(
        L_1\left( B \right)=\alpha_1B 
        \text{ és }
        L_2\left( B \right)=\alpha_2B.
    \)
    Tehát ha valamely $v\in V$ vektorra $Bv\neq 0$,
    akkor 
    \[
        A\left( Bv \right)=
        \alpha_1Bv+
        i\alpha_2Bv=
        \left(\alpha_1+i\alpha_2\right)Bv,
    \]
    ergo $\alpha_1+i\alpha_2$ sajátértéke, és $Bv$ sajátvektora $A$-nak.
\end{proof}

Minden komplex számnak van gyöke, ezért minden legfeljebb másodfokú komplex együtthatós
polinomnak van zérushelye.
Az algebra alaptételével ekvivalens állítás tehát, hogy egy komplex vektortér felett
minden lineáris transzformáció minimálpolinomjának van legfeljebb másodfokú faktora.%
\footnote{Azaz, van legfeljebb két dimenziós nem triviális invariáns altere.}
Az is nyilvánvaló, hogy minden egész szám egyértelműen áll $2^kn$ alakban, ahol $n$ páratlan.

A következő állítás tehát az algebra alaptételének egy ekvivalens megfogalmazása.
\begin{proposition}
    Tekintsünk egy $V$ komplex vektorteret, amelynek dimenziója $2^kn$ alakú,
    ahol n páratlan egész.
    Ekkor $V$ minden lineáris transzformációja minimálpolinomjának 
    van legfeljebb másodfokú faktora.
\end{proposition}
\begin{proof}
    A $k$ szerinti indukció.
    A $k=0$ esetben az előző állítás szerint van sajátvektor is, 
    tehát első fokú faktora is van a minimálpolinomnak.
    Tegyük fel, hogy igaz az állítás minden $k$-nál kisebb szám mellett.
    E feltétel azt jelenti, hogy minden olyan vektortérre igaz az állítás,
    -- így az algebra alaptétele --
    melynek dimenziója $2^l$ páratlan szorosa $l<k$ mellett, 
    azaz amelynek dimenzióját a $d=2^k$ szám nem osztja.
    Alkalmazva a lemmát azt kapjuk, hogy ilyen dimenziójú vektortér kommutáló
    lineáris transzformációinak van közös sajátvektora is.
    Legyen tehát $V$ dimenziója $2^kn$ alakban felírva, ahol $n$ páratlan.

    Rögzítsünk a térnek egy bázisát, és jelölje
    \(
        \mathcal{S}\subseteq L(V)
    \)
    azon lineáris transzformációk összességét, 
    amelyeknek mátrixa az itt rögzített bázisban szimmetrikus.
    Ez egy komplex vektortér,
    amelyre
    \[
        \dim\mathcal{S}=
        \frac{2^kn\left( 2^kn+1 \right)}{2}=
        2^{k-1}n\left(2^kn+1  \right)=
        2^{k-1}n',
    \]
    ahol $n'$ páratlan.
    Alkalmazhatjuk tehát a komplex $\mathcal{S}$ vektortérre az indukciós feltevést.
    Ehhez, rögzített $A\in L\left( V \right)$ lineáris transzformáció mellett, 
    vezessük be az $L_1,L_2:\mathcal{S}\to\mathcal{S}$ függvényeket.
    \[
        L_1\left( B \right)=AB+BA^T \text{ és }
        L_2\left( B \right)=ABA^T.
    \]
    Könnyű számolgatás mutatja, hogy az $L_1$ és $L_2$ lineáris transzformációk kommutálnak:\index{kommutál}
    \begin{multline*}
        \left( L_1\circ L_2\right)B=
        A\left( ABA^T \right)+\left( ABA^T \right)A^T=
        A\left( ABA^T+BA^TA^T \right)
        =\\
        A\left( AB+BA^T \right)A^T =
        \left( L_2\circ L_1 \right)B.
    \end{multline*}
    Létezik tehát közös sajátvektora az $L_1$ és $L_2$ transzformációknak, ergo
    létezik $\lambda,\mu\in\mathbb{C}$ komplex szám, 
    és létezik nem az azonosan zérus $B\in\mathcal{S}$ lineáris transzformáció,
    amelyekre 
    $
    L_1\left( B \right)=\lambda B
    $
    és
    $
    L_2\left( B \right)=\mu B,
    $
    azaz
    $
    AB+BA^T=\lambda B
    $
    és
    $
    ABA^T=\mu B.
    $
    Ebből
    \[
        A\lambda B=
        A\left( AB+BA^T \right)=
        A^2B + ABA^T=
        A^2B + \mu B.
    \]
    Ha tehát $u=Bv\neq 0$, akkor
    \[
        A^2u-\lambda Au+\mu u=0.
    \]
    Ha $u$ nem sajátvektora $A$-nak, akkor a 
    $p\left( t \right)=t^2-\lambda t+\mu$
    egy nem zérus vektor kis minimálpolinomja, 
    ezért a $p$ polinom osztója a minimálpolinomnak.
\end{proof}

\section{Analízis megközelítés}
Az algebra alaptételének (\ref{Th:FundOfAlg}. tétel) bizonyításához felhasználunk néhány az elemi analízisből
jól ismert állítást.
Ezek közül a lényegesebbek az alábbiak:
\begin{enumerate}
\item  Az origó középpontú zárt kör a sík
kompakt részhalmaza.

\item  Kompakt halmazon folytonos függvény felveszi minimumát.

\item  Minden komplex számnak van legalább egy $k$-adik gyöke ($k>0$).

\item  Komplex síkon differenciálható függvények
folytonosak is.
\end{enumerate}

A főtételt könnyen megérthetjük, ha áttekintjük
a felé vezető utat.
Két lényeges pontot kell látnunk.
Az első (\ref{Th:Cauchy1}. állítás) kompaktsági meggondolás, polinomok növekedési ütemére (\ref{Th:Grow}. lemma) támaszkodva.
Ez utóbbi lemma talán önmagában is érdekes, hiszen azt állítja, hogy egy polinom legalább a fokszáma nagyságrendjében növekszik.
A másik döntő lépés (\ref{Th:Cauchy2}. állítás) az Argand-féle becslésen (\ref{Th:Argand}. lemma) nyugszik.
Ez a holomorf függvényekre vonatkozó nyílt leképezés tételnek az itt éppen elegendő speciális esete.

\begin{lemma}
\label{Th:Grow}Legyen $f:\mathbb{C}\rightarrow \mathbb{C}$ nem a konstans nulla,
komplex $n$-edfokú polinom.
Ekkor létezik olyan $r>0$ valós szám, hogy minden $z\in \mathbb{C}$ $,\left| z\right| >r$ esetén $\left| f\left( z\right) \right| >\frac{1}{2}\left| a_{n}\right| \left|
z^{n}\right| $.
\end{lemma}

\begin{proof}
Nyilván $f\left( z\right) =a_{n}z^{n}+a_{n-1}z^{n-1}+\ldots +a_{1}z+a_{0}
$ alakú, ahol $a_{n}\neq 0$.
Világos, hogy $z\neq 0$ esetén 
\[
f\left( z\right) =z^{n}\left( a_{n}+a_{n-1}\frac{1}{z}+\ldots +a_{1}\frac{1}{z^{n-1}}+a_{0}\frac{1}{z^{n}}\right) \text{.}
\]
Legyen $h:\mathbb{C}\rightarrow \mathbb{C}$ komplex polinom a következőképpen definiálva: 
\[
h\left( w\right) :=a_{n-1}w+a_{n-2}w^{2}+\ldots +a_{1}w^{n-1}+a_{0}w^{n}.
\]
Ekkor minden $z\in \mathbb{C}$, $z\neq 0$ mellett 
\begin{equation}
f\left( z\right) =z^{n}\left( a_{n}+h\left( 1/z\right) \right) .
\label{Eq:C}
\end{equation}
A $h$ folytonos $0$-ban, és $h\left( 0\right) =0$, így létezik
olyan $\delta >0$ valós szám, melyre $w\in \mathbb{C}$, $\left|
w\right| <\delta $ esetén $\left| h\left( w\right) \right| <\frac{1}{2}\left| a_{n}\right| $.
\'{I}gy ha $\left| z\right| >1/\delta $, akkor $\left| 1/z\right| <\delta $, amiből következik, hogy 
\[
\left| h\left( 1/z\right) \right| <\frac{1}{2}\left| a_{n}\right| .
\]
A háromszög-egyenlőtlenség és (\ref{Eq:C}) miatt 
az $r:=1/\delta $ választással minden 
$z\in \mathbb{C},\left| z\right| >r$ mellett 
%\begin{multline*}
\[
\left| f\left( z\right) \right|  
=
\left| z^{n}\right| \left| a_{n}+h\left(1/z\right) \right| \geq \left| z^{n}\right| \left( \left| a_{n}\right|
-\left| h\left( 1/z\right) \right| \right) 
> 
\left| z^{n}\right| \left( \left| a_{n}\right| -\frac{1}{2}\left|
a_{n}\right| \right) =\frac{1}{2}\left| a_{n}\right| \left| z^{n}\right| .\qedhere
%\end{multline*}
\]
\end{proof}

\begin{proposition}
\label{Th:Cauchy1}Legyen $f:\mathbb{C}\rightarrow \mathbb{C}$ komplex polinom.
Ekkor létezik $c\in \mathbb{C}$ komplex szám, melyre 
\[
\left| f\left( c\right) \right| =\inf \left\{ \left| f\left( z\right)
\right| :z\in \mathbb{C}\right\} .\qedhere
\]
\end{proposition}
\begin{proof}
Most úgy válasszuk meg az $r$ pozitív valós számot, 
hogy egyrészt az előző lemma, másrészt az
$\frac{1}{2}\left|a_{n}\right| r^{n}>\left| a_{0}\right|$ 
feltétel is teljesüljön.
Ekkor
persze minden $z\in \mathbb{C}$, $\left| z\right| >r$ esetén 
\[
\left| f\left( z\right) \right| >\frac{1}{2}\left| a_{n}\right| \left|
z^{n}\right| >\left| a_{0}\right| =\left| f\left( 0\right) \right| 
\]
is teljesül.
Ez azt jelenti, hogy ha bevezetjük az $\alpha :=\inf \left\{ \left|
f\left( z\right) \right| :z\in \mathbb{C}\text{, }\left| z\right| \leq
r\right\} $ jelölést, akkor minden $z\in \mathbb{C},\left| z\right| >r$
esetén $\left| f\left( z\right) \right| \geq \left| f\left( 0\right)
\right| \geq \alpha .$ Persze ha $\left| z\right| \leq r$ ez utóbbi
akkor is teljesül, ezért 
\[
\alpha \leq \inf \left\{ \left| f\left( z\right) \right| :z\in \mathbb{C}\right\} .
\]
Mivel a fordított irányú egyenlőtlenség triviális,
azt kapjuk, hogy 
\[
\inf \left\{ \left| f\left( z\right) \right| :z\in \mathbb{C}\text{, }\left|
z\right| \leq r\right\} =\inf \left\{ \left| f\left( z\right) \right| :z\in 
\mathbb{C}\right\} .
\]
De láttuk, hogy $\left\{ z\in \mathbb{C}:\left| z\right| \leq r\right\}
\subseteq \mathbb{C}$ a komplex számsík kompakt halmaza, így az $f$
polinom és az abszolútérték-függvény folytonossága miatt létezik $c\in \mathbb{C}$, $\left| c\right| \leq r$, amelyre 
\[
\left| f\left( c\right) \right| =\alpha =\inf \left\{ \left| f\left(
z\right) \right| :z\in \mathbb{C}\text{, }\left| z\right| \leq r\right\} =\inf
\left\{ \left| f\left( z\right) \right| :z\in \mathbb{C}\right\}. \qedhere
\]
\end{proof}

\begin{lemma}[Argand]
\label{Th:Argand}Legyen $k\in \mathbb{N}$, $k>0$ egész és $b\in \mathbb{C}$, $b\neq 0$ komplex szám, valamint $g:\mathbb{C}\rightarrow \mathbb{C}$, $g\left( 0\right) =0$ olyan függvény, amely a $0\in \mathbb{C}$ pontban
folytonos.
Tekintsük a következőképpen definiált $h:\mathbb{C}\rightarrow \mathbb{C}$ leképezést: 
\[
h\left( z\right) :=1+bz^{k}+z^{k}g\left( z\right) .
\]
Ekkor létezik $z\in \mathbb{C}$ komplex szám, melyre $\left| h\left(
z\right) \right| <1.$
\end{lemma}

\begin{proof}
Azt fogjuk megmutatni, hogy található $d\in \mathbb{C}$ és $t\in 
\mathbb{R}$, $t\in \left( 0,1\right) $ melyekre $\left| h\left( dt\right)
\right| <1.$\[
h\left( dt\right) =1+bd^{k}t^{k}+d^{k}t^{k}g\left( dt\right) .
\]
Válasszuk $d$ komplex számot úgy, hogy $bd^{k}=-1$ teljesüljön, azaz $d$ legyen a $-1/b$ komplex szám egyik $k$ -adik gyöke.
Ekkor 
\[
h\left( dt\right) =1-t^{k}+d^{k}t^{k}g\left( dt\right) .
\]
Amiből 
\[
\left| h\left( dt\right) \right| \leq 1-t^{k}+t^{k}\left| d^{k}g\left(
dt\right) \right| =1-t^{k}\left( 1-\left| d^{k}g\left( dt\right) \right|
\right) .
\]
Ebből látszik, hogy elegendő megválasztani $t\in \left(
0,1\right) $ -et olyan módon, hogy $\left| d^{k}g\left( dt\right)
\right| <1.$ Ez pedig nyilván megtehető $g\left( 0\right) =0$ és 
$g$ -nek a $0$ pontban feltett folytonossága miatt.
\end{proof}

\begin{proposition}
\label{Th:Cauchy2}Legyen $f:\mathbb{C}\rightarrow \mathbb{C}$ legalább elsőfokú polinom.
Ekkor minden $c\in \mathbb{C}$, $f\left( c\right) \neq 0$
komplex számhoz létezik olyan $\hat{c}\in \mathbb{C}$ komplex szám,
melyre 
\[
\left| f\left( \hat{c}\right) \right| <\left| f\left( c\right) \right| .
\]
\end{proposition}

\begin{proof}
Tekintsük a 
\[
h\left( z\right) :=\frac{f\left( z+c\right) }{f\left( c\right) }
\]
legalább elsőfokú polinomot.
Vegyük észre, hogy $h\left(
0\right) =1$, így e polinom 
\[
h\left( z\right) =1+a_{k}z^{k}+\ldots +a_{n}z^{n}
\]
alakú, ahol $a_{k}\neq 0$ valamely $k\geq 1$ -re, hiszen az $f$ és
ebből következően a $h$ polinom nem konstans.
Tovább alakítva: 
\[
h\left( z\right) =1+a_{k}z^{k}+z^{k}\left( a_{k+1}z+\ldots
+a_{n}z^{n-k}\right) .
\]
Világos, hogy a fenti $h$ polinomra alkalmazható az előző
lemma, így létezik $u\in \mathbb{C}$, melyre $\left| h\left( u\right)
\right| <1.$ Ez viszont azt jelenti, hogy 
\[
\left| \frac{f\left( u+c\right) }{f\left( c\right) }\right| <1
\]
amiből $\hat{c}:=u+c$ választással kapjuk, hogy 
\[
\left| f\left( \hat{c}\right) \right| =\left| f\left( u+c\right) \right|
<\left| f\left( c\right) \right| .\qedhere
\]
\end{proof}

\begin{FA}\label{Th:FundOfAlg}
    Legyen $f\left( t \right)\in\mathbb{C}[t]$ nem konstans polinom.
    Ekkor $f$-nek van gyöke a komplex számok körében.
\end{FA}

\begin{proof}
Láttuk (\ref{Th:Cauchy1}. állítás), hogy van olyan $c\in \mathbb{C}$ melyre 
\[
\left| f\left( c\right) \right| =\inf \left\{ \left| f\left( z\right)
\right| :z\in \mathbb{C}\right\} \text{.}
\]
Ha $f(c)\neq 0$ lenne, akkor lenne (\ref{Th:Cauchy2}. állítás) $\hat{c}\in \mathbb{C}$ melyre 
\[
\left| f\left( \hat{c}\right) \right| <\left| f\left( c\right) \right| =\inf
\left\{ \left| f\left( z\right) \right| :z\in \mathbb{C}\right\} 
\]
is teljesülne, ami nem lehetséges.
\end{proof}

\section{A komplex számok egyértelműsége}

Az alábbiakban azt fogjuk megvizsgálni, hogy lehet-e a sík pontjain
a komplex számok bevezetésénél megadott szorzástól eltérő
módon bevezetni szorzás műveletet úgy, hogy ez a valós számokon
már megszokott szorzás kiterjesztése legyen, és a sík ellátva
a szokásos összeadással valamint evvel a szorzásnak nevezett művelettel test legyen.
Meg fogjuk mutatni, hogy ez nem lehetséges.
Sőt
azt is látni fogjuk, hogy az $\mathbb{R}$ -től illetve a $\mathbb{C}$ -től
eltekintve nincs véges dimenziós $\mathbb{R}$ feletti vektortér, amely
test lesz olyan összeadásnak illetve szorzásnak nevezett művelettel, 
amely az $\mathbb{R}$ -ben szokásos összeadás és szorzás kiterjesztése.

\begin{proposition}
\label{Th:CUnique1}Legyen $\mathbb{G}$ az $\mathbb{R}$ valós számtestet
tartalmazó olyan test, amelyben az $\left( +,\cdot \right) $ műveletek az $\mathbb{R}$ -ben szokásos $\left( +,\cdot \right) $ műveletek kiterjesztései, valamint amely kétdimenziós vektortér $\mathbb{R}$ felett.
Ekkor $\mathbb{G}$ test izomorf $\mathbb{C}$-vel.
\end{proposition}

\begin{proof}
Meg fogjuk mutatni, hogy létezik $w\in \mathbb{G}$ melyre $w^{2}=-1$.
Ekkor
készen is leszünk, mert nyilván $w\notin \mathbb{R}$ így $\lin\{1,w\}=\mathbb{G}$ amiből könnyen látható, hogy
az $\alpha +w\beta \longmapsto \alpha +i\beta $ megfeleltetés
izomorfizmus $\mathbb{G}$ és $\mathbb{C}$ között.
\newline
Legyen $v\in \mathbb{G}\setminus \mathbb{R}$.
Ilyen $v$ létezik, mivel $\mathbb{G}
$ kétdimenziós.
Világos, hogy az $\left\{
1,v,v^{2}\right\} $ vektorrendszer lineárisan összefüggő,
hiszen három vektor egy kétdimenziós vektortérben.
\'{I}gy léteznek $\alpha ,\beta \in \mathbb{R}$ valós számok melyekre $v^{2}=\alpha +\beta v$.
Most legyen $\gamma :=\frac{-\beta }{2}
$.
Ekkor nyilván $v^{2}=\alpha -2\gamma v$.
Most tekintsük a $\left(
v+\gamma \right) ^{2}$ kifejezést.

\[
\left( v+\gamma \right) ^{2}=v^{2}+2\gamma v+\gamma ^{2}=\alpha +\gamma ^{2}
\]
Azt kaptuk tehát, hogy létezik $r\in \mathbb{R}$ valós szám
melyre $\left( v+\gamma \right) ^{2}=r$.
De vegyük észre, hogy ha $r\geq 0$ lenne, 
akkor $t=\sqrt{r}\in\mathbb{R}$
jelöléssel $\left( v+\gamma \right)^2=t^2$ következne, amiből pedig $v\in \mathbb{R}$ következtetésre juthatnánk ellentétben a $v$ -re kiindulásul tett feltétellel.
Ebből már
világos, hogy ha $w$ -t 
\[
w:=\frac{v+\gamma }{\sqrt{\left| r\right| }}\in \mathbb{G}
\]
módon definiáljuk akkor $w^{2}=\frac{r}{\left| r\right| }=-1$,
hiszen $r<0$.
\end{proof}

\begin{definition}
Legyen $\mathbb{G}$ az $\mathbb{R}$ valós számtestet tartalmazó olyan
test amelyben az $\left( +,\cdot \right) $ műveletek az $\mathbb{R}$ -ben
szokásos $\left( +,\cdot \right) $ műveletek kiterjesztései.
Az $x\in \mathbb{G}$ elemet $\mathbb{R}$ felett algebrainak nevezzük, ha létezik nem konstans zéró valós együtthatós polinom,
melynek $x$ gyöke.
\end{definition}

\begin{proposition}
Legyen $\mathbb{G}$ az $\mathbb{R}$ valós számtestet tartalmazó olyan
test amelyben az $\left( +,\cdot \right) $ műveletek az $\mathbb{R}$ -ben
szokásos $\left( +,\cdot \right) $ műveletek kiterjesztései,
valamint amely véges dimenziós vektortér $\mathbb{R}$ felett.
Ekkor $\mathbb{G}$ minden eleme algebrai $\mathbb{R}$ felett.
\end{proposition}

\begin{proof}
Legyen $\mathbb{G}$ dimenziója $n$ és $v\in \mathbb{G}$ tetszőleges
vektor.
Tekintsük az 
\[
\left\{ 1,v,v^{2},\ldots ,v^{n}\right\} 
\]
vektorrendszert.
Ez nyilván lineárisan összefüggő,
hiszen $n+1$ vektor egy $n$ dimenziós vektortérben.
\'{I}gy van nem
triviális $0$ -t adó lineáris kombinációja.
Azaz léteznek $\alpha _{0},\alpha _{1},\ldots \alpha _{n}$ nem csupa nulla valós számok melyekre 
\[
\sum_{i=0}^{n}\alpha _{i}v^{i}=0
\]
De ez pont azt jelenti, hogy ha a $p$ polinomot 
\[
p\left( x\right) :=\sum_{i=0}^{n}\alpha _{i}x^{i}
\]
módon definiáljuk, akkor $p\left( v\right) =0$.
\end{proof}

\begin{proposition}[Weierstrass]
Legyen $\mathbb{G}$ az $\mathbb{R}$ valós számtestet tartalmazó olyan
test, amelyben az $\left( +,\cdot \right) $ műveletek az $\mathbb{R}$-ben
szokásos $\left( +,\cdot \right) $ művelet kiterjesztései,
valamint amelynek minden eleme algebrai $\mathbb{R}$ felett.
Ekkor két eset
lehetséges: Vagy $\mathbb{G}$ test-izomorf $\mathbb{R}$-rel, vagy $\mathbb{G}$
test-izomorf $\mathbb{C}$-vel.
\end{proposition}

\begin{proof}
Tegyük fel, hogy $\mathbb{G}$ tartalmaz $\mathbb{R}$ -től különböző $v$ vektort.
Ekkor azt kell megmutatni, hogy $\mathbb{G}$ izomorf $\mathbb{C}$ -vel.\newline
Először azt mutatjuk meg, hogy 
\begin{equation}
v^{2}\in \lin\left\{ 1,v\right\} 
\end{equation}
\label{Eq:Alg}Mivel $v$ algebrai $\mathbb{R}$ felett, ezért létezik $p$
valós együtthatós polinom melyre $p\left( v\right) =0$.
De láttuk, hogy valós együtthatós polinom első- és másodfokú tényezők szorzatára bomlik (\ref{pr:RealPolFact}.),
ami azt jelenti, hogy van olyan $q$ első, vagy másodfokú valós együtthatós polinom melyekre $q\left( v\right) =0$.
Ha $q$ első
fokú lenne az azt jelentené, hogy található $\alpha ,\beta
\in \mathbb{R}$ valós számok melyekre $\alpha +\beta v=0$, azaz $v\in \mathbb{R}$ lenne.
Tehát $q$ pontosan másodfokú.
Ez viszont azt
jelenti, hogy léteznek $\left\{ \alpha ,\beta ,\gamma \right\} \subset 
\mathbb{R}$ valós számok $\gamma \neq 0$, melyekre $\alpha +\beta
v+\gamma v^{2}=0$.
Ebből persze 
\[
v^{2}=\frac{-\alpha }{\gamma }+\frac{-\beta }{\gamma }v
\]
már könnyen következik, bizonyítva (\ref{Eq:Alg})-et.\newline
Most megmutatjuk, hogy $\mathbb{G}$ tartalmaz $\mathbb{C}$ -vel izomorf testet.
Tekintsük a 
\[
K:=\lin\left\{ 1,v\right\} 
\]
kétdimenziós alterét $\mathbb{G}$-nek.
Világos, hogy ennek test
voltához elegendő megmutatni, hogy a $\mathbb{G}$-beli műveletekre zárt.
Az összeadásra való zártság triviális a $K$
altér mivoltából, a szorzásra való zártság pedig
(\ref{Eq:Alg}) következménye.
\'{I}gy tehát $K$ két dimenziós test kiterjesztése $\mathbb{R}$ -nek, ami azt jelenti (\ref{Th:CUnique1}.), 
hogy $K$ test izomorf $\mathbb{C}$ -vel.\newline
Utoljára megmutatjuk, hogy $\mathbb{G}$ minden $w$ eleme $\mathbb{C}$ -beli
is.
Mivel $w$ algebrai $\mathbb{R}$ felett, ezért létezik $p$ valós együtthatós polinom, melyre $p\left( w\right) =0$.
De tekinthetjük $p$ -t a komplex számtest feletti polinomnak is, így $p$
felbomlik elsőfokú komplex polinomok szorzatára (\ref{pr:PolFact}).
Ez viszont azt jelenti, hogy létezik olyan $c$ komplex szám,
melyre $w-c=0$, tehát $w\in \mathbb{C}$ valóban teljesül.
\end{proof}

%% a vége következik

\backmatter
\pagestyle{empty}
%\bibliography{\jobname}
\printbibliography
\printindex

\end{document}
% arara: latexmk: { 
% arara: --> engine: lualatex,
% arara: --> options: [ '-pvc' ]
% arara: --> }

\begin{thebibliography}{MMMMMMMMM}
    \bibitem[Dancs István, Puskás Csaba (2006)]{PCSDI}
        Dancs István, Puskás Csaba: \textit{Vektorterek}, Aula kiadó 2001, Budapest, ISBN:963 9345 53 9, BCE Catalogue: bcek.379187.
\end{thebibliography}

Created: Sat 20 Jul 2019 05:02:12 AM CEST
Last Modified : Mon 09 Mar 2020 04:42:27 PM CET

